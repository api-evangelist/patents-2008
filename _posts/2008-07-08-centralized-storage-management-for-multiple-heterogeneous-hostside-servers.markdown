---

title: Centralized storage management for multiple heterogeneous host-side servers
abstract: Centralized management of both host-side storage objects on multiple heterogeneous host-side servers and logical data containers on a storage system is performed by a management server. In one embodiment, the management server automatically provisions a logical data container on the storage server according to the storage virtualization strategy without administrator interaction at the storage system. In another embodiment, the management server automatically performs a snapshot operation on logical data containers on the storage system according to the storage virtualization strategy without administrator interaction at the storage system. In another embodiment, the management server centrally monitors for out-of-space events in the storage system and automatically correlates the out-of-space events in the storage system to out-of-space events for the host-side file systems.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08024442&OS=08024442&RS=08024442
owner: Network Appliance, Inc.
number: 08024442
owner_city: Sunnyvale
owner_country: US
publication_date: 20080708
---
This invention relates to the field of network data storage systems and in particular to centralized storage management for multiple heterogeneous host side servers.

Various forms of network data storage systems are known today. These forms include network attached storage NAS storage area networks SANs and others. Network storage systems are commonly used for a variety of purposes such as providing multiple users with access to shared data backing up critical data e.g. by data mirroring etc. These data storage systems use storage virtualization to abstract the logical storage from the physical storage to provide a resource pool of storage resources. However host side servers which interact with these data storage systems use various types of different virtualizations than the storage virtualization on the storage systems. For example every vendor of resources such as vendors of applications databases servers and storage devices attempts to add value from virtualization but has consequently adopted a different storage virtualization strategy for their particular resource. Storage virtualization as used herein refers to the process of abstracting logical storage from physical storage. Each of the host side servers employ different host side virtualization strategies which include a set of rules that govern how host side storage objects are represented logically on the host side server. The set of rules may be stored in memory of the host side server.

One goal in storage resource management SRM is to have the ability to deliver peak resource utilization on a shared resource pool to thereby minimize wasted idle storage resources. Due to the competition among the various vendors to virtualize resources most systems are not complete data center management solutions resulting in computing environments that include multiple heterogeneous virtualization strategies that may not take full advantage the storage virtualization of the underlying storage system. Since the host side file systems include one of multiple heterogeneous virtualization strategies they are referred to herein as heterogeneous host side servers. For example a typical computing environment may include multiple heterogeneous host side servers that utilize a storage system that employs a storage virtualization strategy that is different than the host side virtualization strategies of the host side servers. In conventional computing systems a conventional data center environment may have a host side server and a storage system which are both separately managed by the server administrator and the storage administrator respectively such as illustrated in . The server administrator is responsible for managing the physical and software resources of one or more host side servers. Part of those software resources is the server stack host side file system volume manager virtual disks NFS mounts etc . The storage administrator on the other hand manages the physical and software resources of one or more storage systems such as logical units of data e.g. logical unit numbers LUN logical containers e.g. volumes aggregates etc and NFS exports . The server administrator has a set of challenges. The first is that the server stacks are different and are there are no tools for effectively managing them as anything but distinct host side file systems. The second is that the server IO stack is dependent on physical and software resources managed by a different administrative group the storage administrator. As a result the server administrator is unable to exploit the virtualization value of centralized storage resources without involving the storage administrator. Although host side storage objects can be manually mapped to storage objects in the storage system conventional systems do not provide any tools to allow a server administrator to manage the data in the storage object without administrator interaction such as by a storage administrator at the storage system. For example to perform an operation like a snapshot or a snapshot restore the entire set of storage resources e.g. storage objects must be manipulated requiring both human co ordination and software that can map between the two sets of storage resources the host side storage resources of the host side server and the storage resources of the storage system. A snapshot operation creates a copy of a set of files and directories as they were at a particular point in time.

For example the conventional SRM tools require administrator interaction at the storage system to provision storage resources on the storage system. Conventional SRM tools which may be used in such environments as described above simply connect an initiator host bus adapter HBA to a target HBA in provisioning storage. That is the server administrator notifies the storage administrator that a storage resource of a specified capacity is needed and the storage administrator allocates the appropriate storage resource notifying the server administrator of the target location for the particular storage resource. The target HBA of the storage resource in the storage system is then manually mapped to the initiator HBA of the storage resource on the host side server using the SRM tool. The conventional SRM tools are not used to create and or manage file system on the host side server nor create and or manage the storage resources on the storage system but to manually map the host side storage objects using human coordinate between the server and storage administrators. Also the conventional SRM tools do not provision storage according to the underlying storage system and thus do not take advantage of the benefits of the storage virtualization such as thin provisioning and snapshot operations. Thin provisioning is a provisioning operation that allows space to be allocated to servers on a just enough and just in time basis as compared to pre allocating a large amount of space to account for possible data growth. Thin provisioning may be used in many applications where access to the same storage resource pool is used allowing administrators to maintain space in the resource pool to service the data growth requirements of the many applications on an ongoing basis. Thin provisioning may allow organizations to purchase less storage capacity up front and defer storage capacity upgrades with actual increase in usage. In contrast in fat provisioning typically large amounts of storage capacity are pre allocated to individual applications. Most of the storage resources in fat provisioning remain unused e.g. not written to resulting in poor utilization rates.

Similarly conventional SRM tools may not take advantage of such features of the underlying storage systems such as snapshot operations. Snapshot operations may generate a copy of a set of files and or directories as are at a particular point in time. As described above to perform an operation like a snapshot or a snapshot restore the entire set of storage resources e.g. storage objects must be manipulated requiring both human co ordination and software that can map between the two sets of storage resources the host side storage resources of the host side server and the storage resources of the storage system.

Also since the conventional provisioning tools in servers do not provision storage according to the underlying storage system but are limited to provisioning storage according to their own specific virtualization strategy the conventional provisioning tools do not provide a heterogeneous tool for multiple heterogeneous host side servers that employ different virtualization strategies. Some other conventional systems attempt to provide tools for achieving end to end provisioning however these tools are limited to the specific virtualization strategy employed for the particular system and do not allow centralized storage provisioning for multiple heterogeneous host side servers that employ different virtualization strategies.

Also since the conventional SRM tools do not provision storage according to the underlying storage system the conventional SRM tools cannot detect and report out of space conditions especially when there are storage based snapshots and thin provisioning. From a traditional file system perspective the storage space is entirely contained within the file system but in a centralized file system of multiple heterogeneous host side servers the out of space conditions on the storage system need to be correlated to the out of space conditions on each of the host side servers.

Centralized management of both host side storage objects on multiple heterogeneous host side servers and logical data containers on a storage system is performed by a management server. The management server centrally manages the host side storage objects and the logical data containers according to the respective virtualization strategies. By centrally managing both the host side storage objects and the logical data containers the management server may take advantage of the underlying storage virtualization of the storage system such as by provisioning a logical data container on the storage server without administrator interaction at the storage system or performing a snapshot operation without administrator interaction at the storage system. Also by centrally managing both the host side storage objects and the logical data containers the management server may centrally monitor for out of space events in the storage system and automatically correlated the out of space events in the storage system to out of space events for the corresponding host side file systems.

The embodiments described herein are directed to solving the data center resource optimization problems described above namely server storage resource optimization and management by integrating with the various host side virtualization strategies employed on the various host side servers and the storage virtualization strategy employed by the underlying storage system. As described above the host side virtualization strategies each include a set of rules that governs how the host side storage objects are logically represented on the host side server whereas a storage virtualization strategy includes a set of rules that governs how the logical data containers and or logical units of data are logically represented on the storage system. The set of rules for the host side virtualization strategies may be stored in memory on the respective host side servers and the set of rules of the storage virtualization strategy may be stored in memory on the storage system. For example when an operating system of one of the host side servers receives a request to create a host side file system the operating system may create the host side file system according to the set of rules stored in memory. Similarly when a storage system receives a request to create a file system the operating system of the storage system may create the file system according to the set of rules stored in memory. The embodiments described herein allow data centers to use storage virtualization available on the storage system regardless of the host side virtualization strategy that is being used to virtualize other resources in the system. The embodiments described herein allow centrally managing the underlying storage objects from a host side perspective without the storage administrator. The embodiments described herein are able to centrally manage the server stack across multiple heterogeneous servers and to use storage virtualization without requiring coordination with the storage administration staff e.g. without administrator interaction at the storage system .

The storage system may include one or more storage servers and one or more storage devices. The storage devices may be for example conventional magnetic disks optical disks such as CD ROM or DVD based storage magneto optical MO storage or any other types of non volatile storage devices suitable for storing large quantities of data. The storage devices in the storage devices A and B can be organized as one or more RAID groups in which ease the nodes A and B access the storage devices A and B using an appropriate RAID protocol.

The management server communicates with the management console through a first interface and receives user input from the management console such as request to perform various storage management operations. The management server may also perform storage management operations without administrator interaction from the server administrator. The management server communicates with the storage system over the network through a second interface to centrally manage the logical data container e.g. LUNs volumes aggregates or the like in the storage system . The management server communicates with the multiple host side servers and over the network through a third interface. The management server centrally manages the host side storage objects of the heterogeneous host side servers through the third interface. The storage system employs a storage virtualization strategy and each of the host side servers employs a different host side virtualization strategy. For example the host side server operates a first operating system having a first host side virtualization type and the host side server operates a second operation system having a second host side virtualization type. It should be noted that the management server communicates with both the storage system and the multiple host side servers and over the same network . In other embodiments the management server may communicate with the storage server and the multiple host side servers and over more than one network. In one embodiment the network is a LAN. Alternatively the network may be a metropolitan area network MAN a virtual private network VPN a wide area network WAN a global area network GAN such as the Internet or other types of network or combination of networks.

The management server as part of centrally managing both the logical data containers and the host side storage objects may provision a logical data container in the storage system as a host side storage object in the host side file system residing on one of the host side servers e.g. client based servers without administrator interaction at the storage system . The management server receives a request to provision the logical data container for the host side storage object and sends a request to the storage system through the second interface to provision the logical data container. The management server then provisions the host side file system on the particular host side server through the first interface based on the provisioning information from the storage system .

In another embodiment the management server includes a SRM tool to coordinate end to end storage provisioning of the host side storage objects of the heterogeneous host side servers homogeneously with respect to the storage arrays of the storage system. The SRM tool allows an administrator via the management console to manage virtualization of the host side file systems according to the different host side virtualization strategies and to manage virtualization of the storage system according to the storage virtualization strategy to coordinate the end to end provisioning of the host side storage object. This centralized management of both types of virtualization may be performed without administrator interaction at the storage system and or at the host side servers. The host side management may be done by communicating with a host side agent residing on each of the host side servers which can manipulate the host side storage objects on the respective host side file system. In one embodiment host side agents and reside on the host side servers and respectively. As described in more detail below in conjunction with a monitoring manager e.g. a distinct process performed by the management server communicates with the host side agents and to centrally manage the host side storage objects on the host side servers and . The host side storage objects may be manipulated by the host side agents and .

The management server centrally manages both the host side storage objects of multiple heterogeneous host side servers and the logical data containers on the storage system without administrator interaction at the storage system. The management server may automatically provision a logical data container on the storage server according to the storage virtualization without administrator interaction at the storage server for example upon receiving a request to provision the logical data container. This may be done by automatically sending a request to the storage system to automatically allocate the logical data container on the storage system which corresponds to the respective host side storage object according to the storage virtualization strategy without administrator interaction at the storage system. The logical data container may be thinly provisioned. The request to provision the logical data container may originate from the server administrator through the management console . Alternatively the request may originate from a user on one of the multiple host side servers in response to a request to provision a host side storage object. In another embodiment the management server may also automatically provision a host side storage object or a host side file system on one of the host side servers which corresponds to the provisioned logical data container on the storage system without administrator interaction at the host side server.

The management server as part of centrally managing both the host side storage objects and the logical data containers may also perform other storage operations to take advantage of the storage virtualization on the storage system . In one embodiment the management server automatically performs a snapshot operation on the logical data containers on the storage system according to the storage virtualization strategy. This may be done by sending a request from the management server to the storage system to perform the snapshot operation. The management server interacts with the storage system directly without administrator interaction at the storage system to perforin the operations. The request to perform the snapshot operation may originate from the server administrator through the management console . Alternatively the request may originate from a user on one of the multiple host side servers.

The management server may manage a server stack across the multiple host side server e.g. and as well as utilize the storage virtualization strategy to provision a logical data container in the storage system for each file system of the multiple host side servers. The management server centrally manages the host side storage objects and the logical data containers using a storage resource model such as illustrated in . The storage resource model is an abstract model that describes how the storage and server storage resources are connected to each other as a graph. In particular the storage resource model represents how the host side storage objects in the multiple heterogeneous host side servers are connected to the logical data containers on the storage system . The storage resource model abstracts out the details of the storage and presents the storage in the form of a graph which can be used to build a variety of solutions for centrally managing both the storage resources on the storage system and the host side storage resources on the multiple host side servers and . The storage resource model may be a single manageable entity which is a first class object rather than just a collection of distinct objects. The object defines the object relationships of how various host side resources and storage resources are connected to each other. In one embodiment using the storage resource model the management server permits the server administrator to perform various operations such as provisioning operations snapshot operations snapshot restore operations or the like as well as monitor the health and status of the storage resource model.

It should be noted that host side virtualization strategies typically use specific device drivers which present physical disks or logical storage to the host side server. Typically a software layer above the physical device drive intercepts the I O requests performing the meta data lookup and I O redirection. Essentially every operating system has its host side virtualization. These virtualization tasks are performed by a logical volume manager LVM such as in Unix and Linux based systems or a logical disk manager LDM such as in the Windows based systems. These different managers employ different host side virtualization strategies depending on the platform type of the host side server. These host side virtualization strategies optimize storage utilization only a per host basis and the software for implemented the virtualization strategy is unique to the particular type of host.

In one embodiment the host side virtualization strategy uses a file based access protocol such as the Common Internet File System CIFS protocol or Network File System NFS protocol over TCP IP when accessing information in the form of files and directories. In another embodiment the host side virtualization strategy uses a block based access protocol such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel Protocol FCP when accessing information in the form of blocks. In one embodiment the storage virtualization strategy is a storage device based virtualization. In another embodiment the storage virtualization strategy is a network based virtualization. The network based virtualization may be implemented in a storage system having a SAN architecture. The SAN architecture allows remote computer storage devices such as disk arrays tape libraries or the like to host side servers in such a way that to the host side operating system the devices appear as locally attached using for example the Fibre Channel FC protocol. Alternatively the storage system may have a NAS architecture which uses file based protocols such as NFS or Server Message Block SMB also referred to as CIFS where it is clear that the storage is remote and computers request a portion of an abstract file rather than a disk block. Whether the storage system is NAS or SAN storage virtualization refers to the process of completely abstracting logical storage from physical storage. The physical storage resources may be aggregated into storage pools from which the logical storage is created. The storage system may present to the client e.g. host side server a logical space for data storage and may transparently handle the process of mapping it to the actual physical location. The storage system may virtualize multiple disk arrays made by different vendors scattered over the network into a single monolithic storage device which can be managed uniformly.

The host side server is representative of a host side server that operates the Unix based operating system. The host side server employs a second host side virtualization strategy which is different than the first and which uses various objects to represent the files and directories of a host side file system e.g. VxFS . The storage administrator provisions a LUN to the host. The server administrator mounts the LUN as a base device which may be partitioned. Each of the partitions may be assembled into a volume group as a collection of raw storage. VxVM is then used to create a VxVM volume inside of the volume group. A VxFS file system is then created within the VxVM volume. Using the Unix based operating system the host side storage object is represented by a base device . In one embodiment the base device is represented as a mounted root directory e.g. root such as in file systems of the Unix based operating system. The storage resource model maps the base device to one or more LUN e.g. logical data container on the underlying storage system .

The host side server is representative of a host side server that operates the Linux operating system. The host side server employs a third host side virtualization strategy which is different than the first and second and which uses various objects to represent the files and directories of the host side server . The storage administrator provisions a LUN to the host. The server administrator mounts the LUN as a base device which may be partitioned. Each of the partitions may be assembled into a volume group as a collection of raw storage. LVM is then used to create a logical volume inside of the volume group. A new ext3 file system is then created thin within the LVM. Using the Linux operating system the host side storage object is represented by a base device . In one embodiment the base device is represented as a mounted root directory e.g. root such as in file systems of the Linux operating system. The storage resource model maps the base device to one or more LUN e.g. logical data container on the underlying storage system .

The host side server is representative of a host side server that also operates the Linux operating system but employs a fourth host side virtualization strategy which is different than the first second and third and which uses a NFS client as the host side storage object to represent the files and directories of the host side server . Unlike the host side servers where the file system is contained within the host side server using NFS the file system may be implemented in the storage server. The host side server connects to the file system using an NFS client. As a result some of the host side storage objects within the host side servers may not be necessary. Instead those logical objects may exist reside of the storage system . The storage resource model maps the NFS client to a volume e.g. logical data container on the underlying storage system . The storage system may have a file system which is created within the volume. A volume is a logical data set which is an abstraction of physical storage combining one or more physical storage devices or parts thereof into a single logical data container e.g. data storage object and which is managed as a single administrative unit such as a single file system. Typically file systems have a one to one mapping to volumes where volumes can be constructed from one or more storage devices e.g. disks . Each volume is generally although not necessarily managed as a separate file system. Each volume stores a copy of a data set and each node has its own data sets as well as has access to the other node s data sets. The volume may be made up of one or more LUNs and the volume may be part of an aggregate having one or more volumes. In the depicted embodiment of the storage virtualization strategy uses multiple aggregates which include one or more volumes which include one or more LUNs . In other embodiments more or less layers of abstraction may be used and other types of logical data containers may be used.

The management server may implement a data fabric manager DFM server as described in more detail below in conjunction with .

The DFM server may provide the ability to restrict access to storage resources and the ability to have a consolidated view of all of the server resources. The unified view of the server storage resources may be provided through a management console which includes a new abstraction for host storage resources. Using the host side agents and on the heterogeneous host side servers the DFM server can centrally manage both the storage objects in the storage system and the corresponding host side storage objects. The management server allows a server to optimally provision storage for multiple heterogeneous host side servers in a way to take advantage of the underlying storage virtualization. The management server may centrally provision host side storage objects such as host side file systems across many heterogeneous hosts e.g. hosts implementing heterogeneous operating system such as Windows Linux Solaris operating systems or the like. The DFM server may also centrally provision logical data containers on the storage system to correspond to the host side storage objects without administrator interaction at the storage system. The DFM server may also perform a snapshot operation on logical data containers on the storage server according to the storage virtualization strategy without administrator interaction at the storage system upon receiving user input from a user on the management server.

The DFM server includes an operations manager which communicates with the management console via the management console MC application programming interface API with the host side servers via the host side API client and with the storage systems through the storage system API client . The host side API client communicates with the host side agents and as described above. In this embodiment the host side agents are host side agents for the Windows operating system SDW and for the Unix operating system SDU . The SDW servers and SDU servers communicate with the DFM server through the APIs and respectively. The SDW server and SDU server each provides a layer of abstraction between an application running on the host side servers and the underlying storage system . In one embodiment the application may use virtual disks e.g. LUNs on the storage system as if they were locally connected drives or mount points. The SDW server and SDU server allow the host side servers to take advantage of the storage virtualization strategies of the underlying storage system such as snapshot operations flexible volumes cloning space management technologies and the like. The SDW server and SDU allow the DFM server e.g. operations manager to manipulate host side storage objects e.g. raw devices files base devices file systems or the like .

The SDW server and SDU may perform various operations such as create destroy snapshot remote copy and clone operations of any host side storage objects. The operations manager can also manipulate storage objects on the storage systems through the storage system API client . Each storage system communicates with the DFM server through the APIs and . The operations manager acting as an API server centrally manages both the host side storage objects on the host side server and the storage resources e.g. logical data containers logical units of data etc on the storage system . The operations manager may also be used to define global roles data protection polices provisioning policies or the like. The operations manager communicates with the DFM database which is a data store that stores for example information regarding each object in the storage resource model information regarding global roles data protection policies and provisioning policies access information information regarding discovered host side agents information regarding the health and status of each of the host side agents monitoring results from the monitoring manager described below such as out of space events or the like. The DFM database may also store configuration information as well as historical information. The data of the DFM database may be stored according to a data schema such as a relational database having multiple tables.

The monitoring manager communicates with the SDW server the SDU server the storage systems using API and respectively. The monitoring manager performs centralized monitoring of the host side storage objects on the host side server and the storage resources on the storage systems . The monitoring manager may discover host side agents e.g. SDW and SDU servers and host side storage objects. All discovered resources may be stored in the DFM database . As the status of the resources changes the resources may be updated in the DFM database . The monitoring manager may also monitor other items such as operational status of a server provisioned LUNs file systems host volumes disk groups NFS exports the LUNs backed up by snapshots the HBA status or the like. The monitoring manager may also monitor various thresholds on both the host side servers and the storage systems such as the volume space reservation threshold the volume out of space threshold the volume nearly full threshold the host side file system out of space threshold the host side file system nearly full threshold or the like. It should be noted that each of the host side agents may generate events about the status of the host side storage objects and report them to the monitoring manager when they are detected or upon request from the monitoring manager .

The event manager communicates with the monitoring manager and the DFM database . When certain events are detected by the monitoring manager the event manager may store the event in the DFM database and or perform some action such as notifying the host side servers the management console or a third party management console of the detected event. For example the event manager may generate an email when triggered by a particular type of event. The policies regarding the event notification may be stored in the DFM database .

In one embodiment the various blocks of the DFM server represent a collection of distinct processes that run on a single management server. Alternatively the various blocks of the DFM server represent a combination of hardware and software components for centrally managing the host side storage resources on the host side servers and the storage resources on the storage systems .

In one embodiment the host side storage object is a host side file system. In another embodiment the host side storage object is a base device. Alternatively the host side storage object may be other types of host side storage resources. In one embodiment logical data containers may be thinly provisioned logical data containers. In one embodiment the logical data container is a flexible volume. Alternatively the logical data containers may be fat provisioned data containers. Although the embodiment of illustrates that request to perform storage provisioning originates from the management console in other embodiments the request may originate from a user on the host side server .

Although the embodiment of illustrates that request to perform a snapshot operation originates from the management console in other embodiments the request may originate from a user on the host side server .

Storage administrators may need a technique to define and limit the amount of storage that is used by individuals who have access to a shared data storage resource. The administrator may wish to limit storage to particular individuals for administrative reasons financial reasons or the like. The administrators may also wish to control the rate of growth of data stored in the storage system. In order to limit the amount of storage used by an individual some file systems have a grant limit also referred to herein as quota of storage resources that can be used by a user to limit the storage usage by a user on one or more logical data containers such as a volume. The grant limit is a limit on the amount of storage resources that can be allocated to the user. There are different types of grant limits for example usage quota block quota file quota inode quota or the like. Usage and block quota limit the amount of disk area that can be used. File and inode quotas limit the number of files and directories that can be created. Specific quotas can be applied to individual users or alternatively a default user quota can be established that applies to all users utilizing space within the logical data container. An inode stores basic information about a file directory or file system object. Quotas may also be applied to a group of users in a similar manner as with individual users. That is a system or storage administrator defines a usage or file quota specific to a certain user or group of users. Alternatively quotas may be implemented for other entities than an individual user or a group of users such as Human Resources HR Engineering or the like. In addition administrators typically define a warning level i.e. a soft quota at which users are informed that they are nearing their grant limit which is less than the effective limit i.e. a hard quota. Also there may be a grace interval which allows users to temporarily violate their quotas by certain amounts if necessary. When an entity meets or exceeds a defined threshold the storage system detects and or reports an out of space event. An out of space event occurs when the entity requests allocation of storage space that would exceed the entities specified grant limit. In defining and implementing grant limits on storage server an administrator can prevent one user from consuming an entire file system s resources or create a system of tiered access in which users can have different levels of restriction. This may be used for example to provide different levels of service based upon the needs of individual clients.

In one embodiment the grant limits for one or more entities may be stored in the DFM database . Alternatively the grant limits may be stored in other memory locations in the either the management server or the storage system . Table 1 illustrates exemplary entries of grant limits for two entities according to one embodiment.

Each of the entries include the name of the entities e.g. Joe and Kostadis a value representative of the used capacity by the entity e.g. 500 MB and 1 TB and a grant limit of the total capacity for the entity e.g. 1 GB and 10 TB . In other embodiments additional information may be stored in each entry such as the type of storage resources to be used for the particular entity.

The method starts by the server administrator through the management console requesting that a new file system be provisioned on the host side server e.g. host side server F operation . The request may include the storage capacity of the file system such as 500 megabytes MB . The DFM server upon receiving a request to provision a host side storage object e.g. a host side file system on the host side server employing a host side virtualization strategy provisions storage on the storage system to correspond to the host side storage object based on the grant limits. In order to provision a logical data container on the storage system which employs a storage virtualization strategy the DFM server maps to a storage server on the storage system e.g. Filer G operation queries the database to find available space on the designated storage server operation and queries the database to determine whether the requested storage size specified in the request exceeds a grant limit for the requesting entity operation . When the specified storage size does not exceed the grant limit e.g. return Yes the DFM server sends a request to the storage system to create a logical data container e.g. volume having the specified storage size in the designated storage server operation . However when the specified storage size exceeds the grant limit the logical data container is not created not illustrated and the DFM server can notify the server administrator through the management console . Once the storage system notifies the DFM server that the logical data container is created e.g. return vol the DFM server can send a request to the host side agent to provision the file system on the host side server corresponding to the created logical data container on the storage system operation . It should be noted that in this embodiment only a logical data container is created and not both a logical data container and a logical data unit. In other embodiments the host side agent upon receiving the request in operation may perform the operations described with respect to including operations and . Once the host side agent has created the host side file system the host side agent notifies the DFM server that the base device was created on the host side server operation . The DFM server may update the DFM database to include the newly created storage resources. Once the host side agent has notified the DFM server of the base device creation the DFM server may also notify the server administrator through the management console that the host side file system is created not illustrated .

It should be noted that the operations and may be performed without administrator interaction at the host side server and the operation may be performed without administrator interaction at the storage system . It should also be noted that although the embodiments of perform operations with respect to the host side agent on the host side server these operations may also be performed to provision storage take snapshots or the like in centrally managing both the host side storage resources on the multiple heterogeneous host side servers and the storage resources on the storage system.

In addition to storage provisioning both host side file systems and storage systems may use grant limits to detect and or report out of space events. Because the server administrator is centrally managing both the host side storage objects and the logical data containers on the storage system the embodiments described herein centrally monitor for out of space events in the storage system and automatically correlate the out of space events in the storage system to out of space events for the host side file systems on the host side servers. This may be done by the management server e.g. using the DFM server . The management server may detect and report the out of space events to the respective host side file system that corresponds to the logical data container in which the event was detected. The centralize management of the out of space events may also be monitored using the management console. As described above from a traditional file system perspective the storage space is entirely contained within the file system but when the storage system uses a storage virtualization strategy that uses snapshot operations thin provisioning and the like space outside the host side file system needs to be monitored e.g. the underlying storage system for out of space events. For example multiple host side file systems may be created as described herein and the underlying storage in the storage system corresponding to the host side file systems may be thinly provisioned within one or more logical data container. Although each of the host side file systems is within the specified grant limits collectively the host side file systems may be exceeding a grant limit on the storage system creating an out of space event. However the host side file systems may continue to allocate storage space since individually the host side file system have not exceeded the respective grant limits. The out of space event on the storage system which if not detected and reported to the host side servers may lead to data corruption and storage system failure when the host side file systems collectively exceed the grant limits. The embodiments described below are directed to automatically correlating out of space events occurring on the storage system to out of space events for the host side file systems on the host side servers.

The second graphical representation is of multiple host side file systems each stored in a flexible volume. In particular a first file system is stored in a first flexible volume a second file system is stored in a second flexible volume a third file system is stored in a third flexible volume and a fourth file system is stored in a fourth flexible volume . Each of the flexible volumes is thinly provisioned logical data containers which are part of another logical data container namely aggregate which has an out of space aggregate threshold . Each of the four flexible volumes has an out of space volume threshold which may be set at different levels for each of the flexible volumes . These thresholds of the flexible volumes may correspond to host side file system out of space thresholds. In this embodiment no out of space warning is generated at the host side file since the file systems have not individually exceeded any of the out of space volume thresholds . However collectively the host side file systems exceed the out of space aggregate threshold creating an out of space event on the storage system while no out of space event is detected on each of the host side servers.

By monitoring the out of space events on the storage system the management server may automatically correlate the detected out of space events to out of space events for the corresponding affected host side file system. For example this may happen when either data in one of the flexible volumes exceeds the out of space volume threshold or the data in the aggregate exceeds an out of space aggregate threshold . The management server may report the out of space events to the respective host side servers. In one embodiment the monitoring may be performed by the monitoring manager of the DFM server as described with respect to above. The monitoring manager monitors various thresholds on both the host side servers and the storage systems such as the volume space reservation threshold the volume out of space threshold the volume nearly full threshold the host side file system out of space threshold the host side file system nearly full threshold or the like. It should be noted that each of the host side agents and may generate events about the status of the host side storage objects and report them to the monitoring manager when detected or upon request from the monitoring manager . When out of space events are detected by the monitoring manager the event manager may store the event in the DFM database and or perform some action such as notifying the host side servers the management console or a third party management console of the detected event. For example the event manager may generate an email when an out of space event occurs on the storage system .

It should be noted that some of the operations of the method may not be performed for some types of host side file systems for example for host side servers that employs host side virtualization strategies that do not use disk groups host side volumes etc. In another embodiment the method finds all file systems e.g. NFS file systems that are mounted over a logical data container stored in the storage system for each of the out of space events and creates an event warning of the out of space event for each of the file systems. In another embodiment the method finds all logical units of data of a logical data container for each of the out of space events. For each of the logical units of data the method finds all host side storage objects that are mapped to the particular logical unit of data. For each of the host side storage objects the method finds a file system that corresponds to the particular host side storage object. The method determines when the host side file system is found and consequently creates an event warning of the out of space event for the found host side file system. When the host side file system is not found the method creates an event warning of the out of space event for the found host side storage object.

In one embodiment the automatic correlation of the host side storage objects and the logical data containers is performed using the following algorithm. It should be noted that in other embodiments other algorithms may be used based on the type of host side virtualization strategy being employed on the particular host side servers.

It should be noted that the methods described here may be performed by processing logic that may comprise hardware circuitry dedicated logic or the like software such as is run on a general purpose computer system or a dedicated machine or a combination of both. In another embodiment the methods described herein may be performed by processing logic of the management server of . In another embodiment the methods described herein may be performed as part of the SRM tool of .

The exemplary computer system includes a processor a main memory e.g. read only memory ROM flash memory dynamic random access memory DRAM such as synchronous DRAM SDRAM or Rambus DRAM RDRAM etc. a static memory e.g. flash memory static random access memory SRAM etc. and a secondary memory e.g. a data storage device which communicate with each other via a bus .

Processor represents one or more general purpose processing devices such as a microprocessor central processing unit or the like. More particularly the processor may be a complex instruction set computing CISC microprocessor reduced instruction set computing RISC microprocessor very long instruction word VLIW microprocessor processor implementing other instruction sets or processors implementing a combination of instruction sets. Processor may also be one or more special purpose processing devices such as an application specific integrated circuit ASIC a field programmable gate array FPGA a digital signal processor DSP network processor or the like. Processor is configured to execute the processing logic for performing the operations and steps discussed herein.

The computer system may further include a network interface device . The computer system also may include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT an alphanumeric input device e.g. a keyboard a cursor control device e.g. a mouse and a signal generation device e.g. a speaker .

The secondary memory may include a machine readable storage medium or more specifically a computer readable storage medium on which is stored one or more sets of instructions e.g. software embodying any one or more of the methodologies or functions described herein. The software may also reside completely or at least partially within the main memory and or within the processing device during execution thereof by the computer system the main memory and the processing device also constituting machine readable storage media. The software may further be transmitted or received over a network via the network interface device .

While the machine readable storage medium is shown in an exemplary embodiment to be a single medium the term machine readable storage medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term machine readable storage medium shall also be taken to include any medium that is capable of storing or encoding a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present invention. The term machine readable storage medium shall accordingly be taken to include but not be limited to solid state memories and optical and magnetic media.

The embodiments described herein are directed to a centralized monitoring tool that inspects both storage objects on a storage system and host side storage objects on multiple heterogeneous host side servers. As contrasted with conventional systems which typically only provide visibility into the virtualization on the host side server and requires communication with the storage administrator for visibility into the virtualization on the storage system the embodiments described herein may provide complete visibility into both the virtualization on the host side servers as well as the virtualization on the storage system such as to determine why the storage is or is not performing without requiring communication to the storage administrator and or administrator interaction at the storage server. Using this increase in visibility the server administrator may take corrective action without coordination with the server administrator. The complete visibility may also help the server administrator understand the storage resources are being used from top to bottom in abstraction.

The embodiments described herein may present the workflow of operations e.g. provisioning and snapshot operations and resulting information in a way that is understandable to the server administrator e.g. host side storage objects base device host side file systems volume groups host volumes etc as opposed to being understandable to the storage administrator e.g. LUN volumes aggregates etc . The embodiments described herein are able to understand the underlying storage infrastructure in order to take advantage of the storage virtualization strategy employed by the storage system. In particular the management server understands the storage virtualization strategy of the storage system and the host side agent understands the particular host side virtualization strategy e.g. understands the host side file system . The embodiments described herein coordinate interaction between the logical data containers on the storage server and the host side agents of multiple heterogeneous host side servers.

It should be noted that the storage system described herein may be one of the storage systems available from NetApp Inc. of Sunnyvale Calif. Also as described herein the capability to centrally manage the server stack and use storage virtualization may be provided through a host side agent. In one embodiment the host side agent is the SNAPDRIVE server available from NetApp Inc. of Sunnyvale Calif. which has the ability to monitor and manage the host side storage objects. The SNAPDRIVE server may be used to provision logical data containers on the respective host side server allowing the management server to centrally provision logical data containers and logical units of data for host side objects residing on multiple heterogeneous host side servers. The SNAPDRIVE server may also be used to allow policy based provisioning acting as a provisioning manager. The host side agents may be for example the SNAPDRIVE server referred to herein as SDW server for a host running the Windows operating system the SNAPDRIVE server for a Unix based host running a Unix based operating system referred to herein as SDW server or similar agents on hosts running other types of operating systems such as the Linux operating system or the like.

Using the embodiments described herein one possible advantage may be that a server administrator may quickly perform end to end storage provisioning that includes both the host side file system and the underlying storage array without involving the storage administrator. The end to end provisioning may take into account both the storage objects and the host side storage objects to present a global view of the storage infrastructure. Since the end to end storage provisioning is performed independent of the type of file system that is being provisioned the amount of understanding of each type of heterogeneous file system by the server administrator decreases such as the host side virtualization e.g. how each of the file systems volume managers disk managers operate .

Also as described above one of the basic challenges when a host file system is stored in a storage system that implements storage based snapshots and thin provisioning is how to report out of space conditions. Since the storage space for the host side file system may be thinly provisioned or include snapshot data the out of space conditions on the storage system need to be correlated and reported to the server administrator as out of space conditions for the host side file systems on the host servers. From a traditional file system perspective the storage space is entirely contained within the file system however when using an underlying storage system there is storage space that exists outside of the host file system. For example when a storage virtualization strategy employs multiple flexible volumes in an aggregate each of the flexible volumes may not individually be out of space but the collective space of the aggregate may exceed an out of space threshold. This out of space event occurs on the storage systems. The embodiments described herein are able to coordinate out of space events detected on the storage server to out of space events to the respective host side file systems that are affected by the out of space events on the storage system. Without coordination of the out of space events between the storage system and the host side servers events in the storage layer may be invisible at the host layer. Using these embodiments the server administrator is able to know when the storage resource is running out of space and or that the server administrator is running out of snapshots.

The preceding description sets forth numerous specific details such as examples of specific systems components methods and so forth in order to provide a good understanding of several embodiments of the present invention. It will be apparent to one skilled in the art however that at least some embodiments of the present invention may be practiced without these specific details. In other instances well known components or methods are not described in detail or are presented in simple block diagram formats in order to avoid unnecessarily obscuring the present invention. Thus the specific details set forth are merely exemplary. Particular implementations may vary from these exemplary details and still be contemplated to be within the spirit and scope of the present invention.

Some portions of the detailed descriptions are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be borne in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion it is appreciated that throughout the description discussions utilizing terms such as processing or computing or calculating or determining or displaying or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

The algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with programs in accordance with the teachings herein or it may prove convenient to construct more specialized apparatus to perform the required method steps. The required structure for a variety of these systems will appear from the description herein. In addition the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein.

Although the operations of the method s herein are shown and described in a particular order the order of the operations of each method may be altered so that certain operations may be performed in alternative orders or so that certain operation may be performed at least in part concurrently with other operations. In another embodiment instructions or sub operations of distinct operations may be in an intermittent and or alternating manner.

Embodiments of the present invention include various operations as described above. These operations may be performed by hardware components software firmware or a combination thereof. The various operations may be performed by executing one or more computer programs to perform functions of the embodiments by operating on input data and generating output data. The operations may be performed sequentially or alternatively in parallel. The various operations may also be implemented in for example special purpose logic circuitry e.g. a field programmable gate array FPGA digital signal processor DSP Application specific integrated circuit ASIC programmable logic device PLD or the like .

The present invention also relates to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but is not limited to any type of disk including floppy disks optical disks CD ROMs and magnetic optical disks read only memories ROMs random access memories RAMS EPROMs EEPROMs magnetic or optical cards or any type of media suitable for storing electronic instructions and each coupled to a computer system bus.

Certain embodiments may be implemented as one or more computer program products. The one or more computer programs may be tangibly embodied in an information carrier e.g. in a machine readable storage device or in a propagated signal for execution by or to control the operation of one or more general purpose or special purpose processors to perform the described operations. A machine readable medium includes any mechanism for storing information in a form e.g. software processing application readable by a machine e.g. a computer . The machine readable storage medium may include but is not limited to magnetic storage medium e.g. floppy diskette optical storage medium e.g. CD ROM disks digital video disk DVD ROM disks magneto optical storage medium read only memory ROM random access memory RAM erasable programmable memory e.g. EPROM and EEPROM flash memory or another type of medium suitable for storing electronic instructions. The machine readable transmission medium may include but is not limited to electrical optical acoustical or other form of propagated signal e.g. carrier waves infrared signals digital signals etc. .

Additionally some embodiments may be practiced in distributed computing environments where the machine readable medium is stored on and or executed by more than one computer system. In addition the information transferred between computer systems may either be pulled or pushed across the communication medium connecting the computer systems.

In the foregoing specification the invention has been described with reference to specific exemplary embodiments thereof. It will however be evident that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The specification and drawings are accordingly to be regarded in an illustrative sense rather than a restrictive sense.

