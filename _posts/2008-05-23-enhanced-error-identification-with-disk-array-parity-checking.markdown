---

title: Enhanced error identification with disk array parity checking
abstract: When parity checking in a disk array such as a RAID-6 system determines data and parity information is unsynchronized, additional calculations are performed to determine whether the error may be attributed to faulty data on a disk drive or to a more systemic problem such as a faulty controller. In particular, for each particular error detected, the parity generating information is analyzed to determine if each error involves a common disk index. If so, the data can be corrected on that disk; if not other corrective procedures are implemented.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08196018&OS=08196018&RS=08196018
owner: International Business Machines Corporation
number: 08196018
owner_city: Armonk
owner_country: US
publication_date: 20080523
---
This application is a divisional of U.S. patent application Ser. No. 10 994 088 filed on Nov. 19 2004 now U.S. Pat No. 7 392 458 by Carl Edward Forhan et al. ROC920040141US1 and is related to the following U.S. Patent Applications all filed by Carl Edward Forhan Robert Edward Galbraith and Adrian Cuenin Gerhard Ser. Nos. 11 873 085 11 873 086 11 873 087 and 11 873 088 all filed Oct. 16 2007 and entitled RAID ENVIRONMENT INCORPORATING HARDWARE BASED FINITE FIELD MULTIPLIER FOR ON THE FLY XOR and all divisionals of Ser. No. 10 994 099 filed Nov. 19 2004 Ser. No. 11 867 407 entitled METHOD AND SYSTEM FOR IMPROVED BUFFER UTILIZATION FOR DISK ARRAY PARITY UPDATES which is a continuation of Ser. No. 10 994 086 filed Nov. 19 2004 now issued as U.S. Pat. No. 7 290 199 Ser. No. 11 923 280 entitled METHOD AND SYSTEM FOR INCREASING PARALLELISM OF DISK ACCESSES WHEN RESTORING DATA IN A DISK ARRAY SYSTEM which is a continuation of Ser. No. 10 994 098 filed Nov. 19 2004 and Ser. No. 12 106 856 Entitled RECOVERING FROM ABNORMAL INTERRUPTION OF A PARITY UPDATE OPERATION IN A DISK ARRAY SYSTEM which is a continuation of Ser. No. 10 994 097 filed Nov. 19 2004. In addition this application is related to U.S. patent application Ser. No. 12 126 503 filed on even date herewith ROC920040141US2 which is a continuation of the aforementioned Ser. No. 10 994 088 application. The entire disclosures of these applications are incorporated by reference herein.

The present invention relates to data protection methods for data storage and more particularly to systems implementing RAID 6 and similar data protection and recovery strategies.

RAID stands for Redundant Array of Independent Disks and is a taxonomy of redundant disk array storage schemes which define a number of ways of configuring and using multiple computer disk drives to achieve varying levels of availability performance capacity and cost while appearing to the software application as a single large capacity drive. Typical RAID storage subsystems can be implemented in either hardware or software. In the former instance the RAID algorithms are packaged into separate controller hardware coupled to the computer input output I O bus and although adding little or no central processing unit CPU overhead the additional hardware required nevertheless adds to the overall system cost. On the other hand software implementations incorporate the RAID algorithms into system software executed by the main processor together with the operating system obviating the need and cost of a separate hardware controller yet adding to CPU overhead.

Various RAID levels have been defined from RAID 0 to RAID 6 each offering tradeoffs in the previously mentioned factors. RAID 0 is nothing more than traditional striping in which user data is broken into chunks which are stored onto the stripe set by being spread across multiple disks with no data redundancy. RAID 1 is equivalent to conventional shadowing or mirroring techniques and is the simplest method of achieving data redundancy by having for each disk another containing the same data and writing to both disks simultaneously. The combination of RAID 0 and RAID 1 is typically referred to as RAID 0 1 and is implemented by striping shadow sets resulting in the relative performance advantages of both RAID levels. RAID 2 which utilizes Hamming Code written across the members of the RAID set is not now considered to be of significant importance.

In RAID 3 data is striped across a set of disks with the addition of a separate dedicated drive to hold parity data. The parity data is calculated dynamically as user data is written to the other disks to allow reconstruction of the original user data if a drive fails without requiring replication of the data bit for bit. Error detection and correction codes ECC such as Exclusive OR XOR or more sophisticated Reed Solomon techniques may be used to perform the necessary mathematical calculations on the binary data to produce the parity information in RAID 3 and higher level implementations. While parity allows the reconstruction of the user data in the event of a drive failure the speed of such reconstruction is a function of system workload and the particular algorithm used.

As with RAID 3 the RAID scheme known as RAID 4 consists of N data disks and one parity disk wherein the parity disk sectors contain the bitwise XOR of the corresponding sectors on each data disk. This allows the contents of the data in the RAID set to survive the failure of any one disk. RAID 5 is a modification of RAID 4 which stripes the parity across all of the disks in the array in order to statistically equalize the load on the disks.

The designation of RAID 6 has been used colloquially to describe RAID schemes that can withstand the failure of two disks without losing data through the use of two parity drives commonly referred to as the P and Q drives for redundancy and sophisticated ECC techniques. Although the term parity is used to describe the codes used in RAID 6 technologies the codes are more correctly a type of ECC code rather than simply a parity code. Data and ECC information are striped across all members of the RAID set and write performance is generally lower than with RAID 5 because three separate drives must each be accessed twice during writes. However the principles of RAID 6 may be used to recover a number of drive failures depending on the number of parity drives that are used.

Some RAID 6 implementations are based upon Reed Solomon algorithms which depend on Galois Field arithmetic. A complete explanation of Galois Field arithmetic and the mathematics behind RAID 6 can be found in a variety of sources and therefore only a brief overview is provided below as background. The Galois Field arithmetic used in these RAID 6 implementations takes place in GF 2 . This is the field of polynomials with coefficients in GF 2 modulo some generator polynomial of degree N. All the polynomials in this field are of degree N 1 or less and their coefficients are all either 0 or 1 which means they can be represented by a vector of N coefficients all in 0 1 that is these polynomials look just like N bit binary numbers. Polynomial addition in this Field is simply N bit XOR which has the property that every element of the Field is its own additive inverse so addition and subtraction are the same operation. Polynomial multiplication in this Field however can be performed with table lookup techniques based upon logarithms or with simple combinational logic.

Each RAID 6 check code i.e. P and Q expresses an invariant relationship or equation between the data on the data disks of the RAID 6 array and the data on one or both of the check disks. If there are C check codes and a set of F disks fail F C the failed disks can be reconstructed by selecting F of these equations and solving them simultaneously in GF 2 for the F missing variables. In the RAID 6 systems implemented or contemplated today there are only 2 check disks check disk P and check disk Q. It is worth noting that the check disks P and Q change for each stripe of data and parity across the array such that parity data is not written to a dedicated disk but is instead striped across all the disks.

Even though RAID 6 has been implemented with varying degrees of success in different ways in different systems there remains an ongoing need to improve the efficiency and costs of providing RAID 6 protection for data storage. The mathematics of implementing RAID 6 involve complicated calculations that are also repetitive. Accordingly efforts to improve the simplicity of circuitry the cost of circuitry and the efficiency of the circuitry needed to implement RAID 6 remains a priority today and in the future.

For example one aspect of RAID 4 and higher implementations is that once parity data for a parity stripe is initially generated later writes performed on the array typically require the parity to be updated by combining new data with old data and existing parity data to produce the new parity data. In RAID 4 and RAID 5 implementations these update operations often referred to as delta updates require each RAID write to include a read from two drives old data old parity the calculation of the difference between the new and old data the application of that difference to the old parity to obtain the new parity and the writing of the new data and parity back onto the same two drives which typically requires four I O operations to be performed. In RAID 6 implementations a delta update typically takes six I O operations given the need to update two parity drives.

Since delta update operations operate as modifications of prior data a problem with a single delta update operation can cause the parity data to become out of sync with the data with the error being propagated to future delta update operations. A number of problems could occur in a delta update operation e.g. if a disk returns incorrect data on a read of old data or old parity data if a disk writes incorrect new data or new parity data or if the RAID hardware or software XOR s the data incorrectly.

Out of sync parity if left undetected could cause a data integrity problem if a disk fails and the parity is needed to recreate data for that disk. Considering that millions or billions of delta updates can be performed over a relatively short period of time the risk of a problem with a single delta update and thus the parity getting out of sync can be unacceptable for many implementations.

As a result many RAID implementations employ parity checking which typically runs in the foreground or the background e.g. during periods of inactivity and which checks all of the parity stripes to ensure that the parity data is in sync. The parity checking is only performed for a good non exposed array for which the parity is expected to be valid.

When invalid parity data is detected during parity checking however conventional RAID implementations are unable to determine where the problem originated e.g. what particular drive caused the problem or if a hardware software problem rather than a particular drive was the cause of the problem. As a result such implementations typically alert a user or systems administrator of the problem requiring manual intervention to determine the root cause of the problem.

To address these and other problems associated with the prior art embodiments consistent with the invention utilize a parity checking algorithm in a disk array environment that is capable of assisting in the isolation of the particular source of a fault therein e.g. to identify a particular disk drive or to identify a source other than a disk drive as the root cause of a problem detected as a result of parity checking.

One aspect of the present invention relates to the performance of parity checking in a disk array environment in which each parity stripe across a plurality of disk drives has an associated first and second parity value. In accordance with this aspect the associated first parity value is verified for each of a plurality of data values and if the respective first parity value is determined to be in error then analysis is performed of first and second error values respectively associated with first and second parity equations. Based on the analysis it is determined whether one of the plurality of disk drives is faulty.

Another aspect of the present invention relates to the performance of parity checking in a disk array environment such as a RAID 6 environment in which a plurality of bytes over a plurality of disks are checked where each such byte has an associated first parity value and a second parity value. In accordance with this aspect each byte for which a first parity equation is not satisfied is identified. For each identified byte a first error value is calculated using the first parity equation and a second error value is calculated using a second parity equation. Based on the first error value and the second error value for each identified byte a faulty disk drive in the RAID 6 environment may be identified.

The embodiments discussed hereinafter utilize a parity checking algorithm in a disk array environment e.g. a RAID 6 environment that is capable of determining whether one of a plurality of disk drives in the RAID environment is faulty. In this regard some embodiments of the invention may be capable of determining whether one of the disk drives in a RAID environment is faulty or whether the fault lies in another component of the environment e.g. an adapter or controller . Some embodiments may additionally or alternatively determine which specific disk drive among a plurality of disk drives in a disk array environment is faulty.

Presented hereinafter are a number of embodiments of a disk array environment implementing a parity checking and error detection algorithm consistent with the invention. However prior to discussing such embodiments a brief background on RAID 6 is provided followed by a description of an exemplary hardware environment within which parity checking and error detection consistent with the invention may be implemented.

The nomenclature used herein to describe RAID 6 storage systems conforms to the most readily accepted standards for this field. In particular there are N drives of which any two are considered to be the parity drives P and Q. Using Galois Field arithmetic two independent equations can be written . . . 0 1 . . . 0 2 where the operator used herein represents an Exclusive OR XOR operation.

In these equations is an element of the finite field and dis data from the xdisk. While the P and Q disk can be any of the N disks for any particular stripe of data they are often noted as dand d. When data to one of the disks i.e. d is updated the above two equations resolve to old new 3 new old 4 new old 5 

In each of the last two equations the term to the right of the addition sign is a constant multiplied by the change in the data i.e. . These terms in equations 4 and 5 are often denoted as K and K respectively.

In the case of one missing or unavailable drive simple XOR ing can be used to recover the drive s data. For example if dfails then dcan be restored by 6 

In the case of two drives failing or being exposed the above equations can be used to restore a drive s data. For example given drives 0 through X and assuming drives A and B have failed the data for either drive can be restored from the remaining drives. If for example drive A was to be restored the above equations reduce to . . . 7 Exemplary Hardware Environment

With this general background of RAID 6 in mind attention can be turned to the drawings wherein like numbers denote like parts throughout the several views. illustrates an exemplary computer system in which a RAID 6 or other disk array may be implemented. For the purposes of the invention apparatus may represent practically any type of computer computer system or other programmable electronic device including a client computer a server computer a portable computer a handheld computer an embedded controller etc. Moreover apparatus may be implemented using one or more networked computers e.g. in a cluster or other distributed computing system. Apparatus will hereinafter also be referred to as a computer although it should be appreciated the term apparatus may also include other suitable programmable electronic devices consistent with the invention.

Computer typically includes at least one processor coupled to a memory . Processor may represent one or more processors e.g. microprocessors and memory may represent the random access memory RAM devices comprising the main storage of computer as well as any supplemental levels of memory e.g. cache memories non volatile or backup memories e.g. programmable or flash memories read only memories etc. In addition memory may be considered to include memory storage physically located elsewhere in computer e.g. any cache memory in a processor as well as any storage capacity used as a virtual memory e.g. as stored on the disk array or on another computer coupled to computer via network e.g. a client computer .

Computer also typically receives a number of inputs and outputs for communicating information externally. For interface with a user or operator computer typically includes one or more user input devices e.g. a keyboard a mouse a trackball a joystick a touchpad and or a microphone among others and a display e.g. a CRT monitor an LCD display panel and or a speaker among others . Otherwise user input may be received via another computer e.g. a computer interfaced with computer over network or via a dedicated workstation interface or the like. For additional storage computer may also include one or more mass storage devices accessed via a storage controller or adapter e.g. removable disk drive a hard disk drive a direct access storage device DASD an optical drive e.g. a CD drive a DVD drive etc. and or a tape drive among others. Furthermore computer may include an interface with one or more networks e.g. a LAN a WAN a wireless network and or the Internet among others to permit the communication of information with other computers coupled to the network. It should be appreciated that computer typically includes suitable analog and or digital interfaces between processor and each of components and as is well known in the art.

In accordance with the principles of the present invention the mass storage controller advantageously implements RAID 6 storage protection within an array of disks .

Computer operates under the control of an operating system and executes or otherwise relies upon various computer software applications components programs objects modules data structures etc. e.g. software applications . Moreover various applications components programs objects modules etc. may also execute on one or more processors in another computer coupled to computer via a network e.g. in a distributed or client server computing environment whereby the processing required to implement the functions of a computer program may be allocated to multiple computers over a network.

In general the routines executed to implement the embodiments of the invention whether implemented as part of an operating system or a specific application component program object module or sequence of instructions or even a subset thereof will be referred to herein as computer program code or simply program code. Program code typically comprises one or more instructions that are resident at various times in various memory and storage devices in a computer and that when read and executed by one or more processors in a computer cause that computer to perform the steps necessary to execute steps or elements embodying the various aspects of the invention. Moreover while the invention has and hereinafter will be described in the context of fully functioning computers and computer systems those skilled in the art will appreciate that the various embodiments of the invention are capable of being distributed as a program product in a variety of forms and that the invention applies equally regardless of the particular type of computer readable signal bearing media used to actually carry out the distribution. Examples of computer readable signal bearing media include but are not limited to recordable type media such as volatile and non volatile memory devices floppy and other removable disks hard disk drives magnetic tape optical disks e.g. CD ROM s DVD s etc. among others and transmission type media such as digital and analog communication links.

In addition various program code described hereinafter may be identified based upon the application within which it is implemented in a specific embodiment of the invention. However it should be appreciated that any particular program nomenclature that follows is used merely for convenience and thus the invention should not be limited to use solely in any specific application identified and or implied by such nomenclature. Furthermore given the typically endless number of manners in which computer programs may be organized into routines procedures methods modules objects and the like as well as the various manners in which program functionality may be allocated among various software layers that are resident within a typical computer e.g. operating systems libraries API s applications applets etc. it should be appreciated that the invention is not limited to the specific organization and allocation of program functionality described herein.

It will be appreciated that the embodiment illustrated in is merely exemplary in nature. For example it will be appreciated that the invention may be applicable to other disk array environments where multiple parity values are associated with each parity stripe and or multiple independent equations are utilized in calculating parity data. It will also be appreciated that a disk array environment consistent with the invention may utilize a completely software implemented control algorithm resident in the main storage of the computer or that some functions handled via program code in a computer or controller can be implemented in hardware logic circuits and vice versa. Therefore the invention should not be limited to the particular embodiments discussed herein.

Embodiments consistent with the invention implement error detection within a parity checking algorithm to assist in isolating the sources of errors in a RAID environment such as a RAID 6 environment.

As noted above within a RAID 5 system the parity P is updated according to a commonly known procedure that takes four I O operations. First the old data d is read from a disk then the new data d is written to the disk next the old parity data P is read from the disk and finally the newly calculated parity P is written to the disk. Should anything go wrong e.g. a power or controller failure incorrect XOR calculation a write error etc. during these four steps the data integrity of the disks may be placed in jeopardy. Also of note delta update operations operate as modifications of prior data so a problem with a single delta update operation can cause the parity data to become out of sync with the data with the error being propagated to future delta update operations.

As also noted above in conventional RAID 5 systems background parity checking has been utilized to ensure the data integrity of a RAID system. For example a background process would run during periods of inactivity that checked that the stripes of data and parity were correct. However within RAID 5 if an error was found insufficient information exists to determine whether a particular disk or an adapter controller or other component is at fault. Or if it is a disk at fault then which particular disk is at fault.

Similarly background parity checking has been utilized to ensure data integrity in RAID 6 systems. As described above with RAID 6 there are two independent equations that can be written for each parity stripe and are typically used to restore data in the case of multiple drive failures. Nonetheless conventional RAID 6 systems like RAID 5 systems are still incapable of determining the potential source of an error.

However it has been found that in addition to restoring data the independent equations utilized in parity checking in a RAID 6 system can be used to identify additional information about errors that might be located during parity checks including of note whether a particular disk drive is a source of an error detected during a parity check.

The aspects of RAID 6 that allow additional error investigation to be performed are explained below by way of an example. In particular there is a disk d that is assumed to have incorrect data. As a result the equations 1 and 2 are not satisfied i.e. they do not add together to equal 0 . Instead the equations with the faulty disk are . . . . . . 8 . . . . . . 9 where is the difference between the faulty disk s expected and actual data. Rearranging equations 8 and 9 gives 

Because of the properties of addition in Galois Field arithmetic i.e. the XOR operation dcan be corrected as shown in step by the equation new old 

In one exemplary embodiment of the parity checking algorithm described above the data or symbol size on the disks is 8 bits. By performing the parity checking over a number of bytes it can be determined with confidence whether a disk is at fault or not. For example the parity checking may operate on 256 disk blocks at a time each of which are 520 bytes in length. These values are exemplary in nature and other disk block size symbol size and number of blocks are contemplated within the scope of the present invention. However using the exemplary values above a parity checking routine will check 133 120 bytes and thus solve the primary equation 1 that many times. If at least one of these checks indicate a failure then the following conditions are checked in step for each byte 

If they both are zero then parity for that byte is correct. If one is zero and the other is non zero the error suggests that something other than a particular disk is at fault.

If all three of these conditions are satisfied then the errors are likely caused by faulty data on a disk which can be reported for further corrective action. Moreover as noted above based upon the identification of a particular disk as the source of faulty data in some instances the faulty data on the disk may also be corrected. On the other hand if one of these three conditions is not met then the errors are likely caused during the parity encoding operation and is the fault of the RAID controller or control software. In such instances individual disk data should not be corrected.

Thus embodiments of the present invention provide a method and system that utilizes RAID 6 parity information to not only detect unsynchronized data and parity but to perform further analysis which helps identify whether the data on the disk is faulty or the fault is likely the result of a RAID controller or component other than a specific disk.

Various modifications may be made to the illustrated embodiments without departing from the spirit and scope of the invention. For example it may be desirable in some embodiments to resync parity after detection of a parity error. Other modifications will be apparent to one of ordinary skill in the art having the benefit of the instant disclosure. Therefore the invention lies in the claims hereinafter appended.

