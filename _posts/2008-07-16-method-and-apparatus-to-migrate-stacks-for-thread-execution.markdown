---

title: Method and apparatus to migrate stacks for thread execution
abstract: A method and an apparatus that generate a request from a first thread of a process using a first stack for a second thread of the process to execute a code are described. Based on the request, the second thread executes the code using the first stack. Subsequent to the execution of the code, the first thread receives a return of the request using the first stack.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09135054&OS=09135054&RS=09135054
owner: Apple Inc.
number: 09135054
owner_city: Cupertino
owner_country: US
publication_date: 20080716
---
The present invention relates generally to multi thread systems. More particularly this invention relates to migrating thread stacks for thread context switching.

Advancement of computing systems has allowed a software program to run as one or more execution entities such as threads processes and so forth. Usually such a software program may cause thread context switching running from one thread to another. As a result resources are allocated dynamically to coordinate activities among execution entities. For example a synchronization mechanism is activated when more than one threads in a process concurrently request a single thread service which allows only one thread to access at a time. Usually synchronization mechanism requires allocation of synchronization resources such as events mutexes or locks etc. Therefore available resources for execution entities are reduced and the performance of a computing system can be compromised when synchronizing threads.

Although it may be possible to dedicate a single thread to perform a single thread task such a thread is likely to idle most of the time wasting valuable computing resources when no request is present for its service. Additionally a thread has to communicate with the single thread to obtain its service. Often times communications between threads may incur message passing queuing and or notifications etc. which again may drain away resources from computing systems.

Therefore system performance can be improved if multiple threads are synchronized leveraging existing mechanisms already established such as thread context switching in a multi threading system without requiring allocating additionally resources.

An embodiment of the present invention includes methods and apparatuses that generating a request from a first thread of a process using a first stack for a second thread of the process to execute a code. Based on the request the second thread executes the code using the first stack. Subsequent to the execution of the code the first thread receives a return of the request using the first stack.

In an alternative embodiment a first thread in a process executes a first code to update a stack associated with a first stack trace. A second thread in the same process executes a second code to update the stack. The updated stack is associated with a second stack trace on top of the first stack trace. A stack trace is displayed to provide debug information for the second thread. The displayed stack traced includes both the first stack trace and the second stack trace.

In an alternative embodiment a first thread in a process generates a first request using a first stack for a main thread of the process to execute a code. A second thread of the same process generates a second request using a second stack for the main thread to execute the code. Separately the main thread executes the code using the first stack according to the first request and executes the code using the second stack according to the second request.

Other features of the present invention will be apparent from the accompanying drawings and from the detailed description that follows.

A method and an apparatus for stack migration are described herein. In the following description numerous specific details are set forth to provide thorough explanation of embodiments of the present invention. It will be apparent however to one skilled in the art that embodiments of the present invention may be practiced without these specific details. In other instances well known components structures and techniques have not been shown in detail in order not to obscure the understanding of this description.

Reference in the specification to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment can be included in at least one embodiment of the invention. The appearances of the phrase in one embodiment in various places in the specification do not necessarily all refer to the same embodiment.

The processes depicted in the figures that follow are performed by processing logic that comprises hardware e.g. circuitry dedicated logic etc. software such as is run on a general purpose computer system or a dedicated machine or a combination of both. Although the processes are described below in terms of some sequential operations it should be appreciated that some of the operations described may be performed in different order. Moreover some operations may be performed in parallel rather than sequentially.

In one embodiment stack migration may be designed to provide a method and an apparatus that migrate a stack from one thread to another thread. A single existing thread such as a main thread in a process may execute a code using migrated stacks from multiple threads of the same process in a synchronous manner. In one embodiment a thread in a process may call an API application programming interface that causes a stack migration associated with a main thread to execute a code as part of parameters passed to the API. A thread may execute a code using a stack migrated from another thread waiting for a return from the execution of the code. In one embodiment stack migration may cause a debug system to display a single stack trace associated with executions by more than one threads.

In one embodiment system may include a thread request to schedule a target thread to perform a task. A running thread may generate a thread request for a target thread to perform a task. In one embodiment a thread request such as Target thread request may include an identifier for a target thread such as Target thread ID to identify Target thread an identifier for a running thread which generates the request such as Source thread ID to identify Source thread and information on the intended task such as Request task info which might include a pointer to a function code. In one embodiment a thread request may be stored in a request queue such as Request queue to be processed in an order according to a thread management module . A thread management module may update a schedule for running threads such as Thread schedule based on a request queue . For example Thread schedule may include Target thread id which identifies Target thread as currently running thread and Source thread id which identifies Source thread as a thread scheduled to run subsequently.

In one embodiment system may include a stack jumping module which performs stack migration between a source thread and a target thread identified in a thread request. A stack jumping module may migrate a stack from a source thread such as Source stack to a target thread such as Target thread according to a thread request such as Target thread request . In one embodiment a stack jumping module may update a thread context for migrating stacks. For example when running Target thread a stack jumping module may update a thread context including a local storage pointer and a stack pointer pointing respectively to Target stack and Target TLS of Target thread . A running thread may be the one selected to run from among more than one threads already scheduled. In one embodiment no more than one running thread may be associated with a single processor at a time. A stack jumping module may perform a request task such as according to Request task info of Target thread request subsequent to updating a thread context for stack migration.

If the target thread is active performing a current task when a thread request for the target thread is generated in one embodiment the processing logic of process may not run the target thread for the request before the current task is completed. A thread may be active when scheduled to run in a thread schedule such as Thread schedule of . At block the processing logic of process running a source thread may wait for a return from an execution of a code by the requested target thread. In one embodiment the processing logic of process may detect whether a return from an execution of a code is available via a storage location. In another embodiment the processing logic of process may depend on a hardware interrupt which indicates to a waiting source thread that a return an execution of a code by a requested target thread is available.

At block according to one embodiment the processing logic of process may schedule a target thread identified according to a request such as for example based on a Target thread ID in Target thread request of . An identified target thread may be associated with a target stack such as Target stack associated with Target thread of . The processing logic of process may update a schedule such as Thread schedule of to schedule an identified target thread. In one embodiment when an identified target thread according to a request is active in a schedule a corresponding source thread such as Source thread ID of may also be active waiting for a return in the schedule. At block the processing logic of process may perform a task specified in a thread request from a target thread using a target stack associated with the target thread. A stack pointer of a thread context for the processing logic of process such as Stack pointer in Thread context of may point to the associated target stack used by the target thread. The processing logic of process may perform a task at block according to a schedule established for the target thread at block .

Subsequently at block the processing logic of process may assign a source stack associated with a source thread to the target thread for executing a code. Thus the source stack may be migrated to replace the target stack associated with the target thread before executing the code. A thread request such as Target thread request of may identify both the source thread and the target thread. In one embodiment the processing logic of process may update a stack pointer in a thread context from a pointer pointing to a target stack to a pointer pointing to a source stack for stack migration. At block the processing logic of process may execute a code from the target thread using the source stack. During the execution of the code there may be no updates on the target stack. In one embodiment a thread request identifying a target thread may include a pointer to the code executed such as in Request task info of . When the execution of the code from the target thread is completed a return may be indicated by an update in a storage location or by generating a hardware interrupt for the source thread. At block in one embodiment the processing logic of process may detect a return from the execution of the code when running the source thread using the source stack. In one embodiment the processing logic of process may run the source thread using the source stack to determine if the execution of the code is complete. A schedule such as Thread schedule of may include identifiers for both the source thread and the target thread to indicate that both threads are currently active waiting to be selected for running as scheduled.

According to one embodiment the processing logic of process may migrate a stack associated with a thread calling the API to the scheduled main thread at block . A main thread may be associated with a main stack separate from the migrated stack. Prior to migrating a stack the processing logic of process may perform operations according to the API from the main thread using a main stack associated with the main thread. In one embodiment the processing logic of process may switch a thread context from the thread calling the API at block to the main thread as scheduled at block . During thread context switch the processing logic of process may update a local storage pointer such as Local storage pointer of in a thread context such as Thread context of to point to a local storage associated with a main thread and update a stack pointer such as Stack pointer of of a thread context to point to a main stack associated with the main thread. To migrate a stack to a main thread the processing logic of process may update a stack pointer of a current thread context to point to the stack. At block in one embodiment the processing logic of process may execute a code via an API from the main thread using the stack migrated from the thread calling the API. While the code is being executed from a main thread a thread calling the API may be scheduled in a thread schedule. The processing logic of process may perform a thread context switch to run the thread calling an API to determine if a return from the API is available. In one embodiment subsequent to completing the execution of the code at block the processing logic of process may perform a thread context switch to run a thread calling an API to receive a return from the API.

At block the processing logic of process may migrate the first stack of the first thread to a second thread associated with a second stack. Both the first thread and the second thread may belong to one single process. To migrate the first stack in one embodiment the processing logic of process may update from a second thread such as Target thread of a stack pointer in a thread context pointing to the second stack such as Stack pointer pointing to Target stack of to point to the first stack associated with the first thread such as Source stack of Source thread in . Subsequently at block in one embodiment the processing logic of process may execute a second code from the second thread to push a second stack state to the first stack on top of the first stack state of the first stack. The second stack state may be generated according to the execution of the second code from the second thread.

In one embodiment at block the processing logic of process may receive a debug request such as from Debug module of according to a debug command from a user. The processing logic of process may perform debug operations such as inspecting debug information including a stack state for a thread. At block in one embodiment the processing logic of process may retrieve a current stack state from the second thread to present a stack trace in response to the debug request. The processing logic of process may present a stack trace via a user interface such as User interface module of to a user based on the retrieved stack state of the second thread including both the first stack state and the second stack state. In one embodiment the processing logic of process may present a separate stack trace for the first thread including also both the first stack state and the second stack state.

As shown in the computer system which is a form of a data processing system includes a bus which is coupled to a microprocessor s and a ROM Read Only Memory and volatile RAM and a non volatile memory . The microprocessor may retrieve the instructions from the memories and execute the instructions to perform operations described above. The bus interconnects these various components together and also interconnects these components and to a display controller and display device and to peripheral devices such as input output I O devices which may be mice keyboards modems network interfaces printers and other devices which are well known in the art. Typically the input output devices are coupled to the system through input output controllers . The volatile RAM Random Access Memory is typically implemented as dynamic RAM DRAM which requires power continually in order to refresh or maintain the data in the memory.

The mass storage is typically a magnetic hard drive or a magnetic optical drive or an optical drive or a DVD RAM or a flash memory or other types of memory systems which maintain data e.g. large amounts of data even after power is removed from the system. Typically the mass storage will also be a random access memory although this is not required. While shows that the mass storage is a local device coupled directly to the rest of the components in the data processing system it will be appreciated that the present invention may utilize a non volatile memory which is remote from the system such as a network storage device which is coupled to the data processing system through a network interface such as a modem or Ethernet interface or wireless networking interface. The bus may include one or more buses connected to each other through various bridges controllers and or adapters as is well known in the art.

Portions of what was described above may be implemented with logic circuitry such as a dedicated logic circuit or with a microcontroller or other form of processing core that executes program code instructions. Thus processes taught by the discussion above may be performed with program code such as machine executable instructions that cause a machine that executes these instructions to perform certain functions. In this context a machine may be a machine that converts intermediate form or abstract instructions into processor specific instructions e.g. an abstract execution environment such as a virtual machine e.g. a JAVA Virtual Machine an interpreter a Common Language Runtime a high level language virtual machine etc. and or electronic circuitry disposed on a semiconductor chip e.g. logic circuitry implemented with transistors designed to execute instructions such as a general purpose processor and or a special purpose processor. Processes taught by the discussion above may also be performed by in the alternative to a machine or in combination with a machine electronic circuitry designed to perform the processes or a portion thereof without the execution of program code.

An article of manufacture may be used to store program code. An article of manufacture that stores program code may be embodied as but is not limited to one or more memories e.g. one or more flash memories random access memories static dynamic or other optical disks CD ROMs Compact Disc Read Only Memory DVD Digital Versatile Disc ROMs EPROMs Erasable Programmable Read Only Memory EEPROMs Electrically Erasable Programmable Read Only Memory magnetic or optical cards or other type of machine readable media suitable for storing electronic instructions. Program code may also be downloaded from a remote computer e.g. a server to a requesting computer e.g. a client by way of data signals embodied in a propagation medium e.g. via a communication link e.g. a network connection .

The preceding detailed descriptions are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the tools used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of operations leading to a desired result. The operations are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be kept in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the above discussion it is appreciated that throughout the description discussions utilizing terms such as processing or computing or calculating or determining or displaying or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

The present invention also relates to an apparatus for performing the operations described herein. This apparatus may be specially constructed for the required purpose or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but is not limited to any type of disk including floppy disks optical disks CD ROMs and magnetic optical disks read only memories ROMs RAMs EPROMs EEPROMs magnetic or optical cards or any type of media suitable for storing electronic instructions and each coupled to a computer system bus.

The processes and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with programs in accordance with the teachings herein or it may prove convenient to construct a more specialized apparatus to perform the operations described. The required structure for a variety of these systems will be evident from the description below. In addition the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein.

The foregoing discussion merely describes some exemplary embodiments of the present invention. One skilled in the art will readily recognize from such discussion the accompanying drawings and the claims that various modifications can be made without departing from the spirit and scope of the invention.

