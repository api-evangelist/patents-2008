---

title: Utilizing cache information to manage memory access and cache utilization
abstract: A method and system of managing data access in a shared memory cache of a processor are disclosed. The method includes probing one or more memory addresses that map to a subset of the shared memory cache and sensing a plurality of events in the one or more memory addresses. Cache utilization information is then obtained by reading a hardware performance counter of the processor. The hardware performance counter is incremented based on the occurrence of the plurality of events. Based upon the cache utilization information, an occurrence of one of the plurality of events is reduced.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07831773&OS=07831773&RS=07831773
owner: VMware, Inc.
number: 07831773
owner_city: Palo Alto
owner_country: US
publication_date: 20080827
---
The present patent application is a continuation application of U.S. patent application Ser. No. 11 410 474 filed on Apr. 24 2006 now U.S. Pat. No. 7 434 002 entitled UTILIZING CACHE INFORMATION TO MANAGE MEMORY ACCESS AND CACHE UTILIZATION. 

Processors and memory in many computing systems such as Non Uniform Memory Access NUMA systems are arranged in nodes that allow for communications between the nodes. In a typical NUMA system each node is comprised of some number of processors together with local memory and possibly a shared memory cache plus some interconnection mechanism to access other remote nodes. Likewise many modem processors which contain multiple cores and or multiple threads may have many of the same attributes of a NUMA system such as shared cache and non uniform communication speeds for instance intra node processor communication speeds may be faster than inter node processor communication speeds . These arrangements allow the processors in such systems to share caches and to share memory. The ability to access remote memory is useful however several performance penalties are suffered by computing systems when accessing remote memory. These penalties are experienced by physical computing systems and processors virtual computing systems and processors and systems composed of both physical and virtual components.

One performance penalty is the slower access times experienced when accessing remote memory rather than local memory. For instance many programs access memory when they run. If a large amount of the memory being accessed by one of the programs is remote memory rather than local memory the program will run less efficiently due to the waiting for remote memory access. The cumulative effect across multiple programs is that they all run slower.

Another performance penalty results from overuse of signal lines that connect nodes together. Because signal lines have limited capacity a single program that engages in heavy remote memory access can slow down many other programs due to an overtaxing of the bandwidth on the nodal interconnects. This problem is only exacerbated by multiple programs that have heavy remote access memory requirements.

The use of memory caches attempts to solve these problems. However normal caches often fall short due to their limited size the conflicting uses of the cache that occur when multiple programs run on a computer system and the inability of other portions of the computer system to gain visibility into how data is stored and managed within the cache.

Many NUMA systems attempt to solve the performance problems associated with heavy remote access of data by providing some facilities for software to obtain a measure of memory locality. A popular approach along these lines in high end purpose built systems such as SGI Origin computers is providing per page performance counters. In computing terms system memory is typically divided into addressable units commonly known as pages. A page is typically a particular fixed size contiguous portion of memory determined by computing hardware. In x86 type hardware for example a page is a 4 KB or 2 MB contiguous portion of memory. However it is well known that pages of other sizes may be defined and managed. These performance counters allow software to measure the number of accesses to a page from each node. However this approach incurs a very large overhead in terms of memory consumption and hardware implementation costs. Additionally lower end commodity NUMA systems only provide a set of very coarse per node counters for remote memory access. These coarse counters have no ability to correlate accesses to the particular memory locations or source processors that caused the access.

Some commodity processors such as MIPS R4000 processors also allow very limited software access to cache data but the access is only used for error correction and system initialization. Other approaches utilize cache sampling techniques that require heavy hardware support and are thus expensive and cumbersome to implement in production systems such as commodity processors.

A related problem exists in the form of conflict misses that occur due to the sharing of caches that are not fully associative. Caches are often shared by multiple processors in a multi node system by multiple software programs that run either sequentially or simultaneously by multiple threads in a multi threading system or by multiple processor cores in a multi core systems. Such systems can be either physical virtual or some combination. Conflict misses occur when a cache s hash function maps several commonly utilized pieces of data to the same portion of the cache.

Traditional operating systems use static cache coloring techniques to mitigate the problem of conflict misses by allocating physical cache pages in ways that are expected to evenly utilize the different portions of the cache. Processor designs have also increased associativity of caches to help overcome the problem of conflict misses. While static cache coloring and widely set associative caches mitigate conflict miss problems on commodity processors the possibility for conflict grows substantially when the processor in question contains multiple cores that share a common cache or multiple hardware threads such as simultaneous multi threading SMT that share a common cache.

In such a shared cache conflict misses will occur for instance when the number of active cache lines at a given index summed over all active hardware threads cores exceeds the cache s associativity. With four cores sharing an eight way set associative cache for instance if each core heavily utilizes three cache lines that map to a given cache index serious conflict misses can impede performance. However such a scenario may cause no problems at all on a dual core processor that is eight way set associative.

One software technique that addresses this problem is compiler directed coloring. In this technique during the compilation stage a designer selects portions of a cache that will be addressed by a program. This technique is useful in single application systems or in situations where there is complete control over all programs that run on a system. However in most instances even if a designer can control cache access patterns by specially compiling one program other programs such as commercial programs will have unknown and potentially conflicting cache access patterns. Therefore it is generally unhelpful to alter cache access patterns of one process through compiler directed coloring when there is no knowledge of what areas of the cache other processes will utilize.

A second technique used in some processors is called dynamic page remapping. Dynamic remapping is a technique whereby the operating system copies data from one page to another then uses virtual memory to make the process reference the new page. However this remapping mechanism is only useful if the operating system can figure out which pages are useful to remap in order to increase efficiency.

Some RISC Reduced Instruction Set Computer chips have a Software managed Translation Look aside Buffer STLB and the operating system can use this hardware to figure out which pages are being accessed frequently. This access information allows the operating system to determine which pages are worth remapping. However commodity processors such as Intel IA32 AMD x86 and other x86 64 type commodity processors do not have an STLB so this information gathering technique is impossible on them.

Thus applications operating systems and virtualization software e.g. Virtual Machine Monitors VMMs or hypervisors designed for simultaneous multi threading multi core processors or shared cache use will suffer performance penalties if portions of their shared caches are over utilized. Likewise performance penalties are also suffered by physical and virtual computing systems which too heavily rely on remote access of memory. Hence the inefficiencies experienced with caches and memory access are a significant and growing problem in today s computing environment.

The advantages of virtual machine technology have become widely recognized. Among these advantages is the ability to run multiple virtual machines on a single host platform. This makes better use of the capacity of the hardware while still ensuring that each user enjoys the features of a complete computer. Depending on how it is implemented virtualization can also provide greater security since the virtualization can isolate potentially unstable or unsafe software so that it cannot adversely affect the hardware state or system files required for running the physical as opposed to virtual hardware.

As is well known in the field of computer science a virtual machine VM is a software abstraction or virtualization of an actual physical computer system. shows one possible arrangement of a computer system that implements virtualization. shows a plurality of virtual machines VMs n and a plurality of virtual machine monitors VMMs n coupled to an exemplary system hardware platform . An optional kernel used in non hosted systems is also shown.

In a virtual machine VM which in this system is a guest is installed on a host platform or simply host which includes system hardware and one or more layers or co resident components comprising system level software such as OS or similar kernel VMMs n or some combination of these. As software the code defining VM will ultimately execute on the actual system hardware .

As in almost all computers this system hardware will typically include one or more CPUs some form of memory volatile and or non volatile one or more storage devices such as one or more disks and one or more devices which may be integral or separate and removable. In many existing virtualized systems the hardware processor s are the same as in a non virtualized computer with the same platform for example the Intel x86 platform. Because of the advantages of virtualization however some hardware vendors have proposed developed or released processors that include specific hardware support for virtualization.

Each VM will typically mimic the general structure of a physical computer and as such will usually have both virtual system hardware and guest system software . The virtual system hardware typically includes at least one virtual CPU virtual memory at least one storage device such as virtual disk and one or more virtual devices . Note that virtual disk and physical disk are also devices but are shown separately in because of the important roles they play. All of the virtual hardware components of VM may be implemented in software to emulate corresponding physical components. The guest system software typically includes a guest operating system OS and drivers as needed for example for the various virtual devices .

To permit computer systems to scale to larger numbers of concurrent threads systems with multiple CPUs have been developed. Many conventional hardware platforms therefore include more than one hardware processor . In many such platforms each processor is a separate chip and may share system resources such as main memory and or at least one I O device. Multi core architectures have also been developed for example IBM POWER4 and POWER5 architectures Intel dual core processors AMD dual core processors as well as the Sun UltraSparc IV in which more than one physical CPU is fabricated on a single chip with its own set of functional units such as a floating point unit and an arithmetic logic unit ALU and can execute threads independently. Multi cored processors typically share some resources such as some cache.

Note that a single VM may be configured with more than one virtualized processor. To permit computer systems to scale to larger numbers of concurrent threads systems with multiple CPUs have been developed. These symmetric multi processor SMP systems are available as extensions of the PC platform and from other vendors. Essentially an SMP system is a hardware platform that connects multiple processors to a shared main memory and shared I O devices. Virtual machines may also be configured as SMP VMs. for example illustrates multiple virtual processors . . . VCPU VCPU . . . VCPUm within the VM .

Yet another configuration is found in a so called multi core architecture in which more than one physical CPU is fabricated on a single chip with its own set of functional units such as a floating point unit and an arithmetic logic unit ALU and can execute threads independently multi core processors typically share only very limited resources such as some cache. Still another technique that provides for simultaneous execution of multiple threads is referred to as simultaneous multi threading in which more than one logical CPU hardware thread operates simultaneously on a single chip but in which the logical CPUs flexibly share not only one or more caches but also some functional unit s and sometimes also the translation lookaside buffer TLB . This invention may be used regardless of the type physical and or logical or number of processors included in a VM.

If VM is properly designed applications running on VM will function as they would if run on a real computer. This occurs even though the applications are running at least partially indirectly that is via the guest OS and virtual processor s . Executable files will be accessed by guest OS from virtual disk or virtual memory which will be portions of the actual physical disk or physical memory allocated to VM . Applications may be installed within VM in a conventional manner using guest OS . Guest OS retrieves files required for the execution of such installed applications from virtual disk in a conventional manner.

Some interface is generally required between the guest software within a VM and the various hardware components and devices in the underlying hardware platform. This interface which can be generally referred to as virtualization software may include one or more software components and or layers possibly including one or more of the software components known in the field of virtual machine technology as virtual machine monitors VMMs hypervisors or virtualization kernels. Because virtualization terminology has evolved over time and has not yet become fully standardized these terms do not always provide clear distinctions between the software layers and components to which they refer. For example hypervisor is often used to describe both a VMM and a kernel together either as separate but cooperating components or with one or more VMMs incorporated wholly or partially into the kernel itself however hypervisor is sometimes used instead to mean some variant of a VMM alone which interfaces with some other software layer s or component s to support the virtualization. Moreover in some systems some virtualization code is included in at least one superior VM to facilitate the operations of other VMs. Furthermore specific software support for VMs may be included in the host OS itself. Unless otherwise indicated the invention described below may be used in virtualized computer systems having any type or configuration of virtualization software.

Moreover shows virtual machine monitors that appear as separate entities from other components of the virtualization software. Furthermore some software components used to implemented one illustrated embodiment of the invention are shown and described as being within a virtualization layer located logically between all virtual machines and the underlying hardware platform and or system level host software. This virtualization layer can be considered part of the overall virtualization software although it would be possible to implement at least part of this layer in specialized hardware. The illustrated embodiments are given only for the sake of simplicity and clarity and by way of illustration as mentioned above the distinctions are not always so clear cut. Again unless otherwise indicated or apparent from the description it is to be assumed that the invention can be implemented anywhere within the overall structure of the virtualization software and even in systems that provide specific hardware support for virtualization

The various virtualized hardware components in the VM such as the virtual CPU s the virtual memory the virtual disk and the virtual device s are shown as being part of the VM for the sake of conceptual simplicity. In actuality these components are usually implemented as software emulations included in the VMM. One advantage of such an arrangement is that the VMM may but need not be set up to expose generic devices which facilitate VM migration and hardware platform independence.

Different systems may implement virtualization to different degrees virtualization generally relates to a spectrum of definitions rather than to a bright line and often reflects a design choice with respect to a trade off between speed and efficiency on the one hand and isolation and universality on the other hand. For example full virtualization is sometimes used to denote a system in which no software components of any form are included in the guest other than those that would be found in a non virtualized computer thus the guest OS could be an off the shelf commercially available OS with no components included specifically to support use in a virtualized environment.

In contrast another concept which has yet to achieve a universally accepted definition is that of para virtualization. As the name implies a para virtualized system is not fully virtualized but rather the guest is configured in some way to provide certain features that facilitate virtualization. For example the guest in some para virtualized systems is designed to avoid hard to virtualize operations and configurations such as by avoiding certain privileged instructions certain memory address ranges etc. As another example many para virtualized systems include an interface within the guest that enables explicit calls to other components of the virtualization software.

For some para virtualization implies that the guest OS in particular its kernel is specifically designed to support such an interface. According to this view having for example an off the shelf version of Microsoft Windows XP as the guest OS would not be consistent with the notion of para virtualization. Others define para virtualization more broadly to include any guest OS with any code that is specifically intended to provide information directly to any other component of the virtualization software. According to this view loading a module such as a driver designed to communicate with other virtualization components renders the system para virtualized even if the guest OS as such is an off the shelf commercially available OS not specifically designed to support a virtualized computer system. Unless otherwise indicated or apparent this invention is not restricted to use in systems with any particular degree of virtualization and is not to be limited to any particular notion of full or partial para virtualization.

In addition to the sometimes fuzzy distinction between full and partial para virtualization two arrangements of intermediate system level software layer s are in general use a hosted configuration and a non hosted configuration which is shown in . In a hosted virtualized computer system an existing general purpose operating system forms a host OS that is used to perform certain input output I O operations alongside and sometimes at the request of the VMM. The Workstation product of VMware Inc. of Palo Alto Calif. is an example of a hosted virtualized computer system which is also explained in U.S. Pat. No. 6 496 847 Bugnion et al. System and Method for Virtualizing Computer Systems 17 Dec. 2002 .

As illustrated in in many cases it may be beneficial to deploy VMMs on top of a software layer a kernel constructed specifically to provide efficient support for the VMs. This configuration is frequently referred to as being non hosted. Compared with a system in which VMMs run directly on the hardware platform use of a kernel offers greater modularity and facilitates provision of services for example resource management that extend across multiple virtual machines. Compared with a hosted deployment a kernel may offer greater performance because it can be co developed with the VMM and be optimized for the characteristics of a workload consisting primarily of VMs VMMs. The kernel also handles any other applications running on it that can be separately scheduled as well as a console operating system that in some architectures is used to boot the system and facilitate certain user interactions with the virtualization software for example using applications .

Note that the kernel is not the same as the kernel that will be within the guest OS as is well known every operating system has its own kernel. Note also that the kernel is part of the host platform of the VM VMM as defined above even though the configuration shown in is commonly termed non hosted moreover the kernel may be both part of the host and part of the virtualization software or hypervisor. The difference in terminology is one of perspective and definitions that are still evolving in the art of virtualization.

In one embodiment a method of managing data access in a shared memory cache of a processor is disclosed. The method includes probing one or more memory addresses that map to a subset of the shared memory cache and sensing a plurality of events in the one or more memory addresses. Cache utilization information is then obtained by reading a hardware performance counter of the processor. The hardware performance counter is incremented based on the occurrence of the plurality of events. Based upon the cache utilization information an occurrence of one of the plurality of events is reduced.

In another embodiment a computer storage device containing a computer program product having computer readable program code embedded therein for causing a computer system to perform a method of optimizing utilization of a shared memory cache is disclosed. The method includes probing one or more memory addresses that map to a subset of the shared memory cache and sensing a plurality of events in the one or more memory addresses. Cache utilization information is then obtained by reading a hardware performance counter of the processor. The hardware performance counter is incremented based on the occurrence of the plurality of events. Based upon the cache utilization information an occurrence of one of the plurality of events is reduced.

In yet another embodiment a method of optimizing memory access in a processor is disclosed. The method includes inspecting a cache metadata of a shared cache memory to determine memory utilization by one or more processes and dynamically relocating data to reduce cache color contention within the shared cache memory based upon the cache metadata inspection.

Reference will now be made in detail to embodiments of the present invention examples of which are illustrated in the accompanying drawings. While the invention will be described in conjunction with various embodiments it will be understood that they are not intended to limit the present invention to these embodiments. On the contrary the present invention is intended to cover alternatives modifications and equivalents which may be included within the spirit and scope the various embodiments as defined by the appended claims. Furthermore in the following detailed description numerous specific details are set forth in order to provide a thorough understanding of the present invention. However the present invention may be practiced without these specific details. In other instances well known methods procedures components and circuits have not been described in detail as not to unnecessarily obscure aspects of the present embodiments.

Unless specifically stated otherwise as apparent from the following discussions it is appreciated that throughout the present detailed description discussions utilizing terms such as inspecting analyzing optimizing receiving requesting evicting referencing determining tracking improving reporting probing running measuring reading using comparing scheduling ascertaining utilizing rescheduling moving copying reducing relocating storing recoloring and remapping or the like refer to the actions and processes of a computer system or similar electronic computing device. The computer system or similar electronic computing device manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices. The present invention is also well suited to the use of other computer systems such as for example optical computers. Additionally it should be understood that in embodiments of the invention one or more of the steps can be performed manually.

As an overview one embodiment of the present invention is a method of utilizing cache metadata to optimize memory access. In one such method embodiment software of the present invention inspects cache metadata associated with a set of cache locations. The cache is a part of a physical system or a virtual machine. Cache metadata comprises one or more pieces of cache information such as a cache tag or tag identifying a memory address recency of use status information for a cache location such as Least Recently Used LRU bits or Most Recently Used MRU bits a protocol state of a cache location information about which hardware thread or core brought a particular piece of data into the cache or other cache metadata information.

In one embodiment of the present invention the cache metadata is received in response to requesting the cache metadata from cache hardware. Elements of the present invention analyze the cache metadata to determine memory utilization such as remote memory utilization of one or more processes supported by the cache. In various embodiments analysis of cache metadata comprises one or more techniques such as manipulating data at cache locations periodically inspecting cache metadata to check for changes determining an amount of space used in the cache by remote memory locations determining a frequency with which one or more particular remote memory locations are stored within the cache tracking remote memory utilization over a period of time and determining a cache utilization rate such as a hit miss conflict eviction or thrashing rate for a portion of the cache. Elements of the present invention optimize memory access based on results of the analysis of the cache metadata. In some embodiments the memory optimization comprises locally storing selected information. In some embodiments the memory optimization comprises dynamically relocating data to reduce cache color contention within the cache.

As an overview one embodiment of the present invention is a method of optimizing utilization of a shared cache. In one such method embodiment software of the present invention probes a set of locations in a cache while an observed process is running descheduled or interrupted. For example this probing can comprise running a piece of code to repeatedly access one or more memory addresses that map to a subset of a cache while an observed process is running. The present invention then determines cache misses that occur during the probing. In one embodiment the present invention determines cache misses inferentially via latency of replies from the cache.

In another embodiment the present invention determines cache misses by reading a hardware performance counter of the processor or cache that increments based on the occurrence of a relevant event the cache or in a memory system associated with the cache. Some examples of relevant events that can trigger incrementing of such a performance counter are a cache hit a cache miss a message sent out to the memory bus from the cache and etc. Software of the present invention then uses techniques to directly or indirectly determine a variety of cache information such as which portions of the cache are used by the observed process what processes of several observed processes highly contend for the same areas of a cache which processes of several observed processes have substantially non conflicting cache access patterns and which pages and or addresses of memory are heavily accessed.

In some embodiments cache metadata is not used to determine information about the cache thus making this technique useful in processors where cache metadata is not available to software applications. Elements of the present invention then optimize the use of the shared cache by remapping pages of memory to reduce contention or improve access efficiency recoloring pages of memory to reduce cache contention scheduling non conflicting processes to run together with one another when possible and scheduling highly conflicting processes to run separately from one another when possible.

With respect to structure system is comprised of a memory access manager a memory re mapper a cache re colorer a shared cache manager a cache and a process scheduler . It is appreciated that in some embodiments of the present invention memory re mapper and cache re colorer are implemented as a single memory remapping element such as memory layout optimizer as cache recoloring is one type of memory remapping. However for purposes of clarity and simplicity of discussion memory re mapper and cache re colorer are shown and discussed as separate elements. As shown in system is comprised of two separate sub systems system and system that share some common components. System is a system for managing memory access. System utilizes cache metadata to optimize memory access and is described in detail in conjunction with . System is a system for managing shared cache access. System utilizes cache information to optimize utilization of a shared cache and is described in detail in conjunction with . Finally though process scheduler is included in some embodiments of system and system do not utilize process scheduler .

In one embodiment of the present invention cache is coupled to memory access manager shared cache manager and process scheduler . Memory access manger is additionally coupled to memory re mapper cache re colorer . Shared cache manager is additionally coupled to process scheduler memory re mapper and cache re colorer . It is appreciated that in some embodiments not illustrated memory access manager and shared cache manager are coupled to one another coupled to a centralized control or formed as single entity. However for purposes of clarity of explanation functions of cache manager and memory access manager are illustrated and described separately.

Memory access manager of the present invention directly extracts actual cache metadata from hardware of cache in one embodiment. Memory access manager utilizes the cache metadata to optimize memory access by controlling memory re mapper to remap memory and cache re colorer to recolor cache . The operation of memory access manager is described in detail in conjunction with and .

Shared cache manager of the present invention directly and indirectly ascertains information about how a shared cache is used in one embodiment. For example in one embodiment Memory access manager utilizes software probing techniques to indirectly determine how cache is being used without use of cache metadata provided by cache hardware. Shared cache manager employs the ascertained cache information to optimize shared utilization of cache by controlling memory re mapper to remap memory and cache re colorer to recolor cache. The operation of shared cache manager is described in detail in conjunction with and .

Memory re mapper of the present invention remaps portions of memory accessed by cache . Techniques for memory remapping such as moving migrating or copying replicating portions of memory from remote locations to closer locations are generally known in the art. Typically moving comprises copying data from one storage area such as a memory page on one node to another location such as a page on another node. The virtual page that is being accessed by a process is then remapped so that it points to the physical page containing the moved data. The old physical page that previously held the moved data may then be deallocated or unmapped. Typically replicating comprises copying data from one storage area to a new location or storage area so that a process can access the data from a location that is more local to the process. Replicating is normally done for read only pages. The location that the memory is copied from is left intact and is not de allocated. Copy on write techniques are then used handle writes to replicated read only pages.

In embodiments of the present invention memory re mapper employs memory remapping techniques under direction of shared memory access manager shared cache manager or both to move or copy selected portions of memory. In embodiments of the present invention the selected portions of memory are typically those that memory access manager shared cache manager or both have identified as remote memory that is frequently accessed by cache remote memory that is heavily used in cache or memory that is stored in a location of a cache that is heavily contended for by multiple processes. Embodiments of the present invention optimize memory access times and system interconnect use by moving or copying the selected portions of memory to a local cache or to a local storage area such as system Random Access Memory RAM .

Cache re colorer of the present invention is used to accomplish recoloring of memory pages to control which portions or colors of cache into which they are mapped. In one embodiment to recolor a page in a system with a physically indexed cache cache re colorer selects an old page O from an over utilized cache subset and copies its data to a new free page N from a less utilized subset. Cache re colorer then changes the virtual address mapping so that the virtual addresses which formerly pointed to page O now point to page N. Such recoloring is useful to ease contention caused by several processes heavily using one region or particular color of a cache. These and other techniques for recoloring cache are generally known in the art and may be employed by cache recolorer . In embodiments of the present invention cache re colorer employs cache recoloring techniques under direction of shared memory access manager shared cache manager or both to dynamically relocate selected data within cache to reduce cache color contention within cache . In embodiments of the present invention the selected data are typically data or pages that memory access manager shared cache manager or both have identified as subject to heavy cache utilization such as thrashing or else are located in areas of a cache that are heavily contended for by multiple processes that share a cache .

Process scheduler of the present invention is a pre existing portion of an operating system running directly on physical hardware or running as a guest operating system in a virtual machine . Process scheduler schedules processing time to multiplex processes on a central processing unit CPU . When process scheduler is utilized as a portion of system or system shared cache manager feeds data such as control inputs into process scheduler to manage the scheduling of processes that access shared cache . In an embodiment that utilizes a coupling to process scheduler the present invention modifies the existing process scheduler in an operating system so that it can receive control inputs from shared cache manager . Shared cache manager uses this coupling to exploit information about cache to direct process scheduler to advantageously schedule multiplexing or simultaneous running of processes that use disjoint or mostly disjoint areas of cache and are therefore non conflicting. Likewise shared cache manager uses this coupling to advantageously schedule conflicting processes to run separately when cache information indicates that processes heavily contend for the same regions of cache . This coupling to process scheduler is especially useful in embodiments of the present invention utilized with processors that have multiple cores and or multiple simultaneous threads.

Cache of the present invention represents one or more virtual machine and or physical computer system caches coupled to components of system . In some embodiments cache is an internal processor cache such as a Level 1 or Level 2 cache in a single core processor or multi core processor. In some embodiments cache is external to a processor such as a Level 3 cache on a motherboard or node in a NUMA Non Uniform Memory Access system. In a NUMA embodiment cache might cache only remote data or both remote and local data. In some NUMA embodiments it is appreciated that a NUMA cache is a Level 4 cache since in these embodiments Level 3 cache already exists at the chip level. In some embodiments cache is connected to interconnect hardware such as for example a card in a Peripheral Component Interconnect PCI slot.

In embodiments of the present invention that use memory access manager cache represents one or more computer memory caches which allow software inspection of cache metadata. In some such embodiments cache supports a software query for the metadata of a single cache line. In other such embodiments cache supports a software query of multiple cache lines at once. In various embodiments of the present invention cache provides information such as one or more of the following pieces of cache metadata a cache tag which indicates the main memory address that is being cached at this cache index cache permissions protocol states of the cache such as dirty shared exclusive valid invalid and etc. information about which hardware thread or core brought a particular piece of data into the cache cache miss data such as from a hardware performance counter and in a set associative cache information regarding the set in which cache data resides and or any associated least recently used LRU or most recently used MRU state.

In one embodiment cache allows dumping of the cached data itself while in another embodiment it does not. In one embodiment cache optionally includes a feature to allow direct invalidation of one or more cache entries. In one embodiment comprising a system with multiple levels of cache each level of cache in the hierarchy can be individually queried. In one embodiment comprising a multi level cache the multi level cache supports an inclusion property such that all lines in a smaller lower level cache are included in the larger higher level caches.

A cache tag is a numerical identifier that can be translated into the virtual or physical address of a portion of memory a cache line a page or other unit . The translation from tag to address may require lookup in a hardware managed or software managed table. A cache tag may include an address space identifier ASID to associate the virtual address with the process that loaded the memory. In a NUMA system a cache tag not only allows the unique determination of memory address es being cached in a cache line associated with a cache tag but also allows unique determination of the NUMA node which that memory is associated with i.e. the NUMA node for which this is local memory . Similarly given a memory address and static details about the cache hardware information about cache color can be computed. Another way of saying this is that cache color and NUMA node information are typically specified by a subset of the bits in the memory address.

In embodiments of the present invention that use shared cache manager cache represents one or more shared caches such as a node cache a single core processor cache a multi core processor cache a cache in a simultaneous multi threading processor and or a cache shared simultaneously or sequentially by a plurality of traditional or virtual machine processes. In some embodiments that use shared cache manager cache does not allow software inspection of cache metadata. In other such embodiments utilizing shared cache manager cache allows some level of software inspection of cache metadata as described above.

In the present invention a process comprises any software manageable encapsulation of a runnable unit of code and its state such as a standard traditional process task or job within an operating system a thread or lightweight process within an operating system a virtual central processing unit CPU and or a virtual machine monitor VMM .

The following discussion sets forth in detail the operation of present technology for managing memory access and shared cache utilization. With reference to flow diagrams and each illustrate exemplary steps used by various embodiments of the present invention. Flow diagrams and include processes that in various embodiments are carried out by a physical or virtual processor under the control of computer readable and computer executable instructions. The computer readable and computer executable instructions reside for example in data storage features such as computer usable volatile memory computer usable non volatile memory and or data storage unit of . The computer readable and computer executable instructions are used to control or operate in conjunction with for example processor A and or processors A B and C of . Although specific steps are disclosed in flow diagrams and such steps are exemplary. That is embodiments are well suited to performing various other steps or variations of the steps recited in flow diagrams and . It is appreciated that the steps in flow diagrams and may be performed in an order different than presented and that not all of the steps in flow diagrams and may be performed.

In of in one embodiment cache inspector of the present invention inspects cache metadata associated with a set of cache locations. As part of this inspection cache inspector receives cache metadata from hardware of cache which is configured to provide metadata to software applications. In some embodiments hardware of cache automatically supplies or makes this cache metadata available to cache inspector . In other embodiments hardware of cache only supplies this cache metadata to cache inspector upon a request of a specific set of cache metadata that cache inspector supplies to the cache hardware of cache . Various types of cache metadata have been previously described and can comprise information such as a cache tag of a cache location a recency of use status of a cache location LRU or MRU for example a protocol state of a cache location or other cache metadata information. Metadata received by cache inspector is passed to cache analyzer . In one embodiment of the present invention cache is a cache on a commodity processor that has hardware configured to provide elements of cache information to software. In another embodiment of the present invention cache is a node cache in a NUMA node that has hardware configured to provide elements of cache information to software. Such an embodiment is illustrated in which is described below.

In of in one embodiment cache analyzer of the present invention analyzes the cache metadata to determine memory utilization supported by cache . In one embodiment this analysis comprises periodically inspecting refreshed metadata associated with a set of cache locations to determine changes in the set of memory locations stored in the cache. For instance such periodic inspection can be accomplished every 10 milliseconds every 50 milliseconds or at some other periodic interval. One such embodiment employs statistical techniques to analyze changes and trends in remote memory utilization that occur in the inspected cache metadata over time. Results of analyses performed by cache analyzer are passed to cache optimizer .

In one embodiment cache analyzer of the present invention determines an amount of space utilized in cache by one or more particular sets of remote memory locations that are stored in cache . Cache analyzer performs this determination through comparisons statistical techniques estimations and or approximations. For example in one embodiment cache analyzer compares space used by a particular set of remote memory locations to the space used by other remote memory locations and or to predetermined standards. Based on such comparisons cache analyzer determines whether it is suitably advantageous to locally store the information instead of referencing remotely stored information.

In one embodiment cache analyzer of the present invention determines a comparative frequency of use of remote memory locations stored in cache . In such an embodiment cache analyzer tracks the frequency that particular remote memory locations are stored in inspected cache locations. In one embodiment cache analyzer of the present invention tracks how frequently particular data are used by examining LRU metadata from cache . Cache analyzer employs memory use algorithms and or compares this frequency information against predetermined guidelines or against the frequency of other storage of other remote memory locations. Based on results of such algorithms and or comparisons cache analyzer determines if a particular remote memory location or piece of data is accessed frequently enough that it is suitably advantageous to store the information locally instead of repeatedly accessing it remotely.

In one embodiment cache analyzer of the present invention determines a cache utilization rate for a particular set of cache locations. Cache analyzer monitors cache information that is stored in and evicted from the monitored location over time. Cache analyzer then determines if it is suitably advantageous to remap heavily used data to a different cache color to improve the utilization of cache . Cache analyzer makes this determination based on the previously described monitoring and based on a comparison to a predetermined utilization rate standard. As part of cache utilization analysis and other cache analysis some embodiments of cache analyzer are capable of sending a software command to cache which will forcibly evict data from a cache location. Similarly some embodiments of cache analyzer are also capable of referencing a known memory address to insert the contents of the memory address into a particular location of cache . These capabilities are useful for setting a cache location to a known state so that changes in the cache location can be easily monitored and analyzed. For instance utilization rates and recency of use information can be determined by calculating how long it takes for a known memory address to be changed or evicted from a cache location by processes that use cache .

In of cache optimizer of the present invention optimizes memory access based on results of the cache analyses performed by cache analyzer in one embodiment. Cache optimizer sends control inputs to memory re mapper cache re colorer or both to carry out the optimization of memory access. In one embodiment of the present invention cache optimizer directs memory re mapper to remap remote information to local storage such as local RAM to improve locality of data storage. This local storage is useful for instance for information determined by cache analyzer to have a high degree of occupancy in cache or a high frequency of access in cache . In one embodiment cache optimizer directs cache re colorer to dynamically relocate data within cache . This is relocation is useful for instance to recolor cache data when cache analyzer determines that the data is subject to an overly high utilization rate. Such relocation reduces the cache miss rate and improves overall utilization of cache . In one embodiment cache optimizer improves locality of memory access by preferentially scheduling an observed process on a node that has been determined to contain data frequently accessed by said process. In such an embodiment cache optimizer communicates with the operating system or process scheduler through a coupling not shown to these systems for carrying out this preferential scheduling. In such an embodiment instead of moving memory to be closer to a process cache optimizer instead preferentially moves the process closer to the memory. Such preferential scheduling is commonly known in the art as an alternative or complement to memory remapping.

In one embodiment cache optimizer reports information to a user or a higher level program to facilitate optimization tasks directed by the user and or by higher level programs to tune the performance of a computer system. Reported information comprises information such as results of analysis from cache analyzer cache utilization information and a cache heat map that includes utilization miss statistics for each cache line and or color. In one embodiment this information reporting comprises an application programming interface API through which higher level programs can obtain this information. In another embodiment the information reporting comprises a facility for recording this information in a user readable log file.

In operation of system processor of Node A is able to access memory stored at Node B . This is useful however because of the relatively small size of nodal cache memory access can be time consuming if a large amount of memory at Node B frequently needs to be accessed remotely. This is because the small size of local node cache can be inadequate to store the large amount of frequently accessed data. Through the processes of cache inspection as described at of cache inspector inspects metadata such as cache tags of cache lines of local node cache . Through the processes of cache analysis as described at of cache analyzer analyzes the metadata such as cache tags to determine which remote memory addresses are being frequently accessed by processor . The results of this analysis are used by cache optimizer to initiate the processes of optimizing memory access as was described at of . Accordingly cache optimizer directs memory re mapper to move the identified data to local storage such as RAM so that processor can access the data locally rather than remotely. In this fashion system and process operate to reduce bandwidth use on NUMA interconnect eliminate the increased latency associated with accessing the identified data remotely and decrease demands upon local NUMA node cache so that it operates more efficiently. Moreover the above illustrated example of the optimization of memory access of system is similar for embodiments using other cache metadata for analysis. The illustrated example is also similar for embodiments inspecting other types of caches such as the cache of processor or other previously described caches and for embodiments using other analysis and optimization techniques that have been previously described.

For the purposes of this discussion the term multi threaded processor is used to refer to either a multi core central processing unit CPU with some level of shared cache or a Simultaneous Multi Threading SMT CPU with some level of shared cache. The sub units e.g. cores or hardware threads are referred to as logical processors. A cache subset should be understood to mean any interesting manageable sub unit of the cache which may be a page color in the traditional sense the set of pages whose lines all map to the same region of the cache by virtue of the fact that their low order page number bits match the high order bits of the cache indices in this subset . Processes comprise any software manageable encapsulation of a runnable unit of code and its state such as a standard traditional process task or job within an operating system a thread or lightweight process within an operating system a virtual central processing unit CPU and or a virtual machine monitor VMM .

One embodiment of the present invention detects each process utilization of each cache color using software probes. On a multithreaded processor a probe is a piece of code that runs on one logical processor while the other logical processor s are executing one or more processes that are being observed. To obtain a single sample the probe selects a single cache subset S and repeatedly access one or more memory addresses that map to subset S of the cache. Each time a memory address is accessed it will either be in the cache already a hit or it will need to be fetched from memory a miss . Elements of the present invention determine misses either by measuring the amount of time necessary to fetch data from the memory address since misses are much slower than hits or by reading a hardware performance counter of the processor or cache that increments based on occurrence of a relevant event in cache such as for example a cache hit or miss or in a memory system associated with said cache such as for example a message sent out to the memory bus . Either approach can be used in batch form i.e. one timing determination or one counter read for several cache accesses by the probe .

After the present invention causes the probe data to be brought into the cache for the first time there should be no misses unless another logical processor has accessed the same portion of the cache and caused some of the probe data to be evicted this is true when the probe data are used on a single processor so there are no cache invalidations due to multiprocessor coherency issues . Elements of the present invention observe the number of misses over a given interval to estimate the examined processes utilization of cache subset S. In some embodiments the probe varies the number of cache lines accessed per cache index in order to measure the associativity required to hold all the lines of the examined process working set. For instance if the cache is eight way associative it may be the case that the process will only access two lines within a particular cache index. In such a case if the probe accesses seven lines within that cache index it will observe misses. However if the probe accesses only six lines within that cache index it will not observe misses. The information gathered by this technique of the present invention is extremely useful in determining which processes can fit together with minimal conflicts. In one embodiment using this technique the present invention also obtains pre index contention statistics in addition to per subset contention statistics.

In one embodiment the present invention uses this probe technique on a non multithreading processor. For example the present invention interfaces with the operating system to interrupt a process for example with a non maskable interrupt and accesses the probe data as previously described. Alternatively rather than interrupting a process the present invention interfaces with the operating system to access the probe data when the process is descheduled or interrupted by the operating system for other reasons. Elements of the present invention determine the number of cache evictions since the last sample by measuring the number of cache misses observed as previously described .

The present invention uses additional techniques as required to determine which of several processes is causing conflict misses when probing a multithreaded processor with more than two logical processors. For example in one embodiment the present invention suspends the execution of several hardware threads during the sampling interval to simplify analysis. In one embodiment instead of reading the performance counter on the probe s logical processor the present invention reads performance counters on all the other logical processors while at the same time aggressively accessing data within a given subset S of the cache. This interference type probing will greatly increase the number of misses observed on a logical processor that heavily utilizes the subset S but has a lesser effect on logical processors that are not utilizing subset S.

All of these probing techniques of the present invention produce the same result a per process description of cache subset utilization. Elements of the present invention then utilize this information to intelligently reduce the occurrence of conflict misses such as by controlling memory mapping to preferentially allocate future pages from cache subsets that are known to be under utilized and or by advantageously controlling a process scheduler to ensure that non conflicting access patterns run together either simultaneously on a multithreaded type processor or sequentially on any processor type .

In one embodiment the present invention also recolors pages in response to contention that is discovered as a result of software probing. To recolor a page in a system with a physically indexed cache the system selects an old page O from an over utilized cache subset and copies its data to a new free page N from a less utilized subset. The system then changes the virtual address mapping so that the virtual addresses which formerly pointed to page O now point to page N.

In one embodiment the present invention also remaps some pages that are discovered to be heavily used as a result of software probing or cache metadata inspection. At a high level there are two steps involved. The first step is obtaining cache utilization statistics via metadata inspection or cache probing. Techniques for performing this first step have been previously described and they will typically identify a set of physical addresses pages colors that are overused. The second step is locating the virtual pages associated with processes that map to these overused physical addresses pages colors. This second step typically produces a set of virtual addresses pages that map to the physical addresses pages found in the first step. The present invention finds these heavily used pages through use of novel virtual memory techniques and standard virtual memory techniques that are known in the art. The present invention uses these techniques to identify which virtual page corresponds to a physical page that was identified as being in an over utilized cache subset. For example in one embodiment in a system using binary translation BT such as certain types of virtual machines elements of the present invention use a novel technique to observe addresses being accessed and use this information to identify the page location of the addresses being accessed.

In one embodiment such as for example a system without BT elements of the present invention use a novel technique to interrupt a running process and scan its registers and relevant memory locations such as the stack for addresses that are currently in use. In one embodiment the present invention uses a well known technique to examine the accessed bits on the page table entries for pages of an identified over utilized cache subset. In one embodiment the present invention also uses a well known technique to remove page table mappings for pages of an identified over utilized cache subset thus taking a page fault that will identify a page in use. In one embodiment on a processor with a software visible or software managed TLB the present invention uses a well known technique to examine the TLB entries to determine which pages are currently in use.

In other embodiments where cache metadata inspection for example inspection of cache tags is not possible elements of the present invention exploit a system s cache coherency hardware to identify hot pages. For example a remote processor in the same cache coherency domain as the one executing the process under observation can load data from many different pages into its cache. If the process under observation writes to one of these cache lines it will cause an invalidation and the remote processor will observe a cache miss when it attempts to re access the same cache line. This detection of cache misses can utilize the techniques described previously such as timing the cache access or examining a hardware performance counter. Embodiments of the present invention combine these techniques as required with the previously described virtual memory techniques to obtain greater probing accuracy. For example in one embodiment after finding several candidate addresses in the stack of a process the present invention clears the accessed bits on the corresponding page and later re examines them for changes.

In of in one embodiment cache prober of the present invention probes a set of locations in cache . The probing is performed while one or more observed processes are running descheduled or interrupted. For example in one embodiment cache prober accesses the set of locations at approximately the same time that the observed process is accessing cache . On a multi threaded processor cache prober can perform the probing simultaneously with the running of an observed process. On a non multi threaded processor cache prober accomplishes the probing by directing the operating system to interrupt the observed process. As an example cache prober can direct the operating system to issue a non maskable interrupt to interrupt the observed process so that the probing can take place. On a multi threaded computer chip with more than two processors several hardware threads can be suspended by cache prober during the probing to simplify data collection and analysis.

In embodiments of the present invention an observed process comprises any software manageable encapsulation of a runnable unit of code and its state such as a standard traditional process task or job within an operating system a thread or lightweight process within an operating system a virtual central processing unit CPU and or a virtual machine monitor VMM . In embodiments observing multiple processes a combination of the identified processes may be observed. For example in one embodiment an observed process is a virtual processor associated with a virtual machine.

In one embodiment cache prober of the present invention runs a piece of code to repeatedly access one or more memory addresses that map to a subset of cache to perform probing. This causes cache to repeatedly attempt to supply information from the accessed memory addresses. In one embodiment the code can be varied to access memory addresses that map to various subsets of cache . Information obtained by cache prober as part of probing is passed to cache utilization determiner . By observing the number of cache misses due to access by the observed process or access by cache prober cache prober identifies how heavily the probed portion of cache is utilized. In one embodiment cache prober utilizes multiple versions of the piece of code probing code such that the probing code itself and associated local data such as stack variables always reside in a different cache line color that the one being probed. This allows cache prober to avoid having the probing code itself interfere with the cache portion being probed.

In one embodiment cache prober uses such repeated access to measure the elapsed time required to fetch the requested data from the accessed memory addresses via cache . Cache prober infers hits and misses by tracking and comparing the elapsed fetch times from repeated accesses. In one embodiment cache prober measures elapsed fetch times for a pre determined period and statistically analyzes this measured data to determine which fetches are cache hits or misses. For example in one embodiment cache prober infers cache hits from elapsed access times that statically cluster around a shorter mean elapsed time as measured over a period of time. Likewise in one embodiment cache prober infers cache misses from elapsed times that exceed the shorter mean elapsed time by some predetermined amount.

In one embodiment cache prober simply associates cache misses with fetches that exceed a pre specified amount of time to process since the latency difference between a cache hit and a cache miss is typically one or two orders of magnitude. For example a Level 2 cache hit may take about ten cycles alternatively 5 nanoseconds on a 2 GHz processor while a Level 2 cache miss that must go all the way to main memory may take about 200 cycles approximately 100 nanoseconds on a 2 GHz processor . Thus in one embodiment cache prober infers that any fetch latency that exceeds a pre determined latency such as for example 50 processor cycles worth of time is associated with a cache miss.

In the present invention such statistical and comparative inferences are possible because of the latency that is experienced when cache is required to retrieve data following a cache miss due to the data not being present in the cache. Note that after probe data has been brought into cache for the first time there should be no cache misses unless another process or processor has accessed the same portion of cache and caused some of the probe data to be evicted.

Cache prober of the present invention also varies the number of cache lines accessed per cache index in order to measure the associativity required to hold all the lines of the working set of the observed process in one embodiment. For instance if cache is eight way associative it may be the case that the observed process will only access two lines within a particular cache index. In such a case cache prober will observe misses by accessing seven lines within the cache index of cache . However if cache prober only accessed six lines within that cache index no cache misses would be observed.

In one embodiment of the present invention cache prober reads cache related data such as a count from a hardware performance counter of the processor or of cache that increments based on the occurrence of relevant events in cache such as for example cache hits and or misses or in memory systems associated with cache such as for example a message sent out to a memory bus . Such hardware performance counters are available even on commodity processors including Intel x86 type processors. In one such embodiment cache prober reads the cache data following each repeated access of memory addresses that map to a subset of cache .

In of in one embodiment cache utilization determiner determines which portions of cache are utilized by the observed process that was running during the operation of cache prober . In one embodiment cache prober marks the cache locations associated with the memory addresses accessed by cache prober as being used by the observed process if cache misses are measured in those cache locations. In one embodiment cache prober performs the probing on multiple observed processes to check for cache misses at multiple locations or throughout the entire cache . This allows cache utilization determiner to determine which areas of cache are used by each observed process. Likewise cache utilization determiner determines which processes access conflicting areas of cache . In some embodiments cache utilization determiner calculates a cache utilization rate such as a thrashing rate eviction rate or miss rate for a contended for subset or portion of cache . The cache utilization rate is calculated in one embodiment from cache miss data obtained when cache prober observes multiple processes either individually or simultaneously.

In one embodiment of the present invention cache utilization determiner uses data from repeated samplings of shared cache by cache prober to estimate a time until a probable cache miss. In various embodiments cache location determiner performs this estimate for a single cache color for multiple cache colors or for all cache colors of shared cache . Likewise in various embodiments cache utilization determiner performs this estimate for a single cache location such as a cache line multiple cache locations or all cache locations in shared cache . For example in one embodiment cache utilization determiner estimates the time in microseconds until a fifty percent probability of a cache miss will occur for each color of shared cache . In one embodiment such estimations are passed to cache optimizer .

In some embodiments of the present invention cache utilization determiner also uses standard and novel virtual memory techniques cache metadata inspection techniques when available and other previously described techniques to determine virtual addresses of data and pages that are heavily utilized and thus are good candidates for remapping. In one embodiment for example cache utilization determiner examines the accessed bits on the page table entries for pages of subset of cache that it has determined to be heavily used based on analysis of data collected by cache prober . In one embodiment cache utilization determiner removes the page table mappings for pages that belong to a cache subset that it has marked as over utilized. This causes a page fault that allows cache utilization determiner to identify a page in use. In one embodiment on a processor with a software visible or software managed TLB cache utilization determiner examines TLB entries to determine which pages are currently in use.

In one embodiment in a system using binary translation BT such as many virtual machines cache utilization determiner interfaces with the BT mechanism to observe addresses being accessed and then uses these addresses to identify pages being accessed. In one embodiment without BT cache utilization determiner interrupts a running process and scans its registers and relevant memory locations such as the stack for addresses that are currently in use. Cache utilization determiner then interfaces with the system s cache coherency hardware to identify hot pages that are heavily accessed in a manner that has been previously described. In one embodiment these techniques are also used by cache analyzer of system to identify addresses of data or pages that are candidates for remapping.

In of in one embodiment cache optimizer optimizes utilization of cache based on results of cache utilization determinations performed by cache utilization determiner . In various embodiments cache optimizer improves scheduling of processes mapping of memory coloring of cache or some combination to optimize utilization of cache .

In one embodiment cache optimizer compares cache utilization results such as which portion of cache is accessed for a plurality of observed processes. Cache optimizer determines which of the plurality of observed processes have completely or substantially non conflicting cache access patterns based on this comparison of cache utilization results. For example processes that are determined to have a conflict level below a predetermined threshold are labeled considered to be non conflicting. In one embodiment cache optimizer then directs process scheduler previously described to advantageously schedule combinations of non conflicting processes to run together. This technique is particularly useful on a multi threaded processor or multi core processor with a shared cache where scheduler can be directed to preferentially execute applications on the same shared cache package at the same time if they are known to use different portions of subsets of cache . However on any processor scheduler can be directed to context switch back to back between two applications that do not overtax the same portions of cache .

In one embodiment cache optimizer also determines which of the plurality of observed processes have conflicting cache access patterns based on this comparison of utilization rates. While many processes will have some conflicting cache access patterns cache optimizer determines which processes exceed a pre defined level of conflict that will warrant remapping memory recoloring cache or rescheduling processes in order to gain substantial efficiency improvements in the operation of cache . Conflicting processes that exceed this pre defined level of conflict are marked as having high contention. In one embodiment cache optimizer then directs process scheduler to run these high contention processes independent of one another to optimize cache access times and reduce contention.

In another embodiment cache optimizer directs memory re mapper previously described to remap data accessed by one or more of the high contention processes such that the contention is reduced or eliminated when the high contention processes run simultaneously. In yet another embodiment cache optimizer directs cache re colorer previously described to recolor contended for pages of cache to reduce or eliminate the contention experienced when the high contention processes run simultaneously. For example on a processor with a physically indexed cache cache re colorer can be directed to remap a physical page from an underutilized portion or subset of cache to take the place of a previously mapped physical page that maps to an overtaxed portion or subset of cache .

In one embodiment cache optimizer receives information from cache utilization determiner about the frequency with which an observed process displaces cache information from a location or set of location in cache . This information can be inferred from a cache miss rate that cache prober measures for a location or set of locations in cache while an observed process is running descheduled or interrupted. If the cache miss rate exceeds a pre established threshold such data is deemed to be very frequently accessed. In one embodiment cache optimizer utilizes this information to direct memory re mapper to re map these very frequently accessed portions of cache to local storage if it is determined that cache is obtaining the data from a remote location.

In one embodiment cache optimizer receives address locations from cache utilization determiner for data and or pages that are accessed by a subset of cache that has been identified by the present invention as heavily accessed. Cache optimizer employs this information to direct memory re mapper to remap the data and or pages such that the next time the data and or pages are accessed they will map to a different subset of cache .

In one embodiment cache optimizer responds to the detection of a highly contended for cache color by directing a memory allocator associated with the operating system to give out fewer pages of cache from the highly contended for color and more from the underutilized colors.

With reference now to portions of the present invention are composed of computer readable and computer executable instructions that reside for example in computer usable media of a computer system. That is illustrates one example of a type of a computer that can be used to implement embodiments of the present invention which are discussed below. illustrates an exemplary computer system that can be implemented from physical components virtual components or some combination thereof. It is appreciated that system of is exemplary only and that the present invention can operate on or within a number of different computer systems including general purpose networked computer systems NUMA systems embedded computer systems routers switches server devices client devices various intermediate devices nodes stand alone computer systems physical computer systems virtual computer systems and the like.

Computer system of is well adapted to having peripheral computer readable media such as for example a floppy disk a compact disc a virtual disk and the like coupled thereto. System of includes an address data bus for communicating information and a processor A coupled to bus for processing information and instructions. As depicted in system is also well suited to a multi processor environment in which a plurality of processors A B and C are present. Conversely system is also well suited to having a single processor such as for example processor A. Processors A B and C may be any of various types of microprocessors may have a single core or multiple cores and may be capable of simultaneous multi threading operation.

System also includes data storage features such as a computer usable volatile memory e.g. random access memory RAM coupled to bus for storing and caching information and instructions for processors A B and C. System also includes computer usable non volatile memory e.g. read only memory ROM coupled to bus for storing static information and instructions for processors A B and C. Also present in system is a data storage unit e.g. a magnetic optical disk or virtual disk drive coupled to bus for storing information and instructions. System also includes an optional alphanumeric input device including alphanumeric and function keys coupled to bus for communicating information and command selections to processor A or processors A B and C. System also includes an optional cursor control device coupled to bus for communicating user input information and command selections to processor A or processors A B and C. System of the present embodiment also includes an optional display device coupled to bus for displaying information.

Referring still to optional display device may be a liquid crystal device cathode ray tube plasma display device or other display device suitable for creating graphic images and alphanumeric characters recognizable to a user. Optional cursor control device allows the computer user to dynamically signal the movement of a visible symbol cursor on a display screen of display device . Many implementations of cursor control device are known in the art including a trackball mouse touch pad joystick or special keys on alpha numeric input device capable of signaling movement of a given direction or manner of displacement. Alternatively it will be appreciated that a cursor can be directed and or activated via input from alpha numeric input device using special keys and key sequence commands. System is also well suited to having a cursor directed by other means such as for example voice commands. System also includes an I O device for coupling system with external entities. For example in one embodiment I O device is a modem for enabling wired or wireless communications between system and an external network such as but not limited to the Internet. A more detailed discussion of the present technology for identifying design issues in electronic forms is found below.

In various other components are depicted for system . Specifically when present an operating system applications modules and data are shown as typically residing in one or some combination of computer usable volatile memory e.g. random access memory RAM and data storage unit . In one embodiment the present technology for identifying design issues in electronic forms for example is stored as an application or module in memory locations within RAM and memory areas within data storage unit .

With the above embodiments in mind it should be understood that one or more embodiments of the invention may employ various computer implemented operations involving data stored in computer systems. These operations are those requiring physical manipulation of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. Further the manipulations performed are often referred to in terms such as producing identifying determining or comparing.

Any of the operations described herein that form part of one or more embodiments of the invention are useful machine operations. One or more embodiments of the invention also relates to a device or an apparatus for performing these operations. The apparatus may be specially constructed for the required purposes such as the carrier network discussed above or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular various general purpose machines may be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations.

The programming modules and software subsystems described herein can be implemented using programming languages such as Flash JAVA C C C Visual Basic JavaScript PHP XML HTML etc. or a combination of programming languages. Commonly available protocols such as SOAP HTTP may be used in implementing interfaces between programming modules. As would be known to those skilled in the art the components and functionality described above and elsewhere herein may be implemented on any desktop operating system such as different versions of Microsoft Windows Apple Mac Unix X Windows Linux etc. executing in a virtualized or non virtualized environment using any programming language suitable for desktop software development.

The programming modules and ancillary software components including configuration file or files along with setup files required for providing the method and apparatus for troubleshooting subscribers on a telecommunications network and related functionality as described herein may be stored on a computer readable medium. Any computer medium such as a flash drive a CD ROM disk an optical disk a floppy disk a hard drive a shared drive and storage suitable for providing downloads from connected computers could be used for storing the programming modules and ancillary software components. It would be known to a person skilled in the art that any storage medium could be used for storing these software components so long as the storage medium can be read by a computer system.

One or more embodiments of the invention may be practiced with other computer system configurations including hand held devices microprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers and the like. The invention may also be practiced in distributing computing environments where tasks are performed by remote processing devices that are linked through a network.

One or more embodiments of the invention can also be embodied as computer readable code on a computer readable medium. The computer readable medium is any data storage device that can store data which can thereafter be read by a computer system. Examples of the computer readable medium include hard drives network attached storage NAS read only memory random access memory CD ROMs CD Rs CD RWs DVDs Flash magnetic tapes and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network coupled computer systems so that the computer readable code is stored and executed in a distributed fashion.

While one or more embodiments of the present invention have been described it will be appreciated that those skilled in the art upon reading the specification and studying the drawings will realize various alterations additions permutations and equivalents thereof. It is therefore intended that embodiments of the present invention include all such alterations additions permutations and equivalents as fall within the true spirit and scope of the invention as defined in the following claims. Thus the scope of the invention should be defined by the claims including the full scope of equivalents thereof.

