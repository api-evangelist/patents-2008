---

title: System and method for virtualization using an open bus hypervisor
abstract: A computer system includes an Open Bus Hypervisor having the highest privilege level. An Open Bus Hypervisor is a set of modules that operate on the root level. The Open Bus Hypervisor provides support for processing, filtering and redirecting of low level events. The Open Bus Hypervisor is used primarily for maintenance and support of computer virtualization features, which are implemented within computer system CPU. Additionally, the Open Bus Hypervisor can be used for supporting new hardware and software modules installed on a computer system. A Virtual Machine Monitor (VMM) runs with fewer privileges than the Open Bus Hypervisor. A Primary Virtual Machine (PVM) runs without system level privileges and has a Primary Operating System (POS) running within it.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08091086&OS=08091086&RS=08091086
owner: Parallels Holdings, Ltd.
number: 08091086
owner_city: 
owner_country: BM
publication_date: 20080718
---
The present application claims priority to the U.S. Provisional Patent Application No. 60 951 147 filed Jul. 20 2007 entitled SYSTEM AND METHOD FOR VIRTUALIZATION USING AN OPEN BUS HYPERVISOR which is incorporated herein by reference in its entirety.

The present invention relates to extending functionality of a Primary Operating System POS in a virtualized system and more particularly to a method system and computer program product for implementation of an Open Bus Hypervisor.

The industry trend of virtualization and isolation of computer system resources presents two major challenges virtualization at software level and virtualization at hardware level. A Virtual Machine VM is a type of an isolated Virtual Environment where multiple VMs can run on the same physical machine simultaneously. Each VM instance has a set of its own software components and uses hardware modules of the physical machine where the VM resides.

Often there are multiple VMs created on a host operating system. In such system some resources of the host operating system are isolated and allocated for running each of the VMs. An example of this type of system is a computing environment provided by VMware . The VMware solution provides standardized isolated secured computing environments. This product is typically used as an enterprise level solution where a number of VMware Virtual Machines are distributed throughout the computer system. However the VMware solution does not provide adequate support for hardware virtualization.

Virtualization allows running a number of VMs on the same physical machine. Conventional systems however provide only limited support for a low level i.e. hardware virtualization.

With Virtual Machine VM technology a user can create and run multiple virtual environments on a server at the same time. Each virtual environment such as a VM requires its own operating system OS and can run applications independently. The VM software provides a layer between the computing storage and networking hardware and the software that runs on it.

A Virtual Machine technology requires a mechanism for emulating or otherwise virtualizing the behavior not only of the software but also of the hardware of a real physical machine or a processor. Generally a VM is an environment that is launched on a particular processor that is running an operating system. Normally the operating system installed on such a machine or processor has certain privileges that are not available to user applications. For example many I O commands can be privileged and executable only in the operating system or in privileged mode. Certain areas of memory or certain addresses in memory also may require operating system privilege to be accessed.

A frequent situation that arises in this context is the problem of emulating or more broadly virtualizing a different operating system on the same processor. For example with one version of Microsoft Windows running on the Intel x86 processor for example in a server environment it may be necessary to emulate the behavior of another different version of Windows on the same Intel processor. This second operating system is generally referred to as Guest OS and the code that it executes is generally referred to as guest code. Note that in order for the emulation to be meaningful the Guest OS needs to execute privileged instructions as if it were actually running on the processor. In other words the Guest OS running as a Virtual Machine is itself unaware that it is a Virtual Machine.

Execution of such privileged instructions however is the province of the native operating system. Therefore any attempts by the Guest OS inside a VM to execute privileged instructions must be intercepted so that they can be properly executed or otherwise handled by the VM. The component that is responsible for this interception and emulation of privileged instructions is called a Virtual Machine Monitor or VMM. 

A typical VMM enables a single physical machine or processor to act as if it were several physical machines. A typical VMM under control of a high ranking operating system OS can run a number of different operating systems simultaneously such that each of these different operating systems is its own Virtual Machine.

In other words the VMM can handle one or a number of Virtual Machines each of which represents its own operating system and each of which can run its own application software. Usually the high ranking OS is referred to as a host OS HOS . The multiple operating systems that are running as Virtual Machines are usually referred to as guest operating systems Guest OS s running guest code. At the present time one of the conventional mechanisms for structuring VMMs is a hosted VMM.

In the case of the hosted VMM the VMM itself is not a full fledged operating system. Such a VMM does not include device drivers and cannot control hardware devices such as I O devices directly. Such a hosted VMM is installed into the host operating system HOS and uses HOS API application programming interface to work with the I O devices. Both the VMM and the HOS have system level privileges and exist on a physical computer concurrently. The VMM is responsible for preserving the context of the host operating system when switching from the HOS to the VMM and is responsible for restoring the context of the HOS when switching back to the HOS. The hosted VMM can create any number of Virtual Machines none of which have system level privileges and none of which can work with I O devices directly. The VMM emulates the I O devices for the VMs and uses the HOS to work with the real I O devices.

For each VM a separate process is created and the HOS is responsible for scheduling of both the VMs and other processes in the HOS. Conventional examples of hosted VMMs include VMware GSX Server VMware Workstation MS Virtual PC MS Virtual Server and SVISTA 2004.

Hosted VMMs however have a major problem when it comes to a low level i.e. hardware virtualization. It can be inefficient or even impossible for a hosted VMM to use hardware virtualization technologies in new families of processors such as for example Intel Virtual Machine Extension VMX technology.

Accordingly what is needed is a method and system for implementing an Open Bus Hypervisor that allows VMMs to efficiently utilize the hardware virtualization technologies.

The present invention is directed to a system method and computer program product for implementation of an Open Bus Hypervisor for hardware virtualization that substantially obviates one or more of the problems and disadvantages of the related art.

An Open Bus Hypervisor according to one proposed embodiment is a set of modules that operate in the root mode. The Open Bus Hypervisor provides support for processing filtering and redirecting of low level events. The proposed Open Bus Hypervisor is used primarily for maintenance and support of computer virtualization features which are implemented within computer system CPU. Additionally the Open Bus Hypervisor can be used for supporting new hardware and software modules installed on a computer system.

In one proposed embodiment a computer system that includes an Open Bus Hypervisor having the highest privilege level is provided. A Virtual Machine Monitor VMM runs within than the Open Bus Hypervisor. A Primary Virtual Machine PVM runs without system level privileges and a Primary Operating System POS running within the Open Bus Hypervisor.

The Open Bus Hypervisor can have hardware drivers used by other components for accessing hardware through the Open Bus Hypervisor. The Open Bus Hypervisor can also have other hardware drivers used by the other components for accessing other hardware through the Open Bus Hypervisor.

The system can include a number of VMMs controlling a number of VMs. The system can also have at least one Guest OS running within one of the VMs. The Guest OS can work with virtualized hardware and hardware access requests from the Guest OS or its VMM are translated by the Open Bus Hypervisor to real hardware using the POS.

The Open Bus Hypervisor supports hardware virtualization by exclusively using hardware virtualization solutions such as VT X from INTEL and AMD V from AMD and providing VT X related APIs to other modules of the system kernel. The proposed Open Bus Hypervisor provides for efficient exchange of low level events between Primary OS POS and VMMs. Note that the POS and the VMM work in their own contexts and the Open Bus Hypervisor provides for exchange of the events in different formats.

The Open Bus Hypervisor according to an exemplary embodiment is responsible for handling hardware virtualization features and sharing the hardware virtualization features between different VMMs. The VMMs can be of different types and can even be produced by different vendors.

Additional features and advantages of the invention will be set forth in the description that follows. Yet further features and advantages will be apparent to a person skilled in the art based on the description set forth herein or may be learned by practice of the invention. The advantages of the invention will be realized and attained by the structure particularly pointed out in the written description and claims hereof as well as the appended drawings.

It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are intended to provide further explanation of the invention as claimed.

Reference will now be made in detail to the embodiments of the present invention examples of which are illustrated in the accompanying drawings.

The present invention is therefore directed to extending functionality of a Primary OS POS by implementation of an Open Bus Hypervisor. The Open Bus Hypervisor according to the preferred embodiment extends the functionality of a POS that is not supported in the core of an operating system. An example of such an extensibility is hardware processor features virtualization support such as hardware virtualization technologies VT x and AMD V DMA Remapping technology VT d Trusted Execution Technology TXT etc.

The Open Bus Hypervisor according to the preferred embodiment is a set of modules that operate in the root mode of a processor with hardware virtualization support enabled. The Open Bus Hypervisor provides support for processing filtering and redirecting of low level events. The Open Bus Hypervisor is used primarily for maintenance and support of computer virtualization features that are implemented within computer system CPU. Additionally the Open Bus Hypervisor has extension means that can be used for supporting new hardware and software modules installed on a computer system.

The low level events can be generally of two types hardware related such as for example exceptions interrupts hardware implemented virtualization instructions such as VMENTER VMEXIT VMON VMOFF VMCS etc. and software related such as for example API calls sent from a VM to a Primary OS. An Open Bus Hypervisor processes these events and redirects them to appropriate modules for further processing.

In the preferred embodiment a computer system that includes an Open Bus Hypervisor having the highest privilege level is provided. In the Intel family of processors the various privilege regimes are referred to as privilege levels and are set through appropriate processor registers. In the Intel IA 32 architecture Ring 0 and Ring 1 privilege levels can be used to implement the Open Bus Hypervisor. It should be noted that although the particular examples given in this discussion relate to the Intel architecture the invention is not limited to the Intel architecture and is applicable to any number of processor families and processor architectures.

The Open Bus Hypervisor can have the highest system level privileges for example Ring 0 privileges for the Intel IA 32 architecture or root for the VT X architecture. A Virtual Machine Monitor VMM runs with fewer privileges than the Open Bus Hypervisor. Thus VMM is at least one privilege level lower for example at Ring 1 privilege level with root level enabled. According to the preferred embodiment a Primary Virtual Machine PVM runs without system level privileges and has a Primary operating system POS running within it. The POS can be the same operating system that is used on the computer system without implementation of an Open Bus Hypervisor. The POS can have hardware drivers used by other components for accessing hardware through the POS.

The Open Bus Hypervisor can have hardware drivers used by other components for accessing hardware through the Open Bus Hypervisor. Additionally the POS can have some hardware drivers used by other components for accessing hardware and the Open Bus Hypervisor can have other hardware drivers used by the other components for accessing other hardware through the Open Bus Hypervisor. Thus the POS can have no direct access to any real hardware devices.

The system can include a number of VMMs controlling a number of VMs and the system can have at least one Guest OS running within one of the VMs. The Guest OS can work with virtualized hardware and hardware access requests from the Guest OS or its VMM are translated by the Open Bus Hypervisor to real hardware using the POS.

The Open Bus Hypervisor supports hardware virtualization by using VT X solutions and providing VT X related APIs to other modules of the system kernel. The proposed Open Bus Hypervisor provides for efficient exchange of low level events between POS and VMMs. Note that the POS and the VMM work in their own contexts and the Open Bus Hypervisor provides for exchange of the events in different formats. Generally a context is a collection of related processes and data whose names are not known outside of the context. Contexts partition operating system s name space into smaller more manageable subsystems. They also hide names ensuring that processes contained in them do not unintentionally conflict with those in other contexts.

A process in one context cannot explicitly communicate with and does not know about processes inside other contexts. All interaction across context boundaries must be through a context process thus providing a degree of security. The context process according to the preferred embodiments is implemented within the Open Bus Hypervisor Service subsystem transport support discussed below in more detail.

The Open Bus Hypervisor according to the preferred embodiment is responsible for handling all hardware virtualization features and sharing the hardware virtualization features between different VMMs. The VMMs can be of different types and can even be produced by different vendors. Thus the proposed Open Bus Hypervisor can be used advantageously across different virtualization platforms.

The Open Bus Hypervisor is particularly useful for a new generation of Host OSs with support for hardware based virtualization technology such as Intel s VT X and or VT I technology. Typically during installation on a bare PC such a HOS can detect the presence of VT X and store this information in a registry. When restarted in a Primary VM without VT X support it could refuse to boot. Installation of the HOS under VMM in the Primary VM with already reduced privileges in the absence of VT X can solve this problem. For example the VMM can be installed on top of the HOS and then the HOS can be reinstalled inside the Primary VM.

However in many instances it is not practical to have a VMM that runs under the control of an already installed HOS. The HOS still runs in the Primary VM with reduced privileges and still uses it to access the I O devices. The proposed Open Bus Hypervisor is helpful for a new generation of Host OSs with support for hardware based virtualization technology such as Intel s VT X and or VT I technology.

Virtualization without an Open Bus Hypervisor would require that all the drivers be placed in VMM space. This approach is inefficient since there are hundreds or even thousands of devices that most operating systems need to support. For example there are numerous vendors of hard disk drives and numerous models and types of hard disk drives CD ROM drives DVD ROM drives floppy disks video cards network cards Wi Fi cards and modems mice track balls etc. Most of these drivers are usually written by vendors themselves. Therefore a large volume of untrusted third party code has to be placed within the VMM which runs on the most privileged level and can therefore place the system at risk. But if the VMM displaces the original operating system it needs to have all these drivers installed. In this case the VMM can have a very large logical footprint in the system.

The problem of the VMM having a large logical footprint i.e. having an amount of code that is comparable to the amount of code of a full fledged operating system such as for example LINUX or MS Windows is solved by an Open Bus Hypervisor. The Open Bus Hypervisor can have exclusive control over the physical resources of the system although it can grant certain rights to other components of the system such as to the Primary OS or in some cases to the Guest OS.

The Open Bus Hypervisor according to the preferred embodiment can restrict the ability of both the POS and the VMM to issue interrupt related instructions. The Open Bus Hypervisor processes all interrupts and exceptions in the system and dispatches them to the POS and VMMs based on virtualization policies. Calls related to system resources such as for example processing time and memory allocation are not handled by the Open Bus Hypervisor and are redirected to the POS.

The Open Bus Hypervisor has the following advantages as compared to regular or lightweight Hypervisors 

A common Open Bus Hypervisor architecture and its modular organization is illustrated in . It includes a Hypervisor nano core a Hypervisor POS service subsystem and POS service library . An Open Bus Hypervisor consist of three modules i.e. spaces connected over an open bus nano core . A Primary space includes a POS kernel with a POS service library . The Hypervisor POS service library provides API calls from the POS kernel upon instruction from the Hypervisor POS service subsystem located in Service space.

The API calls come from a VMM located in Hyp Module which also contains a Hyp Module support library . The Hyp Module support library is used in communication over the nano core with the Hypervisor POS service subsystem of Service space. An Open Bus Hypervisor according to the preferred embodiment can have multiple Hyp Modules connected to a nano core . The Open Bus Hypervisor not only provides the API to the POS kernel but determines if these API calls are consistent with the VMM privilege level. Thus the Open Bus Hypervisor implements a security mechanism.

 7 initializing the nano Core subsystem to be ready to provide communication between the Hypervisor Service subsystem and the Hypervisor POS library 

 10 opening primary communication port between the Hypervisor Service subsystem and the Hypervisor POS library.

The locator is used for communication between modules. It locates open ports sends message to these ports and receives return codes from these ports. Thus communication between modules is established. The scheduler is responsible for scheduling the polling cycles i.e. scheduling port querying processes performed by the locator and allocating time intervals for processing requests between Hypervisor modules. The space manager is responsible for creation and management of the Hypervisor modules such as Hyp modules Service space and Primary space. The thread manager is responsible for creating the threads in the POS for processing the requests between modules.

The Hypercall subsystem provides access to the Hypervisor services directly not via ports using special Hypercall switches. The functions that can be called using Hypercalls are mostly limited to sending and receiving port requests. The transport support means is used for the convenience of creating extensions of the Hypervisor.

Transport support layer provides means for making the API calls over the ports as opposed to just data exchange . Transport support layer provides a Hypervisor security mechanism which determines what API calls can be requested and executed by which Hyp Module in . The event tracking subsystem processes low level events such as for example exceptions interrupts VMExits API calls etc.

The Hypervisor POS service library is used for handling i.e. translating communications between the Hypervisor Service subsystem and the POS . It includes a Hypervisor Loader an HPC POS service server an Information service an HPC over OS calls server and a Private Public API wrapper . The Hypervisor Loader is used at an initial stage of loading the Hypervisor when Service space and nano Core are not yet created. If the Hypervisor Loader can not integrate the Hypervisor POS service library into the POS then the Hypervisor can be alternatively implemented as a driver working at the highest privilege level.

The hyp module is a set of instructions with a particular set of properties and settings in the address which is treated as a set of data. The hyp module is loaded in this form into a particular context. Subsequently the hyp module can be called from any other context. At initial launch the hyp module is unpacked using the hypervisor loader . In this discussion two hyp modules are discussed and which have two different functions that can be called. However the hyp modules and can also be executed in the same context. 0 1 refers to an execution thread.

Each module has to open a set of ports for interactions with other modules or with the Hypervisor. API calls are also implemented using port schema depicted in .

The transport layer allows to process remote function calls as if they are executed locally. This is the highest level of inter modular communications. The transport layer requires serialization. According to the exemplary embodiment the serialization is used for 

The primary communication between modules occurs through the ports by sending messages. A message is a data structure stored in memory which is transferred from a sender to a receiver through using a port identifier which are structures referring to the sender and receiver. Each module registers the ports which that module will monitor. The Hypervisor Service Subsystem has a locator that stores which port is associated with which module and is located in which address space. In the same context the message is sent to a particular port. The recipient receives the messages as follows 

The events are received from a sender module space and have the subscriber module space that orders these events. The tracking subsystem located in the Service space tracks the events sent from the sender module space to the subscribers in the subscriber module spaces. Each of the events and has corresponding subscribers and who can belong to a particular module space. The event IDs are registered in the event tracking subsystem with the unique ID values so the events of each subscriber can be tracked by their IDs. The event ID filtering functions can be used by the subscribers. The event tracking subsystem also includes means for hashing the event identifiers.

When a thread calls a function that is located outside the current context or outside a context that is common for a set of currently executing threads the thread calls a function to relay the parameters of the function in the hypervisor service subsystem which is accompanied by switching the context to the Hypervisor. In other for a thread to switch to a different context it is necessary when calling a function in a different context to send the parameters of the function to the hypervisor service subsystem. In other words simultaneous with calling the function the parameters are passed. The Hypervisor also switches context. Also the function call has a corresponding port call which is associated with the function and the context in which the function can be used. To simplify the search for a port port hashing is used as discussed earlier. The Hypervisor selects based on the port s ID module the context in which the function will be executed and switches control to the specified context.

As one example a port to which control is transferred can have a priority or a queue that defines when control can be transferred to that port. In another example the ports can subscribe to events i.e. to the function calls. At each function call an event handler forms a notification regarding the event and the event tracking subsystem defines the ports which subscribe to the events and notifies the appropriate ports regarding the event. Then based on the priority or by the queue turn the context of the port is invoked and the parameters of the call are processed. If there are no ports subscribing to the event the event is ignored. If there is a recipient then a port is looked for out of the set of ports for that recipient which can receive the event call. The events are then distributed to the ports optionally based on priority and queue turn or position. A single event can be associated with multiple ports. An event exists as long as at least one port is still processing the event.

Each module upon initialization informs the system regarding any polling cycles that it might have. A polling cycle is an analog of a thread in the execution context. The polling cycle is responsible for taking the message from the port waiting until the message event appears in the queue. Note that the scheduler uses not ports themselves but the polling cycles which are active at the moment. A lockless implementation of a hash table can be used as well as other structures.

The Service space of the Hypervisor is dedicated to VT X handling. The Hyp Module receives light weight HPC calls. Events such as VMExits can be received by the Hyp Module from Primary space as well as from VMMs i.e. other Hyp modules . Processing and redirection of interrupts is performed by the Support library . Since the Hyp Module and the Hypervisor Service subsystem are both located in Hypervisor Service Space they can be switched to root level operation while POS works on lower privilege level.

A significant advantage of the Open Bus Hypervisor of the preferred embodiment is that it frees a VM developer from having to develop his own set of drivers. By allowing the Primary OS to use its drivers to interact with the hardware the developer of the Open Bus Hypervisor does not face the task of either writing his own set of drivers for each device that might conceivably be connected to the computer a daunting task since such devices today number in the thousands or having to adapt someone else s driver model such as LINUX to working within the Hypervisor itself. This means that the development cost of such an Open Bus Hypervisor based virtualization approach is dramatically less than it otherwise would be.

Again it is worth recalling that many of the complaints about bugs and crashes and general instability of the Windows operating system derives not from mistakes by Microsoft programmers but from mistakes of the third party vendors who supply the driver software. As a practical matter when the Hypervisor developers are confronted with the task of dealing with third party drivers they may be just as likely to write a product that is similarly buggy. Therefore by using as a Primary OS an already existing operating system such as Windows XP Windows NT LINUX etc. the development time for the overall virtualization system can be shortened considerably.

Furthermore it should be noted that new types of hardware appear all the time such as new video cards new storage devices new network cards new wireless devices etc. For most common operating systems the vendors of these devices write the drivers practically as soon as the devices themselves appear on the market and frequently are supplied with the device itself . Thus the VM developer does not need to worry about supporting a never ending stream of new hardware from third party vendors since the vendors themselves will provide the driver support for standard operating systems.

Another advantage is that there is no need to write a separate Service OS whose primary task is device hardware interface and driver management. Writing even a limited purpose operating system is a complex task. In the approach described herein this task can be avoided since the POS serves in place of any such driver dedicated Service OS. The approach described herein permits using any standard operating system as the primary operating system where the Primary OS also functions as a Service OS. The Primary OS can be easily integrated into the Open Bus Hypervisor.

It also worth noting that up till now Hypervisor based systems have generally been found primarily if not exclusively in server based environments primarily due to the fact that installation and maintenance requires relatively skilled IT personnel notwithstanding the fact that first Hypervisor based systems were described two decades ago. The proposed Open Bus Hypervisor implementation is equally applicable to both server based and desktop laptop based environments.

It should also be noted that running the Primary OS with less than the full privileges permits avoiding at least some of the sources of failures and instability. For instance if the Primary OS itself decides to write something that it shouldn t to the hard disk drive for example the Primary OS itself replaces its own boot sector information with erroneous information there is little that the Open Bus Hypervisor can do about that. On the other hand some of the bugs are due to developers mistakes for example where some instruction tries to access an area in memory that it shouldn t or it tries to transfer control to some page that isn t available triggering a page fault. With the Primary OS having less than full privileges mistakes of the second type can be more easily handled when the Open Bus Hypervisor detects them.

The Open Bus Hypervisor takes full advantage of hardware support for virtualization if available on the processor. For example current Intel processors have VT VT X VT I technology that provides hardware based support for virtualization. Similarly AMD Pacifica has new guest mode and other processors have similar schemes. Where appropriate the Open Bus Hypervisor will reserve these highest privileged modes for itself and give lesser privileges for accessing devices to the Primary OS and possibly even lesser privileges than that to the Guest OSs.

In another embodiment a system without a VM present can be implemented. In this case the Hypervisor provides a means for extensibility of POS s functions. According to this embodiment an Open Bus Hypervisor is integrated with the POS and processes the OS features that are not supported by the POS. An Open Bus Hypervisor is extended by a feature provider module for handling additional POS functionality. This module initializes appropriate subsystems hardware means etc. It also generates specific low level events. Other Hyp modules subscribe to these events and filter or process the events for their specific purposes.

With reference to an exemplary computer system where the Open Bus Hypervisor can be implemented includes a general purpose computing device in the form of a user side computer and or server side computer or the like including a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures.

The system memory includes read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within the computer such as during start up is stored in ROM . The computer may further include a hard disk drive for reading from and writing to a hard disk not shown a magnetic disk drive for reading from or writing to a removable magnetic disk and an optical disk drive for reading from or writing to a removable optical disk such as a CD ROM DVD ROM or other optical media.

The hard disk drive magnetic disk drive and optical disk drive are connected to the system bus by a hard disk drive interface a magnetic disk drive interface and an optical drive interface respectively. The drives and their associated computer readable media provide non volatile storage of computer readable instructions data structures program modules and other data for the computer . Although the exemplary environment described herein employs a hard disk a removable magnetic disk and a removable optical disk it should be appreciated by those skilled in the art that other types of computer readable media that can store data that is accessible by a computer such as magnetic cassettes flash memory cards digital video disks Bernoulli cartridges random access memories RAMs read only memories ROMs and the like may also be used in the exemplary operating environment.

A number of program modules may be stored on the hard disk magnetic disk optical disk ROM or RAM including an operating system e.g. Windows 2000 . The computer includes a file system associated with or included within the operating system such as the Windows NT File System NTFS one or more application programs other program modules and program data . A user may enter commands and information into the computer through input devices such as a keyboard and pointing device . Other input devices not shown may include a microphone joystick game pad satellite dish scanner or the like.

These and other input devices are often connected to the processing unit through a serial port interface that is coupled to the system bus but may be connected by other interfaces such as a parallel port game port or universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video adapter . In addition to the monitor computers typically include other peripheral output devices not shown such as speakers and printers.

When used in a LAN networking environment the computer is connected to the local network through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the wide area network such as the Internet. The modem which may be internal or external is connected to the system bus via the serial port interface . In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

Having thus described a preferred embodiment of a system and method for implementation of an Open Bus Hypervisor it should be apparent to those skilled in the art that certain advantages of the described method and apparatus have been achieved. In particular it should be appreciated by those skilled in the art that system and method described in the preferred embodiment provides efficient support for hardware virtualization. It should also be appreciated that various modifications adaptations and alternative embodiments thereof may be made within the scope and spirit of the present invention. The invention is further defined by the following claims.

