---

title: System and method of multi-path data communications
abstract: In a particular embodiment, a multi-path bridge circuit includes a backplane input/output (I/O) interface to couple to a local backplane having at least one communication path to a processing node and includes at least one host interface adapted to couple to a corresponding at least one processor. The multi-path bridge circuit further includes logic adapted to identify two or more communication paths through the backplane interface to a destination memory, to divide a data block stored at a source memory into data block portions, and to transfer the data block portions in parallel from the source memory to the destination node via the identified two or more communication paths.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08190699&OS=08190699&RS=08190699
owner: Crossfield Technology LLC
number: 08190699
owner_city: Austin
owner_country: US
publication_date: 20080728
---
The U.S. Government has a paid up license in this invention and the right in limited circumstances to require the patent owner to license others on reasonable terms as provided for by the terms of government contract W9113M06C0163 awarded by the U.S. Army Space and Missile Defense Command of Huntsville Ala.

The present disclosure is generally related to systems and methods of multi path data communications.

In general data communications between multiple processors of an electronic device or between electronic devices occurs via a communications path which may be a wired or wireless communications path. Such data transfers can occur according to a data communications protocol such as an Ethernet protocol. To transfer data efficiently a number of techniques have been implemented that allow for direct data transfers. One example includes a Remote Direct Memory Access RDMA technique for transferring data directly from a memory of one computing device to a memory of another computing device with limited involvement of the operating systems of either of the computing devices. RDMA permits high throughput low latency networking which can be used in parallel computer clusters. In general an electronic device that supports RDMA may include an input DMA and an output DMA to receive data from and send data to other devices.

Typically when a transmitting device wants to send data to a destination device that has an input DMA the transmitting device sends a request to the input DMA of the destination device. The input DMA of the destination device can then send an Acknowledgement ACK to the transmitting device. When the transmitting device receives the ACK it transfers data to the input DMA of the destination device and the input DMA transfers the data into memory with limited involvement of the operating system of the destination device.

In general the RDMA Consortium defined a suite of protocols at the transport layer that enables cooperating DMA engines at each end of a communication path to move data between memory locations with minimal support from the kernel and with zero copy to intermediate buffers. The RDMA Consortium s specifications are now maintained by the Internet Engineering Task Force IETF . A Remote Direct Memory Access Protocol RDMAP Verbs specification describes the behavior of the protocol off load hardware and software defines the semantics of the RDMA services and specifies how the hardware and software appear to the host software including both the user and kernel Application Programming Interface API . The Verbs specification defines an RDMA READ WRITE operation and a SEND operation that transport data between user space memories or into a receive queue respectively. Further the Verbs specification defines Send and Receive Queues SQ and RQ and queue pairs to control data transport and Completion Queues CQs to signal when an operation is complete. Work Requests WRs are converted into Work Queue Elements WQEs which are processed in turn by the off load engine. An asynchronous event interrupt is generated when work is complete. Also data need not be in contiguous memory at either the source or destination as Scatter Gather Lists SGLs can define the physical memory locations of data segments.

In such RDMA transfers the CPUs caches and or context switches are not used allowing data transfers to continue in parallel with other system operations. When a processor performs a RDMA read or write request the application data is delivered directly to the network reducing latency and enabling fast message transfer. Thus RDMA permits high throughput low latency networking which is especially useful in massively parallel computer clusters. RDMA can reduce operating system overhead associated with networking which can squeeze out the capacity to move data across a network reducing performance limiting how fast an application can get the data it needs and restricting the size and scalability of a cluster.

Unfortunately conventional systems including complex simulation systems having multiple processors struggle to generate process and render realistic multi spectral and hyperspectral graphics in real time to perform complex modeling calculations to acquire real time data or any combination thereof. While RDMA can be used to leverage processing capabilities associated with multiple processors network data transfer throughput rates can offset processing gains. Hence there is a need for systems and methods to enhance data transfer throughput in multi processor systems.

In a particular embodiment a multi path bridge circuit includes a backplane input output I O interface to couple to a local backplane having at least one communication path to a processing node and includes at least one host interface adapted to couple to a corresponding at least one processor. The multi path bridge circuit further includes logic adapted to identify two or more communication paths through the backplane interface to a destination memory to divide a data block stored at a source memory into data block portions and to transfer the data block portions in parallel from the source memory to the destination node via the identified two or more communication paths.

In another particular embodiment a circuit device includes a local backplane including multiple communication paths and a plurality of multi path bridge circuits communicatively interconnected via the multiple communication paths through the local backplane. Each multi path bridge circuit of the plurality of multi path bridge circuits is adapted to utilize available communication paths through the local backplane to transfer data in parallel from a source to a destination.

In still another particular embodiment a method is disclosed that includes identifying available communication paths from a source to a destination via a local backplane and via a network fabric using a multi path bridge circuit associated with the source. The method further includes segmenting a data block stored at a source memory of the source into multiple data block portions corresponding to a number of identified available communication paths using the multi path bridge circuit and concurrently transferring the multiple data block portions from the source memory to a destination memory at the destination via the identified available paths.

In general a multi path bridge circuit is disclosed that includes logic to exploit multiple communication paths through a local backplane and optionally through a network fabric to transfer data from a source to a destination concurrently via the multiple communication paths achieving an order of magnitude increase in bandwidth in data transfer throughput. In a particular example a plurality of multi path bridge circuits can be included in a circuit device to exploit the multiple communication paths through the local backplane to transfer data at a data throughput rate that is related to an aggregate of the data rates associated with the available communication paths. In a particular example the multi path bridge circuits can be used to facilitate data transfers between a source node and a destination node of the same device. In another particular example the multi path bridge circuits can be used to transfer data between instrumentation systems and supercomputing clusters via a local backplane or through a network fabric to allow data processing in real time or near real time even among geographically distributed processing devices.

The multi path bridge circuit further includes a first host interface coupled to a first processor having a first memory . In a particular embodiment the multi path bridge circuit can be integrated with a processor such as the first processor . The multi path bridge circuit may include a second host interface coupled to a second processor having a second memory . In a particular embodiment the first and second host interfaces and may be 32 lane low voltage differential signal LVDS interfaces to communicate data. In general the first and second host interfaces and can include any high speed chip to chip interconnect technology including AMD s HyperTransport HT technology Intel s QuickPath Interconnect QPI technology Rambus FlexIO technology which is currently used in the IBM Sony and Toshiba Cell Broadband Engine other future interconnect technologies or any combination thereof. In a particular embodiment the first and second processors and can be field programmable gate array FPGA circuits.

The multi path bridge circuit includes bridge control logic that is adapted to control operation of the multi path bridge circuit . Further the multi path bridge circuit includes processor memory access logic that can be used to directly access a memory location associated with one of the first memory the second memory or other memories accessible via the media dependent I O interfaces or the backplane I O interfaces . Further the multi path bridge circuit includes I O routing logic that is adapted to determine which input output interfaces of the media dependent I O interfaces and the backplane I O interfaces have available routes for data transfer to a selected destination. In a particular example the destination can be an application or a process executing on a processor a destination memory a destination device a destination processing node or any combination thereof. Additionally the multi path bridge circuit includes data segmentation logic that can be used to divide a particular data block into multiple data block portions which can be transferred from the source to the destination via the identified I O interfaces. Further the multi path bridge circuit includes data cut through bypass or pass through logic that is adapted to allow data to pass from the backplane I O interface to the media dependent I O interface or vice versa or between different channels in the local backplane without interacting with one of the first or second processors or when a destination address associated with a received data block portion is related to a different processing node.

In a particular embodiment the multi path bridge circuit can be implemented as an integrated circuit to provide a multi path data communications to provide an order of magnitude increase in bandwidth or data throughput capacity between computing nodes. In a particular example the multi path bridge circuit provides a remote direct memory access RDMA functionality. In an example the multi path bridge circuit can be used to transfer portions of a data block concurrently via multiple data paths through the backplane I O interface and through the local backplane and or via the media dependent I O interface to support low latency data transfers. In a particular example such concurrent data transfers can be used to support real time or near real time parallel processing in systems that include multiple processing and or instrumentation nodes. In a particular example the multi path bridge circuit is adapted to allow for multi path concurrent data transfers between multiple processing nodes and multi Giga sample per second GSPS instrumentation systems to allow for real time or near real time processing of measurement data.

In a particular illustrative example the local backplane is adapted to support 10 Gigabit per second Gbps data transfer rates which rates are fully supported by the backplane I O interface . In a circuit device having sixteen processing nodes and having sixteen multi path bridge circuits fifteen multi path bridge circuits plus the multi path bridge circuit a particular embodiment of the multi path bridge circuit has a 10 Gbps connection to each of the other processing nodes through the local backplane. By transferring portions of a data block in parallel via the backplane I O interface the multi path bridge circuit can exploit the multiple data paths through the local backplane to achieve an aggregated data throughput that is approximately 150 Gbps to transfer data from one memory to another memory within the circuit device. If the two media dependent I O interfaces have data throughput rates of 10 Gbps and are also exploited the data throughput can reach approximately 170 Gbps.

Further a similar concept can be employed to exploit the dual media dependent I O interfaces of the multi path bridge circuit and of the other fifteen multi path bridge circuits to transfer the multiple data block portions to a destination memory of a remote device via the media dependent interface. For example in a particular embodiment a circuit device includes sixteen multi path bridge circuits such as the multi path bridge circuit . In this example the circuit device communicates with a corresponding circuit device via dual redundant switches where the corresponding circuit device includes a corresponding sixteen multi path bridge chips.

In a particular embodiment the multi path bridge circuit is adapted to use the fifteen channels through the backplane I O interface and the two media dependent I O interfaces to provide the equivalent of seventeen 10 Gbps serial connections in parallel. In this example if the data throughput of each of the media dependent interface connections is 10 Gbps the local backplane and each of the media dependent connections can be exploited to transfer data from the first circuit device to a destination memory at the second circuit device at an aggregated throughput of approximately 170 Gbps fifteen communication paths through the local backplane plus two communication paths through the network .

In general both the Advanced Telecom Computing Architecture AdvancedTCA and VITA Standards Organization VITA 46 48 standards currently define full mesh backplanes which support up to 16 slots with fifteen channels per slot allowing each of sixteen 16 processing nodes to be interconnected. Currently each channel of the backplane I O interface supports bi directional 10G Ethernet communications using a 10 Gigabit physical layer such as IEEE 802.3ap 10GBASE KR. In this particular example each slot operates at a maximum of approximately 150 Gbps of bandwidth in both directions through the local backplane. In general the backplane I O interface is illustrated as supporting fifteen channels which is consistent with a conventional off the shelf local backplane architecture. However it should be understood that the multi path bridge can be used with a backplane I O interface that supports any number of channels and or any number of processors per node. Further while the above examples have used 10 Gbps as a transfer speed it should be understood that the multi path bridge circuit can be adapted to operate at other data rates to fully exploit the data throughput rates of available local backplanes and of available media dependent switch fabric. Since the multi path bridge circuit is coupled to two processing nodes via the first and second dual host interfaces and the multi path bridge circuit allows the two processors that are connected via host interfaces and to share the full 150 Gigabits per second through the backplane I O interface . While the above example includes dual host interfaces and to couple to two processors in a particular example the multi path bridge circuit can be coupled to any number of processors via respective host interfaces.

In a particular example the bridge control logic can be instructions executed at a processor coupled to the multi path bridge circuit via one of the first or second interfaces and . In a particular embodiment a processor or the bridge control logic uses each of these seventeen 10 Gbps connections to transfer data directly from an associated memory to another processor s memory on any other processing node in a multi node device via Remote Direct Memory Access RDMA . Further the multi path bridge circuit can use these seventeen 10 Gbps connections to transfer portions of a data block from its memory to each other processor s memory in a multi node device via RDMA and to transfer the portions to a remote device via the media dependent I O interface associated with the multi path bridge and with multi path bridge circuits associated with other processing nodes of the circuit device having multiple processing nodes to achieve high data throughput via a network fabric.

While is described as providing a multi path remote direct memory access RDMA it should be understood that RDMA operates over a direct data placement DDP protocol layer. In a particular example it is possible to utilize DDP without using RDMA. In a particular embodiment the multi path data transfer can be performed using RDMA DDP another protocol proprietary or industry standard or any combination thereof.

The second server includes multiple processing nodes having multi path bridge circuits and that are coupled to a local communications fabric such as a multi path or full mesh local backplane via backplane I O interfaces and . The multi path bridge circuits and are coupled to the network switch fabric via respective input output I O interfaces and . Further each of the multi path bridge circuits and can be coupled to one or more processors and associated memories. The respective I O interfaces and are coupled to the network switch fabric via communication paths and respectively. It should be understood that the second server can include any number of processing nodes with processors associated memories and an associated multi path bridge chip which processing nodes are fully interconnected via the local communications fabric .

In a particular embodiment each of the multi path bridge chips and include logic to transfer data from a source memory to a destination memory via multiple available communication paths through the network switch fabric and through the local communication fabrics and . In a particular example the multi path bridge circuit can transfer portions of a data block to a memory associated with the multi path bridge circuit via multiple communication paths concurrently. A first portion can be sent directly from the multi path bridge circuit to the multi path bridge circuit via a first communication path through the local communications fabric . A second portion can be sent directly from the multi path bridge circuit to the multi path bridge circuit through the local communications fabric . The multi path bridge circuit forwards the data via another direct communications path between the multi path bridge circuit and the multi path bridge circuit via the local communications fabric . In a particular example the multi path bridge circuit uses all available communications paths through the local communications fabric to forward portions of a data block to a memory associated with the multi path bridge circuit exploiting the multi path bridge fabric to transfer portions of the data concurrently. Further one or more portions may be sent to the memory associated with the multi path bridge via the network I O interface the network switch fabric and the network I O interface . By utilizing available communications paths data throughput can be enhanced by an order of magnitude.

In another particular example using the local communications fabric a data block to be transferred from a memory associated with the multi path bridge to a destination memory associated with the multi path bridge can be split up forwarded to the other multi path bridges and and sent via the network I O interfaces and concurrently to the multi path bridges and . The multi path bridges and forward received data portions to the destination memory of the multi path bridge via the local communications fabric . Thus data transfers across the network switch fabric can also be enhanced to provide greater throughput.

In a particular example the multi path bridge circuit includes logic adapted to segment or divide a data block into a number of portions related to a number of available I O paths. In another particular embodiment the logic is adapted to transmit duplicate portions of the data block via selected paths of the available I O paths to provide data redundancy as well as enhanced data throughput. In a high quality of service QoS implementation the data portions may be sent concurrently and redundantly via multiple available paths to ensure a high quality data transmission.

In a particular example each channel supports bi directional 10G Ethernet communications using a 10GBASE KR physical layer. In this particular example each of the plurality of processing nodes and can operate at a maximum of approximately 150 Gbps of bandwidth in both directions through the multi path local backplane .

The multi path bridge circuit of each of the processing nodes and provides a bridge between a host processor interface and each of the plurality of processing nodes and via the multi path local backplane . Further the multi path bridge circuit of each of the processing nodes and provides a bridge between the multi path local backplane and a network fabric such as the network switch fabric illustrated in .

In a particular example the system includes processing nodes P nodes and and includes instrumentation nodes I nodes and . The instrumentation nodes and couple to the multi path local backplane to support multi Giga sample per second GSPS instrumentation which allows for real time or near real time parallel processing of data produced by multi GSPS instrumentation. In a particular example the system can include multiple Giga sample per second GSPS data acquisition devices instrumentation nodes that are interconnected with the plurality of processing nodes and via the multi path local backplane to provide for example hardware in the loop simulation signal processing and spectrum analysis. The system is adapted to utilize available backplane technologies such as 10 Gigabit per second Remote Direct Memory Access RDMA over IP and multi path local backplanes to facilitate faster data throughput via a multi path RDMA. In general the multi path local backplane can utilize any available transport technology and the multi path bridge circuits incorporated in the plurality of processing and instrumentation nodes and are adapted to implement multi path data communications and can leverage the available bandwidth to achieve higher data throughput.

In a particular example a multi path bridge circuit in conjunction with the passive multi path local backplane provides an order of magnitude increase in data transfer throughput between the plurality of processing nodes and computing nodes and supports real time or near real time parallel processing in multi GSPS instrumentation systems.

In a particular embodiment the instrumentation node I Node receives instrumentation data samples from a multiple Gigabit per second analog to digital converter ADC . The instrumentation node includes a multi path bridge circuit such as the multi path bridge circuit illustrated in which is adapted to communicate the data samples directly to a memory associated with the processing node via the multi path local backplane via a parallel concurrent multi channel RDMA data transmission. In particular the multi path bridge circuit of the instrumentation node transmits a first portion of the data via a direct interconnection between the instrumentation node and the processing node . Additionally the instrumentation node uses all of its other interconnections to the other instrumentation nodes and and to the other processing nodes and to push other portions of the data to the other nodes and for direct transmission of the other portions of the data from the other nodes and to the memory of the processing node via their respective direct connections. Each of the multiple channels is bolded to illustrate the data transmission from the instrumentation to the processing in parallel via different communication paths having different lengths. In a particular example the data can also be sent to multiple processing nodes in parallel.

In a particular example remote direct memory access RDMA over Internet Protocol IP differs from traditional TCP IP Transmission Control Protocol IP in that RDMA eliminates unnecessary buffering in the operating system OS when transmitting and receiving packets. Instead of copying packets into a buffer in the OS before sending or receiving the multi path bridge circuit takes data directly from application user space memory applies the appropriate network layer protocols and Ethernet link layer frame and sends the packet across the network. On the receiving end another multi path bridge circuit receives the packet and places the payload directly into application user space memory. By removing the unnecessary data copying in the kernel and off loading network protocol processing at both ends of the link the multi path bridge circuits alleviate the latency issues normally associated with TCP IP and make Ethernet a viable solution for high speed low latency instrumentation systems.

In general the multi path connectivity provided by the multi path backplane is leveraged by the multi path bridge circuits to aggregate the bandwidth of each connection between a processing node and each of the other processing or instrumentation nodes. In a particular example while a particular direct connection between any two nodes has a bandwidth of approximately 10 Gbps data can be transferred via the multi path backplane via multiple concurrent communication paths at an aggregate rate of 150 Gbps using each available 10 Gbps channel. Further an additional 10 or 20 Gbps can be added by transferring another portion through the external switch fabric. Thus using the multi path backplane data throughput between processing nodes can be aggregated to improve data transfer rates significantly. In particular relative to current networking technology the multi path RDMA technique implemented by the multi path bridge circuit provides an order of magnitude increase in bandwidth between processing nodes in the same backplane and or between processing nodes connected through a switch fabric across a network. Further if the processing nodes are coupled through a dual redundant switch fabric via a network device such as a switch or router an additional increase in data throughput can be realized.

In a particular embodiment each of the first and second plurality of processing nodes and includes logic to facilitate multi path concurrent parallel data communications.

In a particular embodiment a particular multi path bridge circuit breaks segments or divides the data block into data block portions. A first portion of the data block is sent from the memory associated with the processing node via the first switch to the memory associated with the processing node . A second portion of the data block is sent from the memory associated with the processing node via the second switch to the memory associated with the processing node . A third portion of the data block is sent from the memory associated with the processing node via the first multi path local backplane to a second processing node which forwards the third portion to the memory associated with the processing node via the first or second switch or the processing node and the second multi path local backplane . Further other portions of the data block are transferred from the memory of the first processing node via the first multi path local backplane to the other processing nodes which transmit the other portions via the dual redundant switches or to the memory of the processing node via the processing nodes and through the second multi path local backplane .

In a particular example if each connection from the plurality of processing nodes and to the redundant switches and has a data rate of approximately 10 Gbps then data can be transferred from the memory of the processing node to the memory of the processing node at an aggregate rate of approximately 170 Gbps. In general the system is adapted to exploit the first and second multi path local backplanes and to transfer data between processing nodes and to utilize each of the channels through the switches and concurrently to achieve an aggregated data throughput that is greater than any one of the channels. In another particular example where data quality is important the system may transmit redundant portions of the data block concurrently to a destination device to ensure data accuracy. In an example if there are sixteen available communications paths between two devices eight of the paths may be used to transmit different data block portions and another eight may be used to transmit redundant data block portions in parallel. Further the system can be adapted to utilize half of the channels for redundant transmission and half for accelerated parallel transmission to achieve a higher data throughput rate with enhanced quality of service. In general the amount of bandwidth available to a particular transaction can be dynamically allocated across the available links. In a particular example where 15 communication paths are available through the local backplane a particular transaction can dynamically allocate N of the 15 available links where 1 N 15.

In a particular embodiment each of the first and second pluralities of processing nodes and are mapped to a dual redundant star topology i.e. dual redundant switches and . In general each of the first and second switches and provides a switched connection between any two processing nodes in the system . On the other hand the multi path fabric of the first and second multi path local backplanes and allows for each processing node to have as many connections as processing nodes in the backplane. Hence the multi path fabric of the first and second multi path local backplanes and can be exploited to provide much higher bandwidth into and out of any one processing node than a single connection can provide.

In a particular example each of the processing nodes has seventeen available channels to transmit data to any other processing node the fifteen channels through the multi path local backplanes and and two channels through the dual redundant switches and . Any two processing nodes in the network have one direct channel through the multi path local backplane or plus two switched connections between each other through the dual redundant switches and . Using the multi path RDMA the two processors also gain fourteen one hop connections through the remaining multi path channels via the other fourteen nodes multi path bridge circuits. As used herein the term hop refers to a link or communication path between processing nodes either through the backplane fabric or via a network switch fabric. The multi path bridge circuits act as cut through switches between the sending and receiving multi path bridges on the source and destination nodes.

In a particular embodiment the multi path bridge circuit of a particular processing node such as the processing node segments the payload and transmits the resulting segments concurrently via the seventeen available channels. The two switch fabric ports simply send their packets to the Ethernet ports on the receive side of the first and second switches and . The fifteen connections through the backplane interface however route through the other fifteen multi path bridge circuits of the other fifteen processing nodes and which act as cut through bypass or pass through switches to the switch fabric ports on each node to transmit the data segments to the receive side of the first and second switches and . In general the packet segments are sent across the switch fabric simultaneously or concurrently to the fifteen nodes and connected to the receiving node through the other multi path local backplane . The multi path bridge circuits on those nodes and then act as cut through bypass or pass through switches to the channels connected to the multi path bridge circuit of the receiving node . The multi path bridge on the receiving node then combines the payloads from each packet and places the data directly into the memory of the receiving node . Hence for a multi path data transmission through the dual redundant switch fabric the processing node has two one hop switched connections and fifteen three hop connections through the first and second multi path local backplane interfaces and and through the respective multi path bridges.

The first node includes a processor CPU that is coupled to a memory and to a multi path bridge circuit . The node also includes a second processor and a memory which are coupled to the multi path bridge circuit . The multi path bridge circuit includes logic and a physical connection to provide a multi path data communication functionality to facilitate transfer of data between the first node the one or more other nodes and the last node . Additionally the multi path bridge circuit is adapted to transmit data to or receive data from nodes of another electric device via the switch fabric . Each of the one or more additional nodes includes a respective one or more multi path bridge circuits that includes logic and a physical connection to facilitate concurrent multi path transfer of data between the first node the one or more other nodes and the last node and or between nodes of another electric device via the switch fabric . Further the last node includes a first processor and a memory that are coupled to a multi path bridge circuit . Further the multi path bridge circuit is coupled to the second processor and a second memory . The multi path bridge circuit includes logic and physical connections to facilitate multi path concurrent transfer of data between the first second and one or more other nodes and and or between nodes of another electric device via the switch fabric .

In general the multi path bridge circuits and may be integrated circuit devices that include logic to exploit the multi path local backplane and the dual redundant switch connections via the switch fabric to transmit data via multiple data paths having different lengths and in parallel. Further in a particular embodiment the multi path bridge circuits and can be integrated with a processor. For example the multi path bridge circuit can be integrated with the CPU . In this example the host interface can be a data bus internal to the CPU .

The second node server includes a plurality of processing nodes and . The processing node includes a processor and a memory that are coupled to a multi path bridge circuit . The multi path bridge circuit is coupled to a second processor and to a multi path local backplane and to the switch fabric . The multi path bridge circuit can be an integrated circuit that includes logic adapted to facilitate data transfers between the plurality of nodes and and between a single node and one or more nodes of the first node server . The second node server further includes one or more processing nodes each of which includes a multi path bridge circuit to facilitate multi channel parallel data transfers between the plurality of nodes and between a node and a node of the first node server or any combination thereof.

In general the bandwidth between any two multi path bridge circuits is a multiple of the Ethernet switch fabric speed. For dual redundant 10 Gbps connections a sixteen node chassis can achieve up to a maximum 170 Gbps data throughput using the available communication paths. Further the number of channels utilized by the multi path bridge circuits can change on the fly such that the number of communication paths between any two nodes is dynamic. As a result multi path bridge circuits such as the multi path bridge circuits and can support a large number of connections at dynamically changing bandwidths in order to optimize connections for a wide range of algorithms. Further depending on Quality of Service QoS requirements the number of channels utilized for the multi path transfers can vary to provide redundant channels. For example in a particular instance redundant data segments may be transmitted in parallel via each of the blades and via the two switch fabric connections achieving a redundant data transfer rate of approximately 80 Gbps. However real time or near real time systems can only take advantage of this enhanced data throughput if the instruments are able to obtain the same amount of bandwidth into and out of the parallel processing cluster.

In a particular example the sensor actuator is adapted to provide a signal to the ADC DAC that is related to a measured parameter such as a position a temperature a flow rate other data or any combination thereof. The ADC DAC converts the signal into a digital signal that is provided to the FPGA which can store the data at the memory and which can transmit the data to one or more other nodes or across a network fabric via the multi path bridge circuit . In an alternative example the sensor actuator can receive a signal from the ADC DAC to adjust the sensor actuator . For example if the sensor actuator is adapted to adjust a valve a valve adjustment signal can be received via the ADC DAC .

In another particular embodiment the system can include dual 10 Gbps RDMA over Internet Protocol IP Ethernet ports. In this embodiment using multi path RDMA the multi path bridge circuit can transfer data directly between its own memory and tens hundreds or even thousands of processor memories in a supercomputing cluster through an Ethernet switch fabric at very high data throughput rates allowing for real time or near real time analysis of measurement data.

In another particular embodiment the ADC DAC can be interfaced to the multi path local backplane using the FPGA to communicate with the ADC DAC to buffer data in memory and to provide a host interface to the multi path bridge circuit . The multi path bridge circuit can use multi path RDMA to transfer data between the memory and the memory of any processor in the cluster. Furthermore with Institute of Electrical and Electronics Engineers IEEE 1588 Precision Time Protocol PTP support built into the multi path bridge the system can support clock time synchronization with each of the processing nodes.

In a particular example the system can be used with a cluster of processing nodes using multi path RDMA to compute a billion point complex Fast Fourier Transform CFFT of acquired sensor data in near real time. For example using two 8 bit 5 Giga sample per second GSPS ADCs to capture an arbitrary waveform starting at time zero t 0 on the system that is connected to the multi path local backplane . The two ADCs sample the waveform in phase and quadrature phase I and Q . A billion complex data points are sampled every 214.7 ms a billion in this case is actually 2 . The data rate corresponds to a bandwidth of 10 GBps from the two ADCs. Since the ADC interface and the DDR3 synchronous dynamic random access memory SDRAM interface on the FPGA have more than 10 GBps bandwidth the data points can be stored in the memory as fast as they are sampled.

As the data is stored in memory the FPGA simultaneously has the multi path bridge circuit transport the data to a processor somewhere in the cluster using multi path data communications such as multi path RDMA. Since the host interface and the network bandwidth to a processor through the multi path bridge circuit using multi path data communications can be much greater than the sampling rate of the ADC the data can be streamed to the processor s memory as the ADC samples the waveform.

In a particular example the processor that receives the data via the multi path RDMA data transfer can be an IBM Cell Broadband Engine processor a multi core heterogeneous processor with one PowerPC Processor Element PPE and eight Synergistic Processor Elements SPEs . To perform a near real time billion point CFFT on the IBM Cell the processor must have both enough memory to support the large data set and a significant amount of computational power to perform the calculations in a small amount of time. A billion point single precision floating point CFFT requires eight bytes of storage per single precision floating point complex data point and thus needs 8 GB of memory for storage. In this example with 230 GFLOPS single precision floating point performance the Cell can perform a 16M point CFFT in 0.043 s which can be scaled to estimate the time required to calculate a billion point CFFT. The first part of the calculation involves determining the relative complexity factor of a billion point CFFT to a 16M point CFFT. The complexity of a CFFT is determined by the following equation log Equation 1 where N is the number of points. Hence the complexity factor of a billion point CFFT compared to a 16M point CFFT is as follows 1log 1 16log 16 80 Equation 2. Assuming the billion point CFFT is as equally parallelizable on a single Cell as the 16M point CFFT the time a Cell takes to calculate a billion point CFFT is determined by the following equation 80 0.043 s 3.44 seconds Equation 3.

The ADCs can sample one billion complex data points in 214.7 ms. Using only sixteen Cells the multi path RDMA Instrumentation System can perform billion point CFFTs continuously in near real time. Further a multi path RDMA Instrumentation system with one instrumentation module and nine Cell nodes containing eighteen Cells is able to compute a billion point CFFT on a 5 GSPS complex signal in near real time. In general while a 5 GSPS complex ADC signal represents an extreme example the multi path data communication supported by the multi path bridge circuit in conjunction with the switch fabric and the multi path local backplane makes such near real time calculations possible.

In a particular embodiment each of the available communication paths has a different path length. In another particular embodiment each of the available communication paths has a unique path length. In still another particular embodiment the method further includes receiving multiple second data block portions at network I O interfaces of the plurality of processing nodes each of the multiple second data block portions including a memory base address and an address offset. In this instance the method also includes transferring data to a particular memory according to the memory address and the address offset via the multi path backplane to assemble a received data block from the multiple second data block portions.

In another particular example the method includes receiving multiple second data block portions from multiple sources via multiple communication paths at the multi path bridge circuit of the source determining a memory location within the source memory for each of the multiple second data block portions and storing the multiple second data block portions at the determined memory locations. In still another particular embodiment the method includes receiving a third data block portion from at least one source via at least one communication path determining a destination address associated with the received third data block portion and forwarding the received third data block portion to the destination via the local backplane based on the determined destination address. In a particular example the destination address represents an application process executing at a processor associated with the destination. In another particular example the destination address represents a memory location memory base address and an address offset . In a particular embodiment the destination can include one or more processing nodes one or more instrumentation nodes or any combination thereof.

In conjunction with the systems and methods described above a multi path bridge circuit is disclosed that enables multi path data communications to facilitate high throughput data transfers via a local backplane and optionally through a network switch fabric. Further the multi path bridge circuit can be used in both processing and instrumentation nodes to facilitate real time or near real time data collection and processing using distributed processing systems. Additionally the multi path bridge circuit enables multi path data communications to transfer data concurrently or near simultaneously via multiple data paths having different lengths different data rates different topologies or any combination thereof to achieve a network throughput that is related to an aggregate data throughput of the multiple paths. In a particular embodiment a multi path bridge circuit is adapted to exploit a multi path local backplane and multiple I O interfaces to achieve an order of magnitude increase in bandwidth and or data throughput between nodes of a system.

Although the present invention has been described with reference to preferred embodiments workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention.

