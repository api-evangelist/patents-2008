---

title: Database system with active standby and nodes
abstract: A system includes an active node and a standby node and zero or more replica nodes. Each of the nodes includes a database system, such as an in-memory database system. Client updates applied to the active node are written through to the standby node, and the standby node writes the updates through to a primary database and updates the replica nodes. Commit ticket numbers tag entries in transaction logs and are used to facilitate recovery if either of the active node or the standby node fails. Updates applied to the primary database are autorefreshed to the active node and written through by the active node to the standby node which propagates the updates to the replica nodes. Bookmarks are used to track updated records of the primary database and are used to facilitate recovery if either of the active node or the standby node fails.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08868504&OS=08868504&RS=08868504
owner: Oracle International Corporation
number: 08868504
owner_city: Redwood Shores
owner_country: US
publication_date: 20080212
---
Priority benefit claims for this application are made in the accompanying Application Data Sheet if any . To the extent permitted by the type of the instant application this application claims priority to the following application s 

This application is related to the following application s filed simultaneously herewith and which are all owned by the owner of the instant application and to the extent permitted by the type of the instant application this application incorporates by reference for all purposes the following application s 

The invention relates to the field of data processing systems. More specifically the invention relates to reliability and or availability techniques used for database management systems.

A portion of the disclosure of this patent document contains material subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

A database is a collection of related data items. To perform various tasks users access data items stored in the database via transactions. A database management system supports transactions to access the data items stored in the database and uses data structures such as indices to manage access to the data items.

Disk based Relational Database Management Systems RDBMS use disk storage to store and access databases. Much of the work that is done by a conventional disk optimized RDBMS is done under the assumption that the data items and related data structures primarily reside on disk. Optimization algorithms buffer pool management and indexed retrieval techniques are designed based on this fundamental assumption. One problem with disk storage is that access to the data items and to the data structures is relatively slow.

Even when an R DBMS has been configured to hold all of its data items and data structures in main memory its performance is hobbled by assumptions of disk based data residency. These assumptions cannot be easily reversed because they are hard coded in processing logic indexing schemes and data access mechanisms.

In memory relational database systems also called main memory relational database systems are deployed such as in the application tier and operate in physical memory using standard Structured Query Language SQL interfaces. By managing data in memory and optimizing the data structures and data access algorithms in memory database systems are able to provide improved responsiveness and throughput compared even to fully cached disk based RDBMS. For example an in memory database system is designed with the knowledge that the data items reside in main memory and is thus able to take more direct routes to the data items reducing lengths of code paths and simplifying algorithms and data structures.

When the assumption of disk residency is removed complexity is reduced. The number of machine instructions drops buffer pool management disappears extra copies of the data items and or data structures are not needed and indices shrink. The database design becomes simpler and more compact and requests for data items are executed faster. Some in memory database systems provide persistent non volatile storage of data such as by archiving data from main memory to disk or by maintaining a non volatile transaction log. Unlike disk based RDBMS however some in memory database systems do not guarantee persistence of all data such as by not gating a commit of a transaction by the non volatile logging of the transaction.

In memory database systems are sometimes less reliable than disk based systems because of factors such as the storage such as main memory is volatile an environment in which the in memory database systems are deployed is more hostile and a type of hardware platform on which the in memory database systems run is less reliable. Techniques such as data replication and using active and standby systems are used to increase reliability and availability. In one form of replication write through schemes allow modifications to an in memory database system to be written through to a backend primary database system. Write through to a backend database system thus improves data persistence since if the in memory database system fails data that has been written through remains in the backend database system and is retrievable from there. However it is desirable to have techniques with both improved data persistence and improved data availability so that a failure such as a crash of a single in memory database system does not even temporarily cause a loss of access to the data.

A system includes an active node such as node X of a standby node such as node Y of and zero or more replica nodes such as nodes R R R and R of . Each of the nodes includes a respective database system such as an in memory database system. In some embodiments and or usage scenarios the system includes a backend primary database system such as database of . Any of these database systems are optionally and or selectively different database systems from each other and or different versions of a same database system. According to various embodiments at least some of the nodes include and or act as one or more of a front end for at least some updates applied to the backend database system a subset of the contents of the backend database system and a cache of or for the backend database system. In some embodiments and or usage scenarios using nodes including an in memory database system in conjunction with a backend database system provides one or more of higher access bandwidth shorter access latencies and reduced cost per query.

In various embodiments and or usage scenarios the backend database system is a primary database system such as an Enterprise Information System . In some embodiments a middle tier database system includes the active node the standby node and the replica nodes. The middle tier database system isolates the primary database system from large amounts of traffic such as internet traffic by providing a distributed copy of at least some of the data in the primary database system.

In various embodiments the respective database systems on each of the nodes organize at least some of the data in cache groups. Cache groups provide a way to organize one or more tables of the backend database system so that related data such as from multiple tables sharing a key is movable to and or from the nodes as a unit. That is some or all of the one or more tables of the backend database system such as selected rows and or columns and or other selected portions are replicated on one or more of the nodes and cache groups define a unit of the replication such as a row of each of the tables the rows having a same key. In further embodiments cache groups have properties specifying features of their operation such as how data of the cache groups is loaded into the respective database of a particular one of the nodes how data of the cache groups is aged removed from the respective database of the particular node and how data of the cache groups is updated for example by writethrough or by autorefresh . According to various embodiments properties of cache groups are one or more of configurable separately on each of the nodes configurable separately on the backend database system and global across one or more of the nodes and the backend database system.

In some embodiments a particular cache group or in further embodiments a particular table is configurable to be updated by one or more of direct updates such as from a client writethrough updates such as from another one of the nodes and autorefresh updates such as from a backend database system . In other embodiments a particular cache group or in further embodiments a particular table is configurable to be updated by only one of direct updates writethrough updates and autorefresh updates.

Providing the active node and the standby node increases availability in that either one of the active node or the standby node can fail such as by crashing and the surviving node is able to continue operating in some cases taking over functions and or operations of the failed node. According to various embodiments failure of a node is one or more of complete such as by the node crashing a process failure such as a critical process or critical agent of the node failing a communication failure such as a network connection of the node failing a freeze such as the node functioning but becoming unresponsive and any other type of failure that renders the node wholly or partly unable to provide intended functions of the node. Recovery from the failure depends on a type of the failure and uses various recovery measures such as rebooting the failed node and restarting a failed process of the node to restore basic functionality of the node. Additional recovery techniques as described elsewhere herein are used to synchronize the node for example to ensure that contents of the respective database of the node are up to date and to return the node to operation.

To provide the increased availability without or with minimal loss of data updates such as transactions and or commits of transactions to data stored in the system are propagated to all of the nodes and to the backend database system if present. In a first example updates applied to the active node are written through to the standby node and to the replica nodes if any as well as to the backend database system. In a second example updates applied to the backend database system are propagated to the active node by an autorefresh technique and are then written through to the standby node and to the replica nodes if any .

In some usage scenarios providing replica nodes increases the bandwidth of the system in that a larger number of nodes having copies of the data are able to support more read only accesses for example queries . In various embodiments and or usage scenarios the active node and or the standby node support read only access to the data.

Generally a particular update in a database system is associated with a change of data in the database system but not necessarily to a change that is visible outside of the boundaries of a transaction such as a series of updates containing the particular update. In some embodiments updates to the respective database system of a first one of the nodes such as the active node are applied and are written through to others of the nodes as they are applied irrespective of whether the updates are committed. Transaction commits are similarly written through as another form of update . A node receiving the written through updates such as a standby node is able to apply the written through updates to the respective database of the receiving node as the written through updates arrive. But as with the general case described above the written through updates are not visible outside the boundaries of a containing transaction until the written through containing transaction commit is received and applied. In other embodiments updates to the respective database system of a first one of the nodes such as the active node are applied and are written through to others of the nodes on transaction commit boundaries so that intermediate uncommitted updates are not written through. In further embodiments updates to the respective database system of a first one of the nodes such as the active node are applied and are written through to others of the nodes in groups of committed transactions. According to various embodiments a group of committed transactions is one or more of a sequence of transactions that were sequentially committed and a snapshot of changes caused by a sequence of committed transactions that were sequentially committed. A sequence of transactions that were sequentially committed optionally includes multiple sequential updates of a single record whereas a snapshot of changes caused by a sequence of transactions includes a single update for each record that was changed the single update providing the final state of the record. Updates as referred to herein generally refer to updates due to committed transactions as an update within a transaction that is not yet committed is generally only visible within that transaction.

While techniques described herein generally refer to a single active node and a single standby node the techniques are extendible to systems having a plurality of active nodes and or a plurality of standby nodes. In a first example a first one of the nodes serves as a respective active node for a first cache group and a second one of the nodes serves as a respective active node for a second cache group. In a second example a system has one active node and two standby nodes. Similarly there is no restriction to having a single backend database system. For example a first cache group is associated with a first backend database system and a second cache group is associated with a second backend database system.

In some embodiments updates such as updates from one or more client systems such as clients of are applied to the active node and are written through propagated by the active node to the standby node. The standby node writes the updates through to a backend database system. The active node and or the standby node write through the updates to the replica nodes if any . In various embodiments the load on the active node is reduced by having the standby node write through all or a majority of the updates to the replica nodes. In some embodiments and or usage scenarios updates applied to a particular one of the nodes such as to the active node are non durable in that a failure of the particular node possibly results in a loss of a small number of the updates.

In an example of recovery from a failure of the active node the standby node becomes takes over the role of the active node and the client updates are subsequently applied to the new active node. The new active node also takes over the writing through of the updates to any of the replica nodes that were being performed by the failed active node as well as continuing to write through the updates to the backend database system . The failed active node is recovered synchronized with the new active node and becomes the standby node. The new standby node then takes over writing through the updates to the backend database system and optionally and or selectively takes over writing through the updates to replica nodes if any and as required by load balancing and other considerations .

In an example of recovery from a failure of the standby node the active node takes over writing through the updates to the backend database system and to any of the replica nodes that were being performed by the failed standby node. The failed standby node is recovered synchronized with the active node and resumes operation as the standby node including taking over writing through the updates to the backend database system and writing through updates to ones of the replica nodes as required .

When either the active node or the standby node fails ones of the updates that occur between the failure and subsequent recovery are not applied either directly or via writethrough to the failed node. By assigning an associated commit ticket number to each of the updates by storing the commit ticket numbers with other update related information in a transaction log and by sending the commit ticket numbers along with the updates such as from the active node to the standby node the system is able to determine how far behind a failed node is compared to the surviving node and the failed node is synchronizable with the surviving node without having to copy the entire state of the surviving node.

Because of the flow of writethrough updates from the active node to the standby node and from the standby node to the backend database system the standby node contains a subset including a proper subset of the ones of the updates that have been seen and processed by the active node and the backend database system contains a subset including a proper subset of the ones of the updates that have been seen and processed by the standby node. Further a failure of the standby node does not lose any of the updates and the active node is enabled to use the commit ticket numbers stored in the backend database system to ensure that the backend database system has all of the updates. Similarly a failure of the active node optionally and or selectively loses a small number of updates that were in progress when the active node failed but the standby node and the backend database system remain consistent. In some embodiments no updates are lost in the event of a failure of the active node if all of the updates were transactional and durable as the updates are either committed and thus persistent or not yet committed and thus not guaranteed to an application to be complete. 

In some embodiments updates such as updates of the backend database system are autorefreshed to the active node and are then written through by the active node to the standby node and from the active node and or the standby node to the replica nodes if any . In an example of recovery from a failure of the active node the standby node becomes the active node and the autorefresh updates are applied to the new active node. The failed active node is recovered synchronized with the new active node and becomes the standby node. In an example of recovery from a failure of the standby node the failed standby node is recovered synchronized with the active node and resumes operation as the standby node.

When either the active node or the standby node fails ones of the updates that occur between the failure and subsequent recovery are not applied either by autorefresh or via writethrough to the failed node. By maintaining a change log of updated rows of the backend database and by maintaining bookmarks in the change log and other tables to indicate which updates have been applied to each of the active node and the standby node the system is able to determine how far behind the failed node is compared to the updates of the backend database system and the failed node is synchronizable with the backend database system without having to fully replace contents of the respective database of the failed node.

In some embodiments the backend database system is different from the respective database systems of the nodes such as by being a disk based relational database system whereas the respective database systems of the nodes are in memory database systems . In other embodiments the backend database system is identical to the respective database system of at least one of the nodes. In some embodiments the backend database system is treated differently than the respective database systems of the nodes while in other embodiments the backend database system is treated similarly to the respective database systems of the nodes. In some embodiments the backend database system is the respective database system of a particular one of the nodes. For example updates applied directly to the backend database system are in some embodiments propagated by active writethrough from the backend database system acting as an active node with respect to the updates applied directly to the backend database system.

While in some of the embodiments herein certain techniques such as active writethrough are described in part with respect to ones of the nodes and other techniques such as autorefresh are described in part with respect to the backend database system in other embodiments any of the techniques are applicable to either the nodes or to the backend database system and in any combination. Accordingly in some embodiments one of the nodes includes in whole or in part the backend database system and references to nodes herein are considered to include in some embodiments the backend database system.

In some embodiments a master update list MUL maintains a list of updates committed transactions applied at a master node and the MUL is used at least in part to track which of the updates have been propagated such as by being written through to others of the nodes. According to various embodiments the master node is one or more of the active node and the backend database system. In a first example an MUL is a transaction log at an active node such as a transaction log as illustrated in . In a second example an MUL is a change log table in a backend database system such as change log table as illustrated in . Of course in other examples an active node is configured to use a change log table as an MUL or a backend database system is configured to use a transaction log as an MUL.

In further embodiments the master node maintains a record of which of the updates have been acknowledged by the others of the nodes. A particular one of the nodes acknowledges a particular one of the updates when the particular node has committed the particular update so that it is persistent non volatile such as by writing the update or a log thereof to non volatile storage. To keep the MUL from growing without bound the MUL is in some embodiments garbage collected by removing ones of the updates that have been acknowledged by all of the others of the nodes.

In some embodiments the roles of master node active node and standby node are not fixed to specific ones of the nodes but are associated with particular portions of data such as cache groups or tables . For example at the active node a first cache group is updated using autorefresh from the backend database system which thus acts as the master node for the first cache group and a second cache group is updated by client updates and thus the active node is the master node for the second cache group .

In some embodiments an MUL includes a plurality of entries each of the entries includes an update identifier associated with a respective one of the updates and each of the entries is or is able to be associated with an MUL locator. In some embodiments an MUL locator is an index or a pointer to one of the entries in the MUL. In other embodiments an MUL locator is an identifier such as a sequence number stored in a particular one of the entries of the MUL and the particular entry is found by searching for the MUL locator such as by a linear search a binary search a hash based search a B tree search or any other technique of finding a value located in one of a plurality of entries. In a first example of the entries of the MUL each entry in the MUL is a commit record in a transaction log such as commit record in transaction log as illustrated in and an MUL locator is a commit ticket number such as CTN.X as illustrated in stored in each entry. In a second example of the entries of the MUL each entry in the MUL is a row in a table such as entry of change log table as illustrated in and an MUL locator is a bookmark such as bookmark .B as illustrated in stored in each entry.

According to various embodiments the update identifier in each of the MUL entries includes one or more of a log number a key and any other identifier used to identify and or locate at least one of the updates. In some embodiments a log number is a unique identifier such as a pointer used to locate information corresponding to a transaction log entry for the associated update. The corresponding information includes in various embodiments information sufficient to replay or to redo the update. Given a particular one of the MUL entries the log number in the particular entry enables the update associated with the particular entry to be replayed.

In some embodiments a key used as a unique identifier in an MUL entry associated with a particular one of the updates is a copy of a key such as a primary key of a table modified by the update. Given a particular one of the MUL entries associated with a particular one of the updates the key in the particular entry enables a latest version of data that was modified by the particular update to be retrieved. In further embodiments and or usage scenarios two or more of the MUL entries have a same key due to sequential updates to a same row of a table or rows of multiple tables in a cache group the rows having the same key .

In some embodiments an MUL locator is used to identify one or a group of the updates stored in the MUL. In further embodiments when a particular update is written through to a particular one of the nodes the particular update includes a respective MUL locator or alternatively the respective MUL locator is sent in conjunction and or associated with the particular update . The particular node receives the written through update applies the written through update to its respective database and in some embodiments communicates a commit of the written through update by sending an acknowledgment with the respective MUL locator back to the master node. In further embodiments the master node maintains a record of the last MUL locator acknowledged by each of the other nodes.

In some embodiments an update is written through a succession of nodes. In a first example an update applied to a backend database system is written through such as an autorefresh update to an active node which in turn propagates the update as an active writethrough update to a standby node. In a second example an update applied to an active node is written through such as by an active writethrough update to a standby node which in turn propagates the update as a standby writethrough update to a backend database system. In a third example a system has an active node and a plurality of standby nodes and an update applied to an active node is written through to a first one of the standby nodes which in turn propagates the update to a second one of the standby nodes. In a fourth example updates written through to a standby node are in propagate through to one or more replica nodes.

In some embodiments at least some of the nodes such as the standby node applying written through updates maintain in a respective local update list LUL a list of the written through updates applied at the node. Similar to a MUL a LUL includes a plurality of entries each of the entries includes an update identifier associated with a respective one of the updates and each of the entries is or is able to be associated with an LUL locator. Similar to the MUL locator an LUL locator is used to identify entries in the LUL. In one example an LUL is a transaction log at a standby node such as a transaction log as illustrated in and an LUL locator is a commit ticket number such as CTN.Y as illustrated in stored in each entry. In various embodiments the LUL of the standby node is enabled to become the MUL when the standby node takes over for a failed active and in this case master node.

In some embodiments at least some of the nodes receiving a written through update store the respective MUL locator of the written through update in the respective LUL. For example as illustrated in commit record .Y stores an MUL locator CTN.X and an LUL locator CTN.Y . In further embodiments when a particular one of the nodes receiving a written through update including a respective MUL locator propagates the written through update the propagated written through update includes the respective MUL locator and a respective LUL locator or alternatively the respective MUL locator and the respective LUL locator are sent in conjunction and or associated with the propagated written through update . In still further embodiments a subsequent one of the nodes such as a replica node receiving the propagated written through update applies the propagated written through update to its respective database and communicates a commit of the propagated written through update by sending an acknowledgment with the respective MUL locator back to the master node and by sending an acknowledgment with the respective LUL locator back to the particular node. In some embodiments by sending acknowledgments back to both the master node and to the particular node the particular node is able to take over for the master node in the event that the master node fails.

In some embodiments the MUL enables more rapid recovery from failure of a node. In a first example if a particular one of the nodes such as any one of the standby node or one of the replica nodes or in various embodiments and or usage scenarios the active node fails when the particular node comes back up a particular MUL locator is retrieved for the last update acknowledged by the particular node. According to various embodiments the particular MUL locator is retrieved from a last committed entry in a transaction log of the particular node or the particular MUL locator is retrieved from an acknowledgement list maintained on the master node. By using the particular MUL locator the master node or in some embodiments another node acting for the master node is able to identify all updates in the MUL subsequent to the update associated with the particular MUL locator and to send those updates to the particular node to synchronize the particular node. In some embodiments if the master node is the backend database system another one of the nodes such as the active node or the standby node or the particular node itself is enabled to identify all updates in the MUL subsequent to the update associated with the particular MUL locator and to send those updates to the particular node.

Continuing the first example if the particular node is the active node the active node is recovered and remains the active node. In various embodiments and or usage scenarios the roles of the active node and the standby node optionally and or selectively do not switch on a failure of the active node. Recovering the active node without changing the role of the active node is in some embodiments and or usage scenarios less disruptive such as when the failure is of a type that is quickly recoverable for example as failure of a process on the active node . In various embodiments while the active node is being recovered updates to the active node are suspended such as by being queued or by being dropped. In one example client updates to the active node while the active node is being recovered are queued at the client s and or within the active node and are not lost. In another example autorefresh updates to the active node while the active node is being recovered are not transmitted to the active node until after the active node is recovered and no autorefresh updates are lost.

In a second example if the active nodes fails when the active node comes back up the standby node is now the new active node and the active node is now the new standby node. The LUL maintained on the standby node is now the MUL. Recovery of the new standby node is then similar to that of the first example. In some embodiments recovery of the new standby node is with respect to two separate master nodes and the techniques of the first example are applied twice. For example the backend database system is the master node for autorefresh updates to the active node and the active node was the master node for client updates applied to the active node a role now assumed by the standby node .

In various embodiments a system using techniques such as described above to manage and or to recover from failures is resilient to various types of failures such as failure of the active node failure of the standby node failure of one of the replica nodes failure of a network connection of any of the preceding temporary failure of the backend database system or of a network connections to the backend database system and other types of failures. In a first example if the backend database system fails client updates sent to the active node continue to be applied to the respective database of the active node. When the backend database system is recovered the client updates are written through to the backend database system. In a second example if a network connection from the backend database system to the active node is broken clients continue to query the replica nodes and optionally and or selectively the active node and or the standby node . When the network connection is restored autorefresh updates from the backend database system to the active node resume.

According to various embodiments any or all of the components include processing devices such as one or more of processors multiprocessors microprocessors workstations personal computers server computers mainframe computers server blades any other computer computing device or computer system and any combination thereof. In a first example in some embodiments node X and node Y are each server blades in a blade server. In a second example node X is a multiprocessor computer system and database is a sever computer system. In some embodiments some or all of the functionality of any or all of the components is provided by software firmware microcode or other executable and or interpretable code executing independent or cooperatively on the components.

Some of the connections in and in other figures herein are illustrated in some figures as dashed lines such as connections and and others are illustrated as solid lines such as updates and connections and . Some of the connections such as connections are illustrated as dashed lines in one figure such as and as solid lines in another figure such as . In some embodiments the connections illustrated as dashed lines are not present and or are present primarily as a backup until an event such as a failure of a node occurs. For example on a failure of node Y primary connections and are unavailable and are replaced by backup connections from node X .

Some of the connections in and in other figures herein are illustrated as unidirectional arrows while other connections use bidirectional arrows. Unidirectional arrows are generally used herein to indicate a direction of flow most relevant to the embodiment being described and do not imply that any of the connections are solely unidirectional. For example active writethrough updates is illustrated with a unidirectional arrow to indicate a flow for update information but in some embodiments node Y provides acknowledgments of the active writethrough updates to node X via the same connection .

The replica nodes are coupled to node Y by connections and . In one example connections from node Y to replica nodes R R and R are LAN connections within site and connection from node Y to replica node R is an external WAN connection of site . The replica nodes are also illustrated as being coupled to node X by connections . In some embodiments some or all of connections are used to propagate data such as updates to one or more of the replica nodes and corresponding ones of connections and are not used . For example depending on factors such as system speed performance workload and load balancing configurations some or all of the primary connections to the replica nodes are provided by node Y and the remainder of the primary connections if any are provided by node X . In some embodiments the one of node X and node Y that is the standby node provides all or a majority of the primary connections to the replica nodes.

In various embodiments cluster manager is coupled to any or all of the components by management connections including in some embodiments components outside of site . According to various embodiments cluster manager performs functions such as control for system initialization and or recovery system health and or performance monitoring and other similar functions. For example cluster manager is enabled to detect a failure of node X such as by loss of a heartbeat from node X and to initiate system recovery procedures such as by controlling conversion of a role of node Y to active. In some embodiments cluster manager monitors the status of a health monitor running on each of the nodes. In the event that cluster manager detects a failure of a particular one of the nodes a respective set of rules for the failure of the particular node is used by cluster manager to determine and control recovery from the failure.

Clients source updates to node X . In some embodiments updates include part or all of a transaction such as an SQL data manipulation statement optionally in a parsed and or processed form. While updates optionally contain SQL queries that solely read data without modifications such read only accesses are not subject to writethrough. Clients and or other clients not illustrated in also source read only requests to one or more of the nodes including the replica nodes . In some embodiments the replica nodes provide greater capacity and or improved locality to manage the read only requests. In some embodiments a load balancer not illustrated in distributes the read only requests among the nodes.

In an example of normal operation when both node X and node Y are operating one of node X and node Y is an active node and the other is a standby node. As illustrated in node X is the active node and node Y is the standby node. Definition of which of node X and node Y is the active node or the standby node varies for multiple reasons. In a first example in some embodiments after a failure of the active node the roles of the active node and the standby node switch. In a second example in some embodiments node X is the active node with respect to a first set of one or more cache groups and node Y or one of the replica nodes is the active node with respect to a second set of one or more cache groups.

Updates from clients go to the active node node X as illustrated in . Updates are used by the active node to update the respective database of the active node and the updates are written through propagated to the standby node node Y as illustrated in as active writethrough updates . In some embodiments active writethrough updates include other updates received by the active node such as autorefresh updates from database . The active node optionally and or selectively writes through updates and in some embodiments other updates the active node receives to one or more of the replica nodes R R R and R .

Continuing the example of normal operation the standby node node Y as illustrated in receives active writethrough updates . Active writethrough updates are used by the standby node to update a respective database of the standby node and the updates or in some embodiments ones of the updates corresponding to updates are written through to database as standby writethrough updates . The standby node optionally and or selectively writes through active writethrough updates to one or more of the replica nodes R R R and R . In some embodiments the standby node writes through active writethrough updates to all of the replica nodes. In other embodiments the standby node writes through active writethrough updates to any of the replica nodes to which the active node does not write through client updates .

By writing through updates from the active node to the standby node from the standby node to database and from one or both of the active node and the standby node to the replica nodes all of the nodes and database are kept synchronized receiving copies of all of updates . On failure of a node however additional information such as the replication peers table illustrated in is required in some embodiments to maintain and or to restore synchronization.

As illustrated in node X and node Y have similar sub components and structures. Details of the sub components and structures are provided for purposes of example and are not limiting. In other embodiments node X and node Y have different sub components and or structures and support the same or similar connections.

The replica nodes R R R and R as illustrated in are not illustrated in but have in some embodiments a structure similar to that of node X or node Y .

Database includes a respective database and stores tables such as one or more tables . The structure and or sub components of database are in some embodiments similar to those of node X or node Y . In other embodiments database includes a different type of respective database than the respective databases of node X or node Y for example a disk based relational database management system and the structure and or sub components of database is accordingly different from that of node X or node Y . Database is implementable in a variety of ways within the scope of the teachings herein.

Database is accessible from various other components such as from node Y via one or more connections as illustrated by standby writethrough updates communicated via network . According to various embodiments network is any type of network such as a LAN or a WAN or any combination of types of networks such as the Internet.

Clients .J and .K representative of clients as illustrated in provide via network updates to the active node node X as illustrated in . According to various embodiments network is any type of network such as a LAN or a WAN or any combination of types of networks such as the Internet. Additional details are illustrated for client .J to illustrate an example of client operation. Client .J includes application and client libraries . Application represents an application program such as a database application that communicates via client libraries with server process .X in node X to perform updates to database via writethrough to node Y and to database . In some embodiments performing updates via node X provides higher performance than performing the updates directly on database such as when node X includes at least in part a cache of some of the contents of database and or when node X includes an in memory database.

In various embodiments node Y receives accesses such as read only queries via non update accesses . In some embodiments non update accesses are sourced at least in part by clients via network . In other embodiments non update accesses are sourced at least in part by clients other than clients and are provided at least in part by a same or a different network from network . In some embodiments and or usage scenarios non update accesses include updates to data in cache groups for which node Y is the active node.

Each of node X node Y and database includes one or more respective tables table .X table .Y and table . The tables represent contents of the respective database and in some embodiments are used for data accessed by clients .J and .K as well as being used by agents within the nodes such as replication agent .X . In some embodiments on each of at least some of the respective databases one or more of the tables are configured as a cache group. In various embodiments table .X and or table .Y is a subset of the rows and or columns of table .

The respective databases of node X and node Y are accessed by a respective storage manager storage manager .X and storage manager .Y . In some embodiments one or more of the respective databases is an in memory database.

In some embodiments node X and or node Y includes transactions logs .X transactions logs .Y and or checkpoint files .X checkpoint files .Y coupled to storage manager .X storage manager .Y . In further embodiments using transaction logs and or checkpoint files enables data stored in the respective database of node X and or node Y to be persistent despite failures. According to various embodiments transaction logs .X or transactions logs .Y are used for one or more of recovering a transaction if either an application such as one of application and application .X sourcing the transaction or storage manager .X or storage manager .Y fails undoing rolling back transactions replicating changes to other databases and enabling the applications to detect changes to database contents. In some embodiments checkpoint files .X or checkpoint files .Y store a snapshot of the respective database of the active node or the standby node and are used to restore the respective database to a last transactionally consistent state.

Node X and or node Y includes one or more agents to control and or to manage operations of node X of node Y such as cache agent .X cache agent .Y and replication agent .X replication agent .Y . In various embodiments cache agent .X cache agent .Y controls a way in which data is stored by storage manager .X storage manager .Y . For example in some embodiments cache agent .X controls a subset of the contents of database that is resident in tables accessed by storage manager .X.

In some embodiments replication agent .X and replication agent .Y operate cooperatively to control a way in which data is replicated among nodes. In further embodiments the replica nodes R R R and R as illustrated in also include a replication agent to communicate with one or more of replication agent .X and replication agent .Y. Replication agent .X and replication agent .Y communicate via active writethrough to maintain synchronization of the respective databases of node X and node Y .

Optional application .X and optional application .Y represent applications running on the active node and the standby node respectively similar to application running on a separate client .J. In some embodiments application .X and or application .Y source updates similar to updates . In various embodiments application .Y is not present and or is restricted to have read only access to cache groups for which node X is the active node as updates are applied via the active node . In some embodiments application .X has a direct connection to the respective database of node X such as to storage manager .X . In various embodiments the direct connection couples application .X and storage manager .X via a heap of application .X or via a shared memory segment.

In some embodiments one or more of the components of node X and node Y and database are coupled in additional ways not illustrated in . For example in various embodiments cache agent .X and or cache agent .Y are coupled to database to provide bulk loading of data from database via storage manager .X and or storage manager .Y . In various embodiments application .X and or application .Y requests the loading of data from database such as the initial loading of one or more cache groups.

Transaction log includes a number of types of records such as begin record indicating a start of the transaction log update records and indicating updates to tables and commit records and indicating transaction commits of certain ones of preceding update records . In some embodiments transactions logs include other types of records. Of course the number and arrangement of the records is a function of the usage of the database and is one example illustrating some of the types of records.

In some embodiment and or usage scenarios transaction log is purged of records that are no longer necessary for example by garbage collection such as when it is known that the transactions represented by the records have been committed to a backend database system. In further embodiments at least some of the records in transaction log such as the commit records are no longer necessary when written through updates corresponding to the commit records have been acknowledged by all of the ones of the nodes that are configured to receive the written through updates. According to various embodiments commit records in transaction log that are no longer necessary are removed from transaction log by one or more of a replication agent such as replication agent .X as illustrated in a storage manager such as storage manager .X as illustrated in and any combination of the foregoing.

In some embodiments a node has a single transaction log for all transactions performed by the respective database of the node. In other embodiments a node has a plurality of transaction logs such as one transaction log for each cache group.

In some embodiments transaction log is or includes a master update list MUL and or a local update list LUL . In further embodiments a subset of transaction log composed of at least some of the commit records is an MUL or an LUL .

Commit record .X includes log number indicating a position and or a unique identifier in a transaction log commit ticket number for node X CTN.X and zero or more other fields not illustrated in . Commit record .Y includes log number indicating a position and or a unique identifier in a transaction log commit ticket number for node X CTN.X commit ticket number for node Y CTN.Y and zero or more other fields not illustrated in . In some embodiments a format of commit record .X and commit record .Y is identical and commit record .X leaves a CTN.Y field empty.

In some embodiments a commit ticket number CTN is a monotonically increasing number such as a sequence number . In other embodiments a CTN includes a high order portion and a low order portion the high order portion is a timestamp indicating for example when the node assigning the CTN started a respective database and the low order portion is a monotonically increasing number such as a sequence number . By including the timestamp as a high order portion in some embodiments recovery from a failure is simplified because the monotonically increasing numbers in the low order portion are initialized to zero or another fixed starting point and the timestamp ensures that CTNs as a whole are monotonically increasing including before and after the failure .

In some embodiments when a transaction is committed on a node such as node X or node Y of a CTN is assigned for the transaction. For example in some embodiments the CTN is assigned by a storage manager such as storage manager .X as illustrated in when the transaction is committed. The CTN is stored in a commit record corresponding to the transaction in a transaction log such as transaction log as illustrated in . In various embodiments the CTN is an MUL locator or an LUL locator where at least some of the commit records of the transaction log correspond to a master update list or a local update list .

In some embodiments CTNs are propagated as part of and or in conjunction with writethrough updates such as active writethrough updates. In further embodiments updates and the corresponding CTNs propagate through one or more nodes before being written through to a backend database system. In various embodiments all writethrough updates are performed in a same sequence on all of the nodes and on the backend database system.

In more detail a node receiving a writethrough update and a corresponding CTN or a list of corresponding CTNs if there were multiple preceding nodes all performing writethroughs performs the writethrough update assigns a CTN of its own for the writethrough update and stores both the received CTN s and the assigned CTN in a commit record for the writethrough update. The last one in a sequence of nodes receiving the writethrough update propagates the writethrough update as a standby writethrough update to a backend database system. Further both the received CTN s and the assigned CTN of the last node are stored in a table in the backend database system the table having a row for each of the nodes assigning CTNs. Accordingly each node and the backend database system in the flow of writethrough updates maintains knowledge of a sequence of the updates seen and is able to use the CTNs to determine where any given update is in the sequence of updates and on any of the other nodes.

For example when a transaction is committed on an active node such as node X of the active node CTN assigned for the transaction is sent as part of and or in conjunction with update information for the transaction as part of an active writethrough update such as via active writethrough updates of . The active node CTN is also stored in a corresponding commit record in a transaction log on the active node. A standby node such as node Y of receives the update information and performs a corresponding transaction commit. The standby node also assigns a standby node CTN for the transaction. When the transaction is committed on the standby node both the active node CTN and the standby node CTN are stored in the commit record in a transaction log on the standby node. Continuing the example the standby node also stores the active node CTN and the standby node CTN along with corresponding identifiers for the active node and the standby node in a replication peers table in a backend database such as database of . illustrates an example of a replication peers table replication peers table . Thus in some embodiments the CTN information for each node in the path is propagated as part of and or in conjunction with writethrough update information.

In some embodiments maintaining separate CTN assignments on the active node and the standby node simplifies recovery. In other embodiments a single global CTN assignment is performed such as on the active node.

In some embodiments writethrough updates received at one of the nodes are acknowledged by sending an acknowledgment to the active node and or to the standby node containing the CTNs assigned by the node to which the acknowledgement is sent. For example in some embodiments one of the replica nodes receiving a propagated update from the standby node and including a corresponding active node CTN and a corresponding standby node CTN sends an acknowledgement of the update to the active node with the active node CTN and sends an acknowledgement of the update to the standby node with the standby node CTN.

In some embodiments the backend database system is configured to send acknowledgments of updates to the active and or standby nodes with the corresponding CTNs. In other embodiments the standby node is configured to send acknowledgments for the backend database system to the active node based on responses such as indications of commits received from the backend database system. In still other embodiments instead of acknowledgements from or for the backend database system the CTNs that would have been acknowledged are stored in a table in the backend database system such as a replication peers table as illustrated in . Since multiple tables are able to be updated as part of a single transaction such as an atomic transaction the replication peers table is able to provide a consistent and accurate record of the latest update as identified by the CTNs received and applied by the backend database system.

In some embodiments acknowledging the updates using the corresponding CTNs enables a node receiving the acknowledgements to prune its transaction log since any entry that has been acknowledged by all others of the nodes configured to receive the updates is no longer needed for recovery of the others of the nodes. In further embodiments acknowledgements of updates for the backend database system are obtained by accessing such as by querying a replication peers table stored in the backend database system. The replication peers table contains for each of the active node and the standby node a node identifier and the respective CTNs from the active node and the standby node corresponding to a last one of the updates committed by the backend database system.

Continuing the example each of the active node and the standby node records the last acknowledged CTN for each of the replica nodes. If a particular one of the replica nodes fails and is recovered either the active node or the standby node is able to determine from the respective last acknowledged CTN of the particular replica node which ones of the updates the particular replica node has not received and the state of the particular replica node is able to be synchronized by sending the updates between an update corresponding to the respective last acknowledged CTN of the particular replica node and a latest one of the updates. In some embodiments failures of the standby node use a similar recovery mechanism. In various embodiments the synchronization replays a sequence of transactions indicated in the transaction log of for example the active node between the update corresponding to the respective last acknowledged CTN of the particular replica node and the latest update. For example a commit record in the transaction log corresponding to the respective last acknowledged CTN of the particular replica node is located in the transaction log and the associated log number of the commit record is used to find and replay the transactions. In other embodiments a snapshot of a portion of contents of the respective database from for example the active node corresponding to a portions of the respective database modified by the sequence of transactions is sent to the particular replica node to synchronize the particular replica node.

According to various embodiments a replication agent on a particular one of the nodes is enabled to perform one or more of receiving an update from a preceding one of the nodes executing the update such as via a storage manager receiving confirmation such as from a storage manager that the update is committed receiving a CTN for the committed update such as from a storage manager sending an acknowledgement of the committed update to the preceding node and writing through the update to a subsequent one of the nodes. According to various embodiments the acknowledgment includes one or more of a CTN received from the preceding node as part of or along with the update and the CTN assigned by the particular node. According to various embodiments the written through update includes one or more of the CTN received from the preceding node and the CTN assigned by the particular node.

Replication peers table includes a plurality of entries one entry for each of the active and or standby nodes in the system. Each of the entries includes a node identifier and an associated most recently recorded with a corresponding commit of a written through update CTN of the node. According to various embodiments a replication peers table is maintained on one or more of the active node such as node X of the standby node such as node Y of and the backend database system such as database of . Of course as the replication peers table is updated in conjunction with writethrough updates the multiple versions of the replication peers table are not necessarily identical. For example a version of the replication peers table on the standby node has a more recent entry with a more recent CTN than a version of the replication peers table on the backend database system to which a writethrough update including the more recent CTN is in progress .

Maintaining a version of the replication peers table on the backend database system enables any node in the system having an entry in the replication peers table to find a last one of the node s CTNs that made it to the backend database system as well as a latest CTN that made it to the backend database system of other nodes in the system. For example after a failure of the standby node the active node is able to determine from the replication peers table on the backend database system which of the active node s updates were successfully written through to the backend database system and thus which of the active node s updates must be written through again because they were lost when the standby node failed . In some embodiments maintaining a copy of the replication peers table on the standby node enables recovery of the active node in a similar fashion to recovery of the standby node above without a need for the active node to communicate with the backend database system. In further embodiments maintaining a version of the replication peers table on the active node enables more symmetric operation in event of a failure of the standby node.

Rep active standby table includes a plurality of entries one entry for each of the active and or standby nodes in the system. In some embodiments rep active standby table is maintained on the backend database system. Each of the entries includes a respective node identifier identifying a particular one of the nodes an associated role of the particular node and a timestamp indicating when the particular node assumed that role. For example entry includes identifier ID .A role .R and timestamp .T. ID .A serves to identify a particular one of the nodes corresponding to entry . In some embodiments ID .A identifies an agent on the particular node for example a replication agent such as replication agent .X of . Role .R is a field encoding a role of the particular node such as active or standby . In various embodiments timestamp .T is a global timestamp. Use of a global timestamp in some embodiments enables the system to disambiguate multiple ones of the entries having a same role as is possible after one of the nodes has failed. The one of the multiple entries having the latest timestamp is the one that actually has the role.

In the event that the standby node fails the active node takes over operations and or connections of the standby node. As illustrated in propagation of updates to the replica nodes R R R and R is taken over by the active node with connection replacing connection connection replacing connection connection replacing connection and connection replacing connection . Similarly the path to database previously via node Y using active writethrough updates and standby writethrough updates is replaced by node X sending active writethrough updates directly to database . In some embodiments active writethrough updates has a structure and or format more similar to that of standby writethrough updates . For example if database has different interface requirements than node Y such as when database is a different type of database than the respective database of node Y then active writethrough updates matches the interface requirements of database and active writethrough updates matches the interface requirements of node Y .

Recovery from failure of the standby node includes the active node taking over operations and or connections of the standby node as explained above restarting some or all of the standby node as necessary and synchronizing the standby node with the active node specifically synchronizing contents of the respective databases . In some embodiments the synchronizing use techniques such as one or more of techniques explained above with regards to the master update list MUL and MUL locators techniques explained above with regards to transaction logs commit records and commit ticket numbers and techniques explained below with regards to change logs and bookmarks.

In the event that the active node fails the standby node takes over operations and or connections of the failed active node and in some embodiments assumes the active role. Comparing the remaining connections in both are similar and one difference is whether the remaining node is node X or node Y .

As illustrated in updates from clients are redirected to node Y bypassing node X . Standby writethrough updates is replaced by active writethrough updates . While both standby writethrough updates and active writethrough updates are directed to database standby writethrough updates includes information sent by node X such as a commit ticket number of node X for each update that is not present in active writethrough updates .

Recovery from failure of the active node includes the standby node taking over operations and or connections of the active node as explained above restarting some or all of the failed active node as necessary and synchronizing the failed active node with the new active node specifically synchronizing contents of the respective databases . In some embodiments the synchronizing use techniques such as one or more of techniques explained above with regards to the master update list MUL and MUL locators techniques explained above with regards to transaction logs commit records and commit ticket numbers and techniques explained below with regards to change logs and bookmarks.

In some embodiments a system is enabled to update database tables via one or more of database using updates and an active node such as node X using updates . In further embodiments a same database table or in still further embodiments a same cache group is only updatable by one or the other of updates and updates .

In various embodiments database is a same or a similar database to that of node X . In such a case autorefresh is able to use active writethrough to update the respective database of node X and active writethrough is a standby writethrough connection.

In other embodiments database is not able to perform active writethrough. For example in some usage scenarios database is a large commercial database system that does not support active writethrough. In these cases updates applied to database are communicated to node X by autorefresh. All updates received by node X as the active node whether from clients or from database with autorefresh are treated similarly. For example the updates are written through to node Y using active writethrough and are similarly written through by one or more of node X and node Y to the replica nodes R R R and R as illustrated in . One difference however is that updates are not sent to database as standby writethrough updates since updates have already been applied to database . For at least this reason in some embodiments updates sent via active writethrough include an indication of the source clients or database and or the type direct or autorefresh of the update. In addition autorefresh updates received at node X are treated differently than updates from clients in the event of the failure of the active node node X as illustrated in or the standby node node Y as illustrated in . For example in some embodiments a type of master update list called a change log table as illustrated by change log table in is used as part of recovery from a failure of the active node or of the standby node.

In some embodiments connections used in the event of a failure of either of node X or of node Y are similar to those illustrated in and described with regard to . In further embodiments a failure of node X the active node as illustrated in breaks connection autorefresh from database . When node Y the standby node as illustrated in takes over operations and or connections of the failed active node as explained with regard to node Y also takes over the connection to database for autorefresh updates using standby autorefresh .

In some embodiments change log table includes a plurality of entries such as entry entry and entry . Each entry includes a key such as key .K in entry and a bookmark such as bookmark .B. In further embodiments each entry includes other fields such as a table identifier or a cache group identifier. In various embodiments change log table is a table of database . In some embodiments change log table is a type of master update list for updates applied to database with database acting as the master node. Further the bookmarks such as bookmarks .B .B and .B are MUL locators for change log table .

In some embodiments user count table stores a single bookmark .B indicating a last bookmark used to update bookmarks stored in the entries of change log table . In various embodiments user count table is a table of database . An example of using change log table and user count table is described below with regard to .

In some embodiments agent status table includes a plurality of entries such as entry and entry . Each entry includes an agent identifier and a bookmark such as agent identifier .A and bookmark .B in entry . According to various embodiments the agent identifier is one or more of a node identifier and an identifier of an agent running on a node. A particular one of the bookmarks stored in one of the entries of agent status table corresponds to one of the bookmarks in change log table whose associated updates have been already been applied by an agent identified by the agent identifier of the entry to the respective database of the node on which the identified agent runs. In various embodiments agent status table is a table of database .

In some embodiments when selected tables such as tables in cache groups configured to be autorefreshed in database are updated a respective entry is added to change log table for each updated row the respective entry storing the respective key of the updated row and having an empty bookmark. In further embodiments the entries are added by a respective trigger on each of the selected tables the respective trigger activating when the selected table is updated. In still further embodiments the use of triggers enables the adding of a particular one of the entries to be part of a same transaction as used to update one of the selected tables. According to various embodiments change log table is one or more of global for all tables and stores a table identifier global for all cache groups and stores a cache group identifier specific to a cache group with zero or more other change log tables for other cache groups and specific to a table with zero or more other change log tables for other tables. In some embodiments and or usage scenarios a same key is stored in multiple entries in change log table such as when a same row of a same table is updated multiple times.

As illustrated in change log table includes entries having corresponding keys KEY KEY . . . KEY. Further entries and have corresponding bookmarks and . User count table stores bookmark the largest bookmark applied so far to change log table . Entries with an empty bookmark such as entries and correspond to entries added to change log table that have not yet had bookmarks applied.

In some embodiments there is no determined order to the entries of change log table . For example in some embodiments entries are added in response to triggers such as update triggers when selected tables are updated as part of transactions but the updates including the entries added to change log table are not visible outside of the transaction until the transaction is committed. Hence a later added one of the entries is in some usage scenarios visible prior to an earlier added entry.

In some embodiments bookmarks are applied to change log table by an agent such as a cache agent for example cache agent .X as illustrated in . The cache agent sends a bookmark transaction such as in the form of an SQL statement to database . The bookmark transaction is configured to perform a sequence of operations including determining entries of change log table that have an empty bookmark accessing user count table to determine a last used bookmark incrementing the last used bookmark applying the incremented bookmark to all of the determined entries of change log table and storing the incremented bookmark in user count table . In further embodiments the bookmark transaction is atomic and only one node and or one agent is enabled to apply bookmarks to change log table at a time. For example a lock such as a semaphore is used to control access to change log table and user count table for the purpose of applying bookmarks. In some embodiments multiple cache agents such as a cache agent on an active node and a cache agent on a standby node such as cache agents .X and .Y as illustrated in periodically apply bookmarks to change log table . In some embodiments having more than one cache agent on more than one node apply bookmarks ensures that failure of a single node does not stop the application of bookmarks to change log table .

Applying bookmarks to change log table and using user count table as illustrated in yields change log table and user count table as illustrated in . As illustrated in the bookmark stored in user count table has been incremented and the entries of change log table as illustrated in with empty bookmarks have been filled with the new last used bookmark stored in user count table .

Of course in other embodiments bookmarks are other than sequential numbers. For example in various embodiments bookmarks are one or more of sequentially increasing numbers monotonically increasing numbers sequentially decreasing numbers monotonically decreasing numbers and any other ordered sequence of numbers or symbols.

In some embodiments where backend database system is enabled to use commit triggers bookmarks are applied directly to the entries of change log table as the entries are added. In these embodiments the transitory state illustrated in where some of the bookmark numbers are empty does not exist and there is no need to separately apply bookmarks to already added entries as described above.

Returning to in some embodiments updates previously applied to database are autorefreshed to node X or another node such as another active node using information from change log table to determine which of the updates on database are not present on node X . In some embodiments autorefresh of a node is performed by a cache agent running on that node such as cache agent .X as illustrated in . In these embodiments the cache agent is a source of updates similar to a client such as clients as illustrated in or to an application such as application .X as illustrated in .

Node X or one or more agents running on node X maintains a last applied bookmark such as in a table similar to user count table . The last applied bookmark corresponds to a bookmark value in change log table such that updates to database associated such as by the respective key with entries of change log table having bookmark numbers less than or equal to the last applied bookmark have already been applied to the respective database of node X .

In some embodiments autorefresh of the respective database of node X from updates applied to database is performed at least in part as an autorefresh transaction sent to database to retrieve data autorefresh updates associated with keys of determined entries of change log table where the determined entries are a subset of the entries having a bookmark number greater than the last applied bookmark number. In some embodiments the determined entries include ones of the entries having an empty not yet applied in change log table bookmark number as the entries having an empty bookmark number correspond to completed transactions and excluding them produces an inconsistent view of a current state of database . In further embodiments a particular one of the entries having an empty bookmark number is considered in more than one autorefresh update a first autorefresh update when the particular entry has an empty bookmark number and a second autorefresh update after a bookmark has been applied to the particular entry.

In some embodiments the autorefresh transaction is a join that selects entries from change log table having a bookmark number greater than the last applied bookmark number and additionally in further embodiments having an empty bookmark number and then uses keys of the selected entries to return rows of tables or cache groups associated with change log table and having the selected keys. Continuing the example the autorefresh transaction is also configured to return one or more of the bookmark or the largest bookmark associated with each of the keys the largest bookmark in change log table and the bookmark stored in user count table . In various embodiments and or usage scenarios multiple entries in change log table selectively have a same key and the transaction returns a latest value of rows of the tables or the cache groups associated with change log table and having the selected same key. In other words data retrieved from database in this fashion reflects a latest contents e.g. a snapshot of database and some intermediate states of database are never autorefreshed to and thus never present in node X .

In some embodiments node X performs autorefresh as a sequence of operations performed at least in part on node X using the respective database of node X and database . In various embodiments the sequence of operations is a transaction. In further embodiments the sequence of operations is atomic. The sequence of operations includes determining a last applied bookmark sending an autorefresh transaction to database to retrieve autorefresh updates such as from recent updates to database according to the last applied bookmark applying such as by updating the autorefresh updates to the respective database of node X and setting the last applied bookmark to a largest bookmark number returned by the autorefresh transaction. In further embodiments the sequence of operations also includes updating an entry in agent status table with the last applied bookmark number. In various embodiments after the sequence of operations or as a part of the sequence of operations the autorefresh updates are written through from node X to node Y for example from an active node to a standby node and optionally and or selectively to one or more replica nodes such as replica nodes R R R and R as illustrated in . The written through autorefresh updates include the largest bookmark number returned by the autorefresh transaction.

A node such as node Y or in some embodiments such as a replica node receiving written through autorefresh updates along with a largest bookmark number associated with the updates performs a sequence of operations similar to that described above for node X with one difference being the source of the updates and accordingly no need to send an autorefresh transaction to retrieve the updates . For example in some embodiments a node receiving a written through autorefresh updates along with a largest bookmark number associated with the updates performs a sequence of operations such as applying such as by updating the written through autorefresh updates to the respective database of the node setting a last applied bookmark to the largest bookmark number and updating an entry in agent status table with the last applied bookmark number. In further embodiments the sequence of operations on a standby node such as node Y also includes writing through the written through autorefresh updates optionally and or selectively to one or more replica nodes such as replica nodes R R R and R as illustrated in .

In some embodiments replica nodes do not update an entry in agent status table when applying a written through autorefresh updates. In such embodiments a particular one of the replica nodes is synchronized after a failure of the particular replica node with the active node and or the standby node using techniques similar to those used with active writethrough. For example in some embodiments the written through autorefresh updates have commit ticket numbers and for the replica nodes are treated similarly to client updates such as updates as illustrated in . In other embodiments the replica nodes use techniques similar to those described below for the active node and the standby node to recover from a failure.

In some embodiments agent status table as illustrated in is used at least in part to provide information for garbage collecting removing no longer needed entries from change log table . Since agent status table is updated by each of the nodes with a largest bookmark number corresponding to autorefresh updates applied at the node the minimum bookmark number stored in agent status table corresponds to a minimum bookmark number that has been applied at all of the nodes at least at all of the nodes updating agent status table .

In some embodiments a change log table garbage collection process runs on one or more of the nodes such as on the active node and or on the standby node. Running the change log table garbage collection process on more than one node enables change log table garbage collection to continue in the event of failure of a single node.

In some embodiments the change log table garbage collection performs a sequence of operations using database . In various embodiments the sequence of operations is a transaction. In further embodiments the sequence of operations is atomic. The sequence of operations includes determining a minimum bookmark number stored in entries of agent status table and removing any entries in change log table having a bookmark number less than or equal to the minimum bookmark number.

In some embodiments recovery from failure of either the active node such as node X as illustrated in or the standby node such as node Y as illustrated in is similar to the techniques described above with regard to . One difference is in synchronization of autorefresh updates for which database is the master node. If one of the active node or the standby node fails the failing node is enabled to synchronize with latest ones of the autorefresh updates by performing a sequence of operations. In various embodiments the sequence of operations is a transaction. In further embodiments the sequence of operations is atomic. The sequence of operations includes determining a last applied bookmark number of the failed node performing a transaction similar to an autorefresh transaction to retrieve autorefresh updates associated with keys of determined entries of change log table where the determined entries are a subset of the entries having a bookmark number greater than the last applied bookmark number and additionally in further embodiments having an empty bookmark number applying such as by updating the autorefresh updates to the respective database of the failed node setting the last applied bookmark to a largest bookmark number returned by the transaction and in some embodiments updating an entry in agent status table with the last applied bookmark number. According to various embodiments the last applied bookmark number of the failed node is one or more of stored in a table on the failed node and stored in an entry corresponding to the failed node or to an agent of the failed node of agent status table .

In some embodiments operations of the processes in A and B other than for START and END indications are unless otherwise stated performed and or initiated by one or more of a user such as an operator or administrator of a system or by a cluster manager such as cluster manager as illustrated in . In further embodiments operations performed by the cluster manager are initiated by a user and or are a result of a script or other program run on the cluster manager. For example in some embodiments detecting the failure of the active node DETECT FAILURE OF ACTIVE as illustrated in is performed by the user such as by detecting a lack of a response from the active node while in other embodiments detecting the failure of the active node is performed by the cluster manager such as by detecting a lack of a heartbeat from the active node .

In A and B some state transitions are illustrated for a respective replication state of the active node and the standby node for example as illustrated in SET REP STATE TO ACTIVE . In some embodiments the replication state of a node indicates a current role of the node and is one of idle standby and active. In further embodiments a default value of the replication state of a node such as after the node is recovered restarted and or initialized is idle. Initializations of the replication state of a node to idle after the node is recovered restarted and or initialized are not illustrated in A and B. In various embodiments the replication state of a node assumes other values including one or more of failed such as when a failure of the node is detected and recovering such as when the node is in the process of recovering from a failure . In some embodiments the replication state of a node is global for the node while in other embodiments the replication state is per cache group or per table.

In various embodiments the replication state of a node is controllable at least in part by a user and or by a cluster manager. For example in some embodiments a user is able to provide a command such as via a console or via a web browser interface to the cluster manager to change the replication state of the active node from active to idle. In various embodiments not all state transitions of the replication state are directly controllable. For example in some embodiments and or usage scenarios the state transition of the replication state of a node from idle to active is able to be initiated by a user and is performed using a process similar to parts of process as illustrated in .

In some embodiments the active node and the standby node have a respective autorefresh state selected from one of off paused and on. In some embodiments the respective autorefresh state of a node is global for the node while in other embodiments the autorefresh state is controllable per cache group. The autorefresh state indicates whether the node or in various embodiments whether a particular cache group of the node is participating in autorefresh. A node with an autorefresh state of on for a particular cache group is able to control and or to receive autorefresh updates for the particular cache group from the backend database system. A node with an autorefresh state of paused for a particular cache group is a standby for a corresponding node with an autorefresh state of on for the particular cache group . In further embodiments only one node or only one node for each cache group is able to have an autorefresh state of on. In some embodiments a default value of the autorefresh state of a node or each cache group of the node such as after the node is recovered restarted and or initialized is off. Initializations of the autorefresh state of a node or each cache group of the node to off after the node is recovered restarted and or initialized are not illustrated in A and B.

In various embodiments the autorefresh state of a node or each cache group of the node is controllable at least in part by a user and or by a cluster manager. For example in some embodiments a user is able to provide a command such as via a console or via a web browser interface to the cluster manager to change the autorefresh state of the active node or of selected cache groups of the active node from on to off. In various embodiments not all state transitions of the autorefresh state are directly controllable. For example in some embodiments the state transition of the autorefresh state of a node or of selected cache groups of the active node from off to on is able to be initiated by a user and is performed using a process similar to parts of process as illustrated in . In various embodiments the autorefresh state of a node or of a particular cache group of the node is related to the replication state of the node as illustrated by the table below. In some embodiments transitions of the replication state of a node affect the autorefresh state of the node or of the particular cache group . For example a node having a replication state transition from idle to active also transitions a autorefresh state of the node or of the particular cache group from paused to on as illustrated in the first group of three rows in the table below . In various embodiments the autorefresh state of the node or of the particular cache group is off unless the node or the particular cache group is configured for autorefresh.

With reference to the table above transitions from replication states of active or standby to idle occur in some embodiments and or usage scenarios as part of recovery from a failure or in response to a request such as from a user and or a cluster manager. Transitions from replication states of idle to active occur in some embodiments and or usage scenarios when a node becomes the active node such as when the standby node becomes the active node after failure of the active node. Transitions from replication states of idle to standby occur in some embodiments and or usage scenarios when the standby node has synchronized with the active node such as illustrated in .

With regard to A and B some of the transitions of the respective autorefresh state of the active node and the standby node or of cache groups on the active node and the standby node are illustrated such as SET ARF STATE TO PAUSED FOR ALL ARF CACHE GROUPS as illustrated in and at least some of others of the transitions of the respective autorefresh state are in some embodiments given by the table above.

Sub process illustrated in is performed on the standby node such as node Y as illustrated in . Sub process performs duplication of state from the active node such as node X as illustrated in and initialization of some agents of the standby node.

Sub process starts START and begins by duplicating state from the active node to the standby node INITIATE DUPLICATION FROM ACTIVE . According to various embodiments the duplicated state includes one or more of configuration of the respective database of the active node configuration of objects configuration of cache groups replication schemes contents of the respective database of the active node and other state and or control information of the active node. In some embodiments a garbage collector thread of a cache agent is blocked to prevent the garbage collector thread form purging entries in a change log table such as change log table as illustrated in during duplication.

Next a replication policy is optionally and or selectively set SET REPLICATION POLICY and a replication agent is started START REPLICATION AGENT . In some embodiments a default replication policy is used if one is not explicitly set. According to various embodiments a replication agent is configurable via the replication policy for one or more of automatic or manual restart specification of cache groups or tables that are replicated a heartbeat interval a frequency of garbage collecting and other configuration parameters. In various embodiments prior to starting the replication agent a storage manager such as storage manager .Y as illustrated in initializes a replication state of the standby node to idle. The replication agent in some embodiments includes a transmitter thread such as for writing through updates and or for transmitting acknowledgments and a receiver thread such as for receiving written through updates and or acknowledgments .

Next a cache policy is optionally and or selectively set SET CACHE POLICY and a cache agent is started START CACHE AGENT . In some embodiments a default cache policy is used if one is not explicitly set. According to various embodiments a cache agent is configurable via the cache policy for one or more of automatic or manual restart specification of cache groups or tables that are cached a heartbeat interval cache size parameters cache aging parameters and other configuration parameters.

According to various embodiments a cache agent on a particular one of the nodes includes one or more threads such as one or more of a marker thread to apply bookmarks to a change log table such as change log table as illustrated in a refresher thread to control autorefresh of the respective database of the particular node a reporter thread to update an agent status table such as agent status table as illustrated in with a last applied bookmark number and a garbage collector thread to garbage collect the change log table. In further embodiments some or all of these threads such as the refresher thread run solely on the cache agent on the active node.

Next if autorefresh is enabled globally or for any cache groups bookmarks are published PUBLISH BOOKMARKS TO DB to the backend database system such as database as illustrated in . In some embodiments publishing the bookmarks includes updating an agent status table such as agent status table as illustrated in with a last applied bookmark number as described with reference to B A and B . In various embodiments the bookmarks are published by the cache agent or by a reporter thread of the cache agent. In some embodiments the last applied bookmark number is provided by the active node along with corresponding updates as part of duplicating state from the active node to the standby node INITIATE DUPLICATION FROM ACTIVE .

Sub process illustrated in is performed on the standby node such as node Y as illustrated in . Sub process performs synchronization of the standby node with the active node such as node X as illustrated in and ends with the standby node assuming the standby role and taking over certain replication tasks from the active node.

Sub process starts START and begins by synchronizing state SYNCHRONIZE WITH ACTIVE between the active node and the standby node. According to various embodiments the synchronizing includes one or more of verifying that the standby node has completed duplicating state from the active node and or has completed recovering contents of the respective database of the standby node using local logs such as illustrated by RECOVER STANDBY FROM LOCAL LOGS in using recovery techniques such as one or more of the techniques explained above with regards to the master update list MUL and MUL locators with regards to transaction logs commit records and commit ticket numbers and with regards to change logs and bookmarks and other procedures to ensure that the standby node is ready to assume the standby role.

Next a respective replication state of the standby node is set to standby SET REP STATE TO STANDBY . In some embodiments the replication state is set by a replication agent after the standby node is synchronized with the active node SYNCHRONIZE WITH ACTIVE .

In some embodiments setting the replication state of the standby node to standby enables the standby node or in various embodiments a replication agent of the standby node such as replication agent .Y of to start and or to take over certain tasks START OR TAKE OVER REPLICATION TO DB AND REPLICA NODES . When sub process is performed at initialization of the system such as with regard to the standby node is now able to start the certain tasks and or to take over at least some of the certain tasks from the active node in the event that the active node has already started the at least some of the certain tasks . When sub process is performed as part of recovery from a failure the standby node is now able to take over the certain tasks from the active node. In various embodiments the certain tasks include one or more of replication write through to the replica nodes such as replica nodes R R R and R as illustrated in replication write through to the backend database system such as via standby writethrough as illustrated in and other tasks performed by the standby node. The standby node then assumes the standby role and is operational as a standby for the active node.

Process as illustrated in is initiated START on the active node such as node X as illustrated in and portions ON ACTIVE of process are performed on the active node. Further portions ON STANDBY and ON REPLICA of process are performed on the standby node such as node Y as illustrated in and optionally on one or more of the replica nodes such as replica nodes R R R and R as illustrated in .

The portion of process performed on the active node ON ACTIVE begins by initializing the respective database of the active node such as by creating the database associated objects and cache groups and other similar initializations CREATE DB DB OBJECTS CACHE GROUPS . In some embodiments for each of the cache groups that is configured for autorefreshing if any an autorefresh state of the cache group is optionally set to paused SET ARF STATE TO PAUSED FOR ALL ARF CACHE GROUPS . In various embodiments the initial creation time autorefresh state of a cache group is configurable. For example some cache groups are created with an initial autorefresh state of off.

Next a replication scheme is created CREATE REPLICATION SCHEME . In some embodiments a replication scheme describes such as by hostname the ones of the nodes that are involved in the active standby replication. In further embodiments the replication scheme also describes the replica nodes involved in the replication.

Next a respective replication state of the active node is set to active SET REP STATE TO ACTIVE . The active node then assumes the active role. In some embodiments after setting the replication state to active updates are allowed to the respective database of the active node but the updates are not replicated such as to the standby node or to the backend database system until after a replication agent is started on the active node START REPLICATION AGENT . In further embodiments the updates are replicated to the standby node as part of synchronizing the standby node with the active node.

Next a cache policy is set SET CACHE POLICY and a cache agent is started START CACHE AGENT . In various embodiments these operations are similar to those performed with respect to a cache agent on the standby node as described with regard to SET CACHE POLICY and START CACHE AGENT .

Next cache groups of the respective database of the active node are optionally loaded LOAD CACHE GROUPS . In various embodiments cache groups are configured to load initially such as in bulk and or to load dynamically such as on demand . Further in some embodiments cache groups are loadable under user control after a replication agent is started after completing START REPLICATION AGENT . In various embodiments loading a cache group on the active node and in further embodiments when the replication state of the active node is on automatically changes the autorefresh state of the cache group from paused to on.

Next a replication policy is set SET REPLICATION POLICY and a replication agent is started START REPLICATION AGENT . In various embodiments these operations are similar to those performed with respect to a replication agent on the standby node as described with regard to SET REPLICATION POLICY and START REPLICATION AGENT . In some embodiments after the replication agent is started updates are replicated from the active node to the backend database system.

The portion of process performed on the standby node ON STANDBY begins by performing sub process as illustrated in STANDBY DUPLICATION FROM ACTIVE AND AGENT INITIALIZATION . Next sub process as illustrated in STANDBY SYNCHRONIZATION AND START REPLICATION is performed. After performing these sub processes the standby node is synchronized with the active node and has assumed the standby role.

Process then optionally continues on one or more of the replica nodes and the portion of process on the standby node ends END .

The portion of process performed on a particular one of the one or more replica nodes ON REPLICA begins by duplicating state from the standby node to the particular replica node INITIATE DUPLICATION FROM STANDBY . In various embodiments this operation is similar to that performed with respect to duplicating from the active node to the standby node as described with regard to INITIATE DUPLICATION FROM ACTIVE .

Next a replication policy is set SET REPLICATION POLICY and a replication agent is started START REPLICATION AGENT . In various embodiments these operations are similar to those performed with respect to a replication agent on the standby node as described with regard to SET REPLICATION POLICY and START REPLICATION AGENT .

After performing these operations the particular replica node is synchronized with the standby node and is able to receive and to process client queries.

Process as illustrated in is initiated START and begins by detecting a failure of the active node DETECT FAILURE OF ACTIVE . In some embodiments the failure of the active node such as node X as illustrated in is detected by a user and or by a cluster manager such as cluster manager as illustrated in . Since failure of the active node in some embodiments causes roles of the active node and the standby node such as node Y as illustrated in to switch illustrates portions of process that are performed on the active node ON ACTIVE meaning that these portions are performed on the failed active node which becomes the standby node and illustrates portions of process that are performed on the standby node ON STANDBY meaning that these portions are performed on the new active node which was the standby node .

The portion of process performed on the standby node ON STANDBY begins by declaring the failure of the active node DECLARE FAILURE OF ACTIVE and by changing the replication state of the standby node to active SET REP STATE TO ACTIVE . According to various embodiments declaring failure of the active node includes one or more of state changes on the standby node state changes in other portions of the system such as in cluster manager as illustrated in and reporting of the failure of the active node. The standby node then assumes the active role. In some embodiments after setting the replication state to active updates are allowed to the respective database of the standby node but the updates are not replicated to the active node until after a replication agent is started on the active node as part of STANDBY DUPLICATION FROM ACTIVE AND AGENT INITIALIZATION which performs sub process of . In further embodiments the updates are replicated as part of synchronizing the active node with the standby node. In some embodiments updates optionally begin to be replicated from the standby node to the backend database system at this point.

Next the standby node is able to take over various activities of the active node TAKE OVER CLIENT UPDATES TAKE OVER AUTOREFRESH FROM DB and TAKE OVER ANY REPLICATION DONE BY ACTIVE . Client updates if any such as updates as illustrated in are redirected to the standby node as illustrated in . If autorefresh is configured for the standby node or for any cache groups of the standby node the standby node or in some embodiments a cache agent of the standby node such as cache agent .Y as illustrated in takes over responsibility for autorefresh from the backend database system. If the active node was replicating such as by writing through updates to any of the replica nodes such as replica nodes R R R and R as illustrated in the standby node or in some embodiments a replication agent of the standby node such as replication agent .Y as illustrated in takes over the replication.

In some embodiments the portion of process performed on the active node ON ACTIVE begins in response to detecting a failure of the active node DETECT FAILURE OF ACTIVE . In other embodiments the portion of process performed on the active node begins in response to and or under control of one or more of the standby node a user and the cluster manager. In various embodiments some operations of the portion of process performed on the active node are gated by operations on the standby node. For example in some embodiments duplication from the standby node STANDBY DUPLICATION FROM ACTIVE AND AGENT INITIALIZATION is gated by the standby node setting the replication state of the standby node to active SET REP STATE TO ACTIVE . In some embodiments the portion of process performed on the active node begins after the standby node has completed the portion of process performed on the standby node after completing TAKE OVER ANY REPLICATION DONE BY ACTIVE .

Operations to recover the failed active active node begin by optionally and or selectively recovering processing processes or other state of the active node RECOVER RESTART ACTIVE . In a first example if the active node crashed the active node is rebooted. In a second example if a process on the active node failed the process is restarted. Then if there are still agents running on the active node for example if the active node did not crash the agents are stopped STOP AGENTS .

Process then performs sub process as illustrated in STANDBY DUPLICATION FROM ACTIVE AND AGENT INITIALIZATION . Next sub process as illustrated in STANDBY SYNCHRONIZATION AND START REPLICATION is performed. After performing these sub processes the active node is synchronized with the standby node and has assumed the standby role.

In some embodiments such as some embodiments where the active node has not crashed rather than stopping agents and using sub process of the active node recovers state by recovering the respective database of the active node. In various embodiments the active node uses operations similar to those illustrated in to recover the respective database of the active node from local logs and to verify that agents are active and or have been restarted such as RECOVER STANDBY FROM LOCAL LOGS and VERIFY AGENTS RESTARTED as illustrated in . In these embodiments the active node still performs sub process as illustrated in STANDBY SYNCHRONIZATION AND START REPLICATION .

Process as illustrated in is initiated START and begins by detecting a failure of the standby node DETECT FAILURE OF STANDBY . In some embodiments the failure of the standby node such as node Y as illustrated in is detected by a user and or by a cluster manager such as cluster manager as illustrated in .

The portion of process performed on the active node ON ACTIVE begins by declaring the failure of the standby node DECLARE FAILURE OF STANDBY . According to various embodiments declaring failure of the standby node includes one or more of state changes on the active node state changes in other portions of the system such as in cluster manager as illustrated in and reporting of the failure of the standby node.

Next the active node or in some embodiments a replication agent of the active node such as replication agent .X as illustrated in is able to take over various activities of the standby node TAKE OVER REPLICATION TO DB AND REPLICA NODES . If the standby node was replicating such as by writing through updates to any of the replica nodes such as replica nodes R R R and R as illustrated in then in some embodiments the active node or in various embodiments a replication agent of the active node such as replication agent .X as illustrated in takes over the replication. If the active node is receiving client updates such as updates as illustrated in and was writing through the updates via active writethrough as illustrated in to the standby node which in wrote through the updates to the backend database system via standby writethrough updates as illustrated in then in some embodiments the active node or in various embodiments a replication agent of the active node takes over the writing through to the backend database system such as via active writethrough as illustrated in .

In some embodiments the portion of process performed on the standby node ON STANDBY begins in response to detecting a failure of the standby node DETECT FAILURE OF STANDBY . In other embodiments the portion of process performed on the standby node begins in response to and or under control of one or more of the active node a user and the cluster manager. In various embodiments some operations of the portion of process performed on the standby node are gated by operations on the active node. For example in some embodiments the standby is not able to resume replication STANDBY SYNCHRONIZATION AND TAKE OVER REPLICATION if the active node is still in the process of taking over for the failed standby TAKE OVER REPLICATION TO DB AND REPLICA NODES . In some embodiments the portion of process performed on the standby node begins after the active node has completed the portion of process performed on the active node after completing TAKE OVER REPLICATION TO DB AND REPLICA NODES .

Operations to recover the failed standby node begin by optionally and or selectively recovering processing processes or other state of the standby node RECOVER RESTART ACTIVE . In a first example if the standby node crashed the standby node is rebooted. In a second example if a process on the standby node failed the process is restarted. The standby node then optionally and or selectively uses one of two different paths to restore the respective database of the standby node.

In the first path agents still running on the standby node if any are stopped STOP AGENTS . Then sub process as illustrated in STANDBY DUPLICATION FROM ACTIVE AND AGENT INITIALIZATION is performed.

In the second path the respective database of the standby node is recovered RECOVER STANDBY FROM LOCAL LOGS using local logs such as transaction logs .Y and or checkpoint files .Y as illustrated in . In various embodiments the recovery is performed at least in part by a storage manager of the standby node such as storage manager .Y as illustrated in . In further embodiments the storage manager initializes a replication state of the standby node to idle. Then operations are performed to verify that agents on the standby node are active and or have been restarted and agents are restarted as necessary VERIFY AGENTS RESTARTED .

After using one of the two paths to restore the respective database of the standby node sub process as illustrated in STANDBY SYNCHRONIZATION AND START REPLICATION is performed. After performing this sub process the standby node is synchronized with the active node and has assumed the standby role.

While the embodiments described above have been explained with an active and a standby node other embodiments with one or more active nodes and or one or more standby nodes are within the scope of the teachings herein. For example data structures such as commit records are extendable to have additional fields for recording commit information of additional nodes.

While the embodiments described above have been explained with regard to one respective database on each of the nodes other embodiments with one or more respective databases on each of the nodes are within the scope of the teachings herein.

Embodiments of the system described above are enabled to use to perform some or all of the operations described above one or more of dedicated processor systems micro controllers programmable logic devices microprocessors and any combination thereof. According to various embodiments some or all of the operations described above are implemented in one or more of software firmware microcode dedicated hardware and any combination thereof.

For the sake of convenience the operations are described as various interconnected functional blocks and or distinct software modules. This is not necessary however and there are cases where one or more of these functional blocks or modules are equivalently aggregated into a single logic device program and or operation with unclear boundaries. In any event the functional blocks and or software modules are implementable by themselves or in combination with other operations in either hardware and or software.

Having described and illustrated the principles of the invention in representative embodiments thereof it should be apparent that the invention may be modified in arrangement and detail without departing from such principles. We claim all modifications and variation coming within the spirit and scope of the following claims.

