---

title: Software factory semantic reconciliation of data models for work packets
abstract: A method, system, and computer-readable medium is presented for maintaining and supporting a semantic reconciliation of canonical data model exchange formats in support of software factory workflow management, state determination governing model transformations, and tool enabled processes across software development roles and methodologies used by a software factory.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08418126&OS=08418126&RS=08418126
owner: International Business Machines Corporation
number: 08418126
owner_city: Armonk
owner_country: US
publication_date: 20080723
---
The present disclosure relates in general to the field of computers and more particularly to the use of computer software. Still more particularly the present disclosure relates to the creation of semi custom software through the use of a standardized software factory.

Software can be classified as being in one of two main categories off the shelf and custom. As the name implies off the shelf software is pre developed software that has little if any flexibility. Thus the customer must tailor her activities to conform to the software. While such software is initially inexpensive compared to custom software long term costs in time and money for software implementation training business process alterations etc. can be onerous in an enterprise environment. Custom software as the name implies is custom built software that is tailored to existing or planned activities of the customer.

Today software development and particularly custom software development is perceived as more of an art than a science. This is particularly true for custom software that is being created by a third party for an enterprise customer. That is a developer must rely on her experience training intuition and communication skills to create software that is both unique and reliable. This often leads to software of varying degrees of reliability usefulness and value to the customer.

A method system and computer readable medium is presented for maintaining and supporting a semantic reconciliation of canonical data model exchange formats in support of software factory workflow management state determination governing model transformations and tool enabled processes across software development roles and methodologies used by a software factory.

The above as well as additional purposes features and advantages of the present invention will become apparent in the following detailed written description.

Presented herein is a software factory which includes a collection of business and Information Technology IT governance models operational models delivery methods metrics environment and tools bundled together to improve the quality of delivered software systems control cost overruns and effect timely delivery of such systems. The software factory described herein offers a practical solution to developing software systems using multiple sites that are geographically distributed. The issues of varying timezones and the hand over between various teams residing in such timezones are handled by exchanging work packets. A work packet is a self contained work unit that is composed of processes roles activities applications and the necessary input parameters that allow a team to conduct a development activity in a formalized manner with visibility to progress of their effort afforded to the requesting teams.

The novel software factory described herein is a uniquely engineered scalable efficiency model construct that transforms a traditional software development art form into a repeatable scientific managed engineered streamline information supply chain. The software factory incorporates applied system and industrial engineering quality assured efficiencies that provide for the waste eliminating highly optimized performed instrumentation measured monitoring and risk mitigated management of software development.

With reference now to the figures and in particular to an overview of a preferred embodiment of a software factory is presented. As depicted the software factory is a service that interacts with both enterprise customers i.e. client customers as well as enterprise partners i.e. third party vendors . The primary human interface with the enterprise customers is through a Client Business Governance Board CBGB . CBGB represents client stakeholders and client business sponsors that fund a project of the software factory . CBGB can be an internal or external client. That is the same enterprise i.e. internal client may include both CBGB and software factory or a first enterprise i.e. external client may have CBGB while a second enterprise has the software factory . As described in greater detail below a project proposal definition is then run through a software factory induction process in a Software Factory Governance Board SFGB and Software Factory Operations SFO where the project proposal definition is evaluated qualified scored and categorized. The project proposal definition is then subject to a System Engineering Conceptual Requirements Review by the SFGB . Based on the outcome of the review by the SFGB a decision is made to accept the project proposal definition or to send it back to the CBGB for remediation and resubmission through the Software Factory Induction Process.

Thus Software Factory Governance which includes SFGB and SFO provides the guidance constraints and underlying enforcement of all the factory policies and procedures in support of their governing principles in support of the strategic objects of the Software Factory . Software Factory governance consists of factory business IT and operations governance. The principles policies and procedures of these models are carried out by two governing bodies the Business Governance Board and the IT Governance Board both part of SFGB and an enforcement body the Software Factory Operations .

As soon as a project is deemed worthy to proceed the job of creating the custom software is sent to a Design Center where the project is broken into major functional areas including those handled by a Requirements Analysis Team and an Architectural Team .

The Requirements Analysis Team handles the Requirement Management side of the Design Center and is responsible for collecting the business requirements from the lines of business and populating these requirements into the tools. Analysis of business requirements is also carried out in order to derive associated IT requirements. Some requirements e.g. system requirements may have a contractual constraint to use a certain infrastructure. Requirements are analyzed and used in the basis for business modeling. These requirements and representative business contextual event and process models are then verified with and signed off from project stakeholders. Requirements are then base lined and managed within release and version control.

The Architectural Side of the Design Center is handled by the Architecture Team which takes the output of the requirement analysis management side of the design center and uses architectural decision factors functional requirements non functional requirements available technology and constraints to model a design with appropriate example representation into detail design specification that is bundled with other pertinent factors into a work packet for assembly centers to execute.

Work Packets are reusable self contained discrete units of software code that constitute a contractual agreement that governs the relationship among Design Center Software Factory Governance Board Software Factory Operations and Assembly Center . That is each work packet includes governance policies and procedures e.g. including instructions for how work reports are generated and communicated to the client standards e.g. protocol for the work packet reused assets e.g. reusable blocks of code including the requirements instructions and or links pointers associated with those reusable blocks of code work packet instructions e.g. instructions for executing the work packet integration strategy e.g. how to integrate the work packet into a client s security system schedule e.g. when deliverables are delivered to the client exit criteria e.g. a checklist for returning the work packet and or deliverables to the software factory and Input Output I O work products e.g. artifact checklist templates for I O routines . A deliverable is defined as a unit of software that is in condition for delivery to and or execution on behalf of a customer or client. Thus in the context of the present invention a deliverable is defined as an output product of the software factory that is described herein.

Assembly Center s Job Shop s receive and execute the work packets which are specified by the Design Center to create a customized deliverable . A deliverable is defined as a unit of software that is in condition for delivery to and or execution on behalf of a customer or client. This in the context of the present invention a deliverable is defined as an output product of the software factory that is described herein. As shown in exemplary manner the assembly center puts the work packets into a selected low level design to generate a deliverable executable product . While assembly center can be a manual operation in which a coding person assembles and tests work packets in another embodiment this process is automated using software that recognizes project types and automatically assembles work packets needed for a recognized project type

Various tests can be performed in the assembly center including code unit tests integration test system test system integration test and performance test. Code unit test tests the deliverable for stand alone bugs. Integration test tests the deliverable for compatibility with the client s system. System test checks the client s system to ensure that it is operating properly. System integration test tests for bugs that may arise when the deliverable is integrated into the client s system. Performance test tests the deliverable as it is executing in the client s system. Note that if the deliverable is being executed on a service provider s system then all tests described are obviously performed on the service provider s system rather than the client s system.

A User Acceptance Test Team includes a client stakeholder that is charged with the responsibility of approving acceptance of deliverable .

Software factory may utilize enterprise partners to provide human hardware or software support in the generation delivery and or support of deliverables . Such third party contractors are viewed as a resource extension of the software factory and are governed under the same guidelines described above.

If an enterprise partner is involved in the generation of work packets and or deliverables an interface between the software factory and the enterprise partner may be provided by a service provider s interface team and or a product vendor s interface team . Service provided by an enterprise partner may be a constraint that is part of contractual agreement with a client to provide specialized services. An example of such a constraint is a required integrated information service component that is referenced in the integration design portion of the work packet that is sent to assembly center . Again note that third party service providers use a standard integration strategy that is defined by the software factory and as such are subject to and obligated to operate under software factory governance.

Product vendor s interface team provides an interface with a Product Vendor which is an enterprise partner that provides software factory with supported products that maybe used within a software factory solution. Product Vendors are also responsible for providing product support and maintaining vendor s relationships which are managed under the software factory s governance guidelines.

L Support is provided primarily by Software Engineers who provide problem support of Software Factory produced delivered code for customers. That is if a deliverable doesn t run as designed then the software engineers will troubleshoot the problem until it is fixed. These software engineers deliver technical assistance to Software Factory customers with information tools and fixes to prevent known software and possibly hardware problems and provide timely responses to customer inquiries and resolutions to customer problems.

L support is primarily provided by an L Help Desk Call Center . L Help Desk support can be done via self service voice recognition and voice response or by text chat to an automated smart attendant or a call can be directed to a Customer Service Representative CSR . Customer Service Representatives in this role provide first line of help problem support of Software Factory produced deliverables. Such help includes user instruction of known factory solution procedures. For any related customers issues that cannot be resolved through L the L Help Desk will provide preliminary problem identification create trouble ticket entry into trouble tracking system which then triggers a workflow event to dynamically route the problem issue to an available and appropriate L support group queue.

With reference now to a flow chart of exemplary steps taken to create custom software through the use of a software factory is presented. After initiator block which may be a creation of a contract between an enterprise client and a software factory service input from a Client Business Governance Board is received at a software factory block . This input is a detailed description of the custom software needs of the enterprise client. While such input is usually prepared and presented by human management of the enterprise client alternatively this input may be the creation of a Unified Modeling Language UML based description of the needed software. Based on the client s input a project software proposal definition is created by the Software Factory Governance Board of the software factory block . This project software proposal definition is sent to the scheduling dispatching department of the Software Factory Operations which creates a software project.

The software project is then inducted block . As will be described in more detail below the project induction provides an initial introduction of the project to the software factory. Through the use of various parameters including those found in records of other projects checklists et al. the project is initially evaluated. This evaluation includes determining if the software factory has the capacity resources bandwidth etc. needed for the project. If so then a determination is made as to whether the project is qualified for acceptance by the software factory. Such qualification includes but is not limited to determining if the project falls within the guidelines set by a Service Level Agreement SLA between the client enterprise and the software factory whether the project conforms to legal guidelines such as Sarbanes Oxley etc. Based on these and other criteria the project is scored for feasibility profitability and desirability for implementation. If the induction process concludes that the project should proceed then it is categorized into a particular type of project e.g. payroll inventory control database management marketing et al. .

If the induction process does not pass query block indicating that the project should not proceed then the project is returned to the Client Business Governance Board for additional discussions between the Client Business Governance Board and the software factory in order to induct a revised project i.e. reinduct the software project . However if the induction process passes then the software project is parsed into major functional areas block . That is the project is divided up broken apart in order to establish subunits that can later be integrated into a single custom software deliverable .

Work packets are then obtained for all of the functional areas of the software project block . These work packets are reusable components which are described in detail below. The work packets are then stitched together block on an assembly center to create deliverable custom software that meets the criteria for the software project that has been established in the earlier steps. The custom software is then tested in the software factory block . Once testing is completed the custom software is delivered block to the client customer who receives on going support from the support team block . The flow chart ends at terminator block .

While the process has been described for the creation of custom software the same process is used by a software factory for other activities including creating a service for a customer creating standardized software etc. Thus the software factory uses work packets to blend software including reusable artifacts protocols e.g. how software will be transmitted how individuals will be contacted etc. governance requirements e.g. service level agreements that describe how much a service will cost and operating environments hardware and software including operating systems integrated environments such as SAP Rational etc. into a single integrated product which can then be used in a stand alone manner or can be fed into another system product.

Note that software factory is virtual. That is the different components e.g. software factory governance board software factory operations design center assembly center may be located in different locations and may operate independently under the control of information found in work packets . In a preferred embodiment each of the different components of the software factory publishes a set of services that the component can provide and a set of requirements for using these services. These services are functions that are well defined and made visible for outside entities to call.

For example assume that assembly center publishes a service that it can assemble only work packets that include code and protocol that utilize IBM s Rational software development platform. Thus the assembly center has published its service set of services includes assembling work packets and the required protocol set of requirements includes utilize IBM s Rational software development platform to the design center which must decide if it wants or is able to utilize that particular assembly center . If not then another assembly center from another software factory may be called upon by the design center . Behind each offered service are the actual processes that a component performs. These processes are steps taken by the service. Each step is performed by a section of software or may be performed by an individual who has been assigned the task of performing this step. Each step utilizes leveraged tools including the work packets described herein. These work packets then implement the process.

By utilizing published interfaces between the different components of the software factory then different components from different software factories can be interchanged according to the capability offered by and protocol used by each component. This enables a building block architecture to be implemented through the use of different components from different software factories.

There are five phases in the life cycle of a work packet which are shown in . These five phases are 1 Defining block 2 Assembling block Archiving block Distributing block and Pulling for Execution block . As indicated by the top dashed line coming out of asset repository this life cycle may be recursive. That is in one embodiment work packets are modified and upgraded in a recursive manner which includes the steps shown in . Once a work packet is assembled and archived it is stored in an asset repository whence the work packet may be accessed and utilized by an asset manager for assembly into a deliverable by an assembly center . Note that the assembly center can also send to the asset manager a message that requests a particular work packet which can be pulled block into the asset repository by the asset manager . This pulling step block is performed through intelligent routing distribution block to the asset repository and assembly center . The configuration of the routing distribution of the work packet is managed by the asset manager which is software that indexes stores and retrieves assets created and used with the software factory.

A work packet is a self contained work unit that comprises processes roles activities parts of the job applications and necessary input parameters that allow a team to conduct a development activity in a formalized manner with visibility to progress of their effort afforded to requesting teams. A work packet is NOT a deliverable software product but rather is a component of a deliverable software product. That is a work packet is processed integrated into a system tested etc. to create one or more deliverables. Deliverables which were created from one or more work packets are then combined into a custom software such as an application service or system.

Governance Policies and Procedures these policies and procedures include protocol definitions derived from a project plan. That is a project plan for a particular custom software describes how work packets are called as well as how work packets report back to the calling plan.

Standards this component describes details about how work packets are implemented into a deliverable in a standardized manner. Examples of such standards are naming conventions formatting protocol etc.

Reused Assets this component includes actual code or at least pointers to code that is archived for reuse by different assembled deliverables.

Work Packet Instructions this component describes detailed instructions regarding how a work packet is actually executed. That is work packet instructions document what work packets need to be built and how to build them. These instructions include a description of the requirements that need to be met including design protocols code formats and test parameters.

Integration Strategy this component describes how a set of work packets as well as deliverables developed from a set of work packets are able to be integrated into a client s system. This component includes instructions regarding what processes must be taken by the client s system to be prepared to run the deliverable as well as security protocols that must be followed by the deliverable. The component may also include a description of how one deliverable will interact with other applications that are resident to the client s computer system.

Scheduling this component describes when a set of work packets are to be sent to an assembly center plus instructions on monitoring the progress and status of the creation of the work packet.

Exit Criteria this component includes instructions e.g. through the use of a checklist for deploying a deliverable to the client s system. That is this component is the quality criteria that the deliverable must meet before it can be considered completed and acceptable for a project.

Input Work Products this component includes Input Output I O templates that are used to describe specific work products that are needed to execute the activities of the work packet in the assembly center to build the deliverable.

The process of defining a work packet is called a work packet definition process. This process combines critical references from governance factory operations e.g. factory management project management business criteria and design including test artifacts. Structured templates enable governance design center and factory operations to define the referenced artifacts by filling in corresponding functional domain templates thus defining the contents of the work packet. Thus a work packet includes not only reusable software code but also includes governance and operation instructions. For example a work packet may include directions that describe a sequence of steps to be taken in a project which data is to be used in the project which individuals departments job descriptions are to perform each step in the project how assigned individuals departments are to be notified of their duties and what steps data are to be taken and used et al. Thus each work packet includes traceability regarding the status of a job as well as code data individuals to be used in the execution of a project.

Thus work packets are created from unique references to governance factory operations factory mgt project mgt business and design including test artifacts. The packet definition process provides structure templates that enable governance design center and factory operations to define referenced artifacts newly defined artifact identifiers or any reusable part of existing work packet definitions by filling in corresponding functional domain e.g. eXtensible Markup Language XML templates. What can be defined may be controlled by a Document Type Definition DTD . The DTD states what tags and attributes are used to describe content in the deliverable including where each XML tag is allowed and which XML tags can appear within the deliverable. XML tag values are defined and applied to a newly defined XML template for each functional area of a design center. These XML templates are then merged into one hierarchical structure when later assembled into finalized work packets.

With reference now to an overview of the environment in which a packet definition process occurs is presented. The packet definition process calls artifacts metrics and a template to define a work packet. The artifacts may be one or more of governance artifacts intellectual assets produced in the software factory by the Software Factory Governance Board described in business contextual artifacts intellectual assets produced in the software factory by business analysts in the requirement analysis team described in architectural artifacts intellectual assets produced by the architecture team described in test artifacts intellectual assets produced by test architects in the architecture team shown in and project artifacts intellectual assets produced in the software factory by system engineers in the design center shown in .

The metrics may be one or more of governance metrics measurable governance indicators such as business plans factory metrics measurable indicators that describe the capabilities of the software factory including assembly center capacity and system metrics measurable indicators that describe the capabilities of the client s computer system on which deliverables are to be run .

Based on a template for a particular deliverable artifacts and metrics are used by a packet assembly process to assemble one or more work packets.

Template shown in describes how a work packet is to be assembled. The template includes metadata references to key artifacts and metrics which are merged into a formal work packet definition as described above. The work packet is then assembled in a standardized hierarchical way and packaged within a factory message envelope that contains a header and body.

With reference now to a high level flow chart of steps taken to define and assemble work packets is presented. After initiator block which may be an order by the Requirements Analysis Team to the Architecture Team shown in to create a design center defined work packet the requisite packet definitions are created for work packets that are to be used in deliverables block . First a template which preferably is a reusable that has been used in the past to create the type of work packet needed is called block . Based on that called template the needed artifacts block and metrics block are called. Using the template as a guide the called artifacts and metrics are assembled in the requisite work packets block and the process ends.

As stated above work packets are fungible easily interchangeable and reusable for different deliverables . As such they are stored in an archival manner. In order to retrieve them efficiently however they are categorized classified and named. For example consider the header shown in . Header is associated with a specific work packet that includes software code . The name of the work packet is created by the architect who originally created the work packet . Preferably the name is descriptive of the function of the work packet such as Security Work Packet which can be used in the assembly of a security deliverable. The header may describe whether the work packet is proprietary for a particular client such that the work packet may be reused only for that client. A description coded flagged etc. for what the work packet is used for may be included as well as the names of particular components such as the eight components described above .

An alternate header for a work packet is shown in as header . Note that the header for every work packet contains the first four values shown Work Packet ID Work Packet Description Work Packet Type and Parent Packet ID . That is each work packet has a unique identification number Work Packet ID a short description of the work packet Work Packet Description a description of the type of work packet Work Packet Type such as security spreadsheet etc. and the identifier Parent Packet ID of any parent object from which the work packet has inheritance.

With reference now to a high level flow chart of steps taken to archive a work packet is presented. After initiator block an architect defines header components for an asset e.g. a work packet header block . Note that these header components allow an Asset Repository to perform a metadata categorization search of the assets. These header components may be any that the programmer wishes to use including those shown in exemplary manner in . After the header components are defined the architect populates them with descriptors block . A system manager or software then archives stores the work packet including the header block . At a later time a program or programmer can retrieve the work packet by specifying information in the header block . For example if the program or programmer needs a work packet that is of a Security type that follows Standard 100 then Work packet one can be retrieved at Address 1 as depicted in . Note however that this work packet cannot be utilized unless it is to be used in the construction of a deliverable for the client Toyota. The process ends at terminator block .

Before a software factory can receive an order from a client to create work packets and their resultant deliverables applications a determination should be made to determine whether the factory is ready to take on project work. This determination can be made through the use of a scorecard which provides a maturity assessment of the factory. An exemplary scorecard is as follows 

In one embodiment of the present invention all of these steps are taken before a project is taken on by the Software Factory Governance Board described above in . These steps ensure the health and capacity of the software factory to create and assemble work packets into a client ordered deliverable.

As indicated in Step of the Factory Readiness Review process software factory on boarding is a rapid process that uses a series of checklist questionnaires to help with the rapid set up and configuration of the software factory.

The software factory on boarding process is an accelerator process model that enables the roll out configuration of uniquely defined software factor instances. This is a learning process that leverages patterns used in prior on boarding exercises. This evolution provides a pertinent series of checklist questionnaires to qualify what is necessary for a rapid set up and confirmation of a factory instance to support a project. Based on project type assessments installed factory patterns can be leveraged to forecast what is necessary to set up a similar factory operation.

Rapid on boarding provides a calculated line and work cell balancing capability view of leveraged resources thus improving throughput of assembly centers and work cells while reducing manpower requirements and costs. The balancing module instantly calculates the optimum utilization using the fewest operators to achieve the result requested. Parameters can be varied as often as needed to run what if scenarios.

With reference now to a high level flow chart of exemplary steps taken for rapidly on boarding a software factory is presented. After initiator block processes used by a software factory including choke points are determined for a first project block . These processes and perhaps choke points lead to a checklist which describes the processes of the first process block . Examples of processes include but are not limited to the creation of work packets testing work packets etc. Examples of choke points include but are not limited to available computing power and memory in a service computer in which the software factory will run available manpower available communication channels etc. When a new work project comes in to the software factory the checklist can be used by the Software Factory Operations shown in to check processes choke points that can be anticipated by the new work project block . That is assume that the first project and the new project are both projects for creating a computer security program. By using a checklist that identifies similar mission critical processes and or choke points when creating a computer security program a rapid determination can be made by a programmer or automated software as to whether the software factory is capable of handling the new work project. If the checklist is complete indicating that all mission critical resources are ready and no untoward choke points are detected block then the software factory is configured block as before for the first project and the process ends terminator block . However if the resources are not ready then a Not Ready message is sent back to the Software Factory Operations such as to the Software Factory Governance Board block thus ending the process terminator block unless the Software Factory Governance Board elects to retry configuring the software factory either using the rapid on board process or the full process described above .

Before a software project is accepted by the software factory it should first be inducted. This induction process provides an analysis of the proposed software project. The analysis not only identifies what processes and sub processes will be needed to create the software project but will also identify potential risks to the software factory and or the client s computer system.

With reference now to the flow chart shown in a candidate project is submitted to software factory preferably to the Software Factory Governance Board shown in as a factory project proposal . The factory project proposal then goes through a service definition process .

Service definition process utilizes electronic questionnaire checklists to help define a service definition template . Checklists are a collection of drill down checklists that provide qualifying questions related to the candidate project . The questions asked in the checklists are based on pre qualifying questions. That is as shown in pre qualification questions are broad questions that relate to different types of projects. Based on the answers submitted to questions in the pre qualification questions a specific checklist from checklists is selected. Thus assume that pre qualification questions include four questions 1 Who is the client 2 Is the project security related 3 Will the project run on the client s hardware 4 When is the proposed project due Based on answers that are input by the client or the software factory governance board one of the checklists will be selected. That is if the answers for the four questions were 1 Toyota 2 Yes 3 Yes and 4 Six months then a checklist which has questions that are heuristically known from past projects to contain the most relevant questions for such a project is then automatically selected.

Returning to the selected checklists are then used to generate the service definition template which is essentially a compilation of checklists that are selected in the manner described in . Service definition template is then sent to a Service Assessment Review SAR . SAR is a weighted evaluation process that based on answers to qualifying and preferably closed ended yes no questions derived from the service definition template evaluates the factory project proposal for completeness and preliminary risk assessment. SAR provides an analysis of relevant areas of what is known based on answers to questions found in the service definition template and what is unknown could not be determined either because of missing or unanswered questions in the service definition template about the candidate project . Thus the outcome of SAR is a qualification view gap analysis for the factory project proposal which provides raw data to a scoring and classification process .

The scoring and classification process is a scoring and tabulation of the raw data that is output from SAR . Based on the output from SAR the scoring and classification process rates the factory project proposal on project definition completeness trace ability and risk exposure. If the service definition template indicates that third parties will be used in the candidate project then the scoring and classification process will evaluate proposed third party providers through the use of a third party required consent process .

The third party required consent process manages relationships between third party providers and the software factory . Example of such third party providers include but are not limited to a third party contractor provider which will provide software coding services for components of the candidate project a third party service provider which will provide an execution environment for sub components of the candidate project and vendor product support which provides call in and or on site support for the completed project . The determination of whether the third party providers and the software factory can work in partnership on the project is based on a Yes No questionnaire that is sent from the software factory to the third party providers . The questionnaire that is sent to the third party providers includes questions about the third party s financial soundness experience and capabilities development and control process including documentation of work practices technical assistance that can be provided by the third party including available enhancements quality practices including what type of conventions the third party follows such as ISO 9001 maintenance service that will be provided product usage including a description of any licensing restrictions costs contracts used and product warranty.

If the factory project proposal fails this scoring process it is sent back to a remediation process . However if scoring process gives an initial indication that the factory project proposal is ready to be sent to the software factory then it is sent to the service induction process .

Once the factory project proposal has gone through the SAR process and any third party coordination has been met scored and classified the factory project proposal is then inducted pre qualified for approval by the service induction process . During the service induction process the scored and classified project is sent through a Conceptual Requirements Review which utilizes a service repository scorecard to determine if the software factory is able to handle the candidate project . That is based on the checklists evaluations scorecards and classifications depicted in the candidate project receives a final evaluation to determine that the software factory has the requisite resources needed to successfully execute the candidate project . If so then the candidate project becomes a factory project and a contract agreement is made between the client and the service provider who owns the software factory .

As described herein work packets are created in accordance with the client s needs capacities. An optimal way to determine what the client s needs capacities are is through the use of checklists. A standard checklist however would be cumbersome since standard checklists are static in nature. Therefore described now is a process for generating and utilizing dynamic checklists through the use of a Software Factory Meta Morphic Dynamic Restructuring Logic Tree Model. This model provides the means to expedite checklist data collections by dynamically restructuring and filtering non relevant checklist questions depending on answers evaluated in real time. Such a model not only enables a meta data driven morphing of decision trees that adapt to the relevancy of what is deemed an applicable line of questioning but also provides a highly flexible solution to pertinent data collection.

As now described the Software Factory Meta Morphic Dynamic Restructuring Logic Tree Model qualifies answers to checklist questions to determine if a next checklist is relevant to what is needed to determine what type of work packets are needed for the client s project. This expedites the data collection and analysis process and thus provides a scalable flexibility to data collection and logic decision tree processing and constructions.

Referring now to a software diagram shows a relationship between different software objects used to dynamically generate checklists used to determine what work packets are needed to create a deliverable. Objects are used to track and receive answers to a particular checklist while objects are used to evaluate each checklist to determine if it is relevant to the inquiry needed for determining what work packets are needed for a project related to a particular checklist category.

Referring now to a Software Factory Packet Pattern Analysis and Predictive Forecasting Model which is an excerpt of a Software Factory data model shows the relational pattern between areas of pattern analysis. shows a pattern of relationships between different assets project types templates schema tasks and processes. These relationships are a by product of the Software Factory Packet Pattern Analysis and Predictive Forecasting Model shown in .

To tie together the details shown in a high level flow chart of steps taken to dynamically manage checklists used to select appropriate work packets in a software factory is presented in . After initiator block which may be prompted by a client requesting a deliverable from the software factory an initial checklist is presented block . This checklist consists of a series of question groups which are categorized according to a particular type of deliverable. For example a security software program may be associated with a particular checklist category for security software. As described in block answers to the first group of questions are received by the Software Factory Packet Pattern Analysis and Predictive Forecasting Model shown in . If the received answers prompt a new series of questions query block then a dynamically generated new checklist is created block . Note that this new checklist is not merely an existing node in a decision tree. Rather based on received answers a new checklist is dynamically created using stored questions that are tagged and associated with a particular set of answers. Thus if a set of two questions resulted in respective answers True and False this would result in a different set of next questions than what would be generated if the respective answers were True and True or any other combination of answers other than True and False .

Referring now to block answers to the new checklist are evaluated based on their contextual reference and the nature of the questioning objectives. That is based on what question parameters are used for the work packets being generated a determination can be made as to whether additional new checklists need to be constructed query block . If so then the process returns to block in an iterative manner. If not then the process ends terminator block indicating that the checklist process for determining what qualities are needed in the work packets has concluded.

Referring again to block note that leading indicator can influence how answers are evaluated. Such leading indicators include descriptors of the final deliverable that will be generated by the software factory a client s name or field etc. As leading indicators change they can change content relevance and perspective reference points and drive the restructuring of relevant questions that can be restructured along that leading indicator relative perspective.

As thus described for every answer collected by a question posed on a checklist and the scope of the question all answers are evaluated for relevancy scope project type and contextual reference etc. . If a question becomes irrelevant then that question is filtered and not asked in future questionnaires having a similar context. This provides a highly flexible solution for essential pertinent data collection. That is the line of questioning and the decision tree changes with each new iteration thus creating a dynamic logic tree that restructures itself depending on how it used by maintaining a contextual reference base . Like water reforming into a drop no matter how many times and in what manner a set of questions is parsed into segments the set of questions reforms its remnants into a new wholly formed structure.

The software factory described herein should be monitored for a variety of issues. Such monitoring is performed by a Software Factory Analytics and Dashboard which ensures that both a single instance and multiple instances of the Factory can function smoothly. The monitored metrics include project metrics as well as factory operations system business and performance activities. The analytics of the overall health of the factory can be audited and monitored and used as a basis for continual process improvement strategic analysis and planning. This ensures fungibility and consistency provides quality assurance reduces the risk of failure and increases cost effectiveness.

The health of the software factory is monitored through messages on an Enterprise Service Bus ESB which is a bus that is that couples the endpoint processes of the software factory with dashboard monitors. An ESB provides a standard based integration platform that combines messaging web services data transformation and intelligent routing in an event driven Service Oriented Architecture SOA . In an ESB enabled event driven SOA applications and services are treated as abstract endpoints which can readily respond to asynchronous events. The SOA provides an abstraction away from the details of the underlying connectivity and plumbing. The implementations of the services do not need to understand protocols. Services do not need to know how messages are routed to other services. They simply receive a message from the ESB as an event and process the message. Process flow in an ESB can also involve specialized integration services that perform intelligent routing of messages based on content. Because the process flow is built on top of the distributed SOA it is also capable of spanning highly distributed deployment topologies between services on the bus.

As stated above the messages that flow on the ESB contain measurable metrics and states that are received through an event driven Service Oriented Architecture SOA Model. This information is via XML data stream messages which can contain factory operation system business and performance and activity related metrics which provide a relative point of origin for low level measurement. The messages can be used in analytics of the factory s overall health which is audited and monitored and can be used as a basis for continual process improvement strategic analysis and planning. Upon update the data stream is analyzed and the aggregated Key Performance Indicators KPIs are calculated and sent to the dashboard display device where the XML is applied to a style template and rendered for display.

The Health Monitoring System provides factory exception and error reporting system monitoring Performance Monitoring and Reporting Proactive and Reactive Alert Notification Message Auditing and Tracking Reporting Daily View of Activity and Historical Reports. Information collected includes what information regarding the software factory metrics was sent to whom it was sent when it was sent and how many messages were sent via the ESB interface between the software factory and the client s system.

Information in the messages includes timestamps for the sender from the software factory the receiver in the analytic section and the hub the ESB . Derived metrics include 

Referring now to an environment for Software Factory Analytics and Dashboard is presented in a software factory . Note that three exemplary service endpoints are depicted. Service endpoint provides analytic service for measurements taken in the software factory . Service endpoint provides an audit service which determines which analytic measurements should be taken. Service endpoint provides a web service that affords analytic measurements and dashboards to be transmitted in HTML or other web based format to a monitor. Details of a service endpoint include the application service software an application interface a resource adapter a managed connection a client interface an ESB endpoint an invocation and management framework protocol stacks that can be sued for transporting messages across an ESB and a service container an operating system process that can be managed by the invocation and management framework .

Each service endpoint is coupled to the Enterprise Service Bus ESB to which XML message or similar markup language formatted messages can flow to governance monitors factory operations monitors and or system engineering monitors on which the messages generate dashboard progress messages.

With reference now to a flow chart of exemplary steps taken to monitor the health of a software factory is presented. After initiator block which may be prompted by the acceptance of a work project as described above work packets are first defined block . As described above these work packets are then sent to the assembly area. This transmittal is tracked block by sending a message to the ESB shown in . This message contains information about where and when the work packet was sent to the assembly center. If the work packet pulls an artifact such as artifacts described in another message is sent to the ESB for tracking purposes block . Similarly messages are sent to the ESB if there are any on going changes of work activities contained in the work packets block . Execution of the work packets is monitored to ensure that such execution conforms with governance guidelines that have been previously set for the software factory block . Similarly the software factory is monitored to ensure that work packets comply with the architecture of the software factory block .

Quality metrics are also monitored for the execution of the work packets in the assembly center area block . That is as different work packets are executed assembled and tested in the assembly center area the quality of such operations is tracked. These metrics include but are not limited to those described above plus completion rates detection of software defects hazards risks caused by the execution of the work packets and other issues. This information and optionally any other information monitored and tracked in block to is sent on the ESB to a dashboard in a monitoring display as described in above.

With reference now to there is depicted a block diagram of an exemplary client computer in which the present invention may be utilized. Note that some or all of the exemplary architecture shown for client computer may be utilized by software deploying server as well as monitors and shown in .

Client computer includes a processor unit that is coupled to a system bus . A video adapter which drives supports a display is also coupled to system bus . System bus is coupled via a bus bridge to an Input Output I O bus . An I O interface is coupled to I O bus . I O interface affords communication with various I O devices including a keyboard a mouse a Compact Disk Read Only Memory CD ROM drive a floppy disk drive and a flash drive memory . The format of the ports connected to I O interface may be any known to those skilled in the art of computer architecture including but not limited to Universal Serial Bus USB ports.

Client computer is able to communicate with a software deploying server via a network using a network interface which is coupled to system bus . Network interface may include an Enterprise Service Bus not shown such as ESB shown in . Network may be an external network such as the Internet or an internal network such as an Ethernet or a Virtual Private Network VPN . Note the software deploying server may utilize a same or substantially similar architecture as client computer .

A hard drive interface is also coupled to system bus . Hard drive interface interfaces with a hard drive . In a preferred embodiment hard drive populates a system memory which is also coupled to system bus . System memory is defined as a lowest level of volatile memory in client computer . This volatile memory includes additional higher levels of volatile memory not shown including but not limited to cache memory registers and buffers. Data that populates system memory includes client computer s operating system OS and application programs .

OS includes a shell for providing transparent user access to resources such as application programs . Generally shell is a program that provides an interpreter and an interface between the user and the operating system. More specifically shell executes commands that are entered into a command line user interface or from a file. Thus shell as it is called in UNIX UNIX is a registered trademark of The Open Group in the United States and other countries also called a command processor in Windows WINDOWS is a registered trademark of Microsoft Corporation in the United States and other countries is generally the highest level of the operating system software hierarchy and serves as a command interpreter. The shell provides a system prompt interprets commands entered by keyboard mouse or other user input media and sends the interpreted command s to the appropriate lower levels of the operating system e.g. a kernel for processing. Note that while shell is a text based line oriented user interface the present invention will equally well support other user interface modes such as graphical voice gestural etc.

As depicted OS also includes kernel which includes lower levels of functionality for OS including providing essential services required by other parts of OS and application programs including memory management process and task management disk management and mouse and keyboard management.

Application programs include a browser . Browser includes program modules and instructions enabling a World Wide Web WWW client i.e. client computer to send and receive network messages to the Internet using HyperText Transfer Protocol HTTP messaging thus enabling communication with software deploying server .

Application programs in client computer s system memory as well as software deploying server s system memory also include a Software Factory Program SFP . SFP includes code for implementing the processes described in and A . In one embodiment client computer is able to download SFP from software deploying server .

The hardware elements depicted in client computer are not intended to be exhaustive but rather are representative to highlight essential components required by the present invention. For instance client computer may include alternate memory storage devices such as magnetic cassettes Digital Versatile Disks DVDs Bernoulli cartridges and the like. These and other variations are intended to be within the spirit and scope of the present invention.

Note further that in a preferred embodiment of the present invention software deploying server performs all of the functions associated with the present invention including execution of SFP thus freeing client computer from having to use its own internal computing resources to execute SFP .

It should be understood that at least some aspects of the present invention may alternatively be implemented in a computer readable medium that contains a program product. Programs defining functions of the present invention can be delivered to a data storage system or a computer system via a variety of tangible signal bearing media which include without limitation non writable storage media e.g. CD ROM writable storage media e.g. hard disk drive read write CD ROM optical media as well as non tangible communication media such as computer and telephone networks including Ethernet the Internet wireless networks and like network systems. It should be understood therefore that such signal bearing media when carrying or encoding computer readable instructions that direct method functions in the present invention represent alternative embodiments of the present invention. Further it is understood that the present invention may be implemented by a system having means in the form of hardware software or a combination of software and hardware as described herein or their equivalent.

As described above in one embodiment the processes described by the present invention including the functions of SFP are performed by software deploying server . Alternatively SFP and the method described herein and in particular as shown and described in and A can be deployed as a process software from software deploying server to client computer . Still more particularly process software for the method so described may be deployed to software deploying server by another service provider server not shown .

Referring then to step begins the deployment of the process software. The first thing is to determine if there are any programs that will reside on a server or servers when the process software is executed query block . If this is the case then the servers that will contain the executables are identified block . The process software for the server or servers is transferred directly to the servers storage via File Transfer Protocol FTP or some other protocol or by copying though the use of a shared file system block . The process software is then installed on the servers block .

Next a determination is made on whether the process software is to be deployed by having users access the process software on a server or servers query block . If the users are to access the process software on servers then the server addresses that will store the process software are identified block .

A determination is made if a proxy server is to be built query block to store the process software. A proxy server is a server that sits between a client application such as a Web browser and a real server. It intercepts all requests to the real server to see if it can fulfill the requests itself. If not it forwards the request to the real server. The two primary benefits of a proxy server are to improve performance and to filter requests. If a proxy server is required then the proxy server is installed block . The process software is sent to the servers either via a protocol such as FTP or it is copied directly from the source files to the server files via file sharing block . Another embodiment would be to send a transaction to the servers that contained the process software and have the server process the transaction then receive and copy the process software to the server s file system. Once the process software is stored at the servers the users via their client computers then access the process software on the servers and copy to their client computers file systems block . Another embodiment is to have the servers automatically copy the process software to each client and then run the installation program for the process software at each client computer. The user executes the program that installs the process software on his client computer block then exits the process terminator block .

In query step a determination is made whether the process software is to be deployed by sending the process software to users via e mail. The set of users where the process software will be deployed are identified together with the addresses of the user client computers block . The process software is sent via e mail to each of the users client computers block . The users then receive the e mail block and then detach the process software from the e mail to a directory on their client computers block . The user executes the program that installs the process software on his client computer block then exits the process terminator block .

Lastly a determination is made as to whether the process software will be sent directly to user directories on their client computers query block . If so the user directories are identified block . The process software is transferred directly to the user s client computer directory block . This can be done in several ways such as but not limited to sharing of the file system directories and then copying from the sender s file system to the recipient user s file system or alternatively using a transfer protocol such as File Transfer Protocol FTP . The users access the directories on their client file systems in preparation for installing the process software block . The user executes the program that installs the process software on his client computer block and then exits the process terminator block .

The present software can be deployed to third parties as part of a service wherein a third party VPN service is offered as a secure deployment vehicle or wherein a VPN is build on demand as required for a specific deployment.

A virtual private network VPN is any combination of technologies that can be used to secure a connection through an otherwise unsecured or untrusted network. VPNs improve security and reduce operational costs. The VPN makes use of a public network usually the Internet to connect remote sites or users together. Instead of using a dedicated real world connection such as leased line the VPN uses virtual connections routed through the Internet from the company s private network to the remote site or employee. Access to the software via a VPN can be provided as a service by specifically constructing the VPN for purposes of delivery or execution of the process software i.e. the software resides elsewhere wherein the lifetime of the VPN is limited to a given period of time or a given number of deployments based on an amount paid.

The process software may be deployed accessed and executed through either a remote access or a site to site VPN. When using the remote access VPNs the process software is deployed accessed and executed via the secure encrypted connections between a company s private network and remote users through a third party service provider. The enterprise service provider ESP sets a network access server NAS and provides the remote users with desktop client software for their computers. The telecommuters can then dial a toll free number or attach directly via a cable or DSL modem to reach the NAS and use their VPN client software to access the corporate network and to access download and execute the process software.

When using the site to site VPN the process software is deployed accessed and executed through the use of dedicated equipment and large scale encryption that are used to connect a company s multiple fixed sites over a public network such as the Internet.

The process software is transported over the VPN via tunneling which is the process of placing an entire packet within another packet and sending it over a network. The protocol of the outer packet is understood by the network and both points called tunnel interfaces where the packet enters and exits the network.

The process software which consists of code for implementing the process described herein may be integrated into a client server and network environment by providing for the process software to coexist with applications operating systems and network operating systems software and then installing the process software on the clients and servers in the environment where the process software will function.

The first step is to identify any software on the clients and servers including the network operating system where the process software will be deployed that are required by the process software or that work in conjunction with the process software. This includes the network operating system that is software that enhances a basic operating system by adding networking features.

Next the software applications and version numbers will be identified and compared to the list of software applications and version numbers that have been tested to work with the process software. Those software applications that are missing or that do not match the correct version will be upgraded with the correct version numbers. Program instructions that pass parameters from the process software to the software applications will be checked to ensure the parameter lists match the parameter lists required by the process software. Conversely parameters passed by the software applications to the process software will be checked to ensure the parameters match the parameters required by the process software. The client and server operating systems including the network operating systems will be identified and compared to the list of operating systems version numbers and network software that have been tested to work with the process software. Those operating systems version numbers and network software that do not match the list of tested operating systems and version numbers will be upgraded on the clients and servers to the required level.

After ensuring that the software where the process software is to be deployed is at the correct version level that has been tested to work with the process software the integration is completed by installing the process software on the clients and servers.

The process software is shared simultaneously serving multiple customers in a flexible automated fashion. It is standardized requiring little customization and it is scalable providing capacity on demand in a pay as you go model.

The process software can be stored on a shared file system accessible from one or more servers. The process software is executed via transactions that contain data and server processing requests that use CPU units on the accessed server. CPU units are units of time such as minutes seconds hours on the central processor of the server. Additionally the accessed server may make requests of other servers that require CPU units. CPU units describe an example that represents but one measurement of use. Other measurements of use include but are not limited to network bandwidth memory utilization storage utilization packet transfers complete transactions etc.

When multiple customers use the same process software application their transactions are differentiated by the parameters included in the transactions that identify the unique customer and the type of service for that customer. All of the CPU units and other measurements of use that are used for the services for each customer are recorded. When the number of transactions to any one server reaches a number that begins to affect the performance of that server other servers are accessed to increase the capacity and to share the workload. Likewise when other measurements of use such as network bandwidth memory utilization storage utilization etc. approach a capacity so as to affect performance additional network bandwidth memory utilization storage etc. are added to share the workload.

The measurements of use used for each service and customer are sent to a collecting server that sums the measurements of use for each customer for each service that was processed anywhere in the network of servers that provide the shared execution of the process software. The summed measurements of use units are periodically multiplied by unit costs and the resulting total process software application service costs are alternatively sent to the customer and or indicated on a web site accessed by the customer which then remits payment to the service provider.

In another embodiment the service provider requests payment directly from a customer account at a banking or financial institution.

In another embodiment if the service provider is also a customer of the customer that uses the process software application the payment owed to the service provider is reconciled to the payment owed by the service provider to minimize the transfer of payments.

With reference now to initiator block begins the On Demand process. A transaction is created than contains the unique customer identification the requested service type and any service parameters that further specify the type of service block . The transaction is then sent to the main server block . In an On Demand environment the main server can initially be the only server then as capacity is consumed other servers are added to the On Demand environment.

The server central processing unit CPU capacities in the On Demand environment are queried block . The CPU requirement of the transaction is estimated then the server s available CPU capacity in the On Demand environment are compared to the transaction CPU requirement to see if there is sufficient CPU available capacity in any server to process the transaction query block . If there is not sufficient server CPU available capacity then additional server CPU capacity is allocated to process the transaction block . If there was already sufficient available CPU capacity then the transaction is sent to a selected server block .

Before executing the transaction a check is made of the remaining On Demand environment to determine if the environment has sufficient available capacity for processing the transaction. This environment capacity consists of such things as but not limited to network bandwidth processor memory storage etc. block . If there is not sufficient available capacity then capacity will be added to the On Demand environment block . Next the required software to process the transaction is accessed loaded into memory then the transaction is executed block .

The usage measurements are recorded block . The utilization measurements consist of the portions of those functions in the On Demand environment that are used to process the transaction. The usage of such functions as but not limited to network bandwidth processor memory storage and CPU cycles are what is recorded. The usage measurements are summed multiplied by unit costs and then recorded as a charge to the requesting customer block .

If the customer has requested that the On Demand costs be posted to a web site query block then they are posted block . If the customer has requested that the On Demand costs be sent via e mail to a customer address query block then these costs are sent to the customer block . If the customer has requested that the On Demand costs be paid directly from a customer account query block then payment is received directly from the customer account block . The On Demand process is then exited at terminator block .

While the software factory described above provides a portable fungible and uniform environment for creating deliverables through the use of work packets assembled on an assembly center job shop it may be difficult for non technical practitioners e.g. non IT personnel to understand the architecture processes and operational data of the software factory unless expressed in the language of their industry knowledge domain or business role. Thus in one embodiment of the present invention semantics are supplied to link the data and metrics which describe the software factory at a low level to a higher level abstraction found in a semantic data model of the software factory. The semantic data model in support of a software factory instantiation governs a relationship of work packets across a software development lifecycle for the software factory. The semantic model defines the structures used to create the semantic data model operators which trigger software factory events and a canonical format i.e. behavioral and structural rules for processes and practices followed by the software factory to ensure consistency across the software factory components e.g. design centers assembly centers and job shops of a global delivery network. Logic associated with the semantic data model defines the software factory s structures in order to create the semantic data model operators which trigger software factory events within the software factory and behavioral and structural rules that ensure consistency across the software factory components of the global delivery network. Thus the semantic data model allows stakeholders to view the state and configuration of the software factory at a layer of abstraction that supports the language of their industry knowledge domain or business role. The semantic data model in support of a software factory also facilitates the instrumentation of work packet state to ensure workflow metrics are accurately captured in order to determine project status.

In each step of a Software Development Lifecycle SDLC the semantic data model undergoes transformations in order to support the analysis of an overall software factory system state. The semantic data model supports the levels of abstraction needed so that each practitioner involved with software development can query the model in order to fully understand current task assignments at a specific operation.

With reference now to a semantic data model is interpreted by a Semantic Model Transform Algorithm SMTA which is specific for a particular Software Factory which utilizes the architecture of the Software Factory depicted and described above in . SMTA projects data that describes activities within the Software Factory in the format and notation that supports a specific role or responsibility in the SDLC. Thus practitioners are able to view in canonical form the semantic data model . That is the semantic data model is in a format that makes sense to the practitioners since it presents a high level view of the Software Factory using language terms and descriptors that the practitioners readily understand.

The Semantic Model Transform Algorithm may be accessible via standard relational Database Management System DBMS technology either programmatically via an Application Programming Interface API or through a Software Factory custom semantic reconciliation engine neither shown in but are understood to be part of SFP described in .

The Semantic Model Transform Algorithm thus defines semantic data model projections which allow software factory key artifacts to be viewed in the notation required by software development practitioners. The functions supported include but are not limited to Conceptual transformations of data based upon Software Factory State Semantic Meaning of Information Stored and Canonical Form Relationship Between Categories. The conceptual transformation of data depicts to the practitioners what and how in high level terms work packets and or projects are manipulating data signals alarms etc. The semantic meaning of information stored presents the practitioners a high level view of what instructions created by the work packets actually do and how data acted upon by the work packets is transformed modified managed. The canonical form relationship between categories provides a rule based as set by the practitioners pattern that transforms the inner workings of the software factory into a more user friendly depiction found in the semantic data model .

The semantic data model for the Software Factory thus establishes a mechanism whereby the workproducts and state are handed off between the Client Governance Board see element in Software Factory Governance Board see element in Software Factory Operations see element in the Design Center see element in and the Assembly Center or Job Shop see element in to reflect the language of the industry knowledge domain or business role of the teams of these factory components. The semantic data model makes it possible for each practitioner in each of the enumerated Software Factory components to be able to visualize and understand the ontological meaning for the organizational constructs and deliverables that make up a particular software factory component. The semantic data model thus affords various stakeholders the ability to understand software development state and is a primary method and source for the interpretation and analysis of Software Factory operational metrics.

Software Factory Workflow can only be enabled by its state. The workflow state and the associated required transformations are represented by the Software Factory semantic data model throughout the lifecycle of the work packet depicted in . State information is defined as a set of attributes that completely or sufficiently defines the Software Factory Work Packet state. Thus the Work Packet state must be an executable process. The Semantic Data Model for the Software Factory contains entity and event attributes which define each component of Work Packet state. The semantic data model for the Software Factory enables linkages to other sources to comprise a federated state definition.

As work packets are instantiated the semantic data model transforms and governs the relationships between the Business Governance Board and Factory Operations. The semantic data model supports the transformations required to ensure that information can be referenced in the proper context and meaning for each major phase of the software development lifecycle.

With reference now to a high level flow chart depicting exemplary steps taken to permit software factory semantic reconciliation of data models for work packets is presented. After initiator block a semantic data model is created using an SMTA as described in . This semantic data model is in a format that is readily understood by practitioners when presented to them block . The practitioners are able to interact i.e. issue instructions make modifications to artifacts user interfaces databases etc with the semantic data model in a manner that converts the practitioner s high level instructions interactions into lower level instructions e.g. a work packet that the software factory understands block . That is the practitioners interact with the semantic data model which interprets translates these interactions to create launch work packets that the software factory roles can understand and utilize. The process ends at terminator block .

Thus as described herein the present invention provides a method system and computer readable medium for software factory semantic reconciliation of data models for work packets. Note again that computer and or Software Factory Program SFP described in combine to create a computer implemented system for performing the steps and features described in and .

In a preferred embodiment the software factory comprises operations that include collecting a plurality of software artifacts that have been archived during an assembly of previous work packets collecting a plurality of metrics that have been utilized during the assembly of previous work packets receiving a definition of a template for a new work packet wherein the template for the new work packet is created by a packet definition process that defines attributes that are needed in the new work packet under a control of the packet definition process selecting requisite software artifacts from the plurality of software artifacts under the control of the packet definition process selecting requisite metrics from the plurality of metrics and sending the template requisite software artifacts and requisite metrics to a packet assembly process wherein the packet assembly process assembles under the control of the template and the requisite metrics the requisite software artifacts to create the new work packet. Preferably these steps are performed in a software factory which includes the components of a software factory governance section that evaluates the project proposal for acceptance by the software factory a design center composed of a requirements analysis team and an architecture team wherein the design center sections the project proposal into major functional areas that are to be handled by the requirements analysis team and the architecture team and wherein the design center creates the work packets and an assembly center that receives and executes the work packets to create the deliverable custom software.

In one embodiment the design center includes a requirements analysis team wherein the requirements analysis team is responsible for determining system requirements for executing the deliverable custom software on the customer s system and an architectural team wherein the architectural team models the project proposal in accordance with customer constraints and wherein the architectural team bundles the customer constraints together with the work packets for execution in the assembly center.

In one embodiment the work packets include governance procedures standards reused assets work packet instructions integration strategy schedules exit criteria and artifact checklist templates for Input Output routines.

The assembly center and job shop in the software factory may include software that automatically recognizes a project type for the project proposal and wherein the assembly center assembles the work packets into the deliverable custom software in accordance with the project type that is recognized by the assembly center or job shop. In a preferred embodiment the assembly center or job shop conducts an integration test a system test a system integration test and a performance test of the deliverable custom software wherein the integration test tests the deliverable custom software for compatibility with the client s system the system test checks the client s system to ensure that the client s system is operating properly the system integration test tests for bugs that may arise when the deliverable custom software is integrated into the client s system and the performance test tests the deliverable custom software for defects as it is executing in the client s system.

In one embodiment the assembly center or job shop includes a published set of services and a published set of requirements for the assembly center wherein the published set of services and the published set of requirements for the assembly center or job shop are published to the design center and wherein the published set of services describes what assembly services for assembling work packets are offered by the assembly center or job shop and wherein the published set of requirements describes what execution environment must be used by work packets that are provided by the design center for assembly in the assembly center or job shop.

While the present invention has been particularly shown and described with reference to a preferred embodiment it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention. Furthermore as used in the specification and the appended claims the term computer or system or computer system or computing device includes any data processing system including but not limited to personal computers servers workstations network computers main frame computers routers switches Personal Digital Assistants PDA s telephones and any other system capable of processing transmitting receiving capturing and or storing data.

