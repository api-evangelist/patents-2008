---

title: Offloading storage operations to storage hardware
abstract: In a computer system with a disk array that has physical storage devices arranged as logical storage units and is capable of carrying out hardware storage operations on a per logical storage unit basis, the hardware storage operations can be carried out on a per-file basis using various primitives. These primitives include instructions for zeroing file blocks, cloning file blocks, and deleting file blocks, and these instructions operate on one or more files defined in a blocklist, that identifies the locations in the logical storage units to which the files map.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08745336&OS=08745336&RS=08745336
owner: VMware, Inc.
number: 08745336
owner_city: Palo Alto
owner_country: US
publication_date: 20080529
---
This application is related to 1 U.S. patent application Ser. No. 12 129 376 filed on the same date and entitled Offloading Storage Operations to Storage Hardware Using Third Party Server and 2 U.S. patent application Ser. No. 12 129 409 filed on the same date and entitled Offloading Storage Operations to Storage Hardware Using a Switch. 

Enterprise storage systems employ disk arrays that are physically independent enclosures containing a disk array controller a disk cache and multiple physical disk drives. The disk array controller manages the physical disk drives and exposes them to connected computer systems as logical data storage units each identified by a logical unit number LUN and enable storage operations such as cloning snapshotting mirroring and replication to be carried out on the data storage units using storage hardware.

Computer systems that employ disk arrays are typically configured with a file system that executes a logical volume manager. The logical volume manager is a software or firmware component that organizes a plurality of data storage units into a logical volume. The logical volume is available in the form of a logical device with a contiguous address space on which individual files of a file system are laid out. The mapping of the logical volume to the data storage units is controlled by the file system and as a result disk arrays do not know how individual files are laid out on the data storage units. Therefore a disk array cannot invoke its hardware to carry out storage operations such as cloning snapshotting mirroring and replication on a per file basis.

One possible solution for carrying out storage operations in a disk array on a per file basis is to add storage metadata in data structures managed by the disk array. Disk arrays however are provided by a number of different vendors and storage metadata varies by vendor. This solution is not attractive because the file system would then need to be customized for each different vendor. For this reason storage operations such as cloning snapshotting mirroring and replication of files have been typically carried out using software techniques through traditional standard file system calls.

One or more embodiments of the invention provide primitives that enable offloading of storage operations to storage hardware on a per file basis. These primitives include instructions for zeroing file blocks cloning file blocks and deleting file blocks and these instructions support higher level applications such as instant provisioning and thin provisioning.

One embodiment of the invention provides a method of carrying out storage operations in a computer system including a host computer that is connected to a storage system having storage devices represented as logical storage units. The storage operations are carried out on at least one file segment that is stored in at least two non adjacent locations in one or more logical storage units. The method includes the steps of issuing a single instruction from the host computer to the storage system and carrying out the single instruction at the storage system on at least one file segment until completion of the single instruction and independently of the host computer.

Another embodiment of the invention provides a computer system having a host computer and a storage system connected to the host computer having storage devices that are presented to the host computer as one or more logical storage units. In this computer system a data stream from the host computer to the storage system includes an instruction to carry out a storage operation on at least one file segment of the host computer that is stored in one or more logical storage units and the instruction identifies at least two non adjacent locations in the logical storage units where the at least one file segment is stored.

A computer readable storage medium according to an embodiment of the invention has stored therein an instruction to be executed in a storage processor of a storage system that has storage devices represented as logical storage units. The instruction includes an instruction ID and first and second parameters used in executing the instruction wherein the first and second parameters address non adjacent storage locations in one or more logical storage units.

A storage system for virtual machines having files managed through a virtual machine file system according to an embodiment of the invention includes a plurality of storage devices in which files of the virtual machines are stored and at least one storage processor programmed to i manage the storage devices as one or more logical storage units each of which is identified by a logical unit number LUN and has addressable blocks ii receive an instruction from the virtual machine file system and iii to carry out the instruction wherein the instruction contains multiple input parameters and identifies an operation to be carried out on the input parameters and each input parameter addresses a contiguous region of blocks in a logical storage unit.

A further embodiment of the invention is directed to a computer system having a file system for virtual machines and connected to a storage system having storage devices represented as one or more logical storage units. The computer system includes an interface through which storage instructions and storage data are transmitted to the storage system and a processor programmed to issue an instruction to the storage system wherein the instruction contains multiple input parameters and identifies an operation to be carried out on the input parameters and each input parameter addresses a contiguous region of blocks in a logical storage unit.

In some embodiments switch is omitted storage system resides in a separate data center from third party server and third party server communicates with storage system via out of band path and NIC network interface card installed in storage system . In accordance with one or more further embodiments additional switches and storage systems may be included in a system with one or more storage systems residing in different data centers.

In storage system storage system manager which represents one or more programmed storage processors serves as a communication agent to the outside world for storage system and implements a virtualization of physical typically disk drive based storage units referred to in as spindles that reside in storage system . Spindles are collectively referred to herein as spindles . From a logical perspective each of these spindles can be thought of as a sequential array of fixed sized extents . Storage system manager abstracts away complexities of targeting read and write operations to addresses of the actual spindles and extents of the disk drives by exposing to computer system an ability to view the aggregate physical storage space provided by the disk drives as a contiguous logical storage space that may be divided into a set of virtual SCSI devices known as LUNs Logical Units . The virtualization of spindles into such a contiguous logical storage space of LUNs can provide a more efficient utilization of the aggregate physical storage space that is represented by an address space of a logical volume. Storage system manager exposes to computer system an ability to transmit data transfer and control operations to storage system at a LUN block level where a block is a particular contiguous region in a particular LUN. For example a LUN block may be represented as and computer system may transmit to storage system a read or write operation for block in the form of a SCSI operation. The LUN identifier LUN ID is a unique hardware independent SCSI protocol compliant identifier value that is retrievable in response to a standard SCSI Inquiry command.

Storage system manager maintains metadata that includes a mapping hereinafter also referred to as an extent mapping for each of LUNs to an ordered list of extents wherein each such extent can be identified as a spindle extent pair and may therefore be located in any of the various spindles . As such whenever storage system manager receives a LUN block operation from computer system it is able to utilize the extent map of the LUN to resolve the block into an appropriate list of extents located in various spindles upon which the operation is performed. Those with ordinary skill in the art will recognize that while specific storage system manager implementation details and terminology may differ as between different storage device manufacturers the desired consistent result is that the externally visible LUNs implement the expected semantics in this example SCSI semantics needed to respond to and complete initiated transactions.

When storage system is an NAS device storage system manager exposes to computer system an ability to transmit data transfer and control operations to storage system at the file level. In contrast with SAN storage LUNs are managed within the NAS device. Storage system manager manipulates files performs I O for files using block addresses change file length and attributes and the like stored on the NAS device using file handles. When storage system manager receives a file operation from computer system it finds the location of the files being operated on within spindles using the filehandle specified by the file operation and performs the operation.

Returning to computer system operating system is installed on top of hardware platform and it supports execution of applications . Examples of operating system may be Microsoft Windows Linux Netware based operating systems or any other operating system known to those with ordinary skill in the art. Users may interact with computer system through a user interface such as a graphical user interface or a command based shell while executing applications may access computing resources of computer system that are managed by operating system kernel through kernel application programming interface API . Kernel provides process memory and device management to enable various executing applications to share limited resources of computer system . For example file system calls initiated by applications through kernel API are routed to file system . File system in turn converts the file system operations to LUN block operations and provides the LUN block operations to logical volume manager . File system in general manages creation use and deletion of files stored on storage system through the LUN abstraction discussed previously. Logical volume manager translates the volume block operations for execution by storage system and issues raw SCSI operations or operations from any other appropriate hardware connection interface standard protocol known to those with ordinary skill in the art including IDE ATA and ATAPI to device access layer based on the LUN block operations. Device access layer discovers storage system and applies command queuing and scheduling policies to the raw SCSI operations. Device driver understands the input output interface of HBAs interfacing with storage system and sends the raw SCSI operations from device access layer to HBAs to be forwarded to storage system . As previously discussed storage system manager of storage system receives the raw SCSI operations i.e. LUN block level operations and resolves them into the appropriate extents within the spindles of the disk array that are operated upon.

Instances arise during the operation of computer system where files on file system cannot ultimately be stored in contiguous blocks of LUNs presented to computer system by storage system . While there may be enough blocks of free storage space in the aggregate among various LUNs to store such files such blocks are neither large enough nor contiguous and may be dispersed across different LUNs. In such instances files may need to be segmented into multiple component parts at the file system level LUN level and the spindle extent level as further detailed in such that the file components are stored across different blocks of different LUNs. Due to this segmentation operations on such files such as read and write operations also need to be broken up into separate block level LUN operations i.e. raw LUN block level SCSI operations when transmitted to storage system thereby increasing the resources used by computer system to communicate with storage system e.g. CPU cycles DMA buffers SCSI commands in the HBA queue etc. .

One example of an environment that deals with significantly large files or collections of files where the foregoing segmentation may occur is server virtualization. As further discussed below virtualization systems expose the concept of a virtual disk which is implemented as a collection of files stored on a file system. is a functional block diagram of a virtualized computer system with a connected storage system in which one or more embodiments of the invention may be practiced. Similar to computer system of computer system may be constructed on a conventional typically server class hardware platform . As shown in computer system includes HBAs and NIC that enable computer system to connect to storage system . As further shown in virtual machine VMKernel operating system is installed on top of hardware platform and it supports virtual machine execution space within which multiple virtual machines VMs may be concurrently instantiated and executed. Each such virtual machine implements a virtual hardware HW platform that supports the installation of a guest operating system which is capable of executing applications . Similar to operating system of examples of a guest operating system may be Microsoft Windows Linux Netware based operating systems or any other operating system known to those with ordinary skill in the art. In each instance guest operating system includes a native file system layer not shown for example either an NTFS or an ext3FS type file system layer. These file system layers interface with virtual hardware platforms to access from the perspective of guest operating systems a data storage HBA which in reality is virtual HBA implemented by virtual hardware platform that provides the appearance of disk storage support in reality virtual disks or virtual disks to enable execution of guest operating system transparent to the virtualization of the system hardware. Virtual disks may appear to support from the perspective of guest operating system the SCSI standard for connecting to the virtual machine or any other appropriate hardware connection interface standard known to those with ordinary skill in the art including IDE ATA and ATAPI.

Although from the perspective of guest operating systems file system calls initiated by such guest operating systems to implement file system related data transfer and control operations appear to be routed to virtual disks for final execution in reality such calls are processed and passed through virtual HBA to adjunct virtual machine monitor VMM layers that implement the virtual system support needed to coordinate operation with virtual machine kernel . In particular host bus emulator functionally enables the data transfer and control operations to be correctly handled by virtual machine kernel which ultimately passes such operations through its various layers to true HBAs or NIC that connect to storage system . Assuming a SCSI supported virtual device implementation although those with ordinary skill in the art will recognize the option of using other hardware interface standards SCSI virtualization layer of virtual machine kernel receives a data transfer and control operation in the form of SCSI commands from VMM layers and converts them into file system operations that are understood by virtual machine file system VMFS . SCSI virtualization layer then issues these file system operations to VMFS . VMFS in turn converts the file system operations to volume block operations and provides the volume block operations to logical volume manager . Logical volume manager LVM is typically implemented as an intermediate layer between the driver and conventional operating system file system layers and supports volume oriented virtualization and management of the LUNs accessible through HBAs and NIC . As previously described multiple LUNs such as LUNs can be gathered and managed together as a volume under the control of logical volume manager for presentation to and use by VMFS as an integral LUN.

VMFS in general manages creation use and deletion of files stored on storage system through the LUN abstraction discussed previously. Clustered file systems such as VMFS are described in patent application Ser. No. 10 773 613 that is titled MULTIPLE CONCURRENT ACCESS TO A FILE SYSTEM filed Feb. 4 2004. Logical volume manager issues raw SCSI operations to device access layer based on the LUN block operations. Device access layer discovers storage system and applies command queuing and scheduling policies to the raw SCSI operations. Device driver understands the input output interface of HBAs and NIC interfacing with storage system and sends the raw SCSI operations from device access layer to HBAs or NIC to be forwarded to storage system . As previously discussed storage system manager of storage system receives the raw SCSI operations i.e. LUN block level operations and resolves them into the appropriate extents within the spindles of the disk array that are operated upon.

The virtual LUN file is allocated by VMFS as a series of segments in logical address space VMFS volume that is managed by VMFS . Each segment is a contiguous region in VMFS volume where VMFS has been constructed by an administrator of the system by allocating a set of LUNs available from storage system s set of LUNs . As previously discussed in the context of each contiguous region of a file segment that is also contiguous on one of the allocated LUNs is considered a LUN block that can be represented as . As shown in different LUN blocks corresponding to a portion of a file segment may be of different lengths depending on how big the file segment is and what part of that file segment actually corresponds to a contiguous region of an allocated LUN. Therefore a file may have one or more segments and a segment may be composed of one or more blocks from one or more LUNs. In the illustrated example file segment has 2 LUN blocks file segment has 3 LUN blocks file segment has 4 LUN blocks and file segment has 1 LUN block. As shown in file segments in VMFS volume are converted into LUN blocks by lines connecting file segments to LUN blocks in LUNs where LUNs represent the LUN address space. When storage system is a NAS device the file segments are managed within the NAS device.

By resolving all file segments making up virtual disk into an ordered list of their corresponding LUN blocks in the case of for a total of 10 blocks VMFS creates a blocklist e.g. a list of which is representative of virtual disk in LUN block form. As previously discussed in the context of storage system can utilize the extent maps for LUNs to resolve each of the LUN blocks in the blocklist into its corresponding list of pairs spindle extent pairs within spindles . As shown in LUN blocks are converted into spindle extent pairs by lines connecting LUN blocks within LUNs to extents within spindles . Extents within spindle are explicitly labeled in . Extents within other spindles are not labeled in . Those with ordinary skill in the art will recognize that although has been discussed in the context of a virtualized system in which a virtual disk is allocated into file segments non virtualized systems similar to that of may also have files stored in its file system that exhibit similar types of segmentation into LUN blocks.

As previously discussed storage devices such as storage system typically expose LUN block level operations to computer systems communicating with it. For example a standard raw SCSI read or write operation requires a LUN identifier logical block address and transfer length i.e. similar to the encoding described herein . As such in order to perform operations on files such as virtual disk that are managed at VMFS file system level standard raw SCSI operations need to be separately applied to each of the 10 blocks in virtual disk s blocklist. Each I O communication e.g. transmission of a raw SCSI operation by computer system with storage system can take up significant computing resources such as CPU cycles DMA buffers and SCSI commands in an HBA queue.

By exposing LUN blocklist level primitives to the set of operations available to computer systems communicating with storage system disk array vendors provide computer systems an ability to offload resource intensive communication with a disk array into the disk array itself. The disk array can then leverage any proprietary hardware optimizations that may be available internally thereto. In one embodiment such blocklist level primitives may be embedded in a command descriptor block CDB in a pre existing standard command of the communication interface protocol between the computer system and disk array or alternatively may be added as an additional command to the set of standard commands. For example for SCSI supported interactions between a computer system and a disk array certain blocklist level primitives may be able to be embedded into the CDB of SCSI s pre existing WRITE BUFFER command while other blocklist level primitives may require the addition of a new SCSI level command e.g. with its own CDB to augment SCSI s current commands. The following discussion presents three possible blocklist level primitives supported by storage system i.e. zero for zeroing out files clone for cloning files and delete for deleting files . These three blocklist level primitives are in the general form operator source blocklist destination blocklist context identifier and may be utilized to offload atomic components of larger composite virtual machine operations to the disk array. However those with ordinary skill in the art will appreciate that other additional and alternative blocklist level primitives may be supported by the disk array without departing from the spirit and scope of the claimed invention.

At step the file system within VMKernel of the operating system receives a request to zero out a file. For example in a particular embodiment that implements virtualization VMFS in VMKernel may receive a request to zero out a file such as virtual disk e.g. to preserve VM isolation . The file system resolves the file into its component file segments at step where in step represents a list of file segments. Fileid is a unique identifier that distinguishes segments associated with one file from segments associated with another file. At step VMKernel resolves the file segments into logical extents. At step VMKernel resolves each of the logical extents into a corresponding list of LUN blocks . At step VMKernel consolidates these lists of LUN blocks into a sourceblocklist the ordered list LUN blocks representing the relevant file. At step VMKernel generates a new zero blocklist primitive containing the sourceblocklist and embeds it into the CDB of the standard SCSI command WRITE BUFFER. At step VMKernel issues the WRITE BUFFER command to the disk array. At decision step if the disk array supports the new zero blocklist primitive then at step internal disk array mechanisms translate the sourceblocklist to corresponding spindle extents and write zeroes into the extents representing the relevant file.

At decision step if storage system does not support the new zero blocklist primitive then at step for each block in the sourceblocklist VMKernel generates a SCSI WRITE SAME command with the value of zero in the write buffer. At step VMKernel issues the WRITE SAME command to storage system . At step storage system receives the WRITE SAME command internally translates the LUN block into the appropriate spindle extents and write zeroes into the extent representing the block. At decision step VMKernel determines if zeroes should be written for another block in the sourceblocklist and if so steps and are repeated to generate and issue SCSI WRITE SAME commands for another block to storage system . When all of the blocks have been processed VMKernel proceeds to step and execution is complete. Those with ordinary skill in the art will recognize that different functional components or layers of VMKernel may implement steps to . For example in an embodiment that implements virtualization VMFS layer of VMKernel may perform steps to of resolving a file into segments and then into logical extents. Logical volume manager may perform steps to of generating the LUN block operations logical volume manager of VMKernel may convert the sourceblocklist into the raw SCSI WRITE BUFFER operation at step and device access layer of VMKernel ultimately transmits the WRITE BUFFER operation at step to storage system .

Zeroed extents and that correspond to segment within spindles and are shown in . Metadata is configured to store an extent map including the virtual LUN assuming that each spindle extent is 64 Kbyte in size to spindle extent pair mapping as shown in TABLE 1 where s1 s2 and s3 may each correspond to one of spindles . Although each spindle extent is shown as 64 Kbytes other sizes may be used for the spindle extents. The zeroed extents may be unmapped from their respective extent maps by updating metadata . Metadata is updated to indicate that those extents are zeroed without necessarily writing zeroes and proprietary mechanisms may be employed to lazily zero out requested extents using a background process even for non thin provisioned LUNs. For example a flag in metadata for each spindle extent corresponding to segment where the flag indicates that the extent should effectively be presented as zeroes to the user. Techniques for performing lazy zeroing are described in patent application Ser. No. 12 050 805 that is titled INITIALIZING FILE DATA BLOCKS filed Mar. 18 2008. Metadata related to the zero primitive may also be stored as well as configuration information that is described in detail in conjunction with B C A and B.

At step logical volume manager resolves each of the logical extents for each of file A and file B into their corresponding lists of LUN blocks . At step logical volume manager consolidates these lists of LUN blocks into a sourceblocklist and a destinationblocklist for file A and file B respectively which are the ordered list LUN blocks representing the respective files. At step VMKernel generates the new clone blocklist primitive containing the sourceblocklist and destinationblocklist and embeds it into the CDB of the standard SCSI command WRITE BUFFER. At step VMKernel issues the SCSI command to storage system . At decision step if storage system supports the new clone blocklist primitive then at step internal disk array mechanisms clone the destinationblocklist s list of extents with sourceblocklist s list of extents including utilizing any hardware optimizations within storage system such as copy on write techniques .

If however at decision step storage system does not support the new clone blocklist primitive then at step for each block in the sourceblocklist VMKernel generates a SCSI XCOPY command with the of the destinationblocklist. At step VMKernel issues the SCSI XCOPY command to storage system . At step storage system receives the XCOPY command internally translates the LUN block into the appropriate spindle extents and copies the source extent into the destination extent representing the block. At decision step VMKernel determines if more blocks in sourceblocklist remain to be cloned and if so steps and are repeated to generate and issue SCSI XCOPY commands for another block to storage system . When all of the blocks have been processed the clone operation is complete. Those with ordinary skill in the art will recognize that different functional components or layers of VMKernel may implement steps to . For example in an embodiment that implements virtualization VMFS layer of VMKernel may perform steps of generating the LUN block operations logical volume manager of VMKernel may create the sourceblocklist and destinationblocklist at steps and convert it into the raw SCSI XCOPY operation at step and device access layer of VMKernel ultimately transmits the XCOPY operation at step to storage system .

Similar to the zero primitive embodiment of the embodiment of envisions that a SCSI based disk array supports a new blocklist level primitive called delete that takes a context identifier such as virtual machine identifier VMID and a sourceblocklist as parameters and can be embedded into the CBD of the standard SCSI WRITE BUFFER command. At step the file system within the kernel of the operating system receives a request to delete a file. For example in a particular embodiment that implements virtualization VMFS in VMKernel may receive a request to delete a file such as virtual disk . At step the file system resolves the file into its component file segments. At step VMFS resolves the file segments into logical extents and at step logical volume manager resolves each of the file segments into their corresponding list of LUN blocks . At step logical volume manager consolidates these lists of LUN blocks into a sourceblocklist the ordered list LUN blocks representing the relevant file. At step VMKernel generates the new delete blocklist primitive containing the sourceblocklist and embeds it into the CDB of the standard SCSI command WRITE BUFFER. At step VMKernel issues the WRITE BUFFER command to the disk array. At step the internal disk array mechanisms are able to translate the sourceblocklist to corresponding spindle extents and mark them as deleted or update metadata to indicate that the extents should be returned to the free pool.

By exposing file administrative level operations such as zero clone and delete to the set of file operations available to computer systems communicating with a NAS based storage device storage vendors provide computer systems an ability to offload resource intensive communication with the file storage into the NAS device itself which can then leverage any proprietary hardware optimizations that may be available internally to the NAS device. In one embodiment file level primitives may be accessed as I O control commands using a pre existing standard command of the communication interface protocol between the computer system and NAS device or alternatively may be added as an additional commands to the set of standard commands. The following discussion presents three possible file level primitives supported by a NAS based storage system i.e. zero for zeroing out files clone for cloning files and delete for deleting files . These three file level primitives may be utilized to offload atomic components of larger composite virtual machine operations to the storage system. However those with ordinary skill in the art will appreciate that other additional and alternative blocklist level primitives may be supported by the storage system without departing from the spirit and scope of the claimed invention.

Those with ordinary skill in the art will recognize that different functional components or layers of the kernel may implement steps to . Conventional NAS devices may be configured to write zeroes to blocks to perform administrative operations however that functionality is not available to users of the NAS device such as VMs . Without the ZERO BLOCKS command VMs transfer zeroes to the NAS device to write zeroes to the blocks corresponding to a file. In some cases for example when a two terabyte virtual disk is used as many as two terabytes of zeroes are transferred to the NAS device compared with transferring 20 bytes of parameters using the ZERO BLOCKS command in order to offload the storage operation from computer system to the NAS device e.g. storage system . Additionally any administrative optimizations that are provided by the NAS device may also be leveraged through the ZERO BLOCKS command. For example particular NAS devices may be configured to not store zeroes at the time of the command is received.

Similar to the ZERO BLOCKS and CLONE BLOCKS primitives the embodiment of envisions that a NAS device supports a new file level primitive DELETE BLOCKS that takes a filehandle offset and a length as parameters and can be issued as an ioctl command. Those with ordinary skill in the art will recognize that the name of the primitive used and the number of parameters supported by the primitive are implementation choices that are available to persons skilled in the art. At step VMFS within VMKernel receives a request to delete a segment specified by a fileid an offset and length. At step VMKernel determines the filehandle for the file. At step VMKernel prepares the DELETE BLOCKS ioctl command and at step VMKernel issues the ioctl command to the NAS device. At step internal disk array mechanisms are able to delete corresponding spindle extents of the file and keep track of such extents as being freed by the file system such that they may be utilized in internal optimization techniques for tasks such as thin provisioning de duplication mirroring and replication. For example in an embodiment where LUNs may be thin provisioned deletion of a file segment through the foregoing delete blocklist primitive enables the NAS device to unmap the extents associated with the file segment from the extent maps of their associated thin provisioned LUNs thereby returning the unmapped extents to the free pool of extents.

Those with ordinary skill in the art will recognize that the foregoing discussions as well as B A B A B A B and C are merely exemplary and that alternative blocklist and file level primitives may be implemented without departing from the spirit and scope of the claimed invention. Furthermore while this discussion has focused upon transmitting blocklist level primitives where the blocklist is representative of an entire file on the file system those with ordinary skill in the art will recognize that alternative embodiments may work with smaller blocklists such as blocklists at the file segment level. For example in the case of zeroing out virtual disk in an alternative file segment blocklist level embodiment would require 4 instances of issuing the zero blocklist primitive to storage system i.e. one for each of the file segments in comparison to a single instance of the zero blocklist primitive containing a blocklist comprising a consolidation of the 4 smaller blocklists for the 4 file segments .

The primitives discussed above can be used to build hardware assisted data protection e.g. snapshotting cloning mirroring and replication and other file management commands that operate at the file level and leverage the disk array s internal capabilities. A snapshot of a virtual disk is a copy of the virtual disk as it existed at a given point in time i.e. a previous version of a virtual disk . A virtualized system such as may use the zero primitive of B A B A B A B and C for a cloning operations for eager zeroed virtual disks b initializing new file blocks in thin provisioned virtual disks c initializing previously unwritten blocks for zeroed virtual disks and d integrating thin provisioned and zeroed virtual disk formats with the disk array s hardware based thin provisioning. Similarly embedding blocklists within the XCOPY primitive as depicted in and the CLONE BLOCKS file primitive of may be utilized for a instant provisioning of virtual disks and b snapshotting of virtual disks. The delete primitive of may be used for a destroying or reformatting files on a space optimized thin provisioned or de duplicated or protected mirrored replicated or snapshotted volume b deleting virtual disks or virtual disks snapshots on a space optimized or protected volume and c integrating thin provisioned and zeroed thick virtual disk formats with the disk array s hardware based thin provisioning.

For example using blocklist with the XCOPY SCSI operation as discussed in or the CLONE BLOCKS command as discussed in enables a virtualized system to provide instant provisioning of virtual disks in the order of a few milliseconds or seconds in comparison to a few minutes or hours without the combined use of blocklists and WRITE BUFFER or XCOPY. Instant provisioning involves making a full copy of a template virtual disk during the process of creating or provisioning a new virtual machine within a computer system. Because virtual disks are represented as significantly large files on the file system performing continual standard SCSI READ and WRITE operations at a LUN block level including use of read and write buffers within the computer system takes up significant time and resources. By converting the files into blocklists and utilizing the WRITE BUFFER or XCOPY SCSI command the effort to perform the cloning can be offloaded to the hardware of the storage system itself.

Similarly the delete primitive of B and C facilitates the management of thinly provisioned virtual disks within a virtualized system. Rather than allocating storage space for a virtual disk in anticipation of future needs a thin provisioned virtual disk is allocated the minimum amount of storage space for its current needs and dynamically provided additional space from a pool of free space when the virtual disk demands it. As discussed in the context of B and C because the delete blocklist primitive or DELETE BLOCKS command frees extents in a storage system and enables the storage system controller to unmap such freed extents from the extent maps of the LUNs previously using those extents these extents can be returned to the free pool of extents utilized by other thinly provisioned virtual disks in need of additional storage space.

The detailed description provided herein with reference to relates to a virtualized computer system. However those of ordinary skill in the art will recognize that even non virtualized computer systems may benefit from such blocklist level primitives any files existing at the file system level i.e. not necessarily representative of virtual LUNs of any computer system may take advantage of such blocklist level primitives. Similarly while the foregoing discussion has utilized the SCSI interface as a primary example of protocol communication between the disk array and computer system those with ordinary skill in the art will also appreciate that other communication protocols may be utilized without departing from the spirit and scope of the claimed invention. In particular as described in conjunction with B and C a NAS device that provides file level access to storage through protocols such as NFS in contrast to a SAN disk array supporting SCSI rather than embedding blocklist primitives into the CDB of pre existing SCSI commands may use functional file primitives may be developed as ioctl control functions for NFS s standard ioctl operation.

When hardware based storage operations are to be carried out on VM components such as virtual disks the context of such operations is conveyed as configuration information or to storage system or NAS device respectively through third party server . For example when setting up a hardware thin provisioned virtual disk for VM refer to storage system is instructed to map a context identifier associated with the virtual disk to metadata indicating that VM is thin provisioned storing the thin provisioning attribute in configuration information . Extension receives SCI for composite storage operations and generates configuration information that is supplied to storage system so that storage system will recognize that files associated with the context identifier are thin provisioned. Previously unwritten extents corresponding to VM can be unmapped in the background or at creation time. Furthermore zero writes coming into storage system as zero operations can be silently discarded by storage system . Without configuration information or storage system or NAS device would be unaware that VM is thin provisioned.

Basic operations such as open close delete and the like that do not require the movement of data are performed by VMKernel . The data moving portion of the storage operations such as copying are offloaded from VMKernel to the storage system under control of third party server using vendor specific extension or a plugin as described in conjunction with . Additionally primitives such as zero clone and delete can each convey the context identifier as part of their payload so that an operation can be instantaneously mapped to the properties that govern the context requesting the operation. Furthermore the primitives can also carry per operation directives. For example a primitive may include a directive to make a full clone eagerly copy blocks for a given set of file segments instead of a quick clone copy on write blocks .

At step third party server receives operation specific configuration information that is included in SCI from VMKernel . Third party server transcribes the operation specific configuration information into a vendor specific format to generate configuration information or that is needed for the composite storage operation and supplies it to the storage device e.g. NAS device or storage system through out of band paths and respectively. Alternatively the operation specific configuration information may be transcribed by the storage device to produce configuration information or . The configuration information may be included in the extent map.

When mirroring is performed the configuration information indicates that a relationship exists between the file being mirrored and the mirror file and may indicate whether or not the mirroring is synchronous or asynchronous and a quality of service QOS level. Similarly with replication the configuration information may indicate whether compression or encryption should be employed. The configuration information is used by the storage device to keep the mirror file updated as the file being mirrored is modified e.g. written to snapshotted rolled back and the like. Writes to the file being mirrored or replicated will be reflected to the mirrored or replicated file by storage system or NAS device based on the configuration information or respectively. Writes for replicated extents are reflected by storage system to a storage system in another data center. Importantly the transfer of data is performed by the storage system rather than third party server virtual center application or VMKernel .

At step third party server controls the execution of the composite storage operation by the storage device in order to offload storage operations from VMKernel . For example third party server issues a command to mirror or replicate the file by creating a clone.

Creating the new file F on the storage device is necessary to complete the mirror operations. It is also necessary to provide configuration information to virtual center application and the storage device via third party server so that storage system or NAS device is aware that F is a mirror of F in order for the storage device to properly maintain F as a mirror file. Therefore at step third party server supplies the configuration information to the storage device as previously described in conjunction with . Also without requiring any action by VMKernel virtual center application sets up the mirroring through third party server . The new file F is created F is copied to F and virtual center application provides configuration information to the storage device that indicates F is a mirror of F through third party server .

Since file F is a mirror of file F all writes to F are also reflected to F by the storage device. At step VMKernel receives a command to snapshot file F. A redo log is created by the storage device for F. Since virtual center application was provided configuration information indicating that F is a mirror of F a redo log is also created by VMkernel on instruction from virtual center application for F. Virtual center application then sets up a redo log for F that is a mirror of the redo log for F. As data is written to F through the virtual machine file system the data writes are captured in the redo log file for F and mirrored to the redo log file for F. Virtual center application sends the blocklists for data writes to third party server for F and third party server performs the mirroring for F and the redo log files for F and F.

At step virtual center application receives a command to rollback F to the most recent snaphot. Virtual center application and VMKernel perform the rollback command for F and F. If the redo log had not been created by virtual center application and VMkernel F would not be an accurate mirror of F after step is completed. Because third party server provides the storage device with configuration information or the storage device properly maintains F as a mirror of F including the creation and maintenance of the redo log. If the configuration information is not provided to the storage device VMKernel is burdened with the task of creating and maintaining F and the redo log. Third party server beneficially allows VMkernel or virtual center application to control NAS and SAN capabilities of storage devices NAS device and storage system respectively in order to offload the movement of data from VMKernel .

At step virtual center application receives a command to fracture F from F and third party server updates the configuration information for the storage devices to indicate that file F is no longer a mirror of file F and the F redo log is no longer a mirror of the F redo log. Virtual center application deletes the F redo log through VMkernel . At step virtual center application receives a command to delete file F. VMkernel deletes the F file from VMFS and third party server deletes the F file from the storage devices. The previously described delete primitive may be used by third party server to delete the F file producing updated configuration information or . Virtual center application will also make corresponding changes to VM configuration information at the time of fracture and deletion in order to decouple the affected file from the current state of the VM .

Offloading the data moving portion of storage operations such as copying from VMkernel to the storage devices using third party server improves the efficiency of the host system. Additionally the transfer of configuration information associated with composite storage operations by third party server to storage devices enables the integration of the storage device capabilities into the virtual machine workflow.

Switch shown in may be used to offload storage operations from VMkernel by translating primitives e.g. copy and clone enroute to storage devices that do not support the new blocklist primitives. For example new primitives such as copy and clone may be translated by switch for execution by the physical storage devices when a storage device is not configured to execute the blocklist primitive directly. This translation offloads the data moving portion of the storage operations from the file system. Another advantage of offloading storage operations through switch is that switch functions for storage systems provided by different vendors and therefore allows for interoperability with existing and disparate storage systems. Switch may perform the translation or a virtual target such as another computer system or may be setup by the switch to perform the translation. Additionally per VM policies may be specified and switch may be configured to implement those policies. Example policies include RAID levels snapshot frequency data protection QOS quality of service thin provisioning and the like. The policies effect the handling of the data movement when a storage operation is performed and may be included in SCI configuration information or configuration information .

When a CPP does not include support for translating a particular blocklist primitive VMFS or LVM in the requesting computer system or instructs switch to create a virtual target having a unique LUN identifier such as virtual target server . CPP is then programmed to communicate with virtual target server . Virtual target server is configured as a proxy host that is able to translate blocklist primitives for execution by the storage devices. CPP routes packets with blocklist primitives that it is not configured to translate to virtual target server . Virtual target server translates the blocklist primitive for execution by the storage devices to offload the data movement portion of the storage operations from VMKernel . When the zero blocklist primitive is used CPP or virtual target server may translate the zero blocklist primitive into SCSI WRITE SAME commands with the value of zero for each block in the sourceblocklist as previously described in conjunction with . When the clone blocklist primitive is used CPP or virtual target server may translate the clone blocklist primitive into SCSI XCOPY commands for each block in the sourceblocklist as previously described in conjunction with .

If the operation included in the protocol packet is a simple one then at step a DPP processes the protocol packet. Otherwise at decision step step CPP determines if it is configured to translate the blocklist primitive into commands that can be executed by the storage device. If the CPP is not configured to translate the blocklist primitive then at step the CPP routes the protocol packet to virtual target server for translation. At step virtual target server receives the protocol packet including the translated blocklist primitive translates the blocklist primitive into commands for execution by the target storage device and returns the protocol packet with the translated blocklist primitive to switch for routing to the target destination. A DPP will process the packet and route it to the target destination.

Returning to step if the CPP determines that it is configured to translate the blocklist primitive then at step the CPP translates the blocklist primitive into commands for execution by the target storage device. At step the CPP or DPP that processed the protocol packet at step outputs the protocol packet to the target destination e.g. storage device. The translation of the blocklist primitive into commands by either CPP or virtual target server offloads the storage operations from VMKernel .

Outside of the flow diagram shown in the host system determines whether or not the storage device is configured to execute blocklist primitives. A protocol packet including a blocklist primitive is provided by the host system for translation when a storage device is not configured to execute the primitive. If switch is unable to translate a primitive switch reports an error to the host system and the host system handles the translation.

One or more embodiments of the invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored.

The invention has been described above with reference to specific embodiments. Persons skilled in the art however will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

