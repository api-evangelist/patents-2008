---

title: Loading software on a plurality of processors
abstract: Loading software on a plurality of processors is presented. A processing unit (PU) retrieves a file from system memory and loads it into its internal memory. The PU extracts a processor type from the file's header which identifies whether the file should execute on the PU or a synergistic processing unit (SPU). If an SPU should execute the file, the PU DMA's the file to the SPU for execution. In one embodiment, the file is a combined file which includes both PU and SPU code. In this embodiment, the PU identifies one or more section headers included in the file which indicates embedded SPU code within the combined file. In this embodiment, the PU extracts the SPU code from the combined file and DMA's the extracted code to an SPU for execution.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07748006&OS=07748006&RS=07748006
owner: International Business Machines Corporation
number: 07748006
owner_city: Armonk
owner_country: US
publication_date: 20080602
---
This application is a continuation application of co pending U.S. Non Provisional patent application Ser. No. 10 670 842 entitled Loading Software on a Plurality of Processors filed on Sep. 25 2003.

The present invention relates in general to a system and method for loading software on a plurality of processors. More particularly the present invention relates to a system and method for extracting a processor type from a file and loading the file on a processor that corresponds to the processor type.

Computer systems are becoming more and more complex. The computer industry typically doubles the performance of a computer system every 18 months such as personal computers PDAs and gaming consoles. In order for the computer industry to accomplish this task the semiconductor industry produces integrated circuits that double in performance every 18 months. A computer system uses an integrated circuit for particular functions based upon the integrated circuit s architecture. Two fundamental architectures are 1 a microprocessor based architecture and 2 a digital signal processor based architecture.

An integrated circuit with a microprocessor based architecture is typically used to handle control operations whereas an integrated circuit with a digital signal processor based architecture is typically designed to handle signal processing functions i.e. mathematical operations . As technology evolves the computer industry and the semiconductor industry are using both architectures or processor types in a computer system design.

Software is another element in a computer system that has been evolving alongside integrated circuit evolution. A software developer writes code in a manner that corresponds to the processor type that executes the code. For example a processor has a particular number of registers and a particular number of arithmetic logic units ALUs whereby the software developer designs his code to most effectively use the registers and the ALUs.

As the semiconductor industry incorporates multiple processor types onto a single device and as software developers write code to execute on multiple processor type architectures a challenge found is identifying which files to load on a particular processor type.

Executable files typically employ a runtime loader which loads dependent files onto memory. The runtime loader however assumes that the same processor that is executing the runtime loader executes the dependent files. In a multi processor environment however this may not be the case. In addition in a heterogeneous processor environment the code for a particular file is formatted for a particular processor type and may not run if the code is loaded on a different processor type.

What is needed therefore is a system and method for associating a processor type to a file and loading the file on a processor that corresponds to the associated processor type.

It has been discovered that the aforementioned challenges are resolved by extracting a processor identifier that is included in a file s header and loading the file on a processor that corresponds to the extracted processor identifier.

A computer system includes a processing unit PU and a synergistic processing unit SPU . The PU boots up and initializes the computer system during which time the PU loads an operating system. The operating system performs basic tasks such as recognizing input from a keyboard sending output to a display screen keeping track of files and directories on a disk and controlling peripheral devices such as disk drives and printers. The operating system includes a kernel that is a central module of the operating system and is responsible for memory management process management task management and disk management.

The kernel loads a PU program into the PU s internal memory. During the loading process the kernel identifies a runtime loader that corresponds to the PU program. The runtime loader is responsible for loading objects resolving symbols and loading other files i.e. data programs that correspond to the PU program. The kernel loads the runtime loader into the PU s internal memory and passes control to the runtime loader. The runtime loader identifies files that the PU program depends such as an SPU file. The runtime loader loads the SPU file into the PU s internal memory and extracts a processor identifier from the SPU file s header. For example the SPU file may be an ELF formatted file in which case the file includes a machine type SPU in its ELF header which is a processor identifier that correlates the file to an SPU.

The runtime loader determines that the SPU file should run on an SPU based upon the SPU file s processor identifier and sends the SPU file to an SPU using a DMA command. The SPU receives the SPU file and stores it in the SPU s local memory. The SPU begins executing the SPU file and in turn loads an SPU runtime loader in its internal memory.

During the SPU file s execution the SPU runtime loader retrieves and loads files in which the SPU file depends. For example the SPU file may be a graphics program whereby it requires a plug in module for manipulating data. The SPU runtime loader recognizes that the SPU file requires a plug in and sends a request to the PU. The PU receives the request and retrieves the plug in from system memory. The PU program sends the plug in to the SPU runtime loader using a DMA command whereby the SPU runtime loader stores the plug in in SPU internal memory. The SPU file may also receive data from the PU program using the same technique as described above.

In one embodiment the SPU runtime loader independently retrieves a plug in and or data from system memory without intervention from the PU program. In this embodiment the SPU program may send an acknowledgement message to the PU indicating that the SPU is finished with its processing task.

The foregoing is a summary and thus contains by necessity simplifications generalizations and omissions of detail consequently those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. Other aspects inventive features and advantages of the present invention as defined solely by the claims will become apparent in the non limiting detailed description set forth below.

The following is intended to provide a detailed description of an example of the invention and should not be taken to be limiting of the invention itself. Rather any number of variations may fall within the scope of the invention which is defined in the claims following the description.

The overall architecture for a computer system in accordance with the present invention is shown in .

As illustrated in this figure system includes network to which is connected a plurality of computers and computing devices. Network can be a LAN a global network such as the Internet or any other computer network.

The computers and computing devices connected to network the network s members include e.g. client computers server computers personal digital assistants PDAs digital television DTV and other wired or wireless computers and computing devices. The processors employed by the members of network are constructed from the same common computing module. These processors also preferably all have the same ISA and perform processing in accordance with the same instruction set. The number of modules included within any particular processor depends upon the processing power required by that processor.

For example since servers of system perform more processing of data and applications than clients servers contain more computing modules than clients . PDAs on the other hand perform the least amount of processing. PDAs therefore contain the smallest number of computing modules. DTV performs a level of processing between that of clients and servers . DTV therefore contains a number of computing modules between that of clients and servers . As discussed below each computing module contains a processing controller and a plurality of identical processing units for performing parallel processing of the data and applications transmitted over network .

This homogeneous configuration for system facilitates adaptability processing speed and processing efficiency. Because each member of system performs processing using one or more or some fraction of the same computing module the particular computer or computing device performing the actual processing of data and applications is unimportant. The processing of a particular application and data moreover can be shared among the network s members. By uniquely identifying the cells comprising the data and applications processed by system throughout the system the processing results can be transmitted to the computer or computing device requesting the processing regardless of where this processing occurred. Because the modules performing this processing have a common structure and employ a common ISA the computational burdens of an added layer of software to achieve compatibility among the processors is avoided. This architecture and programming model facilitates the processing speed necessary to execute e.g. real time multimedia applications.

To take further advantage of the processing speeds and efficiencies facilitated by system the data and applications processed by this system are packaged into uniquely identified uniformly formatted software cells . Each software cell contains or can contain both applications and data. Each software cell also contains an ID to globally identify the cell throughout network and system . This uniformity of structure for the software cells and the software cells unique identification throughout the network facilitates the processing of applications and data on any computer or computing device of the network. For example a client may formulate a software cell but because of the limited processing capabilities of client transmit this software cell to a server for processing. Software cells can migrate therefore throughout network for processing on the basis of the availability of processing resources on the network.

The homogeneous structure of processors and software cells of system also avoids many of the problems of today s heterogeneous networks. For example inefficient programming models which seek to permit processing of applications on any ISA using any instruction set e.g. virtual machines such as the Java virtual machine are avoided. System therefore can implement broadband processing far more effectively and efficiently than today s networks.

The basic processing module for all members of network is the processing unit PU . illustrates the structure of a PU. As shown in this figure PE comprises a processing unit PU a direct memory access controller DMAC and a plurality of synergistic processing units SPUs namely SPU SPU SPU SPU SPU SPU SPU and SPU . A local PE bus transmits data and applications among the SPUs DMAC and PU . Local PE bus can have e.g. a conventional architecture or be implemented as a packet switch network. Implementation as a packet switch network while requiring more hardware increases available bandwidth.

PE can be constructed using various methods for implementing digital logic. PE preferably is constructed however as a single integrated circuit employing a complementary metal oxide semiconductor CMOS on a silicon substrate. Alternative materials for substrates include gallium arsinide gallium aluminum arsinide and other so called III B compounds employing a wide variety of dopants. PE also could be implemented using superconducting material e.g. rapid single flux quantum RSFQ logic. PE is closely associated with a dynamic random access memory DRAM through a high bandwidth memory connection . DRAM functions as the main memory for PE . Although a DRAM preferably is a dynamic random access memory DRAM could be implemented using other means e.g. as a static random access memory SRAM a magnetic random access memory MRAM an optical memory or a holographic memory. DMAC facilitates the transfer of data between DRAM and the SPUs and PU of PE . As further discussed below DMAC designates for each SPU an exclusive area in DRAM into which only the SPU can write data and from which only the SPU can read data. This exclusive area is designated a sandbox. 

PU can be e.g. a standard processor capable of stand alone processing of data and applications. In operation PU schedules and orchestrates the processing of data and applications by the SPUs. The SPUs preferably are single instruction multiple data SIMD processors. Under the control of PU the SPUs perform the processing of these data and applications in a parallel and independent manner. DMAC controls accesses by PU and the SPUs to the data and applications stored in the shared DRAM . Although PE preferably includes eight SPUs a greater or lesser number of SPUs can be employed in a PU depending upon the processing power required. Also a number of PUs such as PE may be joined or packaged together to provide enhanced processing power.

For example as shown in four PUs may be packaged or joined together e.g. within one or more chip packages to form a single processor for a member of network . This configuration is designated a broadband engine BE . As shown in BE contains four PUs namely PE PE PE and PE . Communications among these PUs are over BE bus . Broad bandwidth memory connection provides communication between shared DRAM and these PUs. In lieu of BE bus communications among the PUs of BE can occur through DRAM and this memory connection.

Input output I O interface and external bus provide communications between broadband engine and the other members of network . Each PU of BE performs processing of data and applications in a parallel and independent manner analogous to the parallel and independent processing of applications and data performed by the SPUs of a PU.

Local memory is not a cache memory. Local memory is preferably constructed as an SRAM. Cache coherency support for an SPU is unnecessary. A PU may require cache coherency support for direct memory accesses initiated by the PU. Cache coherency support is not required however for direct memory accesses initiated by an SPU or for accesses from and to external devices.

SPU further includes bus for transmitting applications and data to and from the SPU. In a preferred embodiment this bus is 1 024 bits wide. SPU further includes internal busses and . In a preferred embodiment bus has a width of 256 bits and provides communications between local memory and registers . Busses and provide communications between respectively registers and floating point units and registers and integer units . In a preferred embodiment the width of busses and from registers to the floating point or integer units is 384 bits and the width of busses and from the floating point or integer units to registers is 128 bits. The larger width of these busses from registers to the floating point or integer units than from these units to registers accommodates the larger data flow from registers during processing. A maximum of three words are needed for each calculation. The result of each calculation however normally is only one word.

Using this standardized modular structure numerous other variations of processors can be constructed easily and efficiently. For example the processor shown in comprises two chip packages namely chip package comprising a BE and chip package comprising four VSs. Input output I O provides an interface between the BE of chip package and network . Bus provides communications between chip package and chip package . Input output processor IOP controls the flow of data into and out of I O . I O may be fabricated as an application specific integrated circuit ASIC . The output from the VSs is video signal .

The chip package of comprises two PEs and and two VSs and . An I O provides an interface between the chip package and network . The output from the chip package is a video signal. This configuration may function as e.g. a graphics work station.

A final configuration is shown in . This processor consists of only a single VS and an I O . This configuration may function as e.g. a PDA.

A plurality of BEs can be connected together in various configurations using such optical wave guides and the four optical ports of each BE. For example as shown in two or more BEs e.g. BE BE and BE can be connected serially through such optical ports. In this example optical interface of BE is connected through its optical ports to the optical ports of optical interface of BE . In a similar manner the optical ports of optical interface on BE are connected to the optical ports of optical interface of BE .

A matrix configuration is illustrated in . In this configuration the optical interface of each BE is connected to two other BEs. As shown in this figure one of the optical ports of optical interface of BE is connected to an optical port of optical interface of BE . The other optical port of optical interface is connected to an optical port of optical interface of BE . In a similar manner one optical port of optical interface of BE is connected to the other optical port of optical interface of BE . The other optical port of optical interface is connected to an optical port of optical interface of BE . This matrix configuration can be extended in a similar manner to other BEs.

Using either a serial configuration or a matrix configuration a processor for network can be constructed of any desired size and power. Of course additional ports can be added to the optical interfaces of the BEs or to processors having a greater or lesser number of PUs than a BE to form other configurations.

BE also includes switch unit . Switch unit enables other SPUs on BEs closely coupled to BE to access DRAM . A second BE therefore can be closely coupled to a first BE and each SPU of each BE can address twice the number of memory locations normally accessible to an SPU. The direct reading or writing of data from or to the DRAM of a first BE from or to the DRAM of a second BE can occur through a switch unit such as switch unit .

For example as shown in to accomplish such writing the SPU of a first BE e.g. SPU of BE issues a write command to a memory location of a DRAM of a second BE e.g. DRAM of BE rather than as in the usual case to DRAM of BE . DMAC of BE sends the write command through cross bar switch to bank control and bank control transmits the command to an external port connected to bank control . DMAC of BE receives the write command and transfers this command to switch unit of BE . Switch unit identifies the DRAM address contained in the write command and sends the data for storage in this address through bank control of BE to bank of DRAM . Switch unit therefore enables both DRAM and DRAM to function as a single memory space for the SPUs of BE .

As discussed above all of the multiple SPUs of a PU can independently access data in the shared DRAM. As a result a first SPU could be operating upon particular data in its local storage at a time during which a second SPU requests these data. If the data were provided to the second SPU at that time from the shared DRAM the data could be invalid because of the first SPU s ongoing processing which could change the data s value. If the second processor received the data from the shared DRAM at that time therefore the second processor could generate an erroneous result. For example the data could be a specific value for a global variable. If the first processor changed that value during its processing the second processor would receive an outdated value. A scheme is necessary therefore to synchronize the SPUs reading and writing of data from and to memory locations within the shared DRAM. This scheme must prevent the reading of data from a memory location upon which another SPU currently is operating in its local storage and therefore which are not current and the writing of data into a memory location storing current data.

To overcome these problems for each addressable memory location of the DRAM an additional segment of memory is allocated in the DRAM for storing status information relating to the data stored in the memory location. This status information includes a full empty F E bit the identification of an SPU SPU ID requesting data from the memory location and the address of the SPU s local storage LS address to which the requested data should be read. An addressable memory location of the DRAM can be of any size. In a preferred embodiment this size is 1024 bits.

The setting of the F E bit to 1 indicates that the data stored in the associated memory location are current. The setting of the F E bit to 0 on the other hand indicates that the data stored in the associated memory location are not current. If an SPU requests the data when this bit is set to 0 the SPU is prevented from immediately reading the data. In this case an SPU ID identifying the SPU requesting the data and an LS address identifying the memory location within the local storage of this SPU to which the data are to be read when the data become current are entered into the additional memory segment.

An additional memory segment also is allocated for each memory location within the local storage of the SPUs. This additional memory segment stores one bit designated the busy bit. The busy bit is used to reserve the associated LS memory location for the storage of specific data to be retrieved from the DRAM. If the busy bit is set to 1 for a particular memory location in local storage the SPU can use this memory location only for the writing of these specific data. On the other hand if the busy bit is set to 0 for a particular memory location in local storage the SPU can use this memory location for the writing of any data.

Examples of the manner in which the F E bit the SPU ID the LS address and the busy bit are used to synchronize the reading and writing of data from and to the shared DRAM of a PU are illustrated in .

As shown in one or more PUs e.g. PE interact with DRAM . PE includes SPU and SPU . SPU includes control logic and SPU includes control logic . SPU also includes local storage . This local storage includes a plurality of addressable memory locations . SPU includes local storage and this local storage also includes a plurality of addressable memory locations . All of these addressable memory locations preferably are 1024 bits in size.

An additional segment of memory is associated with each LS addressable memory location. For example memory segments and are associated with respectively local memory locations and and memory segment is associated with local memory location . A busy bit as discussed above is stored in each of these additional memory segments. Local memory location is shown with several Xs to indicate that this location contains data.

DRAM contains a plurality of addressable memory locations including memory locations and . These memory locations preferably also are 1024 bits in size. An additional segment of memory also is associated with each of these memory locations. For example additional memory segment is associated with memory location and additional memory segment is associated with memory location . Status information relating to the data stored in each memory location is stored in the memory segment associated with the memory location. This status information includes as discussed above the F E bit the SPU ID and the LS address. For example for memory location this status information includes F E bit SPU ID and LS address .

Using the status information and the busy bit the synchronized reading and writing of data from and to the shared DRAM among the SPUs of a PU or a group of PUs can be achieved.

The result of the successful synchronized writing of the data into memory location is shown in . The written data are stored in memory location and F E bit is set to 1. This setting indicates that memory location is full and that the data in this memory location are current and valid.

As shown in control logic next issues a synchronize read command for memory location of DRAM . Since F E bit associated with this memory location is set to 1 the data stored in memory location are considered current and valid. As a result in preparation for transferring the data from memory location to LS memory location F E bit is set to 0. This setting is shown in . The setting of this bit to 0 indicates that following the reading of these data the data in memory location will be invalid.

As shown in the data within memory location next are read from memory location to LS memory location . shows the final state. A copy of the data in memory location is stored in LS memory location . F E bit is set to 0 to indicate that the data in memory location are invalid. This invalidity is the result of alterations to these data to be made by SPU . The busy bit in memory segment also is set to 0. This setting indicates that LS memory location now is available to SPU for any purpose i.e. this LS memory location no longer is in a reserved state waiting for the receipt of specific data. LS memory location therefore now can be accessed by SPU for any purpose.

As shown in the SPU ID and LS address for this read command next are written into memory segment . In this case the SPU ID for SPU and the LS memory location for LS memory location are written into memory segment . When the data within memory location become current therefore this SPU ID and LS memory location are used for determining the location to which the current data are to be transmitted.

The data in memory location become valid and current when an SPU writes data into this memory location. The synchronized writing of data into memory location from e.g. memory location of SPU is illustrated in . This synchronized writing of these data is permitted because F E bit for this memory location is set to 0.

As shown in following this writing the data in memory location become current and valid. SPU ID and LS address from memory segment therefore immediately are read from memory segment and this information then is deleted from this segment. F E bit also is set to 0 in anticipation of the immediate reading of the data in memory location . As shown in upon reading SPU ID and LS address this information immediately is used for reading the valid data in memory location to LS memory location of SPU . The final state is shown in . This figure shows the valid data from memory location copied to memory location the busy bit in memory segment set to 0 and F E bit in memory segment set to 0. The setting of this busy bit to 0 enables LS memory location now to be accessed by SPU for any purpose. The setting of this F E bit to 0 indicates that the data in memory location no longer are current and valid.

As shown in this figure in empty state a synchronized writing operation is permitted and results in a transition to full state . A synchronized reading operation however results in a transition to the blocking state because the data in the memory location when the memory location is in the empty state are not current.

In full state a synchronized reading operation is permitted and results in a transition to empty state . On the other hand a synchronized writing operation in full state is prohibited to prevent overwriting of valid data. If such a writing operation is attempted in this state no state change occurs and an error message is transmitted to the SPU s corresponding control logic.

In blocking state the synchronized writing of data into the memory location is permitted and results in a transition to empty state . On the other hand a synchronized reading operation in blocking state is prohibited to prevent a conflict with the earlier synchronized reading operation which resulted in this state. If a synchronized reading operation is attempted in blocking state no state change occurs and an error message is transmitted to the SPU s corresponding control logic.

The scheme described above for the synchronized reading and writing of data from and to the shared DRAM also can be used for eliminating the computational resources normally dedicated by a processor for reading data from and writing data to external devices. This input output I O function could be performed by a PU. However using a modification of this synchronization scheme an SPU running an appropriate program can perform this function. For example using this scheme a PU receiving an interrupt request for the transmission of data from an I O interface initiated by an external device can delegate the handling of this request to this SPU. The SPU then issues a synchronize write command to the I O interface. This interface in turn signals the external device that data now can be written into the DRAM. The SPU next issues a synchronize read command to the DRAM to set the DRAM s relevant memory space into a blocking state. The SPU also sets to 1 the busy bits for the memory locations of the SPU s local storage needed to receive the data. In the blocking state the additional memory segments associated with the DRAM s relevant memory space contain the SPU s ID and the address of the relevant memory locations of the SPU s local storage. The external device next issues a synchronize write command to write the data directly to the DRAM s relevant memory space. Since this memory space is in the blocking state the data are immediately read out of this space into the memory locations of the SPU s local storage identified in the additional memory segments. The busy bits for these memory locations then are set to 0. When the external device completes writing of the data the SPU issues a signal to the PU that the transmission is complete.

Using this scheme therefore data transfers from external devices can be processed with minimal computational load on the PU. The SPU delegated this function however should be able to issue an interrupt request to the PU and the external device should have direct access to the DRAM.

The DRAM of each PU includes a plurality of sandboxes. A sandbox defines an area of the shared DRAM beyond which a particular SPU or set of SPUs cannot read or write data. These sandboxes provide security against the corruption of data being processed by one SPU by data being processed by another SPU. These sandboxes also permit the downloading of software cells from network into a particular sandbox without the possibility of the software cell corrupting data throughout the DRAM. In the present invention the sandboxes are implemented in the hardware of the DRAMs and DMACs. By implementing these sandboxes in this hardware rather than in software advantages in speed and security are obtained.

The PU of a PU controls the sandboxes assigned to the SPUs. Since the PU normally operates only trusted programs such as an operating system this scheme does not jeopardize security. In accordance with this scheme the PU builds and maintains a key control table. This key control table is illustrated in . As shown in this figure each entry in key control table contains an identification ID for an SPU an SPU key for that SPU and a key mask . The use of this key mask is explained below. Key control table preferably is stored in a relatively fast memory such as a static random access memory SRAM and is associated with the DMAC. The entries in key control table are controlled by the PU. When an SPU requests the writing of data to or the reading of data from a particular storage location of the DRAM the DMAC evaluates the SPU key assigned to that SPU in key control table against a memory access key associated with that storage location.

As shown in a dedicated memory segment is assigned to each addressable storage location of a DRAM . A memory access key for the storage location is stored in this dedicated memory segment. As discussed above a further additional dedicated memory segment also associated with each addressable storage location stores synchronization information for writing data to and reading data from the storage location.

In operation an SPU issues a DMA command to the DMAC. This command includes the address of a storage location of DRAM . Before executing this command the DMAC looks up the requesting SPU s key in key control table using the SPU s ID . The DMAC then compares the SPU key of the requesting SPU to the memory access key stored in the dedicated memory segment associated with the storage location of the DRAM to which the SPU seeks access. If the two keys do not match the DMA command is not executed. On the other hand if the two keys match the DMA command proceeds and the requested memory access is executed.

An alternative embodiment is illustrated in . In this embodiment the PU also maintains a memory access control table . Memory access control table contains an entry for each sandbox within the DRAM. In the particular example of the DRAM contains 64 sandboxes. Each entry in memory access control table contains an identification ID for a sandbox a base memory address a sandbox size a memory access key and an access key mask . Base memory address provides the address in the DRAM which starts a particular memory sandbox. Sandbox size provides the size of the sandbox and therefore the endpoint of the particular sandbox.

The key masks for the SPU keys and the memory access keys provide greater flexibility to this system. A key mask for a key converts a masked bit into a wildcard. For example if the key mask associated with an SPU key has its last two bits set to mask designated by e.g. setting these bits in key mask to 1 the SPU key can be either a 1 or a 0 and still match the memory access key. For example the SPU key might be 1010. This SPU key normally allows access only to a sandbox having an access key of 1010. If the SPU key mask for this SPU key is set to 0001 however then this SPU key can be used to gain access to sandboxes having an access key of either 1010 or 1011. Similarly an access key 1010 with a mask set to 0001 can be accessed by an SPU with an SPU key of either 1010 or 1011. Since both the SPU key mask and the memory key mask can be used simultaneously numerous variations of accessibility by the SPUs to the sandboxes can be established.

The present invention also provides a new programming model for the processors of system . This programming model employs software cells . These cells can be transmitted to any processor on network for processing. This new programming model also utilizes the unique modular architecture of system and the processors of system .

Software cells are processed directly by the SPUs from the SPU s local storage. The SPUs do not directly operate on any data or programs in the DRAM. Data and programs in the DRAM are read into the SPU s local storage before the SPU processes these data and programs. The SPU s local storage therefore includes a program counter stack and other software elements for executing these programs. The PU controls the SPUs by issuing direct memory access DMA commands to the DMAC.

The structure of software cells is illustrated in . As shown in this figure a software cell e.g. software cell contains routing information section and body . The information contained in routing information section is dependent upon the protocol of network . Routing information section contains header destination ID source ID and reply ID . The destination ID includes a network address. Under the TCP IP protocol e.g. the network address is an Internet protocol IP address. Destination ID further includes the identity of the PU and SPU to which the cell should be transmitted for processing. Source ID contains a network address and identifies the PU and SPU from which the cell originated to enable the destination PU and SPU to obtain additional information regarding the cell if necessary. Reply ID contains a network address and identifies the PU and SPU to which queries regarding the cell and the result of processing of the cell should be directed.

Cell body contains information independent of the network s protocol. The exploded portion of shows the details of cell body . Header of cell body identifies the start of the cell body. Cell interface contains information necessary for the cell s utilization. This information includes global unique ID required SPUs sandbox size and previous cell ID .

Global unique ID uniquely identifies software cell throughout network . Global unique ID is generated on the basis of source ID e.g. the unique identification of a PU or SPU within source ID and the time and date of generation or transmission of software cell . Required SPUs provides the minimum number of SPUs required to execute the cell. Sandbox size provides the amount of protected memory in the required SPUs associated DRAM necessary to execute the cell. Previous cell ID provides the identity of a previous cell in a group of cells requiring sequential execution e.g. streaming data.

Implementation section contains the cell s core information. This information includes DMA command list programs and data . Programs contain the programs to be run by the SPUs called spulets e.g. SPU programs and and data contain the data to be processed with these programs. DMA command list contains a series of DMA commands needed to start the programs. These DMA commands include DMA commands and . The PU issues these DMA commands to the DMAC.

DMA command includes VID . VID is the virtual ID of an SPU which is mapped to a physical ID when the DMA commands are issued. DMA command also includes load command and address . Load command directs the SPU to read particular information from the DRAM into local storage. Address provides the virtual address in the DRAM containing this information. The information can be e.g. programs from programs section data from data section or other data. Finally DMA command includes local storage address . This address identifies the address in local storage where the information should be loaded. DMA commands contain similar information. Other DMA commands are also possible.

DMA command list also includes a series of kick commands e.g. kick commands and . Kick commands are commands issued by a PU to an SPU to initiate the processing of a cell. DMA kick command includes virtual SPU ID kick command and program counter . Virtual SPU ID identifies the SPU to be kicked kick command provides the relevant kick command and program counter provides the address for the program counter for executing the program. DMA kick command provides similar information for the same SPU or another SPU.

As noted the PUs treat the SPUs as independent processors not co processors. To control processing by the SPUs therefore the PU uses commands analogous to remote procedure calls. These commands are designated SPU Remote Procedure Calls SRPCs . A PU implements an SRPC by issuing a series of DMA commands to the DMAC. The DMAC loads the SPU program and its associated stack frame into the local storage of an SPU. The PU then issues an initial kick to the SPU to execute the SPU Program.

In step the PU evaluates the spulet and then designates an SPU for processing the spulet. In step the PU allocates space in the DRAM for executing the spulet by issuing a DMA command to the DMAC to set memory access keys for the necessary sandbox or sandboxes. In step the PU enables an interrupt request for the designated SPU to signal completion of the spulet. In step the PU issues a DMA command to the DMAC to load the spulet from the DRAM to the local storage of the SPU. In step the DMA command is executed and the spulet is read from the DRAM to the SPU s local storage. In step the PU issues a DMA command to the DMAC to load the stack frame associated with the spulet from the DRAM to the SPU s local storage. In step the DMA command is executed and the stack frame is read from the DRAM to the SPU s local storage. In step the PU issues a DMA command for the DMAC to assign a key to the SPU to allow the SPU to read and write data from and to the hardware sandbox or sandboxes designated in step . In step the DMAC updates the key control table KTAB with the key assigned to the SPU. In step the PU issues a DMA command kick to the SPU to start processing of the program. Other DMA commands may be issued by the PU in the execution of a particular SRPC depending upon the particular spulet.

As indicated above second portion of illustrates the steps performed by the SPU in executing the spulet. In step the SPU begins to execute the spulet in response to the kick command issued at step . In step the SPU at the direction of the spulet evaluates the spulet s associated stack frame. In step the SPU issues multiple DMA commands to the DMAC to load data designated as needed by the stack frame from the DRAM to the SPU s local storage. In step these DMA commands are executed and the data are read from the DRAM to the SPU s local storage. In step the SPU executes the spulet and generates a result. In step the SPU issues a DMA command to the DMAC to store the result in the DRAM. In step the DMA command is executed and the result of the spulet is written from the SPU s local storage to the DRAM. In step the SPU issues an interrupt request to the PU to signal that the SRPC has been completed.

The ability of SPUs to perform tasks independently under the direction of a PU enables a PU to dedicate a group of SPUs and the memory resources associated with a group of SPUs to performing extended tasks. For example a PU can dedicate one or more SPUs and a group of memory sandboxes associated with these one or more SPUs to receiving data transmitted over network over an extended period and to directing the data received during this period to one or more other SPUs and their associated memory sandboxes for further processing. This ability is particularly advantageous to processing streaming data transmitted over network e.g. streaming MPEG or streaming ATRAC audio or video data. A PU can dedicate one or more SPUs and their associated memory sandboxes to receiving these data and one or more other SPUs and their associated memory sandboxes to decompressing and further processing these data. In other words the PU can establish a dedicated pipeline relationship among a group of SPUs and their associated memory sandboxes for processing such data.

In order for such processing to be performed efficiently however the pipeline s dedicated SPUs and memory sandboxes should remain dedicated to the pipeline during periods in which processing of spulets comprising the data stream does not occur. In other words the dedicated SPUs and their associated sandboxes should be placed in a reserved state during these periods. The reservation of an SPU and its associated memory sandbox or sandboxes upon completion of processing of an spulet is called a resident termination. A resident termination occurs in response to an instruction from a PU.

On the other hand if a software cell contains MPEG data then in step SPU examines previous cell ID of the cell to identify the MPEG data stream to which the cell belongs. In step SPU chooses an SPU of the dedicated pipeline for processing of the cell. In this case SPU chooses SPU to process these data. This choice is based upon previous cell ID and load balancing factors. For example if previous cell ID indicates that the previous software cell of the MPEG data stream to which the software cell belongs was sent to SPU for processing then the present software cell normally also will be sent to SPU for processing. In step SPU issues a synchronize write command to write the MPEG data to sandbox . Since this sandbox previously was set to the blocking state the MPEG data in step automatically is read from sandbox to the local storage of SPU . In step SPU processes the MPEG data in its local storage to generate video data. In step SPU writes the video data to sandbox . In step SPU issues a synchronize read command to sandbox to prepare this sandbox to receive additional MPEG data. In step SPU processes a resident termination. This processing causes this SPU to enter the reserved state during which the SPU waits to process additional MPEG data in the MPEG data stream.

Other dedicated structures can be established among a group of SPUs and their associated sandboxes for processing other types of data. For example as shown in a dedicated group of SPUs e.g. SPUs and can be established for performing geometric transformations upon three dimensional objects to generate two dimensional display lists. These two dimensional display lists can be further processed rendered by other SPUs to generate pixel data. To perform this processing sandboxes are dedicated to SPUs and for storing the three dimensional objects and the display lists resulting from the processing of these objects. For example source sandboxes and are dedicated to storing the three dimensional objects processed by respectively SPU SPU and SPU . In a similar manner destination sandboxes and are dedicated to storing the display lists resulting from the processing of these three dimensional objects by respectively SPU SPU and SPU .

Coordinating SPU is dedicated to receiving in its local storage the display lists from destination sandboxes and . SPU arbitrates among these display lists and sends them to other SPUs for the rendering of pixel data.

The processors of system also employ an absolute timer. The absolute timer provides a clock signal to the SPUs and other elements of a PU which is both independent of and faster than the clock signal driving these elements. The use of this absolute timer is illustrated in .

As shown in this figure the absolute timer establishes a time budget for the performance of tasks by the SPUs. This time budget provides a time for completing these tasks which is longer than that necessary for the SPUs processing of the tasks. As a result for each task there is within the time budget a busy period and a standby period. All spulets are written for processing on the basis of this time budget regardless of the SPUs actual processing time or speed.

For example for a particular SPU of a PU a particular task may be performed during busy period of time budget . Since busy period is less than time budget a standby period occurs during the time budget. During this standby period the SPU goes into a sleep mode during which less power is consumed by the SPU.

The results of processing a task are not expected by other SPUs or other elements of a PU until a time budget expires. Using the time budget established by the absolute timer therefore the results of the SPUs processing always are coordinated regardless of the SPUs actual processing speeds.

In the future the speed of processing by the SPUs will become faster. The time budget established by the absolute timer however will remain the same. For example as shown in an SPU in the future will execute a task in a shorter period and therefore will have a longer standby period. Busy period therefore is shorter than busy period and standby period is longer than standby period . However since programs are written for processing on the basis of the same time budget established by the absolute timer coordination of the results of processing among the SPUs is maintained. As a result faster SPUs can process programs written for slower SPUs without causing conflicts in the times at which the results of this processing are expected.

In lieu of an absolute timer to establish coordination among the SPUs the PU or one or more designated SPUs can analyze the particular instructions or microcode being executed by an SPU in processing an spulet for problems in the coordination of the SPUs parallel processing created by enhanced or different operating speeds. No operation NOOP instructions can be inserted into the instructions and executed by some of the SPUs to maintain the proper sequential completion of processing by the SPUs expected by the spulet. By inserting these NOOPs into the instructions the correct timing for the SPUs execution of all instructions can be maintained.

Kernel loads PU program into memory which is an program that is executed by PU . During the loading process kernel identifies runtime loader which corresponds to PU program . Runtime loader is responsible for loading objects resolving symbols and loading other files i.e. data programs that correspond to PU program . Kernel loads runtime loader into memory and passes control to runtime loader . Runtime loader starts to identify files that PU program depends such as SPU file . Runtime loader loads SPU file into memory and extracts a processor identifier from SPU file s header. For example SPU file may be an ELF formatted file in which case it includes a machine type in its ELF header which signifies which processor to be loaded see B and corresponding text for further details regarding header information .

Runtime loader determines that SPU file should run on SPU based upon SPU file s processor identifier and sends SPU file to SPU using a DMA command such as DMA . SPU receives SPU file and stores it in memory . Memory is SPU s local memory area. SPU is now able to execute SPU file see and corresponding text for further details regarding code execution .

Runtime loader loads combined file into memory . Runtime loader then analyzes combined file s file header to identify whether there is embedded code included in combined file which is targeted for SPU see B and corresponding text for further details regarding file headers . Runtime loader identifies SPU file from combined file s header information and sends SPU file to memory using DMA . DMA provides a mechanism for PU to send data from its memory e.g. memory directly to SPU s memory e.g. memory . Once SPU receives SPU file SPU is ready to execute SPU file see and corresponding text for further details regarding code execution .

SPU file may also receive data from PU program using the same technique as described above. When SPU file is finished processing data using plug in SPU file sends acknowledgment to PU which indicates that SPU is finished processing data. In one embodiment acknowledgement may include processed data or may include a memory location reference where PU retrieves resultant data.

During SPU file s execution runtime loader retrieves and loads files in which SPU file depends. For example SPU file may be a graphics program whereby it requires a plug in module for manipulating data. Runtime loader recognizes that SPU file requires a plug in such as plug in and identifies the location in system memory in which plug in is located by checking a memory map that corresponds to system memory .

In one embodiment SPU includes a memory management unit MMU . The MMU includes a Direct Memory Access DMA in which SPU access system memory. In this embodiment runtime loader retrieves plug in from system memory using DMA and loads plug in into memory . SPU file uses plug in to process data and sends acknowledgement to PU program when complete.

Section headers include contents of embedded files. Particularly line includes an .spuelf program which is an SPU program that a runtime loader loads into an SPU s local store and is executed independently from the PU. In one embodiment a symbol may be included in the .spuelf file which provides access to the file when it is loaded and mapped into system memory.

In one embodiment SPU code is loaded into system memory implicitly when referenced by PU code at link time. In another embodiment SPU code is loaded into system memory explicitly when it is dynamically determined to load SPU code. In these embodiments a symbol points to the SPU ELF image in system memory of an SPU program or plug in. Since the PU code controls and initiates SPU activity special APIs Application Programming Interfaces may be used which indicate that the SPU code located at a particular symbol i.e. system memory address should be loaded onto an SPU and executed. When one of these APIs is called the SPU ELF image is parsed in memory validated that it is an SPU ELF image using the ELF machine type and then sent to an available SPU using a DMA execution. These APIs may be thought of as extensions to the runtime loader that are dynamically invoked when a PU program wishes to initiate SPU activity and load execute SPU code.

The kernel retrieves a runtime loader from system memory that corresponds to the executable file and loads the runtime loader in memory at step . The runtime loader is responsible for loading files that the executable file depends. For example the executable file may be a retirement prediction program and the runtime loader may load a dependent file such as an interest rate application.

The kernel passes control to the runtime loader at step whereupon the runtime loader retrieves a first dependent file from system memory at step . Using the example described above the runtime loader retrieves the interest rate application from system memory . The runtime loader loads the dependent file into memory at step . The dependent file includes a header such as an ELF header whereby the header includes a processor identifier such as a machine type that distinguishes which type of processor the dependent file should run see B and corresponding text for further details regarding processor identifiers . The runtime loader extracts the dependent file s corresponding processor identifier at step .

A determination is made as to whether the processor identifier corresponds to a PU processor or an SPU processor decision . Using the example described above the runtime loader determines whether the interest rate application should be loaded onto a PU or an SPU. If the processor identifier corresponds to an SPU decision branches to No branch whereupon processing DMA s the dependent file from the PU s memory e.g. memory to SPU step . SPU is the same SPU that is shown in .

On the other hand if the dependent file should be executed on the PU decision branches to Yes branch whereupon DMA steps are unnecessary since the dependent file is already loaded onto the PU s memory e.g. memory .

A determination is made as to whether the executable file has more dependent files to load decision . If the executable file has more dependent files to load decision branches to Yes branch which loops back to retrieve step and process the next dependent file. This looping continues until there are no more dependent files to process at which point decision branches to No branch whereupon processing ends at .

A determination is made as to whether the SPU should start processing data corresponding to the dependent file or whether it should wait for a request from PU decision . For example PU may be configuring multiple processors and PU may not wish the SPU to start processing data until the other processors are configured. If the SPU should wait for a request from PU decision branches to No branch whereupon processing waits until it receives a request from PU step . On the other hand if processing should process data using the dependent file without waiting for a request from PU decision branches to Yes branch bypassing request waiting steps.

The SPU executes the file and the file initiates a runtime loader at step . The runtime loader is responsible for loading objects resolving symbols and loading other files i.e. data programs that correspond to the SPU s dependent file. The runtime loader identifies a plug in and or data that the dependent file requires at step . For example the dependent file may be a graphics application and the graphics application requires an edge smoothing plug in.

A determination is made as to whether the SPU should retrieve the plug in and or data independent of PU . This determination may be accomplished based upon a programming model which is chosen for the overall PU SPU application. For example in some applications the PU code starts the SPU program and then controls the SPU program. In another example the SPU program controls itself after being initiated by the PU. May hardware facilities may be utilized by both the PU and SPU to synchronize their activity and coordinate data movement such as load with reservation and store conditional instructions mailboxes signals DMAs polling on status waiting for interrupts events etc.

If the SPU should process the dependent file independently decision branches to No branch whereupon processing sends a request to PU requesting a particular plug in or data step . Processing receives data and or a plug in from PU and stores it in memory at step . On the other hand if the SPU should retrieve the plug in or data independently decision branches to Yes branch whereupon the SPU retrieves data and or a plug in from system memory and stores it in memory step . SPU processes the data at step and sends an acknowledgement to PU at step .

A determination is made as to whether to continue processing decision . For example PU may request the SPU to continue to have a financial calculator loaded waiting for user input until the user exits from a particular user interface. If processing should continue decision branches to Yes branch which loops back to process more data and or load more plug in s. This looping continues until the SPU should not continue processing data using the dependent file at which point decision branches to No branch whereupon processing ends at .

Each SPC may be configured to perform a different task and accordingly in one embodiment each SPC may be accessed using different instruction sets. If PE is being used in a wireless communications system for example each SPC may be responsible for separate processing tasks such as modulation chip rate processing encoding network interfacing etc. In another embodiment the SPCs may have identical instruction sets and may be used in parallel with each other to perform operations benefiting from parallel processing.

PE may also include level 2 cache such as L2 cache for the use of PU . In addition PE includes system memory which is shared between PU and the SPUs. System memory may store for example an image of the running operating system which may include the kernel device drivers I O configuration etc. executing applications as well as other data. System memory includes the local storage units of one or more of the SPCs which are mapped to a region of system memory . For example local storage may be mapped to mapped region local storage may be mapped to mapped region and local storage may be mapped to mapped region . PU and the SPCs communicate with each other and system memory through bus that is configured to pass data between these devices.

The MMUs are responsible for transferring data between an SPU s local store and the system memory. In one embodiment an MMU includes a direct memory access DMA controller configured to perform this function. PU may program the MMUs to control which memory regions are available to each of the MMUs. By changing the mapping available to each of the MMUs the PU may control which SPU has access to which region of system memory . In this manner the PU may for example designate regions of the system memory as private for the exclusive use of a particular SPU. In one embodiment the SPUs local stores may be accessed by PU as well as by the other SPUs using the memory map. In one embodiment PU manages the memory map for the common system memory for all the SPUs. The memory map table may include PU s L2 Cache system memory as well as the SPUs shared local stores.

In one embodiment the SPUs process data under the control of PU . The SPUs may be for example digital signal processing cores microprocessor cores micro controller cores etc. or a combination of the above cores. Each one of the local stores is a storage area associated with a particular SPU. In one embodiment each SPU can configure its local store as a private storage area a shared storage area or an SPU may configure its local store as a partly private and partly shared storage.

For example if an SPU requires a substantial amount of local memory the SPU may allocate 100 of its local store to private memory accessible only by that SPU. If on the other hand an SPU requires a minimal amount of local memory the SPU may allocate 10 of its local store to private memory and the remaining 90 to shared memory. The shared memory is accessible by PU and by the other SPUs. An SPU may reserve part of its local store in order for the SPU to have fast guaranteed memory access when performing tasks that require such fast access. The SPU may also reserve some of its local store as private when processing sensitive data as is the case for example when the SPU is performing encryption decryption.

In one embodiment SPU code is loaded into system memory implicitly when referenced by PU code at link time. In another embodiment SPU code is loaded into system memory explicitly when it is dynamically determined to load SPU code. In these embodiments a symbol points to the SPU ELF image in system memory of an SPU program or plug in. Since the PU code controls and initiates SPU activity special APIs Application Programming Interfaces may be used which indicate that the SPU code located at a particular symbol i.e. system memory address should be loaded onto an SPU and executed. When one of these APIs is called the SPU ELF image is parsed in memory validated that it is an SPU ELF image using the ELF machine type and then DMA d onto an available SPU. These APIs may be thought of as extensions to the runtime loader that are dynamically invoked when a PU program wishes to initiate SPU activity and load execute SPU code.

One of the preferred implementations of the invention is an application namely a set of instructions program code in a code module which may for example be resident in the random access memory of the computer. Until required by the computer the set of instructions may be stored in another computer memory for example on a hard disk drive or in removable storage such as an optical disk for eventual use in a CD ROM or floppy disk for eventual use in a floppy disk drive or downloaded via the Internet or other computer network. Thus the present invention may be implemented as a computer program product for use in a computer. In addition although the various methods described are conveniently implemented in a general purpose computer selectively activated or reconfigured by software one of ordinary skill in the art would also recognize that such methods may be carried out in hardware in firmware or in more specialized apparatus constructed to perform the required method steps.

While particular embodiments of the present invention have been shown and described it will be obvious to those skilled in the art that based upon the teachings herein changes and modifications may be made without departing from this invention and its broader aspects and therefore the appended claims are to encompass within their scope all such changes and modifications as are within the true spirit and scope of this invention. Furthermore it is to be understood that the invention is solely defined by the appended claims. It will be understood by those with skill in the art that if a specific number of an introduced claim element is intended such intent will be explicitly recited in the claim and in the absence of such recitation no such limitation is present. For a non limiting example as an aid to understanding the following appended claims contain usage of the introductory phrases at least one and one or more to introduce claim elements. However the use of such phrases should not be construed to imply that the introduction of a claim element by the indefinite articles a or an limits any particular claim containing such introduced claim element to inventions containing only one such element even when the same claim includes the introductory phrases one or more or at least one and indefinite articles such as a or an the same holds true for the use in the claims of definite articles.

