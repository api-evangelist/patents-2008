---

title: Constructing an inference graph for a network
abstract: Constructing an inference graph relates to the creation of a graph that reflects dependencies within a network. In an example embodiment, a method includes determining dependencies among components of a network and constructing an inference graph for the network responsive to the dependencies. The components of the network include services and hardware components, and the inference graph reflects cross-layer components including the services and the hardware components. In another example embodiment, a system includes a service dependency analyzer and an inference graph constructor. The service dependency analyzer is to determine dependencies among components of a network, the components including services and hardware components. The inference graph constructor is to construct an inference graph for the network responsive to the dependencies, the inference graph reflecting cross-layer components including the services and the hardware components.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08443074&OS=08443074&RS=08443074
owner: Microsoft Corporation
number: 08443074
owner_city: Redmond
owner_country: US
publication_date: 20080228
---
This U.S. Nonprovisional Patent Application claims the benefit of U.S. Provisional Patent Application 60 893 348 filed on 6 Mar. 2007 and entitled Constructing an Inference Graph . U.S. Provisional Patent Application 60 893 348 is hereby incorporated by reference in its entirety herein.

Using a network based service can be a frustrating experience that is marked by the appearances of familiar hourglass or beachball icons. These icons indicate that a request is in progress and that a user must continue to wait for a response to the request. Unfortunately the request may not be fulfilled for some time if ever. The user is often not provided an accurate indication of when the request is expected to be fulfilled. Moreover when there is a problem the user is rarely provided a reliable indication of where the problem lies and the user is even less likely to be told how the problem might be mitigated.

Even inside the network of a single enterprise where traffic does not need to cross the open Internet users are subjected to the negative effects of network problems. Information technology IT personnel of the enterprise are charged with locating and mitigating these network problems. Unfortunately IT personnel are often also uncertain how to diagnose and remedy such network problems. Although IT personnel are given management tools that indicate when a particular hardware component e.g. a server link switch etc. is overloaded these tools can produce so many alerts that the IT personnel eventually start to ignore them. Such management tools also usually fail to address the integrated and changing nature of enterprise networks. In short network problems tend to persist because current network and service monitoring tools do not scale to the size complexity or rate of change of today s enterprise networks.

Constructing an inference graph relates to the creation of a graph that reflects dependencies within a network. In an example embodiment a method includes determining dependencies among components of a network and constructing an inference graph for the network responsive to the dependencies. The components of the network include services and hardware components and the inference graph reflects cross layer components including the services and the hardware components. In another example embodiment a system includes a service dependency analyzer and an inference graph constructor. The service dependency analyzer is to determine dependencies among components of a network the components including services and hardware components. The inference graph constructor is to construct an inference graph for the network responsive to the dependencies the inference graph reflecting cross layer components including the services and the hardware components.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter. Moreover other method system scheme apparatus device media procedure API arrangement etc. implementations are described herein.

Localizing the sources of network problems in large e.g. enterprise networks is extremely challenging. Dependencies are numerous complex and inherently multi level spanning a plethora of hardware and software components across both the core and the computing infrastructure of the network. An inference graph model is described herein that can be employed to discover and actually exploit these dependencies for fast and accurate network problem localization. Network problems can arise at any level and as a result of any general network component. For example network problems can be within the core of a network e.g. routers links switches etc. with the endhosts of a network e.g. clients that are primarily used by humans servers that primarily provide some service or application to clients and or other servers etc. with services of the network e.g. websites web services DNS etc. and so forth. Examples of general network components that can cause and or be affected by network problems are described herein below with particular reference to .

Embodiments of this inference graph model are adaptable to address user perceptible problems that are rooted in conditions giving rise to partial service degradation as well as hard faults. Furthermore methods systems etc. are described that may be used to construct inference graphs for an operational enterprise network or other networks and that may be used to infer various attributes of the network. The inference graph results may also be leveraged for relatively fast and accurate network problem localization and alerting.

For certain example embodiments a general inference system is described. An inference system can provide IT administrators tools to localize performance degradation issues as well as hard failures that can affect an end user. An example inference implementation can 1 detect the existence of hard faults and performance degradations by monitoring the response times of service requests 2 determine a set of components that might be responsible for a given hard fault or performance degradation and or 3 localize the network problem to the most likely component s using probabilistic techniques. The portions of an example inference system that are described extensively herein pertain primarily to constructing an inference graph that may be used for network problem localization.

A number of challenges may be confronted when implementing an inference system. Three example challenges follow. First even relatively simple requests like fetching a webpage involve multiple services such as DNS servers authentication servers web servers and the backend SQL databases that hold the web page data. Problems at any of these servers can affect the success or failure of the request. Unfortunately the dependencies among different components in IT systems are typically not documented anywhere and they evolve continually as systems grow or new applications are added. Nevertheless an example embodiment of an inference system is able to automatically discover the set of components involved in the processing of requests.

Second both performance degradations and hard faults can stem from problems anywhere in the IT infrastructure i.e. a service a router a link and so forth. Therefore while there is some value in using an inference graph to monitor the services and the network elements separately additional information and or accuracy can be achieved by correlating their interdependencies. Consequently an example embodiment of an inference system is capable of operating across both service and network layers simultaneously. Third failover and load balancing techniques commonly used in enterprise networks make determining the responsible component even more difficult because the set of components involved with a requested service may change from request to request. Consequently an example embodiment of an inference system enables such failover and load balancing techniques to be modeled as part of the inference graph.

In an example embodiment of an inference system a number of aspects may be implemented. First software agents of the inference system may run on each endhost to analyze the packets that the endhost sends and receives so as to determine the set of services that the endhost depends on. The agent may also track the distribution of response times experienced for each service with which the endhost communicates fit a Gaussian model to the empirical data and generate notifications when a response time falls outside the nominal range. Second the inference system may combine the individual views of dependency computed by each endhost e.g. and realized as a service level dependency graph to assemble an inference graph. The inference graph can capture the dependencies between each of the components of the IT network infrastructure. This inference graph can be a multi level graph so as to represent the multiple levels of dependencies that are found in a typical IT network infrastructure for example as servers depend on other servers.

The inference system can use information provided by one or more endhosts to fill in any gaps in the dependency information that is reported from another endhost. When constructing the inference graph the inference system may augment it with information about the routers and links used to carry packets between endhosts. The inference system can therefore encode in a single model each of the components of a network that can affect a service request. The inference graph can then be used by the inference system along with agent observations to localize network problems. Third in addition to the aspects described above that can be performed automatically network operators may be empowered to incorporate into the inference graph model the load balancing and failover mechanisms that are present in their networks.

To focus on performance degradations as well as hard faults certain described embodiments of inference systems address problems that affect the users of the IT infrastructure by using monitored response times as indicators of performance degradations. This can mitigate an issue with current management approaches in which operators are overwhelmed with many relatively meaningless alerts that report parameter based overloading situations that may not even directly affect users. In contrast example inference systems as described herein usually generate alarms in response to user perceptible network problems including performance degradations or hard faults. An aspect of an example approach to recognizing user perceptible performance degradations pertains to characterizing components in terms of three potential states e.g. up down or troubled . These three states are described further herein below with particular reference to .

In the following sections example general principles for inference systems and inference graphs are described with particular reference to . are referenced to describe the determination of service dependencies and the building of service level dependency graphs. The construction of an inference graph from the service level dependency graphs is described with particular reference to . A general device that may be used to implement embodiments for constructing an inference graph is described with reference to .

In an example embodiment each of servers participates in providing one or more services. A service is some functionality provided by one or more endhosts. A service may be defined within a network and or for a given inference system embodiment in any manner. By way of example a service may be defined as an IPaddress port pair. However many alternative definitions of a service are possible. For example a service may be defined as all of the messages sent and received by processes with a particular name or executable running on endhost s . As another example a service may be defined as the messages recognized by a packet parser such as NetMon Ethereal WireShark etc. as part of the same protocol suite or application e.g. Hyper Text Transfer Protocol HTTP Network Time Protocol NTP NETBIOS Remote Procedure Call RPC Server Message Block SMB etc. . An inference system may also use any combination of such definitions as a service. Each client is an endhost that may request a service from a server . Network elements may be machines such as routers and switches e.g. network elements or links such as wireless or wired transmission media e.g. network elements . An example taxonomy for network components that further elaborates on the terminologies of and interrelationships between these components is described further herein below with particular reference to .

In an example embodiment inference graph constructor constructs an inference graph at server . A portion of an example inference graph is described herein below with particular reference to . An inference engine may use a constructed inference graph to probabilistically determine at least one likely cause of one or more user perceivable network problems. Although shown in as being located at a single server inference engine and inference graph constructor may be implemented in alternative manners. For example their operation and or the construction of inference graph may be effectuated in a distributed manner such as at all or a portion of service dependency analyzers at servers at clients some combination thereof and so forth. Also although shown separately in any of inference engine service dependency analyzers and or inference graph constructor may be integrated together into fewer total units or may be separated into a greater number of modular units.

As is introduced above and described further herein below network dependencies may be inferred at least partly by monitoring messages that are communicated between endhosts. These messages are monitored by obtaining packet traces for packets communicated between the endhosts. Service dependency analyzers are responsible for obtaining packet traces for the packets that are exchanged between the endhosts. In short network dependencies may be captured based on monitored messages with the obtained packet traces. Service level dependency graphs may be built from the captured dependencies and an inference graph may be constructed using the service level dependency graphs. Example embodiments for obtaining packet traces building service level dependency graphs and constructing an inference graph are described herein below with particular reference to .

Hardware components may be realized as one or more devices an example of which is described herein below with particular reference to . Examples of hardware components include network elements and endhost machines . Examples of network elements include routers R switches S and links L . Examples of endhost machines include servers clients and middle boxes . Other alternative components which are not specifically illustrated may also fit into component taxonomy . For instance a hub may be a network element .

As is apparent from a review of graph the times are bi modal. Thirteen percent of the requests take 10 longer than normal which results in user perceptible lags of 3 to 10 seconds. As is shown at the first response time mode is indicated to be that the service is considered up with a normal performance. As is shown at the second response time mode is indicated to be that the service is considered troubled with an unacceptable performance.

Conventional network management systems treat each service as being either up or down. This relatively naive model hides the kinds of performance degradations evidenced by the second response time mode at of graph . To account for these types of lengthy delays that can qualify as user perceptible network problems certain example embodiments of the inference system model service availability as a tri state value. This tri state value for a service can be up when its response time is normal down when requests result in an error status code or no response at all and troubled when responses fall significantly outside of normal response times. A response may be considered to fall significantly outside of normal response times when it is an order of magnitude greater than normal when it exceeds a few seconds when it is sufficiently long so as to annoy or inconvenience users when it fails to meet a targeted temporal performance goal but does provide a correct response some combination thereof and so forth. Thus a service may be assigned a troubled status when only a subset of service requests is performing poorly.

In an example embodiment an inference graph is a labeled directed graph that provides a unified view of the dependencies in a network with the graph spanning both service and hardware network components. The structure of the dependencies is multi level.

Thus in an example embodiment nodes in an inference graph are of three types and . First root cause nodes correspond to physical components or services whose failure can cause an end user to experience failures e.g. a network problem such as a performance degradation a hard fault etc. . The granularity of root cause nodes may be a computer e.g. a machine with an IP address a router or an IP link and so forth. Alternatively implementations of an inference system may employ root causes having a finer granularity.

Second observation nodes represent accesses to network services whose performance can be measured by the inference system. There can be a separate observation node for each client that accesses any such network service. The observation nodes thus model a user s experience when using services on the network. Observation nodes can also represent other measurements made of components in the network. For example an observation node can represent the utilization of a network link reporting an up state when the link utilization is below 50 reporting a troubled state when the link utilization is above 50 and reporting a down state when the link is down. Thus each observation node may correspond to at least one measurable quantity of the network. Examples of measurable quantities include by way of example but not limitation response time link utilization machine room temperature some combination thereof and so forth.

Third meta nodes act as glue between the root cause nodes and the observation nodes. Three types of meta nodes are described herein noisy max selector and fail over. However meta nodes may have more fewer and or different types. These meta nodes model the dependencies between root causes and observations. Meta nodes are described further herein below especially in Section 2.2 Probabilistic Modeling for Meta Nodes .

The state of each node in an inference graph is expressed probabilistically by a three tuple P P P Pdenotes the probability that the node is working normally. Pis the probability that the node has experienced a fail stop failure such as when a server is down or a link is broken. Third Pis the probability that a node is troubled as described herein above wherein services physical servers or links continue to function but users perceive relatively poor performance. The sum of P P P 1. It should be noted that the state of a root cause node is independent of any other root cause nodes in the inference graph and that the state of observation nodes can be predicted from the state of their ancestors.

An edge from a node A to a node B in an inference graph encodes the dependency that node A has to be in an up or other state for node B to also be in the up or other state. In other words an edge from a node A to a node B indicates that the state of node A affects the state of node B. Equivalently this indication can be expressed as the state of B depends on the state of A. Edges may also be labeled with a dependency probability that encodes the strength of the dependency of B on A. Thus an edge from a first node A to a second node B encodes a probabilistic dependency that indicates how likely it is that a state of the first node A affects a state of the second node B.

Not all dependencies need be equal in strength. For example a client cannot access a file if the path s to the file server are down. However the client may be able to access the file even when the DNS server is down if the file server name is resolved using the client s local DNS cache. Furthermore the client may need to authenticate more or less often than resolving the server s name. To capture varying strengths of such dependencies edges in an inference graph are associated labeled with a dependency probability DP . A larger dependency probability indicates a stronger dependence.

As illustrated inference graph portion e.g. of an inference graph of includes a number of nodes and edges. Specifically five root cause nodes six meta nodes one observation node and multiple edges are shown. Only some of the edges are designated by the reference numeral for the sake of visual clarity. Two special root cause nodes AT and AD are also shown. Thus rectangles represent physical components and the software services executing thereon the oval represents an external observation the hexagons model potential points of failure and the square rectangles represent un modeled or other external factors.

More specifically in an example embodiment each inference graph has two special root cause nodes always troubled AT and always down AD . These special root cause nodes AT and AD are to model external factors that might cause a user perceived failure and that are not otherwise a part of the model. The state of the always troubled node AT is set to 0 1 0 and the state of the always down node AD is set to 0 0 1 . An edge from each of these special root cause nodes AT and AD is included to each of the observation nodes . The probabilities assigned to these edges are described herein below.

In inference graph portion the fetching of a file from a network file server by a user at client C is modeled. The user activity of fetching a file is encoded as an observation node because the inference system can determine the response time for this action. In this example fetching a file involves the user performing three actions i authenticating itself to the network via Kerberos ii resolving the DNS name of the file server via a DNS server and iii accessing the file server. These actions themselves can also depend on other events and or components to succeed. Consequently these actions are modeled as meta nodes and edges are added from each of them to the observation node of fetching a file .

Generally parent nodes are recursively constructed for each meta node and corresponding edges are added until the associated root cause nodes are reached. Examples of meta nodes include paths between endhost machines meta nodes and name resolution meta node certificate fetch meta node and file fetch meta node . Examples of root cause nodes include DNS servers and routers switches and links on paths to the servers the Kerberos authentication server and the targeted file server . To model a failover mechanism in domain name resolution between the two DNS servers DNS and DNS a fail over meta node F is introduced.

It should be noted that illustrates a portion of an inference graph. Thus a complete inference graph may include accesses made to other network services by the same user at client C as well as accesses to the illustrated file service and other services by other users at other clients in the network. Each access to a different service from an individual client and or user may correspond to a separate observation node in the complete inference graph. An example method for computing dependency probabilities for edges is described herein below in Section 3.1 Discovering Service Level Dependencies with particular reference to B and .

With a probabilistic model the states of parent nodes probabilistically govern the state of a child node. For example suppose a child has two parents A and B. The state of parent A is 0.8 0.2 0 i.e. its probability of being up is 0.8 troubled is 0.2 and down is 0. The state of parent B is 0.5 0.2 0.3 . A question then is what is the state of the child While the probability dependencies of the edge labels encode the strength of the dependence the nature of the dependency is encoded in the meta node. For an example embodiment the meta node provides or describes the state of the child node given the state of its parent nodes and the relevant dependency probabilities .

With reference to noisy max meta node N includes two parents parent P1 and parent P2. It also includes a child C as well as two dependency probabilities dand d. The following variable assignments are given x 1 dand y 1 d. Noisy max meta node N may be understood conceptually as follows. Max implies that if any of the parents are in the down state then the child is down. If no parent is down and any parent is troubled then the child is troubled. If all parents are up then the child is up. Noise implies that the dependency probability on the edge dictates the likelihood with which a parent s state affects the child s state. If the edge s weight is d then the child is not affected by its parent with probability 1 d . Thus noisy max combines the notions of both noisy and max .

Table 1 below presents a truth table for a noisy max meta node N when a child has two parents. Each of the two parents P1 and P2 can be up troubled or down. This results in a 3 3 grid as follows 

Each entry in the truth table of Table 1 is the state of the child i.e. its probability of being up troubled or down when parent P1 and parent P2 have states as per the column and row labels respectively. For instance a troubled label for parent P1 implies that its state is 0 1 0 . As an example of the truth table grid the second row and third column of the truth table can be used to determine the probability of the child being troubled given that parent P1 is down and parent P2 is troubled P Child Troubled ParentP1 Down ParentP2 Troubled is 1 d d. To explain this intuitively the child will be down unless parent P1 s state is masked by noise prob 1 d . Further if both parents are masked by noise the child will be up. Hence the child is in the troubled state only when parent P1 is drowned out by noise and parent P2 is not. Other grid entries can be similarly understood.

With reference to selector meta node S includes two parents parent P1 and parent P2 a child C and two dependency probabilities d and 1 d . It also includes a Selector indicator block. The following variable assignment is given x 1 d. The selector meta node is used to model load balancing scenarios. For example a network load balancer NLB in front of two servers may hash the client s request and distribute requests evenly to the two servers. Attempting to model this scenario using a noisy max meta node does not produce a correct result. With a noisy max meta node the child would depend on each server with a probability of 0.5 because half the requests go to each server. Thus the noisy max meta node would assign the client a 25 chance of being up even when both the servers are troubled which is clearly not accurate.

Generally the selector meta node can be used to model a variety of NLB schemes. For example selector meta nodes can model NLB servers equal cost multipath ECMP routing and so forth. ECMP is a commonly used technique in enterprise networks in which routers send packets to a destination along several paths. A path with ECMP may be selected based on the hash of the source and destination addresses in the packet.

Table 2 below presents a truth table for a selector meta node S when a child has two parents. Each of the two parents P1 and P2 can be up troubled or down. A child node selects parent P1 with probability d and parent P2 with probability 1 d. The child probabilities for the selector meta node are as presented in Table 2 below 

The grid entries in the selector meta node truth table above express the selection made by the child. For example if the child may choose each of the parents with an equal probability of 50 selector meta node S causes the child to have a zero probability of being up when both its parents are troubled. This is discernable from the first number 0 in the P2 troubled row P1 troubled column entry.

With reference to fail over meta node F includes two parents parent P1 and parent P2 a child C and two edge labels primary and secondary . It also includes a Fail over indicator block. Fail over meta nodes embrace the fail over mechanisms commonly used in enterprise networks e.g. with servers . Fail over is a redundancy technique where clients access primary production servers and fail over to backup servers when the primary server is inaccessible. Fail over cannot be accurately modeled by either the noisy max or selector meta nodes because the probability of accessing the backup server depends on the failure of the primary server.

Table 3 below presents a truth table for a fail over meta node F when a child has two parents. Each of the two parents P1 and P2 can be up troubled or down. The truth table for the fail over meta node encodes the dependence when the child primarily contacts parent P1 but fails over to parent P2 when parent P1 does not respond. The child probabilities for the fail over meta node are as presented in Table 3 below 

For a fail over meta node F as long as the primary server parent P1 is up or troubled the child is not affected by the state of the secondary server parent P2 . When the primary server is in the down state the child has a high chance of being up if the secondary server is up. Also in this case primary down secondary up the child has a small chance of being troubled as it expends time accessing the primary server which is down before falling back to the secondary server. These permutations are reflected by the probabilities in Table 3. For example if the primary server parent P1 is down and the secondary server parent P2 is up at the first row and third column the up troubled down probabilities are 0.9 0.1 0 .

Other versions of the truth tables of Tables 1 2 and 3 may alternatively be implemented. For example the P1 down P2 up entry of the fail over meta node truth table may be 1 0 0 instead of 0.9 0.1 0 . Additionally it should be understood that the meta nodes of an inference graph may be merged into other nodes in the graph. For example with reference to meta node may be merged into observation node by incorporating the probability table for the meta node into the observation node .

In an example embodiment an inference system includes a centralized inference engine and distributed service dependency analyzers . Each service dependency analyzer may include a packet trace obtainer O a service dependency determiner D and or a service level dependency graph builder B. An inference system may be implemented without changing the routers or switches the applications and or the middleware of a given enterprise or other institution. An inference system may also be implemented in alternative manners.

Generally an example inference system deployment that is capable of analyzing network symptoms and diagnosing network problems can implement a three step process to localize faults in a network. First each service dependency analyzer is responsible for monitoring the packets sent from and received by one or more endhosts to obtain packet traces . Packet traces may be obtained by packet trace obtainer O. Each service dependency analyzer may run on an endhost itself as an agent A or it may obtain packet traces via sniffing a nearby link or router as a packet sniffer PS . From these packet traces each service dependency analyzer computes the dependencies between the services with which its endhost s communicates and the response time distributions for each specified service. The service dependencies may be computed by service dependency determiner D. A service level dependency graph builder B builds service level dependency graphs from the computed service dependency information. The service level information of these dependency graphs is then relayed to inference graph constructor . Alternatively service level dependency graph builder B may be part of inference graph constructor and or inference graph may be constructed directly from the service level dependency information.

Second a network operator specifies the services that are to be monitored e.g. by IP address and port number . Inference graph constructor aggregates the dependencies between the services as computed by each of the service dependency analyzers and may employ statistical procedures to detect false positives and or false negatives. The former can be rejected and probabilistically estimated values for either can be incorporated into the inference graph. Inference graph constructor combines the aggregated dependency information with network information e.g. network topology information to compute a unified inference graph . The unified inference graph pertains to each of the service activities in which the operator is interested and represents information collected across each of the service dependency analyzers .

Third ongoing client response time evidence that is collected by service dependency analyzers is provided to inference engine . Evidence may also be client server interaction logs trouble tickets Simple Network Management Protocol SNMP counters event logs from clients servers or network elements e.g. syslog combinations thereof and so forth. Inference engine analyzes evidence given the current inference graph . In other words inference engine applies the response time evidence observations reported by service dependency analyzers to inference graph to attempt to identify fault suspects e.g. the root cause node s for links routers servers clients etc. that are responsible for any observed network problems. Inference engine may also provide suggested actions e.g. running trace routes analyzing a particular server etc. for remedying the network problems that are potentially caused by fault suspects . The first and second steps may be executed periodically or when triggered by a change in a dependency so as to capture and incorporate any changes in the network. The third step may be executed periodically when requested by an operator or when prompted by a service dependency analyzer that is observing relatively longer response times.

In an example embodiment service dependency analyzer A communicates with network of and observes packets being transmitted over it. Inference system chatter is communicated across network . Inference system chatter includes incoming inference system chatter I and outgoing inference system chatter O. Incoming inference system chatter I includes requests for trigger probes summary requests and so forth. Outgoing inference system chatter O includes dependency graphs observed evidence probe results and so forth.

Packet capture unit is responsible for obtaining packets traces from network . Packet capture unit is an example of a unit that may be used to implement at least part of a packet trace obtainer O of . The obtained packet traces are passed to an identify service level dependencies unit and a detect faults monitor evidence unit . Identify service level dependencies unit may be used to implement at least part of service dependency determiner D and or service level dependency graph builder B. Detect faults monitor evidence unit may monitor incoming performance evidence and detect when services become troubled or down. Units and communicate with process commands and send reports unit . Process commands and send reports unit controls unit unit and probe unit which performs probes over network .

Service dependency analyzer A may be implemented for example as a user level service daemon in a MICROSOFT WINDOWS operating system. Packet capture unit may use WinPCAP NetMon etc. to perform its functions. Probe unit may use troute ping wget etc. to perform its functions. Although is described in terms of an agent version of a service dependency analyzer that runs on an endhost of the illustrated units may alternatively be implemented as parts of a packet sniffer version of a service dependency analyzer that runs on a network element of or a device connected to a network element.

The acts of the flow diagrams that are described herein may be performed in many different environments and with a variety of devices such as by one or more processing devices of . The orders in which the methods are described are not intended to be construed as a limitation and any number of the described blocks can be combined augmented rearranged and or omitted to implement a respective method or an alternative method that is equivalent thereto. Although specific elements of other FIGS. are referenced in the description of the flow diagrams the methods may be performed with alternative elements.

In an example embodiment at block dependencies among components of a network are determined with the network including both service components and hardware components. For example dependencies between services and hardware components of a network may be determined. Block may be implemented with subblocks and .

At subblock messages among endhosts are monitored. For example messages between a client and one or more servers may be monitored by a packet trace obtainer O of at least one service dependency analyzer . At subblock the dependencies among the components of the network are captured based on the monitored messages. For example the dependencies may be captured by a service dependency determiner D of service dependency analyzer . For instance dependencies may be inferred by detecting temporal proximity of monitored messages. In other words the dependencies may be computed based on the messages that are detected by the act of monitoring and responsive to times or differences between the times at which the messages are detected. Dependency may be inferred for example based on the monitored messages and responsive to a predefined dependency interval. An example approach to using a dependency interval is described herein below with particular reference to .

At block service level dependency graphs are built responsive to the captured dependencies among the components. The action s of block may be performed by a service level dependency graph builder B e.g. of a service dependency analyzer . For example service level dependency graph builder B may build service level dependency graphs to represent captured dependencies among the components and . An example method for building a service level dependency graph is described herein below with particular reference to . An example of a service level dependency graph is described herein below with particular reference to .

At block an inference graph is constructed responsive to the dependencies e.g. as reflected by the service level dependency graphs with the inference graph reflecting cross layer components that include both the services and the hardware components. The actions of block may be performed by an inference graph constructor . For example inference graph constructor may construct an inference graph from service level dependency graphs with the inference graph reflecting services and hardware components of network .

Each service dependency analyzer is responsible for computing the dependencies among the services its endhost accesses. The dependency probability DP of an endhost on service A when accessing service B is the probability an endhost s communication with service B will fail if the endhost is unable to communicate successfully with service A. A value of 1 e.g. 100 indicates a strong dependence in which the endhost machine always contacts service A before contacting service B. For example a client visits a desired web server soon after it receives a response from the DNS providing the server s IP address consequently the dependency probability of using the DNS when visiting a web server is likely to be closer to 1 than to 0. Due to caching however the dependency probability will often be less than 1.

When services are defined in terms of e.g. IP addresses and ports the inference system need not rely on the parsing of application specific headers. A finer grain notion of a service however may be implemented if more specific parsers are employed. Without loss of generality the description below focuses on services that are defined in terms of IP addresses and ports in situations in which application specific parsers are not employed.

In an example embodiment the dependency between servers is computed by leveraging the observation that if accessing service B depends on service A then packet exchanges with services A and B are likely to co occur in time. Using this observation the dependency probability of an endhost on service A when accessing service B is approximated as the conditional probability of accessing service A within a short interval termed the dependency interval herein prior to accessing service B. Generally this conditional probability is computed as the number of times in a packet trace that an access to service A precedes an access to service B within the dependency interval.

There is a tension in choosing the value of the dependency interval too large an interval can introduce false dependencies on services that are accessed with a high frequency while too small an interval can miss some true dependencies. False positives and false negatives can be handled using the techniques described below with particular reference to .

As illustrated is a graph A that includes a time axis for an endhost C. Along time axis are arrows that represent messages that are communicated with services A B and Z. Dependency intervals are also shown. In this example service B has been specified by a network operator as being of interest. A question is which service s if any is endhost C dependent upon when accessing service B. When an access to service B is detected in a packet trace a first dependency interval is established prior to this communication.

If another service is communicated with during first dependency interval this other service is tentatively considered to be one on which endhost C is dependent when accessing service B. Both services A and Z fit this criterion. Services that are communicated with only outside of this first dependency interval are not considered to be dependent services with respect to at least this access of service B. From another perspective such other services may be considered to be rejected as being false positives because they do not fall within first dependency interval . 

However service A or service Z may be a false positive even though it occurs within first dependency interval . To reject spurious false positives using the dependency interval concept a second dependency interval is established prior to first dependency interval . If a service that is communicated with during first dependency interval is also communicated with during second dependency interval this service may be rejected as being a false positive because it is likely a background service communication that is occurring frequently. Service Z would therefore be rejected as a false positive under this criterion but service A would be maintained as a dependent service with respect to service B. The first and second dependency intervals may be of different durations and they may be separated from each other by a period of time.

An example approach to a dependency interval technique may be formulated more rigidly as follows. The dependency interval DI may be fixed to an e.g. 10 ms or any other appropriate value that is likely to discover most of the dependencies. Service dependency determiners D may then apply a straight forward heuristic to eliminate false positives due to chance co occurrence. They first calculate the average interval I between accesses to the same service and estimate the likelihood of chance co occurrence as DI I. They then retain those dependencies in which the dependency probability is much greater than the likelihood of chance co occurrence.

These approaches to computing dependency are likely more reliable when a response from service A precedes a request to service B but without deep packet inspectors it is not feasible to explicitly identify the requests and responses in streams of packets going back and forth between the endhost and service A and between the endhost and service B. Investigation indicates that it is usually sufficient to group together a contiguous sequence of packets to a given service as a single access to that given service. More specifically but by way of example only each of the packets of the same 5 tuple that have interpacket spacings less than some predefined amount X can be grouped together.

More generally a dependency probability which reflects a probability that accessing a specified service is dependent on accessing a dependent service may be computed responsive to a frequency at which accesses to the specified service and to the dependent service co occur in time and based on time differences between the co occurrences. An example of this approach includes the dependency interval technique described above in which a predefined period establishes the applicable time difference between accesses and the expected order of accesses. However the time period may be adjustable or otherwise varied the temporal order of the accesses may be interchanged and so forth.

Each of the service dependency analyzers periodically submit the dependency probabilities they measure to the inference engine. However because some services are accessed infrequently a single host may not have enough samples to compute an accurate probability. Fortunately many clients in an enterprise network have similar host software and network configurations e.g. clients in the same subnet and are likely to have similar dependencies. Therefore the inference engine can aggregate the probabilities of similar clients to obtain more accurate estimates of the dependencies between services.

With reference to a graph B includes a horizontal abscissa axis that is labeled as the reported dependency probability and a vertical ordinate axis that is labeled the number of endhosts . A curve is plotted on graph B. In this example curve is a Gaussian shaped curve having a mean . A predetermined number x with x being some positive real number of standard deviations are indicated as extending away from mean in both the increasing and decreasing directions. Above x away from the mean is a false positive area . Below x away from the mean is a false negative area .

Aggregation of the reported dependency probabilities from a number of endhosts can provide another mechanism to eliminate false dependencies. This mechanism uses a statistical approach. For example a client making a large number of requests to a proxy server will indicate that it is dependent on the proxy for all the services that it accesses. To eliminate these false dependencies the inference system calculates the mean and standard deviation of each dependency probability. It can then exclude reports from clients with a dependency probability more than x standard deviations from the mean.

Alternatively it can replace a dependency probability that falls outside the range of x with the mean dependency probability. For example it may be given that endhosts depend on average on service A with a 90 dependency probability when accessing service B. If an endhost reports a 1 dependency probability this false negative can be replaced with the 90 dependency probability of the average. On the other hand it may be given that endhosts depend on average on service A with a 2 dependency probability when accessing service B. If an endhost reports a 96 dependency probability this false positive can be rejected and or replaced with a 2 dependency probability from the average. Other statistical paradigms besides those related to Gaussian distributions and or alternative statistical approaches may be implemented to handle false positives and false negatives.

In an example embodiment at block at least one service of interest is specified. For example at least one service may be specified by an operator e.g. an IT administrator of a network . The actions of flow diagram may be implemented for each specified service.

At block packet traces are obtained. For example at each service dependency analyzer a packet trace obtainer O may record incoming and outgoing packets that are received and sent respectively. From these recorded packets the origins and destinations of the packets can be ascertained.

At block consecutive packets of the same flow are bunched into respective messages between endhosts. For example by detecting interactions with respective individual services a service dependency determiner D at each service dependency analyzer may bunch consecutive packets into messages pertaining to respective service requests.

At block within a dependency interval which other service s are communicated with prior to communicating with the specified service are ascertained. For example service dependency determiner D may ascertain which other service s are communicated with during a first dependency interval prior to communicating with a specified service. There may be multiple such ascertained dependent services for a single specified service.

At block the dependency probability is computed. For example service dependency determiner D may compute a dependency probability for each such ascertained dependent service. For instance an entire obtained packet trace may be analyzed and the total number of accesses to a specified service determined. A number of times that an ascertained dependent service co occurs during the dependency interval is also determined. The quotient of the number of co occurrences divided by the total number of specified service accesses is the dependency probability. This dependency probability may be adjusted or otherwise affected by false positive negative handling as noted below with reference to block .

At block a service level dependency graph for the specified service is formulated using the ascertained dependent services and the computed dependency probabilities. For example a service level dependency graph builder B of service dependency analyzer may build a service level dependency graph . An example service level dependency graph is described herein below with particular reference to . For each specified service a respective computed dependency probability may be associated with a respective ascertained dependent service and applied to associated edge labels. If there are multiple ascertained dependent services there can be multiple associated dependency probabilities.

At block false positives and false negatives are identified and handled. For example false positives and or false negatives may be identified and handled by service dependency analyzer and or inference graph constructor . For instance service dependency analyzer may reject false positives using one or more dependency interval approaches and inference graph constructor may reject or correct false positives and or correct false negatives using a statistical technique. With either a false positive or a false negative an error may be replaced by a statistically likely value such as the mean dependency probability computed by similar endhosts. The action s of block may be performed for example in conjunction with those of block and or after those of block .

By way of example client accessing the portal observation node is dependant on portal root cause node with a dependency probability of 100 . It is also dependent on WINS root cause node domain controller root cause node DNS root cause node and proxy root cause node with dependency probabilities of 7 6 5 and 6 respectively. Portal root cause node is in turn dependent on search server root cause node and search indexer root cause node with dependency probabilities of 23 and 48 respectively. Search indexer root cause node is itself dependent on server backend root cause node and domain controller root cause node with dependency probabilities of 30 and 7 respectively.

This section describes how an inference graph constructor combines dependencies between services as reported by service dependency analyzers with network topology information to construct a unified inference graph .

In an example embodiment obtained packet traces are input into the inference graph construction process. Obtaining the packet traces may also be considered as part of the inference graph construction process. At block dependency probabilities between services are computed. For example for each respective specified service dependency probabilities for respective dependent services may be computed. These computed dependency probabilities for each specified service with regard to each dependent service and for each client or other endhost may be organized into one or more dependency matrices .

A dependency matrix may indicate the probability that access to a given specified service is dependent on accessing another service for each of the specified services and ascertained dependent services. The dependency matrices may be organized to include dependency probabilities computed from each potential endhost client to form a three dimensional dependency probability matrix having three variables specified services client endhosts and dependent services. Each entry in the matrix is a dependency probability for a dependent service given a specified service as determined at a particular endhost client. Two dimensional dependency matrices that are determined at each endhost client may be used to build service level dependency graphs at block . In an example implementation the actions of blocks and are performed by a service dependency analyzer and the action s of block are performed by an inference graph constructor . However the actions may be otherwise distributed or performed by different module s of processor executable instructions.

Manual input A can include load balancing mechanisms fail over infrastructure and other such network hardware components. In an alternative embodiment some or all of input A may be provided programmatically. For example a script may be used to identify each of the servers providing DNS service. The script may also direct the construction of the inference graph of block to include the insertion of the appropriate load balancing and fail over meta nodes. Service level dependency graphs are built at for each specified service that is reported from each endhost. Network topology information B represents the physical components of the network including network elements. Manual inputs A network topology information B and service level dependency graphs are used to construct an inference graph at block .

Inference graph may be used to localize performance faults in the network at block . For example a statistical analysis may be applied to inference graph by inference engine given both determined service level dependencies and computed dependency probabilities and in view of observed network performance characteristics. Postulated problems at root cause nodes of inference graph in the form of candidates are probabilistically propagated to other states through the nodes of the graph. The states may be propagated using for example the characteristics of meta nodes that are described herein above in Section 2.2 Probabilistic Modeling for Meta Nodes . Those candidates that most accurately reflect observed network performance characteristics are the most likely candidates for causing the network fault.

In an example embodiment for each service S the network operator has chosen to monitor the inference graph constructor first creates a noisy max meta node to represent the service at block . It then creates an observation node for each client or other endhost reporting response time observations of that service at block and makes the service meta node a parent of the observation node at block .

The inference graph constructor then examines the service dependency information of these clients to identify the set of services Dthat the clients were dependent on when accessing S as well as the set of services that service S is dependent on. The inference graph constructor then recurses expanding each service in Das if the operator had expressed interest in monitoring it too. Once the service meta nodes for the dependent services and the recursively determined services have been created at blocks and for each of these nodes the inference graph constructor creates a root cause node to represent the endhost on which the service runs and makes the respective root cause a parent of the respective meta node at block .

The inference graph constructor then adds network topology information to the inference graph e.g. by using traceroute results reported by the service dependency analyzers . For each path between endhosts in the inference graph it adds a noisy max meta node to represent the path and root cause nodes to represent every router switch hub link etc. on the path at block . It then adds each of these root causes as parents of the path meta node also at block .

Network operators may also inform the inference graph constructor where load balancing and or redundancy techniques are employed within the network. The inference graph constructor can update the inference graph by employing the appropriate specialized meta node e.g. selector or fail over at block . Adapting a local environment to the configuration language of the inference graph constructor may involve some scripting. For example to model load balanced servers in a particular enterprise network translation scripts may be created that leverage naming conventions such as dc for domain controllers sitename for the webservers hosting sitename and so forth. The default meta nodes can then be replaced with specialized selector meta nodes. Also a service dependency analyzer may examine its host s DNS configuration to identify where to place a specialized failover meta node to model the primary secondary relationship between its name resolvers or in other redundancy situations.

Inference graph constructor also assigns probabilities to the edges of the inference graph at block . The dependency probabilities from the service level dependency graphs may be directly copied onto corresponding edges in the inference graph. For some special cases example edge probabilities are given below. With reference to the special root cause nodes always troubled AT and always down AD are connected to observation nodes with a specialized probability of e.g. 0.001 which implies that 1 in 1000 failures is caused by a component that is not included in the model. Investigation has indicated that the results are usually insensitive to the precise probabilistic parameter setting within this order of magnitude. Edges between a router and a path meta node may use another specialized probability of e.g. 0.9999 which implies that there is a 1 in 10 000 chance that the network topology or traceroutes were incorrect and that the router is therefore not actually on the path.

Generally a device may represent any computer or processing capable device such as a server device a workstation or other general computer device a data storage repository apparatus a personal digital assistant PDA a mobile phone a gaming platform an entertainment device a router computing node a mesh or other network node a wireless access point some combination thereof and so forth. As illustrated device includes one or more input output I O interfaces at least one processor and one or more media . Media include processor executable instructions .

In an example embodiment of device I O interfaces may include i a network interface for monitoring and or communicating across network ii a display device interface for displaying information on a display screen iii one or more human device interfaces and so forth. Examples of i network interfaces include a network card a modem one or more ports a network communications stack a radio and so forth. Examples of ii display device interfaces include a graphics driver a graphics card a hardware or software driver for a screen or monitor and so forth. Examples of iii human device interfaces include those that communicate by wire or wirelessly to human device interface equipment e.g. a keyboard a remote a mouse or other graphical pointing device etc. .

Generally processor is capable of executing performing and or otherwise effectuating processor executable instructions such as processor executable instructions . Media is comprised of one or more processor accessible media. In other words media may include processor executable instructions that are executable by processor to effectuate the performance of functions by device . Processor executable instructions may be embodied as software firmware hardware fixed logic circuitry some combination thereof and so forth.

Thus realizations for constructing an inference graph may be described in the general context of processor executable instructions. Generally processor executable instructions include routines programs applications coding modules protocols objects components metadata and definitions thereof data structures application programming interfaces APIs etc. that perform and or enable particular tasks and or implement particular abstract data types. Processor executable instructions may be located in separate storage media executed by different processors and or propagated over or extant on various transmission media.

Processor s may be implemented using any applicable processing capable technology and one may be realized as a general purpose processor e.g. a central processing unit CPU a microprocessor a controller etc. a graphics processing unit GPU a derivative thereof and so forth. Media may be any available media that is included as part of and or accessible by device . It includes volatile and non volatile media removable and non removable media storage and transmission media e.g. wireless or wired communication channels hard coded logic media combinations thereof and so forth. Media is tangible media when it is embodied as a manufacture and or as a composition of matter. For example media may include an array of disks or flash memory for longer term mass storage of processor executable instructions random access memory RAM for shorter term storage of instructions that are currently being executed and or otherwise processed link s on network for propagating communications and so forth.

As specifically illustrated media comprises at least processor executable instructions . Generally processor executable instructions when executed by processor enable device to perform the various functions described herein. Such functions include but are not limited to i those that are illustrated in flow diagrams and of and ii those that are performed by service dependency analyzers and inference graph constructor of iii those pertaining to handling false positives and or false negatives e.g. of iv those embodied by service level dependency graphs and or an inference graph of and combinations thereof and so forth.

The devices acts aspects features functions procedures modules data structures techniques components units etc. of are illustrated in diagrams that are divided into multiple blocks and other elements. However the order interconnections interrelationships layout etc. in which are described and or shown are not intended to be construed as a limitation and any number of the blocks and or other elements can be modified combined rearranged augmented omitted etc. in any manner to implement one or more systems methods devices procedures media apparatuses arrangements etc. for constructing an inference graph.

Although systems media devices methods procedures apparatuses mechanisms schemes approaches processes arrangements and other example embodiments have been described in language specific to structural logical algorithmic and functional features and or diagrams it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claimed invention.

