---

title: Method and system for enabling managed code-based application program to access graphics processing unit
abstract: One embodiment of the present invention sets forth a method for enabling an intermediate code-based application program to access a target graphics processing unit (GPU) in a parallel processing environment. The method includes the steps of compiling a source code of the intermediate code-based application program to an intermediate code, translating the intermediate code to a PTX instruction code, and translating the PTX instruction code to a machine code executable by the target graphics processing unit before delivering the machine code to the target GPU.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08570333&OS=08570333&RS=08570333
owner: Nvidia Corporation
number: 08570333
owner_city: Santa Clara
owner_country: US
publication_date: 20081211
---
The present invention generally relates to parallel processing and more particularly to a method and system for enabling managed code based application program to access graphics processing unit in a computed unified device architecture CUDA environment.

Unless otherwise indicated herein the approaches described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

CUDA is a software platform for massively parallel high performance computing on graphics processing units GPU . A GPU is a parallel multi core architecture and each core is capable of running thousands of threads simultaneously. Especially when an application program is designed for this architecture the GPU can offer substantial performance benefits. CUDA development tools generally work alongside a conventional C C compiler so that programmers can use a variation of C program language to code algorithms for execution on the GPU. CUDA also hides the GPU architecture beneath application programming interface API and as the result application programmers are not required to be familiar with the architecture and the low level programming language. Instead predefined graphics functions in the API can be called. Compiling a CUDA program however is not as straightforward as running a C compiler to convert source code into executable object code partly because the CUDA program targets two different processor CPU and GPU architectures and partly because of the hardware abstraction of CUDA.

Although equipped with the conventional C C compiler the present CUDA environment does not support application programs that are written in a programming language that is for creating managed code to be executed under a virtual machine such as Virtual Basic .NET . However for programming convenience and security considerations more and more applications are written in such a programming language. Moreover there are also needs to executed a managed code based application program in the CUDA environment so that the GPU computing resources can be utilized.

Before the managed code based application program could access the GPU hardware resource the application program needs to be translated into the corresponding GPU machine codes. However such machine code translation can be inefficient without any support from the enhanced CUDA environment.

What is needed in the art is thus a method and system that can enable a managed code based application program to access the resources of a GPU in the CUDA environment and can address at least the foregoing issues.

A method for enabling an intermediate code based application program to access a target graphics processing unit GPU in a parallel processing environment is disclosed. The method includes the steps of compiling a source code of the intermediate code based application program to an intermediate code translating the intermediate code to a PTX instruction code and translating the PTX instruction code to a machine code executable by the target GPU before delivering the machine code to the target GPU.

At least one advantage of the present invention is the intermediate code based application program could be utilizing the multiple cores of the GPU in the enhanced CUDA parallel processing environment.

Parallel processing subsystem includes a parallel processing unit PPU and a parallel processing PP memory which may be implemented for example using one or more integrated circuit devices such as programmable processors application specific integrated circuits ASICs and memory devices. PPU advantageously implements a highly parallel processor that includes one or more processing cores each of which is capable of executing a large number of threads concurrently. PPU can be programmed to perform a wide array of computations over various types of data. PPU may transfer data from system memory and or PP memory into internal memory not shown for processing. Results may be written to system memory for access by other system components such as the CPU . In some embodiments PP subsystem may include one PPU operating as a graphics processor and a second PPU configured to perform general purpose computations. The PPUs may be identical or different and each PPU may have independent PP memory .

The CPU operates as the control processor of the host computer managing and coordinating the operation of other system components. In particular CPU issues commands that control the operation of PPU . In some embodiments CPU writes a stream of commands for PPU to a command buffer not shown which may reside in system memory PP memory or another storage location accessible to both CPU and PPU . PPU reads the command stream from the command buffer and executes commands asynchronously with respect to the operation of CPU .

System memory includes an executing image of an operating system a driver and a co processor enabled application program . The operating system provides the detailed instructions for managing and coordinating the operation of the host computer . The driver provides detailed instructions for managing and coordinating operation of the parallel processing subsystem and in particular the PPU . Furthermore driver provides compilation facilities for generating machine code specifically optimized for PPU . A co processor enabled application program incorporates instructions configured to execute on the CPU and PPU functions implemented in an abstract format such as virtual PTX instruction and configured to easily map to machine code for PPU . The machine code for PPU may be stored in system memory or in PP memory as a set of PPU code fragments .

In one embodiment the parallel processing subsystem incorporates circuitry optimized for graphics and video processing including for example video output circuitry and constitutes a graphics processing unit GPU . In another embodiment the parallel processing subsystem incorporates circuitry optimized for general purpose processing while preserving the underlying computational architecture. In yet another embodiment the parallel processing subsystem may be integrated with one or more other system elements such as the memory bridge CPU and I O bridge to form a system on chip SoC .

In conjunction with illustrates a parallel processing subsystem according to one embodiment of the present invention. Parallel processing subsystem includes one or more parallel processing units PPUs each of which is coupled to a local parallel processing PP memory . In general a parallel processing subsystem includes a number U of PPUs where U 1. Herein multiple instances of like objects are denoted with reference numbers identifying the object and parenthetical numbers identifying the instance where needed. PPUs and PP memories may be implemented e.g. using one or more integrated circuit devices such as programmable processors application specific integrated circuits ASICs and memory devices.

As shown in detail for PPU each PPU includes a host interface that communicates with the host computer via communication path which connects to memory bridge or in one alternative embodiment directly to CPU . In one embodiment communication path is a PCI E link in which dedicated lanes are allocated to each PPU as is known in the art. Other communication paths may also be used. Host interface generates packets or other signals for transmission on communication path and also receives all incoming packets or other signals from communication path and directs them to appropriate components of PPU . For example commands related to processing tasks may be directed to a front end unit while commands related to memory operations e.g. reading from or writing to PP memory may be directed to a memory interface . Host interface front end unit and memory interface may be of generally conventional design and a detailed description is omitted as not being critical to the present invention.

Each PPU advantageously implements a highly parallel processor. As shown in detail for PPU a PPU includes a number C of cores where C 1. Each processing core is capable of executing a large number e.g. tens or hundreds of threads concurrently where each thread is an instance of a program one embodiment of a multithreaded processing core is described below. Cores receive processing tasks to be executed via a work distribution unit which receives commands defining processing tasks from a front end unit . Work distribution unit can implement a variety of algorithms for distributing work. For instance in one embodiment work distribution unit receives a ready signal from each core indicating whether that core has sufficient resources to accept a new processing task. When a new processing task arrives work distribution unit assigns the task to a core that is asserting the ready signal if no core is asserting the ready signal work distribution unit holds the new processing task until a ready signal is asserted by a core . Those skilled in the art will recognize that other algorithms may also be used and that the particular manner in which work distribution unit distributes incoming processing tasks is not critical to the present invention.

Cores communicate with memory interface to read from or write to various external memory devices. In one embodiment memory interface includes an interface adapted to communicate with local PP memory as well as a connection to host interface thereby enabling the cores to communicate with system memory or other memory that is not local to PPU . Memory interface can be of generally conventional design and a detailed description is omitted.

Cores can be programmed to execute processing tasks relating to a wide variety of applications including but not limited to linear and nonlinear data transforms filtering of video and or audio data modeling operations e.g. applying laws of physics to determine position velocity and other attributes of objects image rendering operations e.g. vertex shader geometry shader and or pixel shader programs and so on. PPUs may transfer data from system memory and or local PP memories into internal on chip memory process the data and write result data back to system memory and or local PP memories where such data can be accessed by other system components including e.g. CPU or another parallel processing subsystem.

Some or all of PPUs in parallel processing subsystem are graphics processors with rendering pipelines that can be configured to perform various tasks related to generating pixel data from graphics data supplied by CPU and or system memory via memory bridge and bus interacting with local PP memory which can be used as graphics memory including e.g. a conventional frame buffer to store and update pixel data delivering pixel data to display device and the like. In some embodiments PP subsystem may include one or more PPUs that operate as graphics processors and one or more other PPUs that are used for general purpose computations. The PPUs may be identical or different and each PPU may have its own dedicated PP memory device s or no dedicated PP memory device s .

In operation CPU is the master processor of the host computer controlling and coordinating operations of other system components. In particular CPU issues commands that control the operation of PPUs . In some embodiments CPU writes a stream of commands for each PPU to a push buffer not explicitly shown in which may be located in system memory PP memory or another storage location accessible to both CPU and PPU . PPU reads the command stream from the push buffer and executes commands asynchronously with operation of CPU .

It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology including the number and arrangement of bridges may be modified as desired. For instance in some embodiments system memory is connected to CPU directly rather than through a bridge and other devices communicate with system memory via memory bridge and CPU . In other alternative topologies parallel processing subsystem is connected to I O bridge or directly to CPU rather than to memory bridge . In still other embodiments I O bridge and memory bridge might be integrated into a single chip. The particular components shown herein are optional for instance any number of add in cards or or peripheral devices might be supported. In some embodiments switch is eliminated and network adapter and add in cards connect directly to I O bridge .

The connection of PPU to the rest of the host computer may also vary. In some embodiments PP system is implemented as an add in card that can be inserted into an expansion slot of the host computer . In other embodiments a PPU can be integrated on a single chip with a bus bridge such as memory bridge or I O bridge . In still other embodiments some or all elements of PPU may be integrated on a single chip with CPU .

A PPU may be provided with any amount of local PP memory including no local memory and may use local memory and system memory in any combination. For instance a PPU can be a graphics processor in a unified memory architecture UMA embodiment in such embodiments little or no dedicated graphics PP memory is provided and PPU would use system memory exclusively or almost exclusively. In UMA embodiments a PPU may be integrated into a bridge chip or processor chip or provided as a discrete chip with a high speed link e.g. PCI E connecting the PPU to system memory e.g. via a bridge chip.

As noted above any number of PPUs can be included in a parallel processing subsystem. For instance multiple PPUs can be provided on a single add in card or multiple add in cards can be connected to communication path or one or more of the PPUs could be integrated into a bridge chip. The PPUs in a multi PPU system may be identical to or different from each other for instance different PPUs might have different numbers of cores different amounts of local PP memory and so on. Where multiple PPUs are present they may be operated in parallel to process data at higher throughput than is possible with a single PPU.

Systems incorporating one or more PPUs may be implemented in a variety of configurations and form factors including desktop laptop or handheld personal computers servers workstations game consoles embedded systems and so on.

Programs in any computer program language could be compiled into either the managed code or the unmanaged code. Unlike the unmanaged code which is executed directly by the CPU the managed code could be executed under the management of a virtual machine. The managed code that is not executed by the virtual machine and is further translated into hardware specific machine code is referred to as intermediate managed code in the following paragraphs. The virtual machine is a software implementation of a machine that executes programs.

The external graphics system includes a GPU and device memory . The device memory includes a virtual machine storing a virtual machine clustering service a CUDA driver and a CUDA driver API . The external graphics system could be connected to the host computer in any well known manner. Another external graphics system has a GPU and device memory . The device memory includes a virtual machine also storing a virtual machine clustering service another CUDA driver and another CUDA driver API . It is worth noting that in the embodiment shown in the virtual machines and and the virtual clustering services and are software components. It is also worth noting that the GPU or might correspond to any PPU e.g. PPU in . Meanwhile the memory or may include the PP memory e.g. PP memory shown in .

In one implementation the virtual clustering services and are configured to be in communication with each other. The bridge serves as an interface between the managed code based application program and the CUDA driver API . The managed code based application program might be written without incorporating any or all of the parallel computing class and the virtual clustering service and . Even if no component is incorporated the managed code based application program might still be able to access the GPU of the host computer .

In conjunction with is a schematic diagram showing how a managed code based application program can access the resources of a GPU according to one embodiment of the present invention. Here it should be noted that accessing the resources of a GPU by the application program broadly refers to delivering instructions of the application program to the GPU that can be executed directly by the GPU. Source code of the managed code based application program is compiled into intermediate managed code by a compiler . The translator and the virtual machine translation API further translate the intermediate managed code into a parallel thread execution PTX instruction code. The PTX instruction code will be further translated into a corresponding GPU specific machine code by the CUDA driver and the CUDA driver API . The translation from the intermediate code to the PTX instruction code starts with the virtual machine translation API interpreting the meaning of the intermediate managed code into objects. The translator is configured to parse the objects before translating the intermediate managed code into the PTX instruction code and delivering the PTX instruction code to the CUDA driver API and the CUDA driver . In one implementation the generation of the PTX instruction code may come from the same application process for the execution of the managed code based application program . In other words the translation into the PTX instruction code is performed during run time and as such the PTX instruction code could be dynamically loaded up even the application program is still in the process of the execution. In an alternatively implementation the generation of the PTX instruction code is performed in an application process distinct from the process for the execution of the managed code based application program .

The CUDA driver API and the CUDA driver translate the PTX instruction code to GPU specific machine code that could be executed directly by the GPU . Through the translation from the intermediate managed code to the GPU specific machine code the hardware resource of the GPU may be utilized by the managed code based application program .

In conjunction with is a flow chart illustrating a process that the host computer is configured to access external GPU resources in the CUDA environment according to one embodiment of the present invention. In step the virtual machine of the host computer executes the intermediate managed code of the managed code based application program through the management and coordination of the CPU . In step after the compile of the source code into the intermediate managed code the translator translates the intermediate managed code to the PTX instruction code . It is worth noting that in CUDA environment the compile of the source code to the intermediate managed code is performed by a managed code compiler not shown in . As discussed above the generation of the PTX instruction code could be dynamically performed in the same application process of the execution of the managed code based application program or alternatively performed in a different application process. In step the host computer is further configured to encapsulate a set of selectable functions that are necessary for the access to the GPU or . The selectable functions are to be performed on the PTX instruction code by the invoking of the corresponding calls provided by the CUDA driver API . In one implementation the selectable functions includes initialization of CUDA driver for the PTX instruction code loading of the PTX instruction code after the translation of the intermediate managed code allocation of the memory space system memory and device memories and for the transfer of parameters associated with the PTX instruction code configurations of types of those parameters and the transfer of those parameters to the allocated memory space. In one implementation the set of selectable functions is encapsulated into an object. The object with the encapsulated set of selectable functions could be dispatched to the external graphics system or for further processing. Moreover the object could be incorporated into the managed code based application program directly as well. It is worth noting that the parallel computing class is extendable at least meaning additional sets of selectable functions could be added into the parallel computing class . As such the managed code based application program becomes more object oriented in nature.

In step the virtual clustering service queries the availability of the GPU through communicating with the virtual clustering service of the external graphics system having the GPU . The virtual clustering service checks the availability of the GPU such as the availability of the number of the cores in the GPU before responding to the query issued from the virtual clustering service . In step once after the virtual clustering service obtains the information about the availability of the GPU the parallel computing class delivers the PTX instruction code to the virtual clustering service . In step the host computer then allows the virtual clustering service to further feed the received PTX instruction code to the CUDA driver API and the CUDA driver so that the CUDA driver API and the CUDA driver could translate the PTX instruction code to the GPU specific machine code for the GPU . As such the managed code based application program could access the GPU in step . It is worth noting that the operation provides the managed code based application program with an opportunity to utilize the resources of the GPU even though the application program is not specifically written to access the GPU .

Although not shown in both external graphics systems and further have their own bridges that are the same or similar to their counterpart for interfacing the PTX instruction codes and the CUDA driver API and .

In conjunction with is a flow chart illustrating a process for the preparation of the PTX instruction code according to one embodiment of the present invention. Managed code based programming generally allows for program level attribute annotation indicative of the way the annotated part of the programming code should be implemented. The attribute annotation could be compiled along with the source code and referenced during run time of the application program . The result of the compile which includes the intermediate manage code and the attributes associated with the attribute annotations is stored into a file. In one implementation the file is an executable .exe file. In yet another implementation the file is a dynamic link library .dll file. Such file will be loaded during or before run time. In step the translator is configured to scan the content of the loaded file for the existence of the attribute annotation associated with a part of the source code or even a variable in the part of the source code . In step the translator may determine which part of the intermediate managed code should be translated and translates it accordingly. As for the result of the translation the CUDA driver stores the result of the translation into a file for the future use in step . In one implementation the file that stores the result of the translation is a CUBIN file. It is worth noting that the CUBIN file comprises the PTX instruction code and the data to be processed by the GPU.

In step the CUDA driver is configured to independently load the file into the GPU for the debugging and verification of the translation. Alternatively the CUDA driver might not store the result of the translation. Rather the CUDA driver dynamically loads the result of the translation over the course of the execution of the managed code based application program in step .

The above description illustrates various embodiments of the present invention along with examples of how aspects of the present invention may be implemented. One embodiment of the present invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive CD RW disks DVD RW disks flash memory hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored. The above examples embodiments instruction semantics and drawings should not be deemed to be the only embodiments and are presented to illustrate the flexibility and advantages of the present invention as defined by the following claims.

