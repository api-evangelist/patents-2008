---

title: Video conferencing device which performs multi-way conferencing
abstract: In various embodiments, a video conferencing device (e.g., an endpoint) may generate a video frame that includes video images of two or more video conferencing endpoints. The video frame may then be sent to a video conferencing device that may receive the video frame and separate the two or more video images into separate video images. In some embodiments, coordinate information sent along with the video frame (e.g., in metadata) may be used by the video conferencing endpoint to determine the locations of the video images in the video frame to facilitate separation of the video images. By transmitting and receiving video frames with multiple video images (from different video conferencing endpoints), multiple video conferencing endpoints can implement a multi-way video conference call without using an MCU.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08237765&OS=08237765&RS=08237765
owner: LifeSize Communications, Inc.
number: 08237765
owner_city: Austin
owner_country: US
publication_date: 20080619
---
This application claims the benefit of priority of U.S. Provisional Patent Application Ser. No. 60 945 734 titled Videoconferencing Device which Performs Multi way Conferencing filed on Jun. 22 2007 whose inventors are Keith C. King and Wayne E. Mock which is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

This application also claims the benefit of priority of U.S. Provisional Patent Application Ser. No. 60 945 723 titled Virtual Decoders filed on Jun. 22 2007 whose inventors are Keith C. King and Wayne E. Mock which is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

This application also claims the benefit of priority of U.S. Provisional Patent Application titled Virtual Multiway Scaler Compensation Ser. No. 60 949 674 which was filed Jul. 13 2007 whose inventors are Keith C. King and Wayne E. Mock which is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

Video conferencing may be used to allow two or more participants at remote locations to communicate using both video and audio. Each participant location may include a video conferencing endpoint for video audio communication with other participants. Each video conferencing endpoint may include a camera and microphone to collect video and audio from a first or local participant to send to another remote participant. Each video conferencing endpoint may also include a display and speaker to reproduce video and audio received from a remote participant. Each video conferencing endpoint may also be coupled to a computer system to allow additional functionality into the video conference. For example additional functionality may include data conferencing including displaying and or modifying a document for two or more participants during the conference .

Video conferencing involves transmitting video streams between video conferencing endpoints. The video streams transmitted between the video conferencing endpoints may include video frames. The video frames may include pixel macroblocks that may be used to construct video images for display in the video conferences. Video frame types may include intra frames forward predicted frames and bi directional predicted frames. These frame types may involve different types of encoding and decoding to construct video images for display. Currently in a multi way video conference call a multipoint control unit MCU is required to composite video images received from different video conferencing endpoints onto video frames of a video stream that may be encoded and transmitted to the various video conferencing endpoints for display.

In various embodiments a video conferencing device e.g. an endpoint may generate a video frame that includes video images of two or more video conferencing endpoints. The video frame may then be sent to a video conferencing device that may receive the video frame and separate the two or more video images into separate video images. By transmitting and receiving video frames with multiple video images from different video conferencing endpoints multiple video conferencing endpoints may implement a multi way video conference call without using an MCU. In some embodiments coordinate information sent along with the video frame e.g. in metadata may be used by the video conferencing endpoints to determine the locations of the video images in the video frame to facilitate separation of the video images. The metadata may include video image identifiers and location information e.g. coordinates in the video frame of the video images.

In some embodiments the separated video images may be provided to a compositor that may composite the separated video images into a new video image layout. Other video images e.g. from local video or received from other video conferencing endpoints may also be composited into the new video image layout. In some embodiments the new video image layout may be configured to be displayed e.g. as a continuous presence image . In some embodiments participants at each video conferencing endpoint may use their local video conferencing endpoints to customize their continuous presence layout. For example participants may rearrange the video images and or replace one or more video images in the video image layout e.g. with a current video image from their local video source .

In some embodiments a spatially multiplexed output decoder may spatially multiplex video packets received in a time multiplexed video stream. A video stream with video packets from two or more sources may be received along with metadata e.g. with identifying information for the video packets . In some embodiments a decoder may organize the video packets into respective buffers e.g. each buffer including video packets for a respective video image . In some embodiments the spatially multiplexed output decoder may spatially multiplex the video images which are made up of data from the respective video packets into a video frame to be outputted e.g. to a separate buffer . The video images in the video frame may then be demultiplexed in other parts of the system e.g. in a virtual decoder using information provided about the video frame e.g. in metadata formed with the video frame . These stacked images may be disassembled as needed to assemble different composite layouts for display and or to transmit to a different endpoint for facilitating a multi way conference.

While the invention is susceptible to various modifications and alternative forms specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood however that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims. Note the headings are for organizational purposes only and are not meant to be used to limit or interpret the description or claims. Furthermore note that the word may is used throughout this application in a permissive sense i.e. having the potential to being able to not a mandatory sense i.e. must . The term include and derivations thereof mean including but not limited to . The term coupled means directly or indirectly connected .

U.S. patent application titled Speakerphone Ser. No. 11 251 084 which was filed Oct. 14 2005 whose inventor is William V. Oxford is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

U.S. patent application titled Videoconferencing System Transcoder Ser. No. 11 252 238 which was filed Oct. 17 2005 whose inventors are Michael L. Kenoyer and Michael V. Jenkins is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

U.S. patent application titled Speakerphone Supporting Video and Audio Features Ser. No. 11 251 086 which was filed Oct. 14 2005 whose inventors are Michael L. Kenoyer Craig B. Malloy and Wayne E. Mock is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

U.S. patent application titled Virtual Decoders Ser. No. 12 142 263 which was filed Jun. 19 2008 whose inventors are Keith C. King and Wayne E. Mock is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

U.S. patent application titled Video Conferencing System which Allows Endpoints to Perform Continuous Presence Layout Selection Ser. No. 12 142 302 which was filed Jun. 19 2008 whose inventors are Keith C. King and Wayne E. Mock is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

U.S. patent application titled Video Decoder which Processes Multiple Video Streams Ser. No. 12 142 377 which was filed Jun. 19 2008 whose inventors are Keith C. King and Wayne E. Mock is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

U.S. patent application titled Integrated Videoconferencing System Ser. No. 11 405 686 which was filed Apr. 17 2006 whose inventors are Michael L. Kenoyer Patrick D. Vanderwilt Craig B. Malloy William V. Oxford Wayne E. Mock Jonathan I. Kaplan and Jesse A. Fourt is hereby incorporated by reference in its entirety as though fully and completely set forth herein.

The endpoints may include video conferencing system endpoints also referred to as participant locations . Each endpoint may include a camera display device microphone speakers and a codec or other type of video conferencing hardware. In some embodiments endpoints may include video and voice communications capabilities e.g. video conferencing capabilities and include or be coupled to various audio devices e.g. microphones audio input devices speakers audio output devices telephones speaker telephones etc. and include or be coupled to various video devices e.g. monitors projectors displays televisions video output devices video input devices cameras etc. . In some embodiments endpoints may include various ports for coupling to one or more devices e.g. audio devices video devices etc. and or to one or more networks. Endpoints may each include and or implement one or more real time protocols e.g. session initiation protocol SIP H.261 H.263 H.264 H.323 among others. In an embodiment endpoints may implement H.264 encoding for high definition HD video streams.

The network may include a wide area network WAN such as the Internet. The network may include a plurality of networks coupled together e.g. one or more local area networks LANs coupled to the Internet. The network may also include public switched telephone network PSTN . The network may also include an Integrated Services Digital Network ISDN that may include or implement H.320 capabilities. In various embodiments video and audio conferencing may be implemented over various types of networked devices.

In some embodiments endpoints may each include various wireless or wired communication devices that implement various types of communication such as wired Ethernet wireless Ethernet e.g. IEEE 802.11 IEEE 802.16 paging logic RF radio frequency communication logic a modem a digital subscriber line DSL device a cable television modem an ISDN device an ATM asynchronous transfer mode device a satellite transceiver device a parallel or serial port bus interface and or other type of communication device or method.

In various embodiments the methods and or systems described may be used to implement connectivity between or among two or more participant locations or endpoints each having voice and or video devices e.g. endpoints that communicate through network .

In some embodiments the video conferencing system network e.g. endpoints may be designed to operate with network infrastructures that support T1 capabilities or less e.g. 1.5 mega bits per second or less in one embodiment and 2 mega bits per second in other embodiments. In some embodiments other capabilities may be supported e.g. 6 mega bits per second over 10 mega bits per second etc . The video conferencing endpoint may support HD capabilities. The term high resolution includes displays with resolution of 1280 720 pixels and higher. In one embodiment high definition resolution may include 1280 720 progressive scans at 60 frames per second or 1920 1080 interlaced or 1920 1080 progressive. Thus an embodiment of the present invention may include a video conferencing endpoint with HD e.g. similar to HDTV display capabilities using network infrastructures with bandwidths T1 capability or less. The term high definition is intended to have the full breath of its ordinary meaning and includes high resolution .

In some embodiments the endpoint may include a camera e.g. an HD camera for acquiring video images of the participant location e.g. of participant . Other cameras are also contemplated. The endpoint may also include a display e.g. an HDTV display . Video images acquired by the camera may be displayed locally on the display and may also be encoded and transmitted to other video conferencing endpoints in the video conference.

The endpoint may also include a sound system . The sound system may include multiple speakers including left speakers center speaker and right speakers . Other numbers of speakers and other speaker configurations may also be used. The endpoint may also use one or more speakerphones which may be daisy chained together.

In some embodiments the video conferencing endpoint components e.g. the camera display sound system and speakerphones may be coupled to the system codec compressor decompressor box . The system codec box may be placed on a desk or on a floor. Other placements are also contemplated. The system codec box may receive audio and or video data from a network e.g. network . The system codec box may send the audio to the speakerphone and or sound system and the video to the display . The received video may be HD video that is displayed on the HD display. The system codec box may also receive video data from the camera and audio data from the speakerphones and transmit the video and or audio data over the network to another conferencing system. The conferencing system may be controlled by a participant through the user input components e.g. buttons on the speakerphones and or remote control . Other system interfaces may also be used.

In various embodiments the system codec box may implement a real time transmission protocol. In some embodiments a system codec box may include any system and or method for encoding and or decoding e.g. compressing and decompressing data e.g. audio and or video data . In some embodiments the system codec box may not include one or more of the compressing decompressing functions. In some embodiments communication applications may use system codec box to convert an analog signal to a digital signal for transmitting over various digital networks e.g. network PSTN the Internet etc. and to convert a received digital signal to an analog signal. In various embodiments codecs may be implemented in software hardware or a combination of both. Some codecs for computer video and or audio may include MPEG Indeo and Cinepak among others.

In some embodiments the endpoint may display different video images of various participants presentations etc. during the video conference. Video to be displayed may be transmitted as video streams e.g. video stream as seen in between the endpoints e.g. endpoints .

While two video images are shown with respect to video frame it is to be understood that video frames video frames used herein to refer to various video frames etc. may include a video image layout with other combinations and layouts of two or more video images e.g. video frame in has four video images . Additional examples are shown in e.g. video frame may include various video image layouts . Video image layout may include four video images stacked on top of each other. In some embodiments each video image of the stacked video images may be 1280 by 720 pixels e.g. for a total size of 1280 by 2880 other dimensions and number of video images are also contemplated . In some embodiments video image layout may include four images side by side. As another example the video image layout may include two video images e.g. each 640 by 360 pixels arranged side by side in a 1280 by 360 pixel video frame. The video frame may then be separated into two 640 by 360 pixel video images. Other combinations and layouts are also contemplated. In some embodiments the number of video images composited in the video image layout may depend on the number of participating endpoints in the video conference. For example each participating endpoint may have a corresponding video image which may be for example 1280 by 720 in the video image layout of video frame .

As seen in the video streams may be decoded e.g. in video stream decoder prior to being sent to the virtual decoder . In some embodiments the composited video images of the video frames may then be separated into separate video images by the virtual decoder . For example a 1280 by 360 video frame may be separated into two 640 by 360 video images . Other dimensions are also contemplated. The video images may then be scaled and composited into a video image layout that may be different from the video image layout of the received video frame . In some embodiments the virtual decoder may be implemented as a software abstraction on hardware such as a field programmable gate array FPGA . In some embodiments one or more virtual decoders may be implemented on a single ASIC Application Specific Integrated Chip . Other virtual decoder configurations are also contemplated.

In some embodiments the virtual decoder may use coordinate information for the video images in the video frame to find the boundaries of the video images in order to separate the video images . In some embodiments coordinate information may be passed with the video frame to provide the coordinates in the video frame of the start and or stop locations of video images in the composited video image of video frame . For example the coordinate information may include boundary information e.g. see coordinate information in for the video images in the composited video image of video frame . Other coordinate information is also contemplated. The coordinate information may be used by the virtual decoder to crop the respective video images e.g. video images and in the video frame . In some embodiments the coordinate information may be passed as metadata e.g. see with the video frame e.g. in a video frame header . In some embodiments coordinate information may be prepared by an endpoint preparing the video frames for the video stream .

In some embodiments one or more endpoints may arrange the incoming video images into a composite video image with a requested video image layout and define the respective coordinate information for one or more of the video images in the composite video image including the size of the original composite video image. In some embodiments one or more endpoints may need to subsequently scale the composite video image e.g. scale down the composite video image to be sent over a reduced bandwidth network connection to be sent to one or more other endpoints . In some embodiments the composite video image may be scaled to a scaled composite video image in a scaler. The coordinate information may be included in metadata passed with a video frame including the scaled composite video image. In some embodiments the coordinate information may be reformatted e.g. at the sending endpoint or at the receiving endpoint to reflect the new coordinates of one or more of the resized video images in the scaled composite video image. For example when the endpoint receives the scaled composite video image the endpoint may detect the actual size of the scaled composite video image and may determine the new coordinates of one or more of the video images in the scaled composite video image using for example a ratio of the size of the original composite video image to the size of the scaled composite video image detected by the endpoint . These new coordinates may then be used to separate one or more of the resized images in the scaled composite video image to use in compositing a new composite video image. For example see U.S. Provisional Patent Application titled Virtual Multiway Scaler Compensation Ser. No. 60 949 674 which was filed Jul. 13 2007 whose inventors are Keith C. King and Wayne E. Mock which was incorporated by reference above.

At a video frame including two or more video images may be received. For example the video frame may be received as a series of video packets in a video stream at decoder . The decoder may assemble the video packets into their respective video frames for further processing in the virtual decoder .

At coordinate information indicating the location of one or more of the video images in the video frame may be received. For example the coordinate information may be received in metadata see sent along with the video frame . In some embodiments the video frame may include a continuous presence layout of video images e.g. video image layout as seen in .

At the coordinate information may be used to find video image boundaries of the video images within the video frame . In some embodiments the coordinate information may be used to determine where the video images start and stop in the video frame . These start stop locations may be used by the virtual decoder to separate the video images from the video frame . For example as seen in coordinate information for coordinates and may be sent with the video frame . illustrates an example of a use of coordinate information to locate the boundaries of video images e.g. video images in order to separate the video images. For example the User video image may have a left boundary at 0 a top boundary at 0 a right boundary at 639 and a bottom boundary at 359. Similarly the user video image may have a left boundary at 640 a top boundary at 0 a right boundary at 1279 and a bottom boundary at 359. Coordinate information e.g. boundary information for other video images e.g. video images and may also be provided in coordinate information .

At the video images may be separated. In some embodiments separate video images may be defined using the video images in the video frame according to the coordinate information . For example separate video images and as seen in may be defined and or scaled into separate video images and . In some embodiments separating the video images may include for example storing the separated video images and in separate locations of a memory. In some embodiments separating the video images and may include storing start and or stop locations of the video images and in memory. Other means for separating the video images are also contemplated. For example separating may include copying replacing and or modifying data from the video images to be used to create a new composite image.

At a video frame including two or more video images may be received. The video frame may include two or more video images . For example video frame may include image layout see that includes video images and originating from different video conferencing endpoints . A main image may be an image of the video conferencing endpoint with the current speaker and two or more side images e.g. side images and of other video conferencing endpoints participating in the video conference. In some embodiments the video frame may be received from another video conferencing endpoint which for example received one or more of the video images in the image layout from other video conferencing endpoints . The video frame may be received with coordinate information e.g. embedded in metadata received with the video frame . The coordinate information may indicate the start stop locations of one or more of the video images in the video frame . In some embodiments the video frames and coordinate information may be transported together in video stream .

At the video frame may be separated into two or more video images e.g. video images and . The two or more separated video images may correspond to separate video conferencing endpoints . As seen in one separated video image may correspond to the main image and two separate video images and may correspond to each of the two side images e.g. images and . In some embodiments the coordinate information may be used to determine where the video images start and stop in the video frame . These start stop locations may be used by the virtual decoder to separate the video images from the video frame . For example coordinate information for coordinates and may be sent with the video frame . illustrates an example of a use of coordinate information to locate the boundaries of video images in order to separate the video images. For example the User video image may have a left boundary at 0 a top boundary at 0 a right boundary at 639 and a bottom boundary at 359. Similarly the user video image may have a left boundary at 640 a top boundary at 0 a right boundary at 1279 and a bottom boundary at 359. Coordinate information e.g. boundary information for other video images e.g. video images and may also be provided in coordinate information . In some embodiments coordinate information for a respective video image may be placed in a row of information for the respective video image. For example row one of data in metadata may include a call identifier system name number Internet Protocol IP address and left top right bottom coordinates e.g. 0 0 639 and 359 for a respective video image other information may also be included .

In some embodiments the coordinate information may be sent in metadata sent in video stream between video conference endpoints . The metadata may include coordinate information for a video frame with the start and or stop information for a video image e.g. image boundaries and or pixel start stop points corresponding to a video conferencing endpoint identifying information respective to the corresponding video conferencing endpoint and other information.

At one or more of the separated video images e.g. separated video image or may be provided to one or more scalers e.g. scalers . In some embodiments one or more of the video images may be scaled according to a video image layout the video images are to be placed in. For example if the main image and each of the two side images and are to be placed in a video image layout with equal sized video images the main image may be scaled down and the two side video images and may be scaled up. Other scaling combinations are also contemplated. In some embodiments the separated video images may not be scaled e.g. the separated video images may be only rearranged .

At the video images including scaled video images if any may be provided to one or more compositors e.g. compositors . In some embodiments the compositors may composite the video images into a video frame for sending to another video conferencing endpoint. For example to implement a multi way conference one of the separated video images may be composited with for example a video image from a local camera and the composited video frame may be sent to a remote video conferencing endpoint. In some embodiments the compositor may composite the video images into a video image layout specified by a local participant for display.

At the video image layout may be sent to another video conferencing endpoint and or displayed. In some embodiments the video image layout may be different from the video image layout of the video images received at the video conferencing endpoint . illustrates an example of a new video image layout with three similar sized images and on display. illustrates other possible video image layouts e.g. layouts and according to various embodiments. Other video image layouts are also contemplated. Each video conferencing endpoint may be operable to configure its own video image layout e.g. according to a layout requested by a local participant through the video conferencing endpoint . In some embodiments a local participant may cycle through the layout offerings from their video conferencing endpoint e.g. by clicking an icon to cycle to the next available layout .

As shown in the embodiment of each video conferencing endpoint e.g. video conferencing endpoints is capable of receiving two input video streams. Other numbers of input streams are also contemplated. For example video conferencing endpoints may receive three input streams e.g. see . Larger multi way conferences may be conducted as the number of inputs and or outputs on the video conferencing endpoints increase. In some embodiments an input video stream for at least one of the video conferencing endpoints e.g. input video stream may include two or more video images e.g. corresponding to two or more video conferencing endpoints . For example input video streams or may include two video images each. Other numbers of video images in input video streams are also contemplated. For example single video image streams may also be transmitted e.g. single video images and may be transmitted . In some embodiments one or more of the video conferencing endpoints may determine which video conferencing endpoints will send receive which video images to facilitate a multi way video conference. For example video conferencing endpoint may determine for a four way video conference between video conferencing endpoints which combinations of video images each respective video conferencing endpoint needs to send and or receive. The video conferencing endpoint may use pre determined rulesets patterns and or manual designations from participants. For example pattern shown as may be used to determine which video conferencing endpoints should send which video images and to whom by mapping the current video conferencing endpoints in a video call to the pattern shown in . For example one video conferencing endpoint may be mapped as video conferencing endpoint and may transmit a video frame with the video image from itself video conferencing endpoint to another video conferencing endpoint in the call mapped as video conferencing endpoint . Video conferencing endpoint may also send a video frame with video images for video conferencing endpoint and video conferencing endpoint to both the video conferencing endpoint mapped as video conferencing endpoint and to another video conferencing endpoint mapped as video conferencing endpoint . This mapping may be applied to each of the video conferencing endpoints and instructions may be sent to each to indicate which video images to send to which video conferencing endpoints. In some embodiments multiple video conferencing endpoints may make their own determinations e.g. if each is using the same pattern . Other means for determining the video image combinations to send between the video conferencing endpoints are also contemplated. In some embodiments instructions may be communicated to each of the video conferencing endpoints directly from video conferencing endpoint or for example using a round robin transmission. In some embodiments for example at the start of the video conference or if one of the video conferencing endpoints in the video conference experiences an error video conference endpoints may send the video images available to them in the determined pattern and may wait until they receive video images from other video conference endpoints before the video conferencing endpoint can send each video image combination assigned. For example video conferencing endpoint may send a video frame with the video image from itself video conferencing endpoint to video conference endpoint but may wait until receiving video images from video conferencing endpoints and e.g. received from video conferencing endpoint until sending the video images from video conferencing endpoints and to video conferencing endpoints and . Video conferencing endpoint may send the video images from video conferencing endpoints and upon receiving the video image from video conferencing endpoint . During the video conference the video conferencing endpoints may send the video images available to them according to the pattern. For example if video conferencing endpoint receives a video image from video conferencing endpoint which does not include the video image from video conferencing endpoint as designated by the pattern video conferencing endpoint may send the video image from video conferencing endpoint to video conferencing endpoints and without the video image from video conferencing endpoint until video conferencing endpoint receives both video images for video conferencing endpoint and from video conferencing endpoint

As another example as seen in video conferencing endpoints and may each be capable of receiving 3 input video streams. In the 8 way video conference shown in input video streams and may each have three video images input video streams and may each have two video images and input video streams and may each have one video image. Other configurations for the 8 way call are also contemplated. In some embodiments video conferencing endpoints with various capabilities e.g. maximum number of receivable input video streams may be mixed in the same network.

In some embodiments the patterns e.g. patterns and may change dynamically as video conferencing endpoints are added and or dropped during the video conference. Rulesets may be used to compensate and or rearrange transmissions for dropped video conferencing endpoints. In some embodiments a video conference call may only be able to support a maximum number of callers and may return an error message or required system requirements if an attempt is made to add an additional caller past the maximum number.

At managing instructions e.g. see for a multi way video conference may be received from one or more video conferencing endpoints or the managing instructions may be self determined. The managing instructions may specify which video conferencing endpoints in the multi way video conference will send which video images and or combinations of video images to other video conferencing endpoints .

At video conferencing endpoints instructed to send at least a video frame with their video image e.g. a single video image sent as input stream may send their video frame to designated video conferencing endpoints e.g. to video conferencing endpoint .

At after receiving the respective video frames with the single video images designated video conferencing endpoints may composite two or more video images on single video frames as instructed to send to designated video conferencing endpoints. For example after receiving the video image in input video stream from video conferencing endpoint video conferencing endpoint may composite the video image from video conferencing endpoint with the local video source image from video conferencing endpoint onto a single video frame to send to video conferencing endpoint in input stream .

At the composited video frames may be transmitted to designated video conferencing endpoints e.g. according to specific instructions received by corresponding video conferencing endpoints . In some embodiments the video stream may be sent and received through a single Internet Protocol IP port on each video conferencing endpoint .

At the composited video frames with at least two video images each may be received by designated video conferencing endpoints . As noted at the at least two video images may be included in a single video frame. For two video images received on the single video frame a single input decoder may be used prior to sending the video frame to the virtual decoder to separate the composited images.

At one or more of the video images e.g. separated video images and or other video images may be sent to a scaler to scale according to a video image layout e.g. a video image layout requested by a local video conferencing participant or needed for a video frame to transmit to another video conferencing endpoint .

At video images e.g. separated video images and or other video images may be composited. For example the video images may be composited into the requested video image layout that may include two or more of the local video images and the three received video images. In some embodiments the video images may be composited into video frames to send to other video conferencing endpoints .

At the video image layout may be displayed. In some embodiments recomposited video frames may be sent to other video conferencing endpoints e.g. to facilitate the multi way video conference call .

At a pattern e.g. see pattern in and in may be used to determine which video conferencing endpoints in the multi way video conference call will transmit combinations e.g. see combination in of video images to other video conferencing endpoints in the multi way video conference call.

At the pattern may be used to determine which video images to include in the various combinations transmitted between the video conferencing endpoints . Various combinations may include at least two video images each from different video conferencing endpoints e.g. as seen in . The pattern may also include single video images sent by a video conferencing endpoint to other video conferencing endpoints.

At instructions may be transmitted to one or more of the video conferencing endpoints participating in the video conference call. For example video conferencing endpoint may perform and above and may then transmit the instructions to the other video conferencing endpoints involved in the multi way video conference call.

At the first video conferencing endpoint may transmit a first video frame in video stream including video images from the first video conferencing endpoint and a second video frame in video stream including video images from the third video conferencing endpoint and the fourth video conferencing endpoint to the second video conferencing endpoint

At the first video conferencing endpoint may also transmit the second video frame to the fourth video conferencing endpoint

At the second video conferencing endpoint may transmit to the fourth video conferencing endpoint a third video frame in video stream including video images from the first video conferencing endpoint and the second video conferencing endpoint .

At the third video conferencing endpoint may transmit to the first video conferencing endpoint a fourth video frame in video frame including video images from the third video conferencing endpoints and the fourth video conferencing endpoint .

At the fourth video conferencing endpoint may transmit to the third video conferencing endpoint a fifth video frame in video stream including video images from the first video conferencing endpoint and the second video conferencing endpoint and a sixth video frame in video stream including video images from the fourth video conferencing endpoint .

At the fourth video conferencing endpoint may also transmit the fifth video frame in video stream to the first video conferencing endpoint

In this embodiment four video conferencing endpoints may participate in a four way video conference using two or fewer transmissions from each video conference system and two or fewer received transmissions per video conferencing endpoint . In some embodiments the video conferencing endpoints may separate video images out of the received video frames to scale and composite with other images e.g. from the local camera or from other video sources to form new video image layouts e.g. as requested by a local participant at the separate video conferencing endpoints and or to transmit to other video conferencing endpoints .

In some embodiments virtual decoders may be implemented in an integrated system in an application programming interface API . New abstract video sources may be enumerated as source channels. The sources may be configured with a new API that maps the virtual decoder sources to a subsection of the video frames of an incoming real source decoder stream. In some embodiments the mapping may be changed dynamically but may be configured before a video stream is opened with a virtual decoder source. Scalers may be reserved for the video streams. Only n 1 virtual decoders may be needed because one of the virtual streams being sent back may be that of the original video conferencing endpoint.

As seen in the table four video conferencing endpoints may participate in a four way call between each other. The video conferencing endpoint may signal the participants and or each other to determine which video conferencing endpoint will send which inputs. In some embodiments no single video conferencing endpoint may need to act as an MCU but instead the MCU duties may be divided among the four video conferencing endpoints . As video conferencing endpoints join and or leave the video conference the remaining video conferencing endpoints may signal each other changes in assignments in which video conferencing endpoints will send which video streams etc. In some embodiments one video conferencing endpoint may be selected to determine which video conferencing endpoints should send which inputs. In some embodiments multiple video conferencing endpoints may participate in the decision. In some embodiments one or more of the video conferencing endpoints may broadcast their capabilities e.g. number of real inputs to the other video conferencing endpoints to assist in the determination. In some embodiment composited streams sent by the video conferencing endpoints may be arranged into three 1280 by 240 video images. These may consume the resolution in a 720 p frame with the aspect ratio being corrected at the receiving end. This may be easier for the hardware scalers to handle the hardware handlers may prefer vertical offsets in the video images . Other video image sizes are also contemplated.

At a video stream including video packets from two or more sources may be received. In some embodiments the video packets of the video stream may be time multiplexed.

At metadata may be received with the video stream. In some embodiments metadata received with the video stream may be used by the decoder to organize the video packets into respective buffers e.g. buffer for source and buffer for source . For example the metadata may include identification information for the video packets.

At the video packets may be sorted into respective buffers. For example a different buffer may be used to collect the video packets for a video image for each video packet source.

At a video frame may be formed by spatially multiplexing the video images of the different sources e.g. see video frame in . In some embodiments metadata may be generated for the composite video frame that includes coordinate information for the video images in the composite video frame.

Embodiments of a subset or all and portions or all of the above may be implemented by program instructions stored in a memory medium or carrier medium and executed by a processor. A memory medium may include any of various types of memory devices or storage devices. The term memory medium is intended to include an installation medium e.g. a Compact Disc Read Only Memory CD ROM floppy disks or tape device a computer system memory or random access memory such as Dynamic Random Access Memory DRAM Double Data Rate Random Access Memory DDR RAM Static Random Access Memory SRAM Extended Data Out Random Access Memory EDO RAM Rambus Random Access Memory RDRAM etc. or a non volatile memory such as a magnetic media e.g. a hard drive or optical storage. The memory medium may include other types of memory as well or combinations thereof. In addition the memory medium may be located in a first computer in which the programs are executed or may be located in a second different computer that connects to the first computer over a network such as the Internet. In the latter instance the second computer may provide program instructions to the first computer for execution. The term memory medium may include two or more memory mediums that may reside in different locations e.g. in different computers that are connected over a network.

In some embodiments a computer system at a respective participant location may include a memory medium s on which one or more computer programs or software components according to one embodiment of the present invention may be stored. For example the memory medium may store one or more programs that are executable to perform the methods described herein. The memory medium may also store operating system software as well as other software for operation of the computer system.

Further modifications and alternative embodiments of various aspects of the invention may be apparent to those skilled in the art in view of this description. Accordingly this description is to be construed as illustrative only and is for the purpose of teaching those skilled in the art the general manner of carrying out the invention. It is to be understood that the forms of the invention shown and described herein are to be taken as embodiments. Elements and materials may be substituted for those illustrated and described herein parts and processes may be reversed and certain features of the invention may be utilized independently all as would be apparent to one skilled in the art after having the benefit of this description of the invention. Changes may be made in the elements described herein without departing from the spirit and scope of the invention as described in the following claims.

