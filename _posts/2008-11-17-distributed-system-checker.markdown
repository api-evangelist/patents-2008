---

title: Distributed system checker
abstract: A distributed system checker may check a distributed system against events to detect bugs in the distributed system. The events may include machines crashes, network partitions, and packet losses, for example. The distributed system checker may check a distributed system that can have multiple threads and multiple processes running on multiple nodes. To obtain control over a distributed system, a distributed system checker may insert an interposition layer between a process and the operating system on each node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07984332&OS=07984332&RS=07984332
owner: Microsoft Corporation
number: 07984332
owner_city: Redmond
owner_country: US
publication_date: 20081117
---
A bug is an error flaw mistake failure fault or undocumented feature in a computer program that prevents it from behaving as intended thereby producing an incorrect result for example. Many bugs arise from mistakes and errors in a computer program s code or its design and some bugs are caused by compilers producing incorrect code.

Distributed systems are directed to hardware and software systems containing more than one processing element or storage element concurrent processes or multiple programs running under a loosely or tightly controlled regime. In distributed systems a computer program is split up into parts that run simultaneously across multiple computers or nodes with communications centralized via a network. Distributed programs often accommodate heterogeneous environments network links of varying latencies and unpredictable failures in the network or the nodes.

Distributed systems are becoming increasingly crucial as more and more infrastructures are distributed for performance scalability and reliability. Distributed systems are complicated and buggy because they should correctly handle all possible events including rare events such as machine crashes network partitions and packet losses. Failures may come in the form of a node crash a network partition a message loss or disk failures for example.

Distributed systems are difficult to test due to complicated interactions between different components of the system as well as unpredictable failures events and message deliveries. Complicated dependencies within a distributed system make it particularly challenging to enumerate the possible cases that the system must handle. Identifying bugs in distributed systems is also challenging. The current practice of finding bugs in distributed systems typically involves some form of random testing such as network simulation end to end testing or analyzing logs. These techniques are not effective for finding the bugs that appear only in rare cases and are unable to reproduce the bugs when such bugs appear in the tests.

Model checkers have been used to find errors in both the design and the implementation of distributed systems. Traditional model checkers take as input an abstract model of a system and explore the states based on the abstract model. Traditional model checkers require an abstract model of the distributed system that is to be checked. Writing an abstract model of the distributed system is costly and error prone thus making application of model checking on distributed systems prohibitive.

A distributed system checker may check a distributed system against events to detect bugs in the distributed system. The distributed system comprises nodes that each runs one or many processes on an operating system. An interposition layer is provided on each node between the processes and the operating system. The distributed system checker is in control over the distributed system via the interposition layer on each node.

In an implementation the interposition layer is provided on an application programming interface API between the process and the operating system on each node.

In an implementation the interposition layer may simulate events to detect a bug in the distributed system. Events may include a machine crash a network partition a message reorder a message loss or an API failure for example. The interposition layer ensures that these simulated events are deterministic and consistent.

In an implementation the distributed system checker may be centralized and the interposition layer may expose the events in generic form to the centralized distributed system checker. The distributed system checker may be operating system independent.

In an implementation a bounding heuristic may be used in a checking of the distributed system. The bounding heuristic may separate each action that runs on the distributed system into an internal state or an external state. A number of internal states may be bounded between external states.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description. This summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

A node may be defined as a point in a network at which lines intersect or branch a device attached to a network or a terminal or other point in a network where messages can be transmitted received or forwarded. A node can be any device connected to a computer network. Nodes can be computers personal digital assistants cell phones switches routers or various other networked devices for example. Although only four nodes are shown in any number of nodes may be connected to the network and to each other. An example computer which may act as a node is described with respect to .

One or more processes each having one or more threads may run on each node respectively. The process es and thread s on each node may be used to communicate with other nodes and for carrying out distributed protocols.

Each process owns resources allocated by the operating system OS of their node. Resources may include memory file handles sockets device handles and windows. At least one thread exists within each process. If multiple threads can exist within a process then they share the same memory and file resources. Threads provide a way for a program to split itself into two or more simultaneously or pseudo simultaneously running tasks. Threads and processes differ from one operating system to another but in general a thread is contained inside a process and different threads in the same process share some resources while different processes do not.

Each node of a distributed system is prepared to handle messages events and unexpected failures that could occur at any time. A distributed system checker may check code against many possible events to detect bugs in a distributed system. The events may include machines crashes network partitions and packet losses for example. The distributed system checker may check a distributed system that can have multiple threads and multiple processes running on multiple nodes.

Each node may comprise one or more processes respectively. In an implementation one process may be run per node and the terms node and process may be used interchangeably herein in such an implementation. Additionally each node may comprise an OS respectively.

To obtain control over the distributed system the distributed system checker may insert an interposition layer between the process es and the OS on each node. An interposition layer may be provided between the process es and the OS on the client node . Similarly an interposition layer may be provided between the process es and the OS on the primary replica node and an interposition layer may be provided between the process es and the OS on the secondary replica node .

The distributed system checker may be interposed on an application programming interface API between each process and its OS for realistic and deterministic failure simulation. Thus in an implementation for each node the interposition layer may be provided on the API between the process es and the OS. An API is a set of functions procedures methods or classes that an operating system library or service provides to support requests made by computer programs. This may allow the distributed system checker to control the system behavior of the distributed system at the API level and does not require that the distributed system use a particular programming model or simulation framework.

The boundary between the process es and the OS for each node is a stable layer and may be used for the interposition layer. Interposing at a higher layer inside a process loses the generality of checking other processes and interposing lower inside the OS involves modifying a multi million line OS kernel which is highly complex.

In an implementation in which the distributed system checker and the distributed system are based on Microsoft s Windows interposing may be performed at the WinAPI layer which is the native interface for Windows applications. The Windows API referred to as WinAPI is Microsoft s core set of APIs available in the Microsoft Windows operating systems. All Windows programs interact with the Windows API regardless of the language. The distributed system checker may intercept on each of the nodes the place where they call through on a core interface e.g. the WinAPI layer and may provide unexpected events to the nodes to check the distributed system . The distributed system checker may be implemented on any operating system and may be applied to any distributed system.

In an implementation the distributed system checker may be centralized and the interposition layer may expose the events in generic form to the centralized distributed system checker. The distributed system checker may be operating system independent. The distributed system checker may transparently model check unmodified distributed systems running on an unmodified operating system. This transparency may be achieved using the interposition layer to expose the actions in the distributed system and deterministically schedule them at the centralized OS independent checker.

The interposition layer in each node may simulate a variety of events including API failures e.g. memory allocation or disk read failures rare message orders message loss network partitions and machine crashes for example. The distributed system checker may simulate these events via the interposition layer as described further herein. A node in the distributed system may send and receive messages that may be used to approximate a state the node is in. The interposition layer ensures that these simulated events are deterministic and consistent.

In an implementation the distributed system checker imposes no programming model has no language restriction and may be extended with user specified domain knowledge to improve coverage and performance. As described further herein effective state reduction techniques may be used including using state signatures for duplicate state detection and separating system level control and application level state exploration.

The distributed system checker may simulate exceptions as part of the effort for detecting bugs in the distributed system and may observe the environment. Systematic explorations of possible error causes may be performed. The distributed system checker may systematically explore possible actions with its control over the system execution as well as its ability to inject rare events at predetermined points. Such events include machine crashes message losses different message orders and network partitions. The distributed system checker may detect fail stop bugs e.g. program assertion failures segmentation faults etc. deadlocks and infinite loops as well as violations to the global assertions provided by programmers. When the distributed system checker detects an error it may store a trace of the actions that have brought the distributed system into the error state so users can deterministically reproduce the error.

The distributed system checker may simulate failures realistically and deterministically. Failures and other actions are often correlated. When the distributed system checker notifies a node A of the crash of another node B node B should stop sending more messages otherwise the distributed system checker is creating unrealistic scenarios which could lead to difficult to diagnose false positives. Failure simulation is also deterministic. Relying on the OS for failure signaling leads to non determinism. The distributed system checker may keep track of a list of actions that are affected by failures. When injecting a failure the distributed system checker may make its effect immediate by cancelling the affected actions.

In an implementation the distributed system checker may use model checking techniques. Model checking excels at exploring corner cases. A corner case is a situation that occurs only outside of normal operating parameters e.g. one that manifests itself when multiple parameters are simultaneously at extreme levels even though each parameter is within the specified range for that parameter. Corner cases are often harder and more expensive to reproduce test and optimize because they use maximal configurations in multiple dimensions. Given a set of initial states together with some safety properties a model checker may systematically explore the possible states of the system to check whether the properties are violated. A variety of techniques exist to make the state exploration process more efficient. In model checking failures and other rare states are explored similar to common states thereby quickly driving a system into corner cases for errors. The distributed system checker may use implementation level model checking to check code directly and avoid the upfront cost of writing an abstract model.

The distributed system checker may be customized by a user to write distributed assertions for global invariant checking to define state digests for checking state equivalence more precisely and to provide heuristics for guiding the exploration of the state space. These customization capabilities may improve the efficiency and effectiveness of the distributed system checker by leveraging users domain knowledge of the distributed system being checked.

In an implementation a state exploration framework may be provided for incorporating heuristics to detect distributed systems errors effectively. State space reduction techniques and state exploration heuristics may be used. In an implementation a heuristic exhaustively explores network related actions such as sending or receiving messages while bounds the number of thread interleaving it explores. Such a heuristic combined with dynamic partial order reduction may effectively bias the systematic exploration towards protocol level behaviors.

A queue of the state action pairs to be explored may be maintained. Due to the complexity of a distributed system it is often infeasible to explore exhaustively the complete state space. Deciding which new state action pairs to be added to the queue as well as the order in which those state action pairs are added and explored may be used in a model checking process.

In an implementation the checker tags each state with a vector clock and implements a customizable modular framework for exploring the state space so that different reduction techniques and heuristics can be incorporated.

In an implementation the checker takes the first state action pair from the queue controls the system execution to reach that state s if that is not the current state applies the action a reaches a new state s and examines the new resulting state for errors. It then calls a customizable function explore . The explore function takes the path from the initial state to s and then s where each state is tagged with its vector clock and each state transition is tagged with the action corresponding to the transition. For s the enabled actions are provided to the function. The function then produces a list of state action pairs and indicates whether the list should be added to the front of the queue or the back. The checker inserts the list into the queue accordingly and repeats the process.

For dynamic partial order reduction DPOR the explore function works as follows. Let a be the last action causing the transition from s to s . The function looks at every state sbefore s on the path and the action ataken at that state. If a is enabled at s i.e. if s and sare concurrent as judged by the vector clocks and a does not commute with a i.e. the different orders of the two actions could lead to different executions is added to the list of pairs to explore. Once all states are examined the function returns the list and has the checker insert the list to the queue. The function could choose to use depth first search or breadth first search depending on how the list is inserted. Also by ordering the pairs in the list differently the checker may be instructed to explore the newly added branches in different orders e.g. top down or bottom up. 

The explore function can be constructed to favor certain actions such as crash events over others to bound the exploration in various ways e.g. based on the path length number of certain actions on the path etc. and to limit the focus on a subset of possible actions. Having the checker run in different subsets of possible actions could help control the state explosion at the expense of coverage.

The checker may implement both process level and thread level vector clocks. For thread level vector clocks the checker may handle the dynamic creation of threads. Due to such dynamism and the often large number of threads thread level vector clocks tend to introduce significant complexity and overhead. The checker may support process level vector clocks for added flexibility of the framework. This can be considered as an approximation of thread level vector clocks because many distributed systems can be approximated as a state machine with mostly deterministic behavior in response to inputs i.e. messages . In this kind of reactive system the nondeterminism of the system mainly comes from the nondeterministic protocol level behaviors such as delivery of messages while a large number of the interleaving of threads within processes do not have different effects on the protocol level state of the distributed system.

The distributed system checker may explore the possible states and actions. The distributed system checker may take a pair off the queue at and may perform the action on the state at . The pair that may be taken off the queue may be determined based on predetermined heuristics. To perform an action on a state the distributed system checker brings the system to that state first. The distributed system checker spawns and runs processes on top of their native OS. Since it is complicated to checkpoint all processes by saving their states the distributed system checker may use a stateless approach the distributed system checker may checkpoint a state by remembering the actions that created the state and may restore the state by redoing all the actions. Performing the action at may lead the system to an additional state.

At it may be determined if the resulting additional state from leads to an error e.g. a failure in the distributed system. If so the distributed system checker may save a trace of the actions that brought the system into the error state at for debugging purposes e.g. so that a user may replay the trace to reproduce the error.

If no error is detected at it may be determined at if the resulting additional state from is a state that the distributed system checker has seen before. If not the distributed system checker may add the additional state and its possible actions to the queue at . After adding the additional state and actions to the queue or if the additional state was seen before processing may continue at with the distributed system checker taking another pair off the queue and continuing the process.

For systematic exploration the distributed system checker cannot let the OS schedule the processes and threads in the distributed system arbitrarily. The distributed system checker may obtain control of the distributed system using the interposition layer. In an implementation for systematic exploration the distributed system checker determines the threads in the distributed system and determines whether they are enabled e.g. not blocked . is an operational flow of an implementation of a method of control with a distributed system checker. A thread may be created in the distributed system at . At the interposition layer may create an exclusive remote procedure call RPC channel to the distributed system checker for this thread at .

The thread may be blocked at and may wait for a message from the distributed system checker before proceeding. At some point the distributed system checker may schedule the thread by sending a message to the interposition layer and allow the thread to proceed at . The thread may begin running at .

At the thread may issue a system call that is monitored by the interposition layer e.g. acquiring a mutex or mutual exclusion which is used in concurrent programming to avoid the simultaneous use of a common resource by pieces of computer code called critical sections . Once the running thread issues the system call at the interposition layer may block the thread at and may pass control back to the distributed system checker at . This approach ensures a distributed system acts under control of a distributed system checker. In an implementation at any time only one action is running and the execution of the entire distributed system may be trivially serializable and deterministic.

In an implementation the interposition layer may hook two types of WinAPIs 1 those that change the number of threads in the system including CreateThread ExitThread and TerminateThread and 2 those that affect the enabled status of threads such as SuspendThread ResumeThread EnterCriticalSection and LeaveCriticalSection . When these functions are called the distributed system checker may take control and update its internal data structures to track the threads and their statuses.

Asynchronous IOs AIOs represent hidden threads of execution inside the kernel of the OS. The distributed system checker may control AIOs for systematic exploration. This control may be used for exploring rare message orders because the distributed system may use network AIO to boost performance.

Using packet receiving as an example AIO in Windows may work as follows. A user may call WSARecv with an empty completion notification structure to express their intent to receive a packet. WSARecv immediately returns even though the message has yet to arrive. When the message arrives the kernel fills the completion notification structure and notifies the application. Later a user may call GetQueueCompletionStatus to dequeue IO completion notifications.

The distributed system checker may not let the kernel decide the order in which completion notifications are posted. The interposition layer may create a proxy event and a proxy thread for each AIO in order to hijack the kernel s completion notification. When an AIO is submitted the interposition layer replaces the application s completion notification structure with the proxy event for this AIO and lets the proxy thread wait for the event. When the kernel delivers the completion notification for the AIO the proxy thread receives it. Since the proxy thread is also under the distributed system checker s control this mechanism lets the distributed system checker obtain full control over AIO.

In an implementation the interposition layer may handle blocking networking operations such as connect accept send and receive for example and may handle AIO related networking operations such as ConnectEx AcceptEx WSARecv WSASend GetQueuedCompletionStatus and WSAGetOverlappedResult queries the completion status of a single AIO request for example.

Rare message ordering is a cause of errors. Since the interposition layer converts an AIO into a proxy thread and uniformly handles proxy threads and regular threads the distributed system checker can explore the ordering of different AIO events via the same mechanism it uses to explore thread interleavings. For example if a process issues two operations such as WSARecv to receive two messages mand m the distributed system checker can explore all possible message orders by running the proxy thread for mfirst then the thread for m or running the thread for mfirst then the thread for m.

The proxy threads may be considered to implement a simulated network in that the messages that reach proxy threads but not the distributed system are still infight in the network. This mechanism may be used to handle synchronous network operations as well. Such an approach avoids the overhead of building a network simulator and porting the distributed system to run within.

A thread scheduling action may be tagged with a set of information in order to help search heuristics determine the exploration order. When the interposition layer blocks a thread it may notify the distributed system checker which API function the thread is about to issue. Optionally a user may provide functions to compute signatures for given network messages. The interposition layer may send message signatures to the distributed system checker as well. With this information the distributed system checker may perform a search as described further herein.

The distributed system may have non deterministic choices of actions that are not exposed for checking. For example it could call rand time and perform different actions based on the return value. In an implementation non determinism for checking may be handled by setting the actions to be deterministic or exposing and exploring the actions. Determining which option to use is a tradeoff between manual overhead and coverage.

The distributed system checker may provide a method choose N to expose the choices in the distributed system. Given a choice point that has N possible actions a node may insert a call choose N which will appear to fork execution N times returning the values 0 1 . . . N 1 in each node execution respectively. A user may then write code that uses this return value to pick an action out of the N possibilities. The distributed system checker can exhaust the possible actions at this choose call by running the forked children. An example of using a choose call is provided below.

In an implementation a distributed system checker may check a distributed system against the following types of failures for example API failures e.g. memory allocation failures disk read failures etc. message loss network partitions machine crashes timeouts and rare message orders.

Regarding API failures an API may be used by a process to read data from a disk. In normal execution reading the data succeeds however a failure may occur if the underlying disk is bad. If the distributed system is run as is the success case would likely result missing the disk read failure case. Prior model checkers have the user manually insert choose calls. In an implementation a distributed system checker may interpose the API with an interposition layer and may automatically expose and explore the success and failure choices within the API. In an implementation with WinAPI the following types of functions may be checked for failure resource allocators such as malloc or the C equivalent operator new CreateThread and WSASocket WinAPI to allocate a socket and file system operations such as CreateFile ReadFile and WriteFile.

Regarding message loss because an AIO may be converted into a thread message loss may be simulated by letting the proxy thread of an AIO deliver a failed IO completion notification to the distributed system.

In an implementation the distributed system checker can simulate network disconnection between two processes to simulate network partition. The distributed system checker may simulate a network partition between two processes by blocking all communication between them. For this purpose the distributed system checker may use an interposition layer to track the open connections in the checked distributed system. When a connection is established the interposition layer may notify the distributed system checker. When the distributed system checker decides to break the connection between two processes it sends a notification to the interposition layer in each process and marks the socket descriptor corresponding to the connection as broken. The interposition layer keeps a list of such broken sockets and only removes a socket from this list when the distributed system explicitly closes the socket. The distributed system checker may fail any attempt to send or receive through one of the broken sockets immediately without issuing real system calls.

The distributed system checker may simulate a machine crash by physically killing a process i.e. calling WinAPI TerminateProcess . Only states persisted on disk files may be carried over between crash and recovery. To generate possible disk states that could occur after a crash the distributed system checker may construct a set of disk blocks that may be cached in memory and would have been lost after a crash. Suppose the current disk is D and there are two disk blocks band bcached in memory. The distributed system checker may permute these writes to generate disk images that could occur D D b D b and D b b. The distributed system checker checks distributed systems running on top of an OS and may compute the set of potentially cached disk blocks by interposing WinAPI WriteFile WriteFileGather and FlushFileBuffers thus avoiding writing a kernel driver.

Many distributed systems have liveness detectors to detect network failures or machine crashes. The liveness detectors typically wake up periodically and use the absence of beacons from a remote machine for a certain period of time as an indicator that there is a network failure or the remote machine has crashed. When the distributed system checker kills a process it notifies liveness detectors of the processes that have connections to this process. In an implementation the distributed system checker requests the liveness detector wait on a special liveness event liveness notify event instead of waking up periodically. When the distributed system checker partitions the network or kills a machine it sets the event and wakes up the liveness detectors in involved processes.

The distributed system checker may handle timeouts similar to the way it handles liveness detectors. System code may have timeouts such as explicit timeouts where a user explicitly tells the OS the timeout values either by registering a timer or waiting for a known amount of time implicit timeouts where a user may call to get a time of day or similar functions and compare the returns and kernel timeouts where the timeout is completely decided by the kernel.

To handle explicit timeouts in an implementation the distributed system checker may track which failure can trigger a timeout and only fires the timeout when it simulates the failure to avoid false positives. For example a thread can wait for a message and if the message does not arrive in five seconds it times out. The interposition layer may replace this timeout with infinite and may block the calling thread. Only when the corresponding connection is broken or the other communication end is killed will the distributed system checker trigger this timeout.

For implicit timeouts since the distributed system checker does not see the timeout value it returns deterministic values for calls to get a time of day or the WinAPI methods GetTickCount GetSystemTime and GetSystemTimeAsFileTime for example. Users who want to get more coverage can use a distributed system checker provided method to advance the time in combination with a choose call to manually fire implicit timeouts.

For kernel timeouts failures may be made immediate. The distributed system checker may track events that may be affected by a failure. When simulating a failure the distributed system checker may fail the dependent events.

Due to the state explosion problem of model checkers exhaustively exploring the entire search space of a distributed system may be infeasible. Previous model checkers do not distinguish different states and actions by mixing control of the system with exploration of state space. This often leads to the exploration of uninteresting states. In an implementation the distributed system checker separates the mechanisms that control the system to achieve deterministic execution from the mechanisms that guide the system state exploration.

A typical distributed system consists of many processes running on different machines and each process usually has multiple threads. Processes synchronize and communicate with each other by passing messages. In order to make the system less complex to debug many distributed systems follow the state machine approach where each process is approximated as a state machine with deterministic behavior in response to inputs i.e. the messages .

A distributed system checker may be limited to considering application level behaviors such as sending and receiving messages failure of a network and crashing of processes and their interleaving. A bounding heuristic may be provided that bounds state exploration and focuses on exploring more application level behaviors. System level control and application level exploration may thus be separated. A distributed system may involve multiple processes each running multiple threads and the distributed system checker may take control of such a system by controlling all the low level system decisions including thread scheduling. However bugs may be more easily manifested e.g. for analysis debugging troubleshooting etc. at the application level. For example bugs based on thread scheduling are less informative than bugs based on network actions e.g. send or receive messages or failures.

At application events may be distinguished from thread scheduling events. At the application events are set as external states. An external state is a state that the only possible transitions are application level events such as message delivery or node crashing and recovery. In an external state threads are either blocked or idle looping i.e. threads periodically wake up and get back to sleep again . An external state transits into another external state by going through a set of internal states where threads become enabled in response to the application level events. For example in a distributed file system when a node receives a request to update to a file it may update local file states as well as send messages to other nodes for data replication. These actions may be carried out by several threads within the process. The interleaving of these threads may be less informative to consider for debugging than for example one of the replication messages being lost.

At internal states may be bounded with external states. The number of paths following each external state may be bounded to reduce the number of internal states being considered. Due to idle looping the path between two external states might be infinitely long. Therefore for each path the length of internal transitions between two external states may be bounded.

Application level events may occur at an internal state. For example between receiving a message and sending a reply a node may crash or recover. The number of application level transitions may be bounded between two external states. If the bound is 0 the distributed system may behave as if it were event driven each node receives a message processes it and potentially sends replies. It then blocks and waits for the next message. As the bound is increased more states may be considered.

A user may guide state consideration. Given a distributed system without user intervention in an implementation a distributed system checker may explore the event interleavings systematically. However in many cases even with the internal state bounding exhausting all states is infeasible. In many cases a user may know which states to consider.

In an implementation a user may provide the distributed system checker with guidance for choosing the states to consider. In a distributed system checker the transitions are tagged with rich information. A user may modify the techniques of adding and removing information from the queue e.g. of the method of by examining the tagged information of the transitions and giving different priorities and or filtering out certain events.

State signatures may be used to detect the states that have been considered before and are not to be considered again. To avoid redundantly considering the same state twice a model checker may keep a signature for each state it sees. is an operational flow of an implementation of a method of determining a state signature that may be used with a distributed system checker.

In an implementation a state signature of each process in a distributed system may be determined incrementally based on the messages it sends and receives and their order. The state signature for a process may be determined incrementally to avoid saving a history of messages. At a state signature of a process may be determined e.g. using any known function. At the process may send or receive a message. At the state signature may be updated based on the signature of the message that has been sent or received. Processing may continue at as the process sends or receives additional messages. The signature of the distributed system may be determined at as a concatenation of signatures of each process.

Suppose the current state signature of a process is sigp. After the process receives a message the state signature may be updated as sigp hash sigp RECV sigm where sigm is the message signature. After the process sends a message its signature may be updated as sigp hash sigp SEND sigm . These updates resemble the update of vector clock for tracking causality as is well known.

If a process receives the same set of messages in the same order and sends out the same set of messages in the same order then it is highly likely to be in the same state. In an implementation state signatures may be determined only for external states. Since the total number of internal states is bounded between two external states and the distributed system checker uses a random strategy to explore internal states it is unnecessary to determine signatures for internal states.

In an implementation after the distributed system checker simulates a network failure or a machine crash it may notify related or affected processes. The notifications may affect process states and process state signatures may be updated for these processes.

In an implementation because an application level message may be broken down into several socket level sends and receives each message may have a code or a number appended to it to mark the end of the message. The code or number may serve to identify message boundaries that may be used by the distributed system checker in the determination of message signatures.

Numerous other general purpose or special purpose computing system environments or configurations may be used. Examples of well known computing systems environments and or configurations that may be suitable for use include but are not limited to personal computers PCs server computers handheld or laptop devices multiprocessor systems microprocessor based systems network PCs minicomputers mainframe computers embedded systems distributed computing environments that include any of the above systems or devices and the like.

Computer executable instructions such as program modules being executed by a computer may be used. Generally program modules include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. Distributed computing environments may be used where tasks are performed by remote processing devices that are linked through a communications network or other data transmission medium. In a distributed computing environment program modules and other data may be located in both local and remote computer storage media including memory storage devices.

With reference to an exemplary system for implementing aspects described herein includes a computing device such as computing device . In its most basic configuration computing device typically includes at least one processing unit and memory . Depending on the exact configuration and type of computing device memory may be volatile such as random access memory RAM non volatile such as read only memory ROM flash memory etc. or some combination of the two. This most basic configuration is illustrated in by dashed line .

Computing device may have additional features functionality. For example computing device may include additional storage removable and or non removable including but not limited to magnetic or optical disks or tape. Such additional storage is illustrated in by removable storage and non removable storage .

Computing device typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by device and include both volatile and non volatile media and removable and non removable media.

Computer storage media include volatile and non volatile and removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Memory removable storage and non removable storage are all examples of computer storage media. Computer storage media include but are not limited to RAM ROM electrically erasable program read only memory EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of computing device .

Computing device may contain communications connection s that allow the device to communicate with other devices. Computing device may also have input device s such as a keyboard mouse pen voice input device touch input device etc. Output device s such as a display speakers printer etc. may also be included. All these devices are well known in the art and need not be discussed at length here.

Computing device may be one of a plurality of computing devices inter connected by a network. As may be appreciated the network may be any appropriate network each computing device may be connected thereto by way of communication connection s in any appropriate manner and each computing device may communicate with one or more of the other computing devices in the network in any appropriate manner. For example the network may be a wired or wireless network within an organization or home or the like and may include a direct or indirect coupling to an external network such as the Internet or the like.

It should be understood that the various techniques described herein may be implemented in connection with hardware or software or where appropriate with a combination of both. Thus the processes and apparatus of the presently disclosed subject matter or certain aspects or portions thereof may take the form of program code i.e. instructions embodied in tangible media such as floppy diskettes CD ROMs hard drives or any other machine readable storage medium where when the program code is loaded into and executed by a machine such as a computer the machine becomes an apparatus for practicing the presently disclosed subject matter.

In the case of program code execution on programmable computers the computing device generally includes a processor a storage medium readable by the processor including volatile and non volatile memory and or storage elements at least one input device and at least one output device. One or more programs may implement or utilize the processes described in connection with the presently disclosed subject matter e.g. through the use of an API reusable controls or the like. Such programs may be implemented in a high level procedural or object oriented programming language to communicate with a computer system. However the program s can be implemented in assembly or machine language if desired. In any case the language may be a compiled or interpreted language and it may be combined with hardware implementations.

Although exemplary implementations may refer to utilizing aspects of the presently disclosed subject matter in the context of one or more stand alone computer systems the subject matter is not so limited but rather may be implemented in connection with any computing environment such as a network or distributed computing environment. Still further aspects of the presently disclosed subject matter may be implemented in or across a plurality of processing chips or devices and storage may similarly be affected across a plurality of devices. Such devices might include PCs network servers and handheld devices for example.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

