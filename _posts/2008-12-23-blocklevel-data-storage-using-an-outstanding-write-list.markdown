---

title: Block-level data storage using an outstanding write list
abstract: A secure storage appliance is disclosed, along with methods of storing and reading data in a secure storage network. The secure storage appliance is configured to present to a client a virtual disk, the virtual disk mapped to the plurality of physical storage devices. The secure storage appliance is capable of executing program instructions configured to generate a plurality of secondary blocks of data by performing splitting and encrypting operations on a block of data received from the client for storage on the virtual disk and reconstitute the block of data from at least a portion of the plurality of secondary blocks of data stored in shares on corresponding physical storage devices in response to a request from the client.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08386798&OS=08386798&RS=08386798
owner: Unisys Corporation
number: 08386798
owner_city: Blue Bell
owner_country: US
publication_date: 20081223
---
The present disclosure claims the benefit of commonly assigned U.S. patent application Ser. No. 12 272 012 entitled BLOCK LEVEL DATA STORAGE SECURITY SYSTEM filed 17 Nov. 2008. The present disclosure also claims the benefit of commonly assigned U.S. patent application Ser. No. 12 336 558 entitled DATA RECOVERY USING ERROR STRIP IDENTIFIERS filed 17 Dec. 2008.

The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. No. 12 336 559 entitled STORAGE SECURITY USING CRYPTOGRAPHIC SPLITTING filed 17 Dec. 2008. The present disclosure is also related to commonly assigned U.S. patent application Ser. No. 12 336 562 entitled STORAGE SECURITY USING CRYPTOGRAPHIC SPLITTING filed 17 Dec. 2008. The present disclosure is related to commonly assigned U.S. patent application Ser. No. 12 336 564 entitled STORAGE SECURITY USING CRYPTOGRAPHIC SPLITTING filed 17 Dec. 2008. The present disclosure is related to commonly assigned U.S. patent application Ser. No. 12 336 568 entitled STORAGE SECURITY USING CRYPTOGRAPHIC SPLITTING filed 17 Dec. 2008.

The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. No. 12 342 438 entitled STORAGE AVAILABILITY USING CRYPTOGRAPHIC SPLITTING filed 23 Dec. 2008. The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. No. 12 342 464 entitled STORAGE AVAILABILITY USING CRYPTOGRAPHIC SPLITTING filed 23 Dec. 2008.

The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. No. 12 342 547 entitled STORAGE OF CRYPTOGRAPHICALLY SPLIT DATA BLOCKS AT GEOGRAPHICALLY SEPARATED LOCATIONS filed 23 Dec. 2008. The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. No. 12 342 523 entitled RETRIEVAL OF CRYPTOGRAPHICALLY SPLIT DATA BLOCKS FROM FASTEST RESPONDING STORAGE DEVICES filed 23 Dec. 2008.

The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. No. 12 342 636 entitled STORAGE COMMUNITIES OF INTEREST USING CRYPTOGRAPHIC SPLITTING filed 23 Dec. 2008. The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. 12 342 575 entitled STORAGE COMMUNITIES OF INTEREST USING CRYPTOGRAPHIC SPLITTING filed 23 Dec. 2008. The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. 12 342 610 entitled STORAGE COMMUNITIES OF INTEREST USING CRYPTOGRAPHIC SPLITTING filed 23 Dec. 2008.

The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. No. 12 342 379 entitled SECURE NETWORK ATTACHED STORAGE DEVICE USING CRYPTOGRAPHIC SPLITTING filed 23 Dec. 2008.

The present disclosure is related to commonly assigned and concurrently filed U.S. patent application Ser. No. 12 342 414 entitled VIRTUAL TAPE BACKUP ARRANGEMENT USING CRYPTOGRAPHICALLY SPLIT STORAGE filed 23 Dec. 2008.

These related applications are incorporated by reference herein in its entirety as if it is set forth in this application.

The present disclosure relates to data storage systems and security for such systems. In particular the present disclosure relates to a block level data storage security system.

Modern organizations generate and store large quantities of data. In many instances organizations store much of their important data at a centralized data storage system. It is frequently important that such organizations be able to quickly access the data stored at the data storage system. In addition it is frequently important that data stored at the data storage system be recoverable if the data is written to the data storage system incorrectly or if portions of the data stored at the repository is corrupted. Furthermore it is important that data be able to be backed up to provide security in the event of device failure or other catastrophic event.

The large scale data centers managed by such organizations typically require mass data storage structures and storage area networks capable of providing both long term mass data storage and access capabilities for application servers using that data. Some data security measures are usually implemented in such large data storage networks and are intended to ensure proper data privacy and prevent data corruption. Typically data security is accomplished via encryption of data and or access control to a network within which the data is stored. Data can be stored in one or more locations e.g. using a redundant array of inexpensive disks RAID or other techniques.

Mass data storage system illustrated in is an example of an existing mass data storage system. As shown an application server e.g. a database or file system provider connects to a number of storage devices providing mass storage of data to be maintained accessible to the application server via direct connection an IP based network and a Storage Area Network . Each of the storage devices can host disks of various types and configurations useable to store this data.

The physical disks are made visible accessible to the application server by mapping those disks to addressable ports using for example logical unit numbering LUN internet SCSI iSCSI or common internet file system CIFS connection schemes. In the configuration shown five disks are made available to the application server bearing assigned letters I M. Each of the assigned drive letters corresponds to a different physical disk or at least a different portion of a physical disk connected to a storage device and has a dedicated addressable port through which that disk is accessible for storage and retrieval of data. Therefore the application server directly addresses data stored on the physical disks .

A second typical data storage arrangement is shown in . The arrangement illustrates a typical data backup configuration useable to tape backup files stored in a data network. The network includes an application server which makes a snapshot of data to send to a backup server . The backup server stores the snapshot and operates a tape management system to record that snapshot to a magnetic tape or other long term storage device.

These data storage arrangements have a number of disadvantages. For example in the network a number of data access vulnerabilities exist. An unauthorized user can steal a physical disk and thereby obtain access to sensitive files stored on that disk. Or the unauthorized user can exploit network vulnerabilities to observe data stored on disks by monitoring the data passing in any of the networks between an authorized application server or other authorized user and the physical disk . The network also has inherent data loss risks. In the network physical data storage can be time consuming and physical backup tapes can be subject to failure damage or theft.

To overcome some of these disadvantages systems have been introduced which duplicate and or separate files and directories for storage across one or more physical disks. The files and directories are typically stored or backed up as a monolith meaning that the files are logically grouped with other like data before being secured. Although this provides a convenient arrangement for retrieval in that a common security construct e.g. an encryption key or password is related to all of the data it also provides additional risk exposure if the data is compromised.

In accordance with the following disclosure the above and other problems are solved by the following 

In a first aspect a method for securely writing and reading data the method comprising receiving at a secure storage appliance a primary read request for a primary data block at a primary storage location of a volume provided by the secure storage appliance. The method also comprises in response to receiving the primary read request determining at the secure storage appliance whether the primary storage location is locked. Furthermore the method comprises when the primary storage location is locked retrieving the primary data block from an outstanding write list that stores primary write requests that could not be completed when the primary write requests were received by the secure storage appliance. In addition the method comprises when the primary storage location is not locked sending from the secure storage appliance to at least M storage devices in a plurality of N storage devices that store secondary data blocks that result from cryptographically splitting the primary data block secondary read requests to read ones of the secondary data blocks wherein M designates a minimum number of secondary data blocks required to reconstruct the primary data block and N designates a number of secondary storage blocks generated by cryptographically splitting the primary data block wherein M is less than N. Furthermore when the primary storage location is not locked receiving at the secure storage appliance secondary read responses sent by the storage devices the secondary read responses containing the secondary data blocks. In addition when the primary storage location is not locked reconstructing at the secure storage appliance the primary data block using the secondary data blocks contained in the secondary read responses. The method also comprises sending from the secure storage appliance a primary read response that is responsive to the primary read request the primary read response containing the primary data block.

In a second aspect an electronic computing device comprises a processing unit a primary interface a secondary interface and a system memory comprising instructions. When executed by the processing unit the instructions cause the processing unit to receive a primary read request for a primary data block at a primary storage location of a volume provided by the electronic computing device. The instructions also cause the processing unit to in response to receiving the primary read request determine whether the primary storage location is locked. In addition the instructions cause the processing unit to when the primary storage location is locked retrieve the primary data block from an outstanding write list that stores primary write requests that could not be completed when the primary write requests were received by the secure storage appliance. When the primary storage location is not locked the instructions cause the processing unit to send to at least M storage devices in a plurality of N storage devices that store secondary data blocks that result from cryptographically splitting the primary data block secondary read requests to read ones of the secondary data blocks wherein M designates a minimum number of secondary data blocks required to reconstruct the primary data block and N designates a number of secondary storage blocks generated by cryptographically splitting the primary data block wherein M is less than N. In addition when the primary storage location is not locked the instructions cause the processing unit to receive secondary read responses sent by the storage devices the secondary read responses containing the secondary data blocks. When the primary storage location is not locked the instructions cause the processing unit to reconstruct the primary data block using the secondary data blocks contained in the secondary read responses. In addition the instructions cause the processing unit to send a primary read response that is responsive to the primary read request the primary read response containing the primary data block.

In a third aspect a computer readable storage medium comprises instructions that when executed at an electronic computing device cause the electronic computing device to receive a primary write request to store a primary data block at a primary storage location. The instructions also cause the electronic computing device to in response to receiving the primary write request determine whether the primary storage location is locked. In response to determining that the primary storage location is locked the instructions cause the electronic computing device to write the primary write request to an outstanding write list. In response to determining that the primary storage location is not locked the instructions cause the electronic computing device to determine whether the primary write request can be completed. Furthermore when it is determined that the primary write request cannot be completed the instructions cause the processing unit to lock the primary storage location and write the primary write request to the outstanding write list. When it is determined that the primary write request can be completed the instructions cause the processing unit to cryptographically split the primary data block into the secondary data blocks. Furthermore when it is determined that the primary write request can be completed the instructions cause the processing unit to send to the storage devices secondary write requests to write the secondary data blocks. The instructions also cause the processing unit to receive a primary read request for the primary data block at the primary storage location of a volume provided by the electronic computing device. Furthermore instructions cause the processing unit to in response to receiving the primary read request determine whether the primary storage location is locked. When the primary storage location is locked the instructions cause the processing unit to retrieve the primary data block from an outstanding write list that stores primary write requests that could not be completed at the time when the primary write requests were received by the secure storage appliance. When the primary storage location is not locked the instructions cause the processing unit to send to at least M storage devices in a plurality of N storage devices that store secondary data blocks that result from cryptographically splitting the primary data block secondary read requests to read ones of the secondary data blocks wherein M designates a minimum number of secondary data blocks required to reconstruct the primary data block and N designates a number of secondary storage blocks generated by cryptographically splitting the primary data block wherein M is less than N. Furthermore when the primary storage location is not locked the instructions cause the processing unit to receive secondary read responses sent by the storage devices the secondary read responses containing the secondary data blocks. In addition when the primary storage location is not locked the instructions cause the processing unit to reconstruct the primary data block using the secondary data blocks contained in the secondary read responses. Furthermore the instructions cause the processing unit to send a primary read response that is responsive to the primary read request the primary read response containing the primary data block.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Various embodiments of the present invention will be described in detail with reference to the drawings wherein like reference numerals represent like parts and assemblies throughout the several views. Reference to various embodiments does not limit the scope of the invention which is limited only by the scope of the claims attached hereto. Additionally any examples set forth in this specification are not intended to be limiting and merely set forth some of the many possible embodiments for the claimed invention.

The logical operations of the various embodiments of the disclosure described herein are implemented as 1 a sequence of computer implemented steps operations or procedures running on a programmable circuit within a computer and or 2 a sequence of computer implemented steps operations or procedures running on a programmable circuit within a directory system database or compiler.

In general the present disclosure relates to a block level data storage security system. By block level it is intended that the data storage and security performed according to the present disclosure is not performed based on the size or arrangement of logical files e.g. on a per file or per directory level but rather that the data security is based on individual read and write operations related to physical blocks of data. In various embodiments of the present disclosure the data managed by the read and write operations are split or grouped on a bitwise or other physical storage level. These physical storage portions of files can be stored in a number of separated components and encrypted. The split encrypted data improves data security for the data at rest on the physical disks regardless of the access vulnerabilities of physical disks storing the data. This is at least in part because the data cannot be recognizably reconstituted without having appropriate access and decryption rights to multiple distributed disks. The access rights limitations provided by such a system also makes deletion of data simple in that deletion of access rights e.g. encryption keys provides for effective deletion of all data related to those rights.

The various embodiments of the present disclosure are applicable across a number of possible networks and network configurations in certain embodiments the block level data storage security system can be implemented within a storage area network SAN or Network Attached Storage NAS . Other possible networks in which such systems can be implemented exist as well.

Referring now to a block diagram illustrating an example data storage system is shown according to the principles of the present disclosure. In the example of system includes a set of client devices A through N collectively client devices . Client devices can be a wide variety of different types of devices. For example client devices can be personal computers laptop computers network telephones mobile telephones television set top boxes network televisions video gaming consoles web kiosks devices integrated into vehicles mainframe computers personal media players intermediate network devices network appliances and other types of computing devices. Client devices may or may not be used directly by human users.

Client devices are connected to a network . Network facilitates communication among electronic devices connected to network . Network can be a wide variety of electronic communication networks. For example network can be a local area network a wide area network e.g. the Internet an extranet or another type of communication network. Network can include a variety of connections including wired and wireless connections. A variety of communications protocols can be used on network including Ethernet WiFi WiMax Transfer Control Protocol and many other communications protocols.

In addition system includes an application server . Application server is connected to the network which is able to facilitate communication between the client devices and the application server . The application server provides a service to the client devices via network . For example the application server can provide a web application to the client devices . In another example the application server can provide a network attached storage server to the client devices . In another example the application server can provide a database access service to the client devices . Other possibilities exist as well.

The application server can be implemented in several ways. For example the application server can be implemented as a standalone server device as a server blade as an intermediate network device as a mainframe computing device as a network appliance or as another type of computing device. Furthermore it should be appreciated that the application server can include a plurality of separate computing devices that operate like one computing device. For instance the application server can include an array of server blades a network data center or another set of separate computing devices that operate as if one computing device. In certain instances the application server can be a virtualized application server associated with a particular group of users as described in greater detail below in .

The application server is communicatively connected to a secure storage appliance that is integrated in a storage area network SAN . Further the secure storage appliance is communicatively connected to a plurality of storage devices A through N collectively storage devices . Similar to the secure storage appliance the storage devices can be integrated with the SAN .

The secure storage appliance can be implemented in several ways. For example the secure storage appliance can be implemented as a standalone server device as a server blade as an intermediate network device as a mainframe computing device as a network appliance or as another type of computing device. Furthermore it should be appreciated that like the application server the secure storage appliance can include a plurality of separate computing devices that operate like one computing device. In certain embodiments SAN may include a plurality of secure storage appliances. Each of secure storage appliances is communicatively connected to a plurality of the storage devices . In addition it should be appreciated that the secure storage appliance can be implemented on the same physical computing device as the application server .

The application server can be communicatively connected to the secure storage appliance in a variety of ways. For example the application server can be communicatively connected to the secure storage appliance such that the application server explicitly sends I O commands to secure storage appliance . In another example the application server can be communicatively connected to secure storage appliance such that the secure storage appliance transparently intercepts I O commands sent by the application server . On a physical level the application server and the secure storage appliance can be connected via a communication interface that can support a SCSI command set. Examples of such interfaces include Fibre Channel and iSCSI interfaces.

The storage devices can be implemented in a variety of different ways as well. For example one or more of the storage devices can be implemented as disk arrays tape drives JBODs just a bunch of disks or other types of electronic data storage devices.

In various embodiments the SAN is implemented in a variety of ways. For example the SAN can be a local area network a wide area network e.g. the Internet an extranet or another type of electronic communication network. The SAN can include a variety of connections including wired and wireless connections. A variety of communications protocols can be used on the SAN including Ethernet WiFi WiMax Transfer Control Protocol and many other communications protocols. In certain embodiments the SAN is a high bandwidth data network provided using at least in part an optical communication network employing Fibre Channel connections and Fibre Channel Protocol FCP data communications protocol between ports of data storage computing systems.

The SAN additionally includes an administrator device . The administrator device is communicatively connected to the secure storage appliance and optionally to the storage devices . The administrator device facilitates administrative management of the secure storage appliance and to storage devices. For example the administrator device can provide an application that can transfer configuration information to the secure storage appliance and the storage devices . In another example the administrator device can provide a directory service used to store information about the SAN resources and also centralize the SAN .

In various embodiments the administrator device can be implemented in several ways. For example the administrator device can be implemented as a standalone computing device such as a PC or a laptop or as another type of computing device. Furthermore it should be appreciated that like the secure storage appliance the administrator device can include a plurality of separate computing devices that operate as one computing device.

Now referring to a data storage system is shown according to a possible embodiment of the present disclosure. The data storage system provides additional security by way of introduction of a secure storage appliance and related infrastructure functionality into the data storage system as described in the generalized example of .

In the embodiment shown the data storage system includes an application server upon which a number of files and databases are stored. The application server is generally one or more computing devices capable of connecting to a communication network and providing data and or application services to one or more users e.g. in a client server thin client or local account model . The application server is connected to a plurality of storage systems . In the embodiment shown storage systems are shown and are illustrated as a variety of types of systems including direct local storage as well as hosted remote storage. Each storage system manages storage on one or more physical storage devices . The physical storage devices generally correspond to hard disks or other long term data storage devices. In the specific embodiment shown the JBOD storage system connects to physical storage devices the NAS storage system connects to physical storage device the JBOD storage system connects to physical storage devices the storage system connects to physical storage devices and the JBOD storage system connects to physical storage device . Other arrangements are possible as well and are in general a matter of design choice.

In the embodiment shown a plurality of different networks and communicative connections reside between the application server and the storage systems . For example the application server is directly connected to storage system via a JBOD connection e.g. for local storage. The application server is also communicatively connected to storage systems via network which uses any of a number of IP based protocols such as Ethernet WiFi WiMax Transfer Control Protocol or any other of a number of communications protocols. The application server also connects to storage systems via a storage area network SAN which can be any of a number of types of SAN networks described in conjunction with SAN above.

A secure storage appliance is connected between the application server and a plurality of the storage systems . The secure storage appliance can connect to dedicated storage systems e.g. the JBOD storage system in or to storage systems connected both directly through the SAN and via the secure storage appliance e.g. the JBOD storage system and storage system . Additionally the secure storage appliance can connect to systems connected via the network e.g. the JBOD system . Other arrangements are possible as well. In instances where the secure storage appliance is connected to a storage system one or more of the physical storage devices managed by the corresponding system is secured by way of data processing by the secure storage appliance. In the embodiment shown the physical storage devices are secured physical storage devices meaning that these devices contain data managed by the secure storage appliance as explained in further detail below.

Generally inclusion of the secure storage appliance within the data storage system may provide improved data security for data stored on the physical storage devices. As is explained below this can be accomplished for example by cryptographically splitting the data to be stored on the physical devices such that generally each device contains only a portion of the data required to reconstruct the originally stored data and that portion of the data is a block level portion of the data encrypted to prevent reconstitution by unauthorized users.

Through use of the secure storage appliance within the data storage system a plurality of physical storage devices can be mapped to a single volume and that volume can be presented as a virtual disk for use by one or more groups of users. In comparing the example data storage system to the prior art system shown in it can be seen that the secure storage appliance allows a user to have an arrangement other than one to one correspondence between drive volume letters in drive letters I M and physical storage devices. In the embodiment shown two additional volumes are exposed to the application server virtual disk drives T and U in which secure copies of data can be stored. Virtual disk having volume label T is illustrated as containing secured volumes F and F i.e. the drives mapped to the iSCS12 port of the application server as well as a new drive thereby providing a secured copy of information on either of those drives for access by a group of users. Virtual disk having volume label U provides a secured copy of the data held in DB1 i.e. the drive mapped to LUN03 . By distributing volumes across multiple disks security is enhanced because copying or stealing data from a single physical disk will generally be insufficient to access that data i.e. multiple disks of data as well as separately held encryption keys must be acquired 

Referring now to a portion of the data storage system is shown including details of the secure storage appliance . In the embodiment shown the secure storage appliance includes a number of functional modules that generally allow the secure storage appliance to map a number of physical disks to one or more separate accessible volumes that can be made available to a client and presenting a virtual disk to clients based on those defined volumes. Transparently to the user the secure storage appliance applies a number of techniques to stored and retrieved data to provide data security.

In the embodiment shown the secure storage appliance includes a core functional unit a LUN mapping unit and a storage subsystem interface . The core functional unit includes a data conversion module that operates on data written to physical storage devices and retrieved from the physical storage devices . In general when the data conversion module receives a logical unit of data e.g. a file or directory to be written to physical storage devices it splits that primary data block at a physical level i.e. a block level and encrypts the secondary data blocks using a number of encryption keys.

The manner of splitting the primary data block and the number of physical blocks produced is dictated by additional control logic within the core functional unit . As described in further detail below during a write operation that writes a primary data block to physical storage e.g. from an application server the core functional unit directs the data conversion module to split the primary data block received from the application server into N separate secondary data blocks. Each of the N secondary data blocks is intended to be written to a different physical storage device within the data storage system . The core functional unit also dictates to the data conversion module the number of shares for example denoted as M of the N total shares that are required to reconstitute the primary data block when requested by the application server .

The secure storage appliance connects to a metadata store which is configured to hold metadata information about the locations redundancy and encryption of the data stored on the physical storage devices . The metadata store is generally held locally or in proximity to the secure storage appliance to ensure fast access of metadata regarding the shares. The metadata store can be in various embodiments a database or file system storage of data describing the data connections locations and shares used by the secure storage appliance. Additional details regarding the specific metadata stored in the metadata store are described below.

The LUN mapping unit generally provides a mapping of one or more physical storage devices to a volume. Each volume corresponds to a specific collection of physical storage devices upon which the data received from client devices is stored. In contrast typical prior art systems assign a LUN logical unit number or other identifier to each physical storage device or connection port to such a device such that data read operations and data write operations directed to a storage system can be performed specific to a device associated with the system. In the embodiment shown the LUNs correspond to target addressable locations on the secure storage appliance of which one or more is exposed to a client device such as an application server . Based on the mapping of LUNs to a volume the virtual disk related to that volume appears as a directly addressable component of the data storage system having its own LUN. From the perspective of the application server this obscures the fact that primary data blocks written to a volume can in fact be split encrypted and written to a plurality of physical storage devices across one or more storage systems .

The storage subsystem interface routes data from the core functional unit to the storage systems communicatively connected to the secure storage appliance . The storage subsystem interface allows addressing various types of storage systems . Other functionality can be included as well.

In the embodiment shown a plurality of LUNs are made available by the LUN mapping unit for addressing by client devices. As shown by way of example LUNs LUN04 LUNnn are illustrated as being addressable by client devices. Within the core functional unit the data conversion module associates data written to each LUN with a share of that data split into N shares and encrypted. In the embodiment shown in the example of a block read operation or block write operation to LUN04 is illustrated as being associated with a four way write in which secondary data blocks L.through L.are created and mapped to various devices connected to output ports shown in as network interface cards NICs a Fibre Channel interface and a serial ATA interface. An analogous operation is also shown with respect to LUN05 but written to a different combination of shares and corresponding physical disks.

The core functional unit LUN mapping unit and storage subsystem interface can include additional functionality as well for managing timing and efficiency of data read and write operations. Additional details regarding this functionality are described in another embodiment detailed below in conjunction with the secure storage appliance functionality described in .

The secure storage appliance includes an administration interface that allows an administrator to set up components of the secure storage appliance and to otherwise manage data encryption splitting and redundancy. The administration interface handles initialization and discovery on the secure storage appliance as well as creation modifying and deletion of individual volumes and virtual disks event handling data base administration and other system services such as logging . Additional details regarding usage of the administration interface are described below in conjunction with .

In the embodiment shown of the secure storage appliance the secure storage appliance connects to an optional enterprise directory and a key manager via the administration interface . The enterprise directory is generally a central repository for information about the state of the secure storage appliance and can be used to help coordinate use of multiple secure storage appliances in a network as illustrated in the configuration shown in below. The enterprise directory can store in various embodiments information including a remote user table a virtual disk table a metadata table a device table log and audit files administrator accounts and other secure storage appliance status information.

In embodiments lacking the enterprise directory redundant secure storage appliances can manage and prevent failures by storing status information of other secure storage appliances to ensure that each appliance is aware of the current state of the other appliances.

The key manager stores and manages certain keys used by the data storage system for encrypting data specific to various physical storage locations and various individuals and groups accessing those devices. In certain embodiments the key manager stores workgroup keys. Each workgroup key relates to a specific community of individuals i.e. a community of interest and a specific volume thereby defining a virtual disk for that community. The key manager can also store local copies of session keys for access by the secure storage appliance . Secure storage appliance uses each of the session keys to locally encrypt data on different ones of physical storage devices . Passwords can be stored at the key manager as well. In certain embodiments the key manager is operable on a computing system configured to execute any of a number of key management software packages such as the Key Management Service provided for a Windows Server environment manufactured by Microsoft Corp. of Redmond Wash.

Although the present disclosure provides for encryption keys including session keys and workgroup keys additional keys may be used as well such as a disk signature key security group key client key or other types of keys. Each of these keys can be stored on one or more of physical storage devices at the secure storage appliance or in the key manager .

Although illustrate a particular arrangement of a data storage system for secure storage of data additional arrangements are possible as well that can operate consistently with the concepts of the present disclosure. For example in certain embodiments the system can include a different number or type of storage systems or physical storage devices and can include one or more different types of client systems in place of or in addition to the application server . Furthermore the secure storage appliance can be placed in any of a number of different types of networks but does not require the presence of multiple types of networks as illustrated in the example of .

As illustrated in the example of the secure storage appliance comprises a primary interface and a secondary interface . The primary interface enables secure storage appliance to receive primary I O requests and to send primary I O responses. For instance the primary interface can enable secure storage appliance to receive primary I O requests e.g. read and write requests from the application server device and to send primary I O responses to the application server . Secondary interface enables the secure storage appliance to send secondary I O requests to the storage systems and to receive secondary I O responses from those storage systems .

In addition the secure storage appliance comprises a parser driver . The parser driver generally corresponds to the data conversion module of in that it processes primary I O requests to generate secondary I O requests and processes secondary I O responses to generate primary I O responses. To accomplish this the parser driver comprises a read module that processes primary read requests to generate secondary read requests and processes secondary read responses to generate primary read responses. In addition the parser driver comprises a decryption module that enables the read module to reconstruct a primary data block using secondary blocks contained in secondary read responses. Example operations performed by the read module are described below with reference to and . Furthermore the parser driver comprises a write module that processes primary write requests to generate secondary write requests and processes secondary write responses to generate primary write responses. The parser driver also comprises an encryption module that enables the write module to cryptographically split primary data blocks in primary write requests into secondary data blocks to put in secondary write requests. An example operation performed by the write module is described below as well with reference to .

In the example of the secure storage appliance also comprises a cache driver . When enabled the cache driver receives primary I O requests received by the primary interface before the primary I O requests are received by parser driver . When the cache driver receives a primary read request to read data at a primary storage location of a virtual disk the cache driver determines whether a write through cache at the secure storage appliance contains a primary write request to write a primary data block to the primary storage location of the virtual disk. If the cache driver determines that the write through cache contains a primary write request to write a primary data block to the primary storage location of the virtual disk the cache driver outputs a primary read response that contains the primary data block. When the parser driver receives a primary write request to write a primary data block to a primary storage location of a virtual disk the cache driver caches the primary write request in the write through cache . A write through module performs write operations to memory from the write through cache .

The secure storage appliance also includes an outstanding write list OWL module . When enabled the OWL module receives primary I O requests from the primary interface before the primary I O requests are received by the parser driver . The OWL module uses an outstanding write list to process the primary I O requests.

In addition the secure storage appliance comprises a backup module . The backup module performs an operation that backs up data at the storage systems to backup devices as described below in conjunction with .

The secure storage appliance also comprises a configuration change module . The configuration change module performs an operation that creates or destroys a volume and sets its redundancy configuration. Example redundancy configurations i.e. M of N configurations are described throughout the present disclosure and refer to the number of shares formed from a block of data and the number of those shares required to reconstitute the block of data. Further discussion is provided with respect to possible redundancy configurations below in conjunction with .

It should be appreciated that many alternate implementations of the secure storage appliance are possible. For example a first alternate implementation of the secure storage appliance can include the OWL module but not the cache driver or vice versa. In other examples the secure storage appliance might not include the backup module or the configuration change module . Furthermore there can be many alternate operations performed by the various modules of the secure storage appliance .

In the embodiment shown the secure storage appliance connects to the client device via both an IP network connection and a SAN network connection . The secure storage appliance connects to the administrative console by one or more IP connections as well. The key management server is also connected to the secure storage appliance by an IP network connection . The storage devices are connected to the secure storage appliance by the SAN network connection such as a Fibre Channel or other high bandwidth data connection. Finally in the embodiment shown secure storage appliances are connected via any of a number of types of communicative connections such as an IP or other connection for communicating heartbeat messages and status information for coordinating actions of the secure storage appliance and the secure storage appliance . Although in the embodiment shown these specific connections and systems are included the arrangement of devices connected to the secure storage appliance as well as the types and numbers of devices connected to the appliance may be different in other embodiments.

The secure storage appliance includes a number of software based components including a management service and a system management module . The management service and the system management module each connect to the administrative console or otherwise provide system management functionality for the secure storage appliance . The management service and system management module are generally used to set various settings in the secure storage appliance view logs stored on the appliance and configure other aspects of a network including the secure storage appliance . Additionally the management service connects to the key management server and can request and receive keys from the key management server as needed.

A cluster service provides synchronization of state information between the secure storage appliance and secure storage appliance . In certain embodiments the cluster service manages a heartbeat message and status information exchanged between the secure storage appliance and the secure storage appliance . Secure storage appliance and secure storage appliance periodically exchange heartbeat messages to ensure that secure storage appliance and secure storage appliance maintain contact. Secure storage appliance and secure storage appliance maintain contact to ensure that the state information received by each secure storage appliance indicating the state of the other secure storage appliance is up to date. An active directory services stores the status information and provides status information periodically to other secure storage appliances via the communicative connection .

Additional hardware and or software components provide datapath functionality to the secure storage appliance to allow receipt of data and storage of data at the storage devices . In the embodiment shown the secure storage appliance includes a SNMP connection module that enables secure storage appliance to communicate with client devices via the IP network connection as well as one or more high bandwidth data connection modules such as a Fibre Channel input module or SCSI input module for receiving data from the client device or storage devices . Analogous data output modules including a Fibre Channel connection module or SCSI connection module can connect to the storage devices or client device via the SAN network connection for output of data.

Additional functional systems within the secure storage appliance assist in datapath operations. A SCSI command module parses and forms commands to be sent out or received from the client device and storage devices . A multipath communications module provides a generalized communications interface for the secure storage appliance and a disk volume disk and cache provide local data storage for the secure storage appliance .

Additional functional components can be included in the secure storage appliance as well. In the embodiment shown a parser driver provides data splitting and encryption capabilities for the secure storage appliance as previously explained. A provider includes volume management information for creation and destruction of volumes. An events module generates and handles events based on observed occurrences at the secure storage appliance e.g. data errors or communications errors with other systems .

In each of the N secondary data blocks each represent a cryptographically split portion of the primary data block such that the functional block requires only M of the N secondary data blocks where M

Although in the embodiment shown in the parser driver uses the N secondary data blocks to reconstitute the primary data block it is understood that in certain applications fewer than all of the N secondary data blocks are required. For example when the parser driver generates N secondary data blocks during a write operation such that only M secondary data blocks are required to reconstitute the primary data block where M

For example during operation of the parser driver a data conversion routine may generate four secondary data blocks of which two are needed to reconstitute a primary data block i.e. M 2 N 4 . In such an instance two of the secondary data blocks may be stored locally and two of the secondary data blocks may be stored remotely to ensure that upon failure of a device or catastrophic event at one location the primary data block can be recovered by accessing one or both of the secondary data blocks stored remotely. Other arrangements are possible as well such as one in which four secondary data blocks are stored locally and all are required to reconstitute the primary data block i.e. M 4 N 4 . At its simplest a single share could be created M N 1 .

In the embodiment of the data storage system shown two secure storage appliances are shown. Each of the secure storage appliances can be connected to any of a number of clients e.g. the application server as well as secured storage systems the metadata store and a remote server . In various embodiments the remote server could be for example an enterprise directory and or a key manager .

The secure storage appliances are also typically connected to each other via a network connection. In the embodiment shown in the example of the secure storage appliances reside within a network . In various embodiments network can be for example an IP based network SAN as previously described in conjunction with or another type of network. In certain embodiments the network can include aspects of one or both types of networks. An example of a particular configuration of such a network is described below in conjunction with .

The secure storage appliances in the data storage system are connected to each other across a TCP IP portion of the network . This allows for the sharing of configuration data and the monitoring of state between the secure storage appliances . In certain embodiments there can be two IP based networks one for sharing of heartbeat information for resiliency and a second for configuration and administrative use. The secure storage appliance can also potentially be able to access the storage systems including remote storage systems across an IP network using a data interface.

In operation sharing of configuration data state data and heartbeat information between the secure storage appliances allows the secure storage appliances to monitor and determine whether other secure storage appliances are present within the data storage system . Each of the secure storage appliances can be assigned specific addresses of read operations and write operations to process. Secure storage appliances can reroute received I O commands to the appropriate one of the secure storage appliances assigned that operation based upon the availability of that secure storage appliance and the resources available to the appliance. Furthermore the secure storage appliances can avoid addressing a common storage device or application server port at the same time thereby avoiding conflicts. The secure storage appliances also avoid reading from and writing to the same share concurrently to prevent the possibility of reading stale data.

When one of the secure storage appliances fails a second secure storage appliance can determine the state of the failed secure storage appliance based upon tracked configuration data e.g. data tracked locally or stored at the remote server . The remaining operational one of the secure storage appliances can also access information in the metadata store including share and key information defining volumes virtual disks and client access rights to either process or reroute requests assigned to the failed device.

As previously described the data storage system is intended to be exemplary of a possible network in which aspects of the present disclosure can be implemented other arrangements are possible as well using different types of networks systems storage devices and other components.

Referring now to one possibility of a methodology of incorporating secure storage appliances into a data storage network such as a SAN is shown according to a possible embodiment of the present disclosure. In the embodiment shown a secure storage network provides for fully redundant storage in that each of the storage systems connected at a client side of the network is replicated in mass storage and each component of the network switches secure storage appliances is located in a redundant array of systems thereby providing a failsafe in case of component failure. In alternative embodiments the secure storage network can be simplified by including only a single switch and or single secure storage appliance thereby reducing the cost and complexity of the network while coincidentally reducing the protection from component failure .

In the embodiment shown an overall secure storage network includes a plurality of data lines interconnected by switches . Data lines connect to storage systems which connect to physical storage disks . The storage systems correspond generally to smaller scale storage servers such as an application server client device or other system as previously described. In the embodiment shown in the example of storage system connects to physical storage disks storage system connects to physical storage disks and storage system connects to physical storage disks . The secure storage network can be implemented in a number of different ways such as through use of Fibre Channel or iSCSI communications as the data lines ports and other data communications channels. Other high bandwidth communicative connections can be used as well.

The switches connect to a large scale storage system such as the mass storage via the data lines . The mass storage includes in the embodiment shown two data directors which respectively direct data storage and requests for data to one or more of the back end physical storage devices . In the embodiment shown the physical storage devices are unsecured i.e. not cryptographically split and encrypted while the physical storage device stores secure data i.e. password secured or other arrangement .

The secure storage appliances also connect to the data lines and each connect to the secure physical storage devices . Additionally the secure storage appliances connect to the physical storage devices which can reside at a remote storage location e.g. the location of the large scale storage system mass storage .

In certain embodiments providing redundant storage locations the secure storage network allows a user to configure the secure storage appliances such that using the M of N cryptographic splitting enabled in each of the secure storage appliances M shares of data can be stored on physical storage devices at a local location to provide fast retrieval of data while another M shares of data can be stored on remote physical storage devices at a remote location. Therefore failure of one or more physical disks or secure storage appliances does not render data unrecoverable because a sufficient number of shares of data remain accessible to at least one secure storage appliance capable of reconstituting requested data.

In the embodiment shown the data storage network includes two clusters . Each of the clusters includes a pair of secure storage appliances respectively. In the embodiment shown the clusters are labeled as clusters A and B respectively with each cluster including two secure storage appliances shown as appliances A and A in cluster and appliances B and B in cluster respectively . The secure storage appliances within each of the clusters are connected via a data network e.g. via switches or other data connections in an iSCSI Fibre Channel or other data network as described above and indicated via the nodes and connecting lines shown within the data network to a plurality of physical storage devices . Additionally the secure storage appliances are connected to client devices shown as client devices C C via the data storage network . The client devices can be any of a number of types of devices such as application servers database servers or other types of data storing and managing client devices.

In the embodiment shown the client devices are connected to the secure storage appliances such that each of client devices can send I O operations e.g. a read request or a write request to two or more of the secure storage appliances to ensure a backup datapath in case of a connection failure to one of secure storage appliances Likewise the secure storage appliances of each of clusters are both connected to a common set of physical storage devices . Although not shown in the example of the physical storage devices can be in certain embodiments managed by separate storage systems as described above. Such storage systems are removed from the illustration of the data storage network for simplicity but can be present in practice.

An administrative system connects to a maintenance console via a local area network . Maintenance console has access to a secured domain of an IP based network . The maintenance console uses the secured domain to access and configure the secure storage appliances . One method of configuring the secure storage appliances is described below in conjunction with .

The maintenance console is also connected to both the client devices and the physical storage devices via the IP based network . The maintenance console can determine the status of each of these devices to determine whether connectivity issues exist or whether the device itself has become non responsive.

Referring now to an example physical block structure of data written onto one or more physical storage devices is shown according to aspects of the present disclosure. The example of illustrates three strips A B and C collectively shares . Each of strips is a share of a physical storage device devoted to storing data associated with a common volume. For example in a system in which a write operation splits a primary data block into three secondary data blocks i.e. N 3 the strips shares would be appropriately used to store each of the secondary data blocks. As used in this disclosure a volume is grouped storage that is presented by a secure storage appliance to clients of secure storage appliance e.g. secure storage appliance or as previously described such that the storage appears as a contiguous unitary storage location. Secondary data blocks of a volume are distributed among strips . In systems implementing a different number of shares e.g. N 2 4 6 etc. a different corresponding number of shares would be used. As basic as a 1 of 1 configuration M 1 N 1 configuration could be used.

Each of the strips corresponds to a reserved portion of memory of a different one of physical storage devices e.g. physical storage devices previously described and relates to a particular I O operation from storage or reading of data to from the physical storage device. Typically each of the strips resides on a different one of physical storage devices. Furthermore although three different strips are shown in the illustrative embodiment shown more or fewer strips can be used as well. In certain embodiments each of the strips begins on a sector boundary. In other arrangements the each of the strips can begin at any other memory location convenient for management within the share.

Each of strips includes a share label a signature header information virtual disk information and data blocks . The share label is written on each of strips in plain text and identifies the volume and individual share. The share label can also in certain embodiments contain information describing other header information for the strips as well as the origin of the data written to the strip e.g. the originating cluster .

The signature contain information required to construct the volume and is encrypted by a workgroup key. The signatures contain information that can be used to identify the physical device upon which data i.e. the share is stored. The workgroup key corresponds to a key associated with a group of one or more users having a common set of usage rights with respect to data i.e. all users within the group can have access to common data. In various embodiments the workgroup key can be assigned to a corporate department using common data a common group of one or more users or some other community of interest for whom common access rights are desired.

The header information contains session keys used to encrypt and decrypt the volume information included in the virtual disk information described below. The header information is also encrypted by the workgroup key. In certain embodiments the header information includes headers per section of data. For example the header information may include one header for each 64 GB of data. In such embodiments it may be advantageous to include at least one empty header location to allow re keying of the data encrypted with a preexisting session key using a new session key.

The virtual disk information includes metadata that describes a virtual disk as it is presented by a secure storage appliance. The virtual disk information in certain embodiments includes names to present the virtual disk a volume security descriptor and security group information. The virtual disk information can be in certain embodiments encrypted by a session key associated with the physical storage device upon which the strips are stored respectively.

The secondary data blocks correspond to a series of memory locations used to contain the cryptographically split and encrypted data. Each of the secondary data blocks contains data created at a secure storage appliance followed by metadata created by the secure storage appliance as well. The N secondary data blocks created from a primary data block are combined to form a stripe of data. The metadata stored alongside each of the secondary data blocks contains an indicator of the header used for encrypting the data. In one example implementation each of the secondary data blocks includes metadata that specifies a number of times that the secondary data block has been written. A volume identifier and stripe location of an primary data block an be stored as well.

It is noted that although a session key is associated with a volume multiple session keys can be used per volume. For example a volume may include one session key per 64 GB block of data. In this example each 64 GB block of data contains an identifier of the session key to use in decrypting that 64 GB block of data. The session keys used to encrypt data in each strip can be of any of a number of forms. In certain embodiments the session keys use an AES 256 Counter with Bit Splitting. In other embodiments it may be possible to perform bit splitting without encryption. Therefore alongside each secondary data block an indicator of the session key used to encrypt the data block may be provided.

A variety of access request prioritization algorithms can be included for use with the volume to allow access of only quickest responding physical storage devices associated with the volume. Status information can be stored in association with a volume and or share as well with changes in status logged based on detection of event occurrences. The status log can be located in a reserved dedication portion of memory of a volume. Other arrangements are possible as well.

It is noted that based on the encryption of session keys with workgroup keys and the encryption of the secondary data blocks in each strip with session keys it is possible to effectively delete all of the data on a disk or volume i.e. render the data useless by deleting all workgroup keys that could decrypt a session key for that disk or volume.

Referring now to basic example flowcharts of setup and use of the networks and systems disclosed herein are described. Although these flowcharts are intended as example methods for administrative and I O operations such operations can include additional steps modules can be performed in a different order and can be associated with different number and operation of modules. In certain embodiments the various modules can be executed concurrently.

Operational flow is instantiated at a start operation which corresponds to initial introduction of a secure storage appliance into a network by an administrator or other individuals of such a network in a SAN NAS or other type of networked data storage environment. Operational flow proceeds to a client definition module that defines connections to client devices i.e. application servers or other front end servers clients or other devices from the secure storage appliance. For example the client definition module can correspond to mapping connections in a SAN or other network between a client such as application server and a secure storage appliance of .

Operational flow proceeds to a storage definition module . The storage definition module allows an administrator to define connections to storage systems and related physical storage devices. For example the storage definition module can correspond to discovering ports and routes to storage devices within the system of above.

Operational flow proceeds to a volume definition module . The volume definition module defines available volumes by grouping physical storage into logical arrangements for storage of shares of data. For example an administrator can create a volume and assign a number of attributes to that volume. A storage volume consists of multiple shares or segments of storage from the same or different locations. The administrator can determine a number of shares into which data is cryptographically split and the number of shares required to reconstitute that data. The administrator can then assign specific physical storage devices to the volume such that each of the N shares is stored on particular devices. The volume definition module can generate session keys for storing data on each of the physical storage devices and store that information in a key server and or on the physical storage devices. In certain embodiments the session keys generated in the volume definition module are stored both on a key server connected to the secure storage appliance and on the associated physical storage device e.g. after being encrypted with an appropriate workgroup key generated by the communities of interest module below . Optionally the volume definition module includes a capability of configuring preferences for which shares are first accessed upon receipt of a request to read data from those shares.

Operational flow proceeds to a communities of interest module . The communities of interest module corresponds to creation of one or more groups of individuals having interest in data to be stored on a particular volume. The communities of interest module further corresponds to assigning of access rights and visibility to volumes to one or more of those groups.

In creating the groups via the communities of interest module one or more workgroup keys may be created with each community of interest being associated with one or more workgroup keys. The workgroup keys are used to encrypt access information e.g. the session keys stored on volumes created during operation of the volume definition module related to shares to ensure that only individuals and devices from within the community of interest can view and access data associated with that group. Once the community of interest is created and associated with a volume client devices identified as part of the community of interest can be provided with a virtual disk which is presented to the client device as if it is a single unitary volume upon which files can be stored.

In use the virtual disks appear as physical disks to the client and support SCSI or other data storage commands. Each virtual disk is associated on a many to one basis with a volume thereby allowing multiple communities of interest to view common data on a volume e.g. by replicating the relevant session keys and encrypting those keys with relevant workgroup keys of the various communities of interest . A write command will cause the data to be encrypted and split among multiple shares of the volume before writing while a read command will cause the data to be retrieved from the shares combined and decrypted.

Operational flow terminates at end operation which corresponds to completion of the basic required setup tasks to allow usage of a secure data storage system.

Operational flow proceeds to an identity determination module which corresponds to a determination of the identity of the client from which the read request is received. The client s identity generally corresponds with a specific community of interest. This assumes that the client s identity for which the secure storage appliance will access a workgroup key associated with the virtual disk that is associated with the client.

Operational flow proceeds to a share determination module . The share determination module determines which shares correspond with a volume that is accessed by way of the virtual disk presented to the user and with which the read request is associated. The shares correspond to at least a minimum number of shares needed to reconstitute the primary data block i.e. at least M of the N shares . In operation a read module issues secondary read requests to the M shares and receives in return the secondary data blocks stored on the associated physical storage devices.

A success operation determines whether the read module successfully read the secondary data blocks. The success operation may detect for example that data has been corrupted or that a physical storage device holding one of the M requested shares has failed or other errors. If the read is successful operational flow branches yes to a reconstitute data module . The reconstitute data module decrypts a session key associated with each share with the workgroup key accessed by the identity determination module . The reconstitute data module provides the session key and the encrypted and cryptographically split data to a data processing system within the secure storage appliance which reconstitutes the requested data in the form of an unencrypted block of data physical disk locations in accordance with the principles described above in and . A provide data module sends the reconstituted block of data to the requesting client device. A metadata update module updates metadata associated with the shares including for example access information related to the shares. From the metadata update module operational flow proceeds to an end operation signifying completion of the read request.

If the success operation determines that not all of the M shares are successfully read operational flow proceeds to a supplemental read operation which determines whether an additional share exists from which to read data. If such a share exists e.g. MN operational flow proceeds to a fail module which returns a failed read response to the requesting client device. Operational flow proceeds to the update metadata module and end operation respectively signifying completion of the read request.

Optionally the fail module can correspond to a failover event in which a backup copy of the data e.g. a second N shares of data stored remotely from the first N shares are accessed. In such an instance once those shares are tested and failed a fail message is sent to a client device.

In certain embodiments commands and data blocks transmitted to the client device can be protected or encrypted such as by using a public private key or symmetric key encryption techniques or by isolating the data channel between the secure storage appliance and client. Other possibilities exist for protecting data passing between the client and secure storage appliance as well.

Furthermore although the system of illustrates a basic read operation it is understood that certain additional cases related to read errors communications errors or other anomalies may occur which can alter the flow of processing a read operation. For example additional considerations may apply regarding which M of the N shares to read from upon initially accessing physical storage disks . Similar considerations apply with respect to subsequent secondary read requests to the physical storage devices in case those read requests fail as well.

In the example systems and methods disclosed operational flow is instantiated at a start operation . Operational flow proceeds to a write request receipt module which corresponds to receiving a primary write request from a client device e.g. an application server as shown in at a secure storage appliance. The primary write request generally addresses a virtual disk and includes a block of data to be written to the virtual disk.

Operational flow proceeds to an identity determination module which determines the identity of the client device from which the primary write request is received. After determining the identity of the client device the identity determination module accesses a workgroup key based upon the identity of the client device and accesses the virtual disk at which the primary write request is targeted. Operational flow proceeds to a share determination module which determines the number of secondary data blocks that will be created and the specific physical disks on which those shares will be stored. The share determination module obtains the session keys for each of the shares that are encrypted with the workgroup key obtained in the identity determination module e.g. locally from a key manager or from the physical disks themselves . These session keys for each share are decrypted using the workgroup key.

Operational flow proceeds to a data processing module which provides to the parser driver the share information session keys and the primary data block. The parser driver operates to cryptographically split and encrypt the primary data block thereby generating N secondary data blocks to be written to N shares in accordance with the principles described above in the examples of and . Operational flow proceeds to a secondary write module which transmits the share information to the physical storage devices for storage.

Operational flow proceeds to a metadata storage module which updates a metadata repository by logging the data written allowing the secure storage appliance to track the physical disks upon which data has been written and with what session and workgroup keys the data can be accessed. Operational flow terminates at an end operation which signifies completion of the write request.

As previously mentioned in certain instances additional operations can be included in the system for writing data using the secure storage appliance. For example confirmation messages can be returned to the secure storage appliance confirming successful storage of data on the physical disks. Other operations are possible as well.

Now referring to of the present disclosure certain applications of the present disclosure are discussed in the context of 1 data backup systems and 2 secure network thin client network topology used in the business setting. shows an example system for providing secure storage data backup according to a possible embodiment of the present disclosure. In the system shown a virtual tape server is connected to a secure storage appliance via a data path such as a SAN network using Fibre Channel or iSCSI communications. The virtual tape server includes a management system a backup subsystem interface and a physical tape interface . The management system provides an administrative interface for performing backup operations. The backup subsystem interface receives data to be backed up onto tape and logs backup operations. A physical tape interface queues and coordinates transmission of data to be backed up to the secure storage appliance via the network. The virtual tape server is also connected to a virtual tape management database that stores data regarding historical tape backup operations performed using the system .

The secure storage appliance provides a virtual tape head assembly which is analogous to a virtual disk but appears to the virtual tape server to be a tape head assembly to be addressed and written to. The secure storage appliance connects to a plurality of tape head devices capable of writing to magnetic tape such as that typically used for data backup. The secure storage appliance is configured as described above. The virtual tape head assembly provides an interface to address data to be backed up which is then cryptographically split and encrypted by the secure storage appliance and stored onto a plurality of distributed magnetic tapes using the tape head devices as opposed to a generalized physical storage device such as the storage devices of .

In use a network administrator could allocate virtual disks that would be presented to the virtual tape head assembly . The virtual tape administrator would allocate these disks for storage of data received from the client through the virtual tape server . As data is written to the disks it would be cryptographically split and encrypted via the secure storage appliance .

The virtual tape administrator would present virtual tapes to a network e.g. an IP or data network from the virtual tape server . The data in storage on the tape head devices is saved by the backup functions provided by the secure storage appliance . These tapes are mapped to the virtual tapes presented by the virtual tape head assembly . Information is saved on tapes as a collection of shares as previously described.

An example of a tape backup configuration illustrates certain advantages of a virtual tape server over the standard tape backup system as described above in conjunction with . In one example of a tape backup configuration share of virtual disk A share of virtual disk B and other share s can be saved to a tape using the tape head devices . Second shares of each of these virtual disks could be stored to a different tape. Keeping the shares of a virtual tape separate preserves the security of the information by distributing that information across multiple tapes. This is because more than one tape is required to reconstitute data in the case of a data restoration. Data for a volume is restored by restoring the appropriate shares from the respective tapes. In certain embodiments an interface that can automatically restore the shares for a volume can be provided for the virtual tape assembly. Other advantages exist as well.

Now referring to one possible arrangement of a thin client network topology is shown in which secure storage is provided. In the network illustrated a plurality of thin client devices are connected to a consolidated application server via a secured network connection .

The consolidated application server provides application and data hosting capabilities for the thin client devices . In addition the consolidated application server can as in the example embodiment shown provide specific subsets of data functionality and connectivity for different groups of individuals within an organization. In the example embodiment shown the consolidated application server can connect to separate networks and can include separate dedicated network connections for payroll human resources and finance departments. Other departments could have separate dedicated communication resources data and applications as well. The consolidated application server also includes virtualization technology which is configured to assist in managing separation of the various departments data and application accessibility.

The secured network connection is shown as a secure Ethernet connection using network interface cards to provide network connectivity at the server . However any of a number of secure data networks could be implemented as well.

The consolidated application server is connected to a secure storage appliance via a plurality of host bus adapter connections . The secure storage appliance is generally arranged as previously described in . The host bus adapter connections allow connection via a SAN or other data network such that each of the dedicated groups on the consolidated application server has a dedicated data connection to the secure storage appliance and separately maps to different port logical unit numbers LUNs . The secure storage appliance then maps to a plurality of physical storage devices that are either directly connected to the secure storage appliance or connected to the secure storage appliance via a SAN or other data network.

In the embodiment shown the consolidated application server hosts a plurality of guest operating systems shown as operating systems . The guest operating systems host user group specific applications and data for each of the groups of individuals accessing the consolidated application server. Each of the guest operating systems have virtual LUNs and virtual NIC addresses mapped to the LUNs and NIC addresses within the server while virtualization technology provides a register of the mappings of LUNS and NIC addresses of the server to the virtual LUNs and virtual NIC addresses of the guest operating systems . Through this arrangement dedicated guest operating systems can be mapped to dedicated LUN and NIC addresses while having data that is isolated from that of other groups but shared across common physical storage devices .

As illustrated in the example of the physical storage devices provide a typical logistical arrangement of storage in which a few storage devices are local to the secure storage appliance while a few of the other storage devices are remote from the secure storage appliance . Through use of 1 virtual disks that are presented to the various departments accessing the consolidated application server and 2 shares of virtual disks assigned to local and remote storage each department can have its own data securely stored across a plurality of locations with minimal hardware redundancy and improved security.

Although present a few options for applications of the secure storage appliance and secure network storage of data as described in the present disclosure it is understood that further applications are possible as well. Furthermore although each of these applications is described in conjunction with a particular network topology it is understood that a variety of network topologies could be implemented to provide similar functionality in a manner consistent with the principles described herein.

Operation begins when write module receives a primary write request that specifies a primary data block to write to a primary storage location at a virtual disk . In one example implementation the primary storage location may be a range of disk sector addresses. The disk sector addresses specified by the primary storage location may be virtual disk sector addresses in the sense that storage devices may not actually have disk sectors associated with the disk sector addresses but application server device may output primary read requests and primary write requests as though disk sectors associated with the disk sector addresses actually exist.

Write module then updates a write counter associated with the primary storage location at the virtual disk . The write counter associated with the primary storage location may be a variety of different types of data. In a first example the write counter associated with the primary storage location may be an integer. In this first example write module may update the write counter associated with the primary storage location by incrementing the write counter. In a second example the write counter associated with the primary storage location may be an alphanumeric string. In this example write module may update the write counter associated with the primary storage location by shifting characters in the alphanumeric string.

Next encryption module cryptographically splits the primary data block into a plurality of secondary data blocks . As explained above encryption module may cryptographically split the primary data block into the plurality of secondary data blocks in a variety of ways. For example encryption module may cryptographically split the primary data block into the plurality of secondary data blocks using the SECUREPARSER algorithm developed by SecurityFirst Corp. of Rancho Santa Margarita Calif.

After encryption module cryptographically splits the primary data block into the plurality of secondary data blocks write module attaches the updated write counter to each of the secondary data blocks . Write module may attach the updated write counter to each of the secondary data blocks in a variety of ways. For example write module may append the updated write counter to the ends of each of the secondary data blocks append the updated write counter to the beginnings of each of the secondary data blocks or insert the updated write counter at some location in the middle of the secondary data blocks.

As described above the storage locations of a storage device are divided into shares. Each share is reserved for data associated with a volume. In other words a volume has a share of the storage locations of a storage device. Each volume has shares of each of storage devices . For example storage locations through of storage device A may be reserved for data associated with a first volume and storage locations through of storage device A may be reserved for data associated with a second volume. Furthermore in this example storage locations through of storage device B may be reserved for data associated with the first volume and storage locations through of storage device B may be reserved for data associated with the second volume.

After attaching the updated write counter to the secondary data blocks write module identifies a set of secondary storage locations the set of secondary storage locations containing a secondary storage location for each of the secondary data blocks . In one example implementation secure storage appliance stores a volume map that contains entries that map virtual disks to volumes. In addition secure storage appliance stores a different primary storage map for each volume. A primary storage map for a volume contains entries that map primary storage locations to intermediate storage locations. An intermediate storage location is a primary storage location relative to a volume. For example primary storage location of a first virtual disk may map to intermediate storage location of a volume and primary storage location of a second virtual disk may map to intermediate storage location . In addition secure storage appliance stores a different secondary storage map for each volume. A secondary storage map for a volume contains entries that map intermediate storage locations to secondary storage locations within the volume s shares of storage devices . For example secondary storage locations through of storage device A may be reserved for data associated with the volume secondary storage locations through of storage device B may be reserved for data associated with the volume and secondary storage locations through of storage device C may be reserved for data associated with the volume. In this example the secondary storage map may contain an entry that maps intermediate storage location to secondary location of storage device A secondary storage location of storage device B and secondary storage location of storage device C. In this example implementation write module identifies the secondary storage locations for each of the secondary data blocks by first using the volume map to identify a primary associated with the virtual disk specified by the primary write request. Write module then uses the volume storage map of the identified volume to identify an intermediate storage location for the primary storage location. Next write module then uses the secondary storage map to identify the set of secondary storage locations associated with the intermediate storage location.

In a second example implementation secure storage appliance stores a map that contains entries that directly map primary storage locations of virtual disks to sets of secondary storage locations of storage devices . In a third example implementation secure storage appliance uses arithmetic formulas to identify sets of secondary storage locations for virtual storage locations of virtual disks.

After write module identifies the secondary storage locations for each of the secondary data blocks write module generates a set of secondary write requests . Each of the secondary write requests generated by write module instructs one of storage devices to store one of the secondary data blocks at one of the identified secondary storage locations. For example a first one of the secondary write requests instructs storage device A to store a first one of the secondary data blocks at a first one of the identified secondary storage locations a second one of the secondary write requests instructs storage device B to store a second one of the secondary data blocks at a second one of the identified secondary storage locations and so on. Next write module sends via secondary interface secondary write requests to a plurality of storage devices . In one example implementation write module sends the secondary write requests concurrently. In other words write module may send one or more of the secondary write requests before another one of the secondary write requests finishes.

Write module then determines whether all of the secondary write requests were successful . Write module may determine that one of the secondary write requests was not successfully completed when write module received a response that indicates that one of storage devices did not successfully complete the secondary write request. In addition write module may determine that one of the secondary write requests was not successfully completed when write module did not receive a response from one of storage devices within a timeout period. Furthermore write module may determine that a secondary write request sent to a storage device was successful when write module receives a secondary write response from the storage device indicating that secondary write request was completed successfully.

If one or more of the secondary write requests were not successful NO of write module resends the one or more secondary write requests that were not successful . Subsequently write module may again determine whether all of the secondary write requests were successful and so on.

If write module determines that all of the secondary write requests were successful YES of write module may send via primary interface a primary write response that indicates that the primary write request was completed successfully .

Operation begins when read module in secure storage appliance receives a primary read request that specifies a primary storage location at a virtual disk . When secure storage appliance receives the primary read request read module identifies secondary storage locations associated with the primary storage location of the virtual disk . Read module may identify the secondary storage locations associated with the primary storage location of the virtual disk using a volume map an intermediate storage location map and a secondary location map as described above with regard to .

After read module identifies the secondary storage locations read module generates a set of secondary read requests . Each of the secondary read requests is a request to retrieve a data block stored at one of the identified secondary storage locations. After generating the secondary read requests read module sends the secondary read requests to ones of storage devices . As described in detail below with reference to read module may send secondary read requests to selected ones of storage devices . Read module may send the secondary read requests concurrently. In other words read module may send one or more of the secondary read requests before one or more other ones of the secondary read requests have completed.

Subsequently read module receives from storage devices secondary read responses that are responsive to the secondary read requests . Each of the secondary read responses contains a secondary data block.

After read module receives the secondary read responses read module determines whether all of the write counters attached to each of the secondary data blocks are equivalent . In one example implementation the write counters may be equivalent when the write counters are mathematically equal. In another example the write counters may be equivalent when the write counters are multiples of a common number.

If read module determines that all of the write counters are equivalent YES of decryption module reconstructs the primary data block using any minimal set of the secondary data blocks contained in the secondary read responses . The minimal set of the secondary data blocks includes at least the minimum number of secondary data blocks required to reconstruct the primary data block. Furthermore each of the secondary data blocks in the minimal set of secondary data blocks must have an equivalent write counter. In addition the write counters of the secondary data blocks in the minimal set of the secondary data blocks must be greater than the write counters of any other set of the secondary data blocks that has the minimum number of secondary data blocks whose write counters are equivalent. For example if only three secondary data blocks are required to reconstruct the primary data block and read module received five secondary read responses decryption module may use any three of the five secondary data blocks in the secondary read responses to reconstruct the primary data block.

On the other hand if read module determines that one of the write counters is not equivalent to another one of the write counters NO of read module determines whether the secondary read responses include a minimal set of secondary data blocks . If the secondary read responses do not include a minimal set of secondary data blocks NO of read module may output a primary read response that indicates that the primary read response failed . In one example implementation read module may not have sent secondary read requests to all of the data storage devices that store secondary data blocks associated with the primary data block. In this example implementation when the secondary read responses do not include a minimal set of secondary data blocks NO of read module may output secondary read requests to ones of the data storage devices that read module did not previously send secondary request requests to. Furthermore in this example implementation read module may loop back and again determine whether the received secondary read responses include a minimal set of secondary data blocks.

On the other hand if the secondary read responses include a minimal set of secondary data blocks YES read module reconstructs the primary data block using the secondary data blocks in the minimal set of secondary data blocks .

After read module reconstructs the primary data block read module sends to the device that sent the primary read request a primary read response that contains the primary data block . For example if application server device sent the primary read request read module sends to application server device a primary read response that contains the primary data block.

Initially read module receives a primary read request for data stored at a primary storage location . After receiving the primary read request read module identifies a minimum number of secondary data blocks M required to reconstruct the primary data block. . As used in this disclosure the letter M is used to designate the minimum number of secondary storage blocks required to reconstruct a primary data block. Each volume may have a different value for M. In one example implementation read module may identify the value of M for a volume by accessing a configuration table that contains an entry that indicates the value of M for the volume. For example read module may determine that the value of M for a particular volume is three meaning that a minimum of three secondary data blocks are required to reconstruct the primary data block of the volume.

Next read module identifies the M fastest responding ones of storage devices . The set of fastest responding storage devices are the storage devices that are expected to respond fastest to requests sent by secure storage appliance to the storage devices. Read module may identify the fastest responding storage devices in a variety of ways. In a first example read module calculates expected response time statistics for each of storage devices . For instance read module may calculate an expected response time statistic that indicates that the average time it takes for storage device A to respond to a read request sent from secure storage appliance is 0.5 seconds and may calculate an expected response time statistic that indicates that the average time it takes for storage device B to respond to a read request sent from secure storage appliance is 0.8 seconds. In this first example read module uses the expected response time statistics to identify the M fastest responding storage devices. Read module may acquire the expected response time statistics by periodically sending messages to storage devices and determining how long each of storage devices take to respond to the messages. In one example implementation the expected response time statistic for one of storage devices is the average of the times it took the storage device to respond to the most recent fifteen messages.

In a second example read module calculates expected response time statistics for each of storage devices as described in the first example. However in this second example read module also tracks the current busyness of each storage devices . In this second example read module accounts for the current busyness of each of storage devices when identifying the M fastest responding storage devices. For instance if the expected response time statistics indicate that storage device A has the fastest average response time but storage device A is currently very busy read module might not include storage device A among the M fastest responding storage devices. To implement this read module may maintain a running count of the number of I O requests outstanding to each of storage devices . In this example it is assumed that any current I O request is about halfway complete. Consequently the expected response time of one of storage devices is equal to N 0.5 R where N is the number of I O requests outstanding for the storage device and R is the average response time for the storage device.

Ones of storage devices may have different response times for a variety of reasons. For example a first subset of storage devices may be physically located at a first data center and a second subset of the storage devices may be physically located at a second data center. In this example the first data center and the second data center are geographically separated from one another. For instance the first data center may be located in Asia and the second data center may be located in Europe. In this example both the first data center and the second data center may store at least a minimum number of the shares of each volume to reconstruct the data of each volume. Separating data centers in this manner may be useful to prevent data loss in the event a catastrophe occurs at one of the geographic locations. In another instance both the first data center and the second data center store fewer than the minimum number of shares of each volume to reconstruct the data of each volume. In this instance distributing the shares in this manner may protect the data of the volumes in the event that all data at one of the data centers is compromised.

After read module identifies the M fastest responding storage devices read module generates a set of secondary read requests . The set of secondary read requests includes one read request for each of the M fastest responding storage devices. Each of the secondary read requests specifies a secondary storage location associated with at the primary storage location specified by the primary read request.

After generating the secondary storage requests read module exclusively sends secondary read requests to the identified storage devices . In other words read module does not send secondary read requests to ones of storage devices that are not among the M fastest responding storage devices. Read module may send the secondary read requests concurrently.

Subsequently read module determines whether all of the secondary read requests were successful . Secondary read requests might not be successful for a variety of reasons. For example a secondary read request might not be successful when one of storage devices does not respond to one of the secondary read requests. In another example a secondary read request might not be successful when one of storage devices sends to secure storage appliance a secondary read response that indicates that the storage device is unable to read the data requested by one of the secondary read requests.

If read module determines that one or more of the secondary read requests have not been successful NO of read module may send a new secondary read request to a next fastest responding storage device . For example suppose M 2 storage devices includes four storage devices and the expected response time for the four storage devices are 0.4 seconds 0.5 seconds 0.6 seconds and 0.7 seconds respectively. In this example read module would have sent secondary read requests to the first storage device and the second storage device. However because there has been an error reading from either the first storage device or the second storage device read module sends a secondary read request to the third storage device. Alternatively if read module determines that one or more of the secondary read requests have not been successful read module may send new secondary read requests to each storage device that stores a secondary data block associated with the primary data block but was not among the identified fastest responding storage devices. After sending the secondary read request to the next fastest responding storage device read module may determine again whether all of the secondary read requests have been successful .

If read module determines that all of the secondary write requests were successful YES of read module uses the secondary data blocks in the secondary read responses to reconstruct the primary data block stored virtually at the primary storage location specified by the primary read request . After reconstructing the primary data block read module sends a primary read response containing the primary data block to the sender of the primary read request .

Initially configuration change module receives a request to change the redundancy configuration of a volume . The redundancy configuration of a volume is described in terms of a two numbers M and N. As described above the number M designates the minimum number of secondary storage blocks required to reconstruct a primary data block. The number N designates the number of secondary data blocks generated for each primary data block. In one example implementation configuration change module may receive the configuration change request via primary interface . In another example implementation configuration change module may receive the configuration change request via an administrative interface.

The configuration change request instructs secure storage appliance to change the redundancy configuration of data stored in storage devices . For example a volume may currently be using a redundancy configuration where M 3 and N 5 i.e. a 3 5 redundancy configuration . A 3 5 redundancy configuration is a redundancy configuration in which five secondary data blocks are written to different ones of storage devices for a primary data block and in which a minimum of three secondary data blocks are required to completely reconstruct the primary data block. In this example the request to change the redundancy configuration of the volume may instruct secure storage appliance to start implementing a 4 8 redundancy configuration for the volume. A 4 8 redundancy configuration is a redundancy configuration in which eight secondary data blocks are written to different ones of storage devices for a primary data block and in which a minimum of four secondary data blocks are required to completely reconstruct the primary data block.

After receiving the request to change the redundancy configuration of the volume configuration change module determines whether all stripes in the source version of the volume have been processed . As explained above a stripe is a set of secondary data blocks that can be used to reconstruct a primary data block. A volume contains one stripe for each primary data block of the volume. If fewer than all of the stripes in the source version of the volume have been processed NO of configuration change module selects one of the unprocessed stripes in the source version of the volume . Configuration change module may select one of the unprocessed stripes in the source version of the volume in a variety of ways. For example configuration change module may select one of the unprocessed stripes in the source version of the volume randomly from the unprocessed stripes in the source version of the volume.

Configuration change module then sends secondary read requests for secondary data blocks in the selected stripe . In one example implementation configuration change module exclusively sends secondary read requests to the M fastest responding storage devices that store secondary data blocks of the volume. Read module may send the secondary read requests concurrently.

After sending secondary read requests for secondary data blocks in the selected stripe configuration change module may receive at least a minimal set of secondary data blocks in the selected stripe . For example if the redundancy configuration of the source version of the volume is a 3 5 redundancy configuration configuration change module may receive three of the five secondary data blocks of the selected stripe.

When configuration change module receives at least a minimal set of secondary data blocks in the selected stripe configuration change module uses decryption module to reconstruct the primary data block of the selected stripe using the received secondary data blocks in the selected stripe .

After using decryption module to reconstruct the primary data block of the selected stripe configuration change module uses encryption module to generate secondary data blocks for the primary data block using the new redundancy configuration . For example if the new redundancy scheme is a 4 8 redundancy configuration encryption module generates eight secondary data blocks.

Next configuration change module generates a set of secondary write requests to write the new secondary data blocks to secondary storage locations of the destination version of the volume at the destination storage devices . Configuration change module then sends the secondary write requests to appropriate ones of storage devices .

After sending the secondary write requests configuration change module updates stripe metadata to indicate that the selected stripe has been processed . Configuration change module then loops back and again determines whether all stripes in the source version of the volume have been processed and so on.

If all of the stripes in the source version of the volume have been processed YES of configuration change module outputs an indication that the configuration change process is complete .

As a result of processing all of the stripes in the source version of the volume the source version of the volume and the destination version of the volume are synchronized. In other words the source version of the volume and the destination version of the volume contain data representing the same primary data blocks. In one example implementation an administrator is able to configure configuration change module to maintain the synchronization of the source version of the volume and the destination version of the volume until the administrator chooses to break the synchronization of the source version of the volume and the destination version of the volume. To maintain the synchronization of the source version of the volume and the destination version of the volume configuration change module may use encryption module to cryptographically split primary data blocks in incoming primary write requests into sets of secondary data blocks in both redundancy configurations and send secondary write requests to write the secondary data blocks in the original redundancy configuration and secondary write requests to write secondary data blocks in the new redundancy configuration.

As discussed above secure storage appliance may provide a plurality of volumes. Each volume is a separate logical disk. Because each volume is a separate logical disk application server device may treat each volume like a separate disk. For example application server device may send to secure storage appliance a primary read request to read a set of data at blocks to of a first volume and may send to secure storage appliance a primary request to read a set of data at blocks to of a second volume. While each volume is a separate logical disk data in each of the volumes may actually be stored at storage devices . For instance data in a first volume and data in a second volume may actually be stored at storage device A.

Initially write module initializes a queue in write through cache for each volume provided by secure storage appliance . Each of the volumes has a status of either clean or dirty. A volume has a status of clean when the volume s queue does not contain references to any outstanding secondary write requests to the volume. A volume has a status of dirty when the volume s queue contains one or more references to outstanding secondary write requests to the volume. The status of a volume is written to each of the storage devices that stores data associated with the volume. In this way the status of a volume on a storage device indicates to an administrator whether the storage device stores up to date data of the volume.

Subsequently cache driver receives an incoming primary I O request for a primary storage location at a virtual disk associated with one of the volumes . Cache driver may receive the incoming primary I O request before parser driver receives the incoming primary I O request. Upon receiving the incoming primary I O request cache driver determines whether the incoming primary I O request is a primary read request or a primary write request .

If the incoming primary I O request is an incoming primary read request YES of cache driver determines whether write through cache contains a primary write request to write a primary data block to a primary storage location that is also specified by the incoming primary read request . For example if write through cache contains a primary write request to write a primary data block to primary storage location and the incoming primary read request is to read data at primary storage location cache driver may determine that the write through cache contains a primary write request to write a primary data block to a primary storage location that is also specified by the incoming primary read request.

If cache driver determines that write through cache contains a primary write request to write a primary data block to a primary storage location that is also specified by the incoming primary read request YES of cache driver returns a primary read response that contains the primary data block in the primary write request in write through cache . On the other hand if cache driver determines that write through cache does not contain a primary write request to write a primary data block to a primary storage location that is also specified by the incoming primary read request NO of cache driver provides the incoming primary read request to read module so that read module may take steps to retrieve the primary data block at the primary storage location specified by the incoming primary read request .

If the incoming primary I O request is an incoming primary write request NO of cache driver determines whether write through cache contains a primary write request to write a primary data block to a primary storage location that is also specified by the primary write request . If cache driver determines that write through cache contains a primary write request to write a primary data block to a primary storage location that is also specified by the incoming primary write request YES of cache driver updates the primary write request in write through cache such that the primary write request specifies the primary data block specified by the incoming primary write request . Otherwise if cache driver determines that write through cache does not contain a primary write request to write a primary data block to a primary storage location that is also specified by the incoming primary write request NO of cache driver adds the incoming primary write request to write through cache .

After cache driver either updates the primary write request in write through cache or adds the primary write request to write through cache cache driver determines whether the volume s queue contains a reference to the primary write request . If cache driver determines that the volume s queue contains a reference to the primary write request YES of cache driver does not need to perform any further action with regard to the primary write request .

If cache driver determines that the volume s queue does not contain a reference to the primary write request NO of cache driver adds a reference to the primary write request . The reference to the primary write request may indicate a location of the primary write request in write through cache . After adding the reference to the volume s queue cache driver then sends an event notification to write through module . An event notification is a notification that an event has occurred. In this context the event is the updating of the primary write request in write through cache .

Cache driver then marks the volume associated with the incoming primary write request as dirty . In one example implementation when cache driver marks the volume as dirty cache driver may output secondary write requests to each of storage devices that has a share devoted to storing data associated with the volume. In this example implementation each of the secondary write requests instructs the storage devices to store metadata that indicates that the volume is dirty.

Initially write through module receives an event notification from cache driver . Prior to receiving the event notification write through module may be in a suspended state to conserve processing resources of secure storage appliance .

In response to receiving the event notification write through module selects a volume . In some example implementations write through module selects the volume on a random basis. In other example implementations write through module selects the volume on a deterministic basis. After write through module selects the volume write through module determines whether there are one or more references to primary write requests in a queue in write through cache associated with the selected volume . If there are no references to primary write requests in the queue in write through cache associated with the selected volume NO of write through module may loop back and again select a volume .

On the other hand if there are one or more references to primary write requests in the queue in write through cache associated with the selected volume YES of write through module selects one of the references to primary write requests in the queue in write through cache associated with the selected volume . In some example implementations write through module selects the reference on a random basis. In other example implementations write through module selects the reference on a deterministic basis. For instance write through module may select the reference to an oldest primary write request in the selected volume s queue in write through cache .

Write through module then provides the primary write request indicated by the selected reference i.e. the indicated primary write request to write module . When write module receives the indicated primary write request write module performs an operation to execute the indicated primary write request. For example write module may perform the example operation illustrated in to execute the indicated primary write request. In another example write module may perform the example operation illustrated in to execute the indicated primary write request.

After write through module provides to write module the indicated primary write request write through module receives a primary write response from write module . Write through module then determines whether the primary write response indicates that the indicated primary write request was successfully executed . For example the primary write response may indicate that the indicated primary write request was not successful when write module did not receive a secondary write response from a storage device within a timeout period.

If write through module determines that the primary write response indicates that the indicated primary write request was not performed successfully NO of write through module determines whether all queues in write through cache are empty . If all queues in write through cache are empty YES of write through module waits until another event notification is received . If all queues in write through cache are not empty NO of write through module selects one of the volumes and so on.

If write through module determines that the primary write response indicates that the indicated primary write request was performed successfully YES of write through module removes the selected reference from the selected volume s queue in write through cache . In one example implementation the indicated primary write request is not removed from write through cache until the indicated primary write request becomes outdated or is replaced by more recent primary write requests. After removing the selected reference write through module determines whether there are any remaining references in the selected volume s queue in write back cache . If there are remaining references in the selected volume s queue in write back cache YES of write through module determines whether all queues in write through cache are empty as discussed above . If there are no remaining references in the selected volume s queue in write through cache NO if write through module marks the status of the selected volume as clean . In one example implementation to mark the status of the selected volume as clean write through module may output secondary write requests to each of storage devices that has a share devoted to storing data associated with the volume. In this example implementation each of the secondary write requests instructs the storage devices to store metadata that indicates that the volume is clean. Furthermore in some example implementations write through module marks the status of the queue as clean only after waiting a particular period of time after removing the selected primary write request from the selected volume s queue. Waiting this period of time may prevent the selected volume from thrashing between the clean status and the dirty status. After marking the status of the queue as clean write through module may determine whether all of the queues in write through cache are empty as described above .

Initially OWL module receives a primary write request to write a primary data block to a primary storage location of a volume . After OWL module receives the primary write request OWL module determines whether the primary write request can be completed at the current time . There may be a variety of circumstances in which a primary write request cannot be completed. For example OWL module may be unable to complete a primary write request when one or more of storage devices are not currently available. In a second example the selected primary write request to write a secondary data block to a secondary storage location at storage device A cannot be completed at the current time because a backup operation is currently occurring at one or more of storage devices .

If OWL module determines that the primary write request can be completed at the current time YES of OWL module provides the primary write request to write module . When write module receives the primary write request write module performs an operation to securely write the primary write request. For instance write module may use operation in or another operation to securely write the primary write request.

Subsequently OWL module determines whether the primary write request was successful . If the OWL module determines that the primary write request was successful YES of the OWL module outputs a primary write response indicating that the primary write request was successful .

On the other hand if the OWL module determines that the primary write request was not successful NO of or if the primary write request cannot be completed at the current time NO of OWL module writes the primary write request to outstanding write list . Outstanding write list is a secure storage medium at secure storage appliance . All data in outstanding write list may be encrypted such that it would be very difficult to access the data in outstanding write list without an appropriate decryption key.

Outstanding write list may be implemented in a variety of ways. For example outstanding write list may be implemented as a set of linked lists. In this example each of the linked lists is associated with a different volume provided by secure storage appliance . Each of the linked lists comprises an ordered set of elements. Each of the elements contains a primary write request. For instance the linked list associated with a first volume may comprise four elements each of which contain one primary write request. In this example OWL module may write the selected secondary write request to outstanding write list by adding an element to a linked list associated with a volume specified by the primary write request.

After OWL module writes the primary write request to outstanding write list OWL module marks the primary storage location specified by the primary write request as locked . After marking the primary storage location specified by the primary write request as locked write module outputs a primary write response that indicates that the primary write request was completed successfully .

Initially OWL module determines whether outstanding write list is empty . In other words OWL module determines whether outstanding write list contains any outstanding primary write requests. If OWL module determines that outstanding write list is empty YES of OWL module may wait a period of time . After waiting OWL module may again determine whether outstanding write list is empty .

If OWL module determines that outstanding write list is not empty NO of OWL module selects one of the primary write requests in outstanding write list . In some example implementations OWL module may select the secondary write request on a random or a deterministic basis.

After selecting the primary write request OWL module provides the selected primary write request to write module . When write module receives the primary write request write module performs an operation to securely write the primary write request. For instance write module may use operation in or another operation to securely write the primary write request.

Subsequently OWL module determines whether the primary write request was completed successfully . If the primary write request was not completed successfully NO of OWL module may loop back and again determine whether the outstanding write list is empty .

As explained above with reference to write module locked the primary storage location specified by the selected primary write request when OWL module added the selected primary write request to outstanding write list . As explained below with reference to when OWL module receives a primary read request to read data at the primary storage location when the primary storage location is locked read module uses the primary read request in outstanding write list to respond to the primary read request.

Hence when OWL module determines that the primary write request was completed successfully YES of OWL module removes the lock on the primary storage location specified by the selected primary write request . After removing the lock on the primary storage location specified by the selected primary write request OWL module removes the primary write request from outstanding write list . Removing the selected primary write request from outstanding write list may free up data storage space in outstanding write list . OWL module then loops back and again determines whether the outstanding write list is empty .

Initially OWL module receives a primary read request . The primary read request comprises an instruction to retrieve data stored in a volume at a primary storage location. After receiving the primary read request OWL module determines whether there is a lock on the primary storage location .

If OWL module determines that there is no lock on the primary storage location NO of OWL module provides the primary read request to read module . When read module receives the primary read request read module performs an operation to read data of the volume at primary storage location. For instance read module may perform the example operation illustrated in the example operation illustrated in or another operation. After providing the primary read request to read module OWL module receives a primary read response from the read module . OWL module may then send the primary read response to a sender of the primary read request .

On the other hand if OWL module determines that there is a lock on the primary storage location YES of OWL module identifies in outstanding write list a primary write request that comprises an instruction to write primary data block to the primary storage location . After identifying the primary write request OWL module sends to the sender of the primary read request a primary read response that contains the primary data block . In this way read module uses the primary data block stored in outstanding write list to respond to the primary read request.

Initially backup module receives a request to perform a backup operation that backs up data stored at storage devices to a set of backup devices . Backup module may receive the request to perform the backup operation in a variety of ways. In a first example backup module may receive the request to perform the backup operation as an invocation of a function by a process operating on secure storage application or another device. In a second example backup module may receive the request to perform the backup operation via an administrative interface of secure storage appliance . In a third example backup module may receive the request from application server device . In the example of the set of backup devices includes one backup device for each one of storage devices .

When backup module receives the request to perform the backup operation backup module determines whether all of storage devices have been backed up . If one or more of storage device have not yet been backed up NO of backup module selects one of storage devices that has not yet been backed up . After selecting the storage device backup module copies all of the data at the selected storage device to the backup device associated with the selected storage device . Backup module may then loop back and again determine whether all of storage devices have been backed up . If all of storage devices have been backed up YES of backup module reports that the backup operation is complete.

As discussed above each of storage devices may store data associated with a plurality of different volumes and secondary data blocks of the data each of the volumes are distributed among storage devices . Consequently when backup module copies the data at one of storage devices to one of the backup devices data associated with the plurality of different volumes is copied to the backup device. Because each of the backup devices is a physically separate device it may be difficult to reconstruct the data associated with a volume from individual ones of the backup devices. For example if a thief steals one of the backup devices it would be difficult if not impossible for the thief to reconstruct the data of a volume.

It is recognized that the above networks systems and methods operate using computer hardware and software in any of a variety of configurations. Such configurations can include computing devices which generally include a processing device one or more computer readable media and a communication device. Other embodiments of a computing device are possible as well. For example a computing device can include a user interface an operating system and one or more software applications. Several example computing devices include a personal computer PC a laptop computer or a personal digital assistant PDA . A computing device can also include one or more servers one or more mass storage databases and or other resources.

A processing device is a device that processes a set of instructions. Several examples of a processing device include a microprocessor a central processing unit a microcontroller a field programmable gate array and others. Further processing devices may be of any general variety such as reduced instruction set computing devices complex instruction set computing devices or specially designed processing devices such as an application specific integrated circuit device.

Computer readable media includes volatile memory and non volatile memory and can be implemented in any method or technology for the storage of information such as computer readable instructions data structures program modules or other data. In certain embodiments computer readable media is integrated as part of the processing device. In other embodiments computer readable media is separate from or in addition to that of the processing device. Further in general computer readable media can be removable or non removable. Several examples of computer readable media include RAM ROM EEPROM and other flash memory technologies CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium that can be used to store desired information and that can be accessed by a computing device. In other embodiments computer readable media can be configured as a mass storage database that can be used to store a structured collection of data accessible by a computing device.

A communications device establishes a data connection that allows a computing device to communicate with one or more other computing devices via any number of standard or specialized communication interfaces such as for example a universal serial bus USB 802.11a b g network radio frequency infrared serial or any other data connection. In general the communication between one or more computing devices configured with one or more communication devices is accomplished via a network such as any of a number of wireless or hardwired WAN LAN SAN Internet or other packet based or port based communication networks.

The above specification examples and data provide a complete description of the manufacture and use of the composition of the invention. Since many embodiments of the invention can be made without departing from the spirit and scope of the invention the invention resides in the claims hereinafter appended.

