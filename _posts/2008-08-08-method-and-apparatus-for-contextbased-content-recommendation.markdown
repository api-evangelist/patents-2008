---

title: Method and apparatus for context-based content recommendation
abstract: Starting with the people in and around enterprises, the expertise and work patterns stored in people's brains as exhibited in their daily behavior is detected and captured. A behavioral based knowledge index is thus created that is used to produce expert-guided, personalized information.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08095523&OS=08095523&RS=08095523
owner: Baynote, Inc.
number: 08095523
owner_city: San Jose
owner_country: US
publication_date: 20080808
---
This application claims priority to U.S. provisional patent application Ser. No. 60 954 677 filed Aug. 8 2007 and is a continuation in part of U.S. patent application Ser. No. 11 319 928 filed Dec. 27 2005 now U.S. Pat. No. 7 698 270 which application claims priority to U.S. provisional patent application Ser. No. 60 640 872 filed Dec. 29 2004 all of which are incorporated herein in their entirety by this reference thereto.

The invention relates to electronic access to information. More particularly the invention relates to a method and apparatus for context based content recommendation.

One problem with finding information in an electronic network concerns how people are connected as quickly and effectively as possible with the information products services that meet their needs. This has been one of the main goals of Web pages and search engines since the beginning of the World Wide Web. Failure to do so leads to lost business in the case of eCommerce eTravel and eMarketing sites frustrated customers on eSupport sites who likely then call customer support thus wasting a lot of the company s money disinterested viewers reader on eMedia sites who quickly abandon the site thus losing opportunities for advertising revenue and unproductive employees on intranets.

Web site design is a manual attempt to solve the problem of information discovery to organize information in a way that the designer imagines helps a user find what they are looking for. While effective in some cases trying to find information in this way often is slow and ineffective as users resort to poking around a site looking for the information they need. Most users actually abandon a site if they do not find what they are looking for within three clicks. One problem is that the site is static. In more recent years Web analytics has emerged as an attempt to alleviate this problem. Designers can see all of the actions that happen on their site and collect it into reports that aim to provide some guidance on how the site can be redesigned or reconfigured more effectively. While providing some benefit the information provided is often ambiguous and provides only hints rather than concrete suggestions for improvement. At best the process is tedious requires a great deal of manual effort as designers redesign the site in line with learnings and takes a long time. The feedback loop is thus slow and ineffective.

Automatic content recommendation is a completely different strategy that emerged very early in the life of the Web. Search engines such as Google Yahoo and Ask are the common manifestation of such techniques. The basic idea is that the user explicitly describes what they are looking for in the form of a search query and an automatic process attempts to identify the piece of content most often a Web page that best matches their query. The approach for doing this amounts to looking at all possible documents and recommending those where the target query occurs within the text with highest frequency i.e. keyword match. Modern adaptations of this basic technique add layers of sophistication e.g. natural language processing but the key in these approaches is still to use properties of the content itself e.g. words within the document to determine the ultimate relevancy ranking. This represents the first content centric phase of content recommendation see .

Many variations to this approach exist including most notably meta tagging. In this approach the content creator selects a small number of terms to describe the content. These terms are embedded within the content often as HTML meta tags but are not necessarily made visible to the consumer of the content. This is one way to allow search engines to search content that is not text based such as video clips. This approach was very common in the late 1990 s but has since fallen out of favor due to the enormous effort required to keep the meta tags up to date and in synch with changes to the content.

In many ways this first content centric approach on the surface make a lot of sense i.e. if you want to recommend content consider the content itself. A key problem with this approach is that it often brings back lots of documents that may be relevant but not useful. Many documents may exhibit a strong keyword match but are outdated or not truly relevant to the user s current interest. If users do not find a useful result within the first few results they are most likely going to abandon the search.

Keyword match does not really reflect how we find information most efficiently in the real world. In day to day life the best way to find the information products services we are looking for is to ask someone who knows to point us in the right direct. The second phase of content recommendation thus shifts the focus from content to users see . Google s PageRank algorithm though we place it in phase 1 was really a transitional technology that harkened the coming of phase 2. The page rank algorithm s break through was to consider not only the content of the page itself but how it had been linked to from other pages by other Web site designers. This represented a form of voting on the importance of Web pages. Thus pages that were linked to more often were seen as more valuable. While bringing people into the equation the people who were voting were Web designers rather than the consumers of the content i.e. the users. Phase 2 of content recommendation is all about the users. The three most well known approaches that fall into phase 2 are folksonomy profiling behavioral targeting and collaborative filtering.

The first folksonomy represents the most straight forward addition to phase 1. Here users are allowed to tag content themselves. So rather than the Web site designers or a single designer being responsible for coming up with the best set of keywords to describe the content folksonomy lets the community do it. Once this is done those community created tags essentially become part of the content and can be searched using traditional information retrieval search techniques developed in phase 1. A big assumption in this approach is that the subset of the community who takes the time to tag the pages explicitly ultimately produce a description that is valid and representative of the larger community s opinion. This is often not the case.

Profiling Behavioral targeting in its common form also borrows heavily from phase 1 techniques. Here based on a user s prior behavior on a site e.g. the pages clicked or products purchased a profile is built for that user. This profile may in the simple case be based on a collection of pages clicked or products purchased. The profile may also make use of the content itself or meta tags to attempt to discern the user s historical topics of interest. For example if a user purchased many films tagged as horror by content providers in the past then a behavioral targeting system would tend to recommend more horror films to the user. A major assumption here is that a user s historical behaviors are a good predictor of future interest. While sometimes true this assumption tends to fail at least as often as it works. The reason for failure is that people exhibit a variety of behaviors depending on their current interests context and goals. For example someone who bought a few books on guitar as a one time gift for his wife a few weeks ago might continue to be recommended guitar books by a behavioral targeting approach even though he may no longer have interest in that topic. Profiling approaches often also take into account demographic data of users such as age gender and geographic location. The core belief underlying such approaches is If I only knew enough about a user I could predict exactly what they want. However some basic introspection uncovers the fallacy underlying this approach. For example I may know more about my wife than any person or machine. I am in this way the ideal profiling system for her. However I am unable to predict what she might be currently looking for online without some context.

Collaborative filtering is another user centric approach which is arguably the most strictly user centric. Here users are compared to one another based on common purchases click histories or explicit ratings. For example based on a person s previous ratings of movies on a movie site find other people who most agree with that person s ratings and recommend other movies that he liked. Standard people who bought this also bought that approaches are actually a variation on the collaborative filtering approach where a user s most recent action serves as the sole basis for identifying similar users. This approach was made popular by Amazon s recommendation engine. A big assumption in this approach is that some global similarity measure between users based on past behavior is a useful way to predict future interest. This is a flawed assumption however. One may be very similar to some of his co workers in a work context e.g. they are all Java engineers with similar interests regarding programming but quite different from these co workers when outside of the office on the golf course for instance. In the context of golf one likely has a very different peer group. Grouping users at a global level is more often misleading than helpful.

Another weakness in all of the user centric approaches in phase 2 is the reliance on either explicit measures of liking or overly simplistic implicit measures. Explicit measures include asking the user to indicate their liking of a particular piece of content e.g. on a 1 5 scale. Such approaches are almost always biased because they represent a very small percentage of the population. Further the people who are taking the time to do these ratings are not representative of the community as a whole. They tend to be very opinionated or reflect a specific personality type that is willing to spend the time to voice their opinion.

Those approaches that leverage implicit observations as a rule either look at clicks or purchases. Clicks are a flawed way to assess liking because getting someone to click on a result has a lot more to do with an intriguing perhaps even ambiguous title and location on page. It tells one nothing about how a user felt about the content once it is selected for viewing. At the other extreme many systems use purchases as a measure of liking. While purchases are a reasonable way to assess this they are too limited. For example when buying a camera one may seriously consider a number of products before making a decision. All of that information could be valuable to others interested in cameras above and beyond to the one ultimately purchased.

An embodiment of the invention represents Phase 3 in the evolution of content recommendation see . Here the idea is to start by understanding the current user s context i.e. What is their intent What are they looking for Based on this understanding then find the appropriate peer group representing other users who are most like the current user in the context of this identified interest. From there find the content that that peer group identifies as most relevant to the current context.

The approach taken in the invention is context centric or put another way intent centric. The techniques used to achieve this approach are described later and are fundamentally based on the UseRank technology and affinity engine described in part in U.S. patent application Ser. No. 11 319 928 filed Dec. 27 2005 which is incorporated herein in its entirety by this reference thereto. It should be noted that all previous approaches including the content centric and user centric approaches are subsumed and improved by the Phase 3 approach manifested in the invention. Because the invention adds the dimension of context to the picture on top of users and content it is always possible to choose to ignore context and use the system to provide phase 2 functionality such as collaborative filtering or behavioral targeting profiling. However even these previously known approaches are significantly improved in their functionality based on a critical aspect of the invention which provides full spectrum behavioral fingerprints.

The invention represents Phase 3 in the evolution of content recommendation see . Here the idea is to start by understanding the current user s context i.e. What is their intent What are they looking for Based on this understanding then find the appropriate peer group representing other users who are most like the current user in the context of this identified interest. From there find the content that that peer group identifies as most relevant to the current context.

The approach taken in the invention is context centric or put another way intent centric. The techniques used to achieve this approach are described later and are fundamentally based on the UseRank technology and affinity engine described in part in U.S. patent application Ser. No. 11 319 928 filed Dec. 27 2005. It should be noted that all previous approaches including the content centric and user centric approaches are subsumed and improved by the Phase 3 approach manifested in the invention. Because the invention adds the dimension of context to the picture on top of users and content it is always possible to choose to ignore context and use the system to provide collaborative filter profiling. However even these previously known approaches are significantly improved in their functionality based on a critical aspect of the invention which provides full spectrum behavioral fingerprints.

Full spectrum behavioral fingerprints provide a significant advancement over current state of the art implicit ratings which essentially amount to click analysis and purchase behavior. First they take into account a wide variety of user behaviors including but not limited to clicks time spent on a page scrolling and mouse movement explicit actions such as print email bookmark links used frequency of return visits and searches performed. Second all of these behaviors can be cross correlated with the current user s behaviors on other pages and also with the rest of the community s and identified peers behaviors on the current page. From this analysis a probability that the user has found value in a particular piece of content can be discerned and fed into the learning system.

A further aspect of the invention concerns the seamless integration of existing strategies for automatic content recommendation see including search engines and profiling systems and ad servers as well as systems for manual recommendations including merchandising rules and systems . The invention also seamlessly integrates with other information sources related to the content such as product catalogs which can be used for purposes of display filtering or learning.

Finally because of the comprehensive nature of the information collected and the learned affinities the system represents a general wisdom platform on top of which many applications can be built see and such as SocialSearch Content and Product Recommendations Insights eMail and Live connect which are tailored to various applications e.g. eCommerce eMarketing etc. Additional applications include reports and integrations with bid management systems for search engine optimization and search engine marketing mobile and IPTV and custom applications or mashups . Some of these applications are described later. Each of these critical aspects are now described in greater detail

A core advancement in the evolution of content recommendation embodied by invention is the process for identifying and representing the users current topic of interest and converting that interest into a set of useful recommendations and information. The steps in this process are shown in and outlined below 

Step 1 When a user comes to a Web site they immediately begin to establish their current context. They might do this by entering a query into a search box on the site navigating to a particular section or may even have established some context before arriving at the site by doing a search on an external search engine such as Goggle or Yahoo that led to this site. All of this information is captured by an Observer Tag i.e. a piece of HTML JavaScript embedded in the Web site. As the user continues to move through the site they may also show interest in a particular page or piece of content based on their implicit actions. Interest is determined based on various behaviors collected by the Observer Tag and analyzed using the invention s Full Spectrum Behavioral Fingerprint technology described in greater detail below . These pages of interest further contribute to the user s context.

All of this information is stored as the user s current context vector which is a hybrid vector of terms and documents with weights on each entry reflecting how strongly that term or document reflects the user s current context. As a user enters search terms and or clicks navigation links the vector entries corresponding to the terms and phrases entered or clicked are incremented to capture expressed interest. As these actions move further into the past the corresponding entries are decremented or decayed. Similarly documents that a user clicks on or indicates interest in as determined based on their implicit actions increment the corresponding vector entry to a degree based on the level of interest and level of certainty determined by the invention. The result is a representation of the user s current context as a context vector. It is also possible to increment the context vector further based on historical actions and historical contexts of interest of the user. Although this is not generally done in the preferred embodiment because such information can be misleading it is possible to increment the context vector to a lesser degree based on these historical contexts in applications where historical behavior is considered to be relevant.

Step 2 Expand and refine the context vector into an intent vector based on affinities associations learned from the aggregated wisdom collected from observations on the community as a whole over a longer term. For example the user may have entered a query about digital SLR cameras and expressed implicit interest in a Nikon page. In the context vector the entries corresponding to digital SLR camera as well as digital SLR and camera to a lesser degree are incremented as is vector entry corresponding to the particular Nikon page of interest. To create the intent vector the system looks at affinities between the terms and documents in the context vector to other terms. For example the community wisdom may have discovered that the term high resolution may be highly associated with both the term Nikon as well as with the specific Nikon page of interest. The intent vector is thus incremented at the entry corresponding to high resolution. Similarly other documents that are associated to the terms in the context vector may be discovered based on the community wisdom and are incremented in the intent vector. For example a Canon camera page may have strong affinities to SLR camera and become part of the intent vector to some degree. The affinities that allow the expansion of the context vector into an intent vector are determined by the affinity engine which is described below and in detail in U.S. patent application Ser. No. 11 319 928 filed Dec. 27 2005. In summary the affinity engine learns connections between documents and terms documents and documents terms and terms as well as users to other users documents and terms by watching all of the implicit behaviors of the user community on the site and by applying behavioral fingerprinting and the use rank algorithm to determine interest and associations. The ability to translate context into intent effectively is a key aspect of the invention.

Step 3 Identify the group of users who share affinity to the current intent as well as those users who exhibit behavior most like the current user within the context of that intent. In U.S. patent application Ser. No. 11 319 928 filed Dec. 27 2005 these are called experts and peers respectively but here we combine them both under the name peers. This peer group is represented by a user vector and each user entry in the vector may have a weight indicating how strong of a peer he is to the current user in this context. Although not generally recommended the system is also capable of limiting the peer group to those users who match the current user based on a set of predefined attributes such as age gender location or other demographic variables. Similarly the peer group can be limited to those users who visit the site at the same time of day e.g. morning afternoon evening. In some cases these attributes can be used to influence the peer weights for users such as giving slightly more peer weight to those users who also best match the user along these predefined variables. The invention can also learn these weights by comparing behavioral patterns e.g. documents found useful in the simplest case within and across each of the predefined attribute groups. Higher intra group similarity in behavior compared to inter group similarity indicates the group is differentiated and thus a higher weight of influence for groupness is warranted when influencing the peer group. Similarity can be measured by looking at similarity of documents and terms used through one of many similarity calculations e.g. cosine similarity on users aggregated interest vectors.

Step 4 Look at which documents have highest affinity to the identified peers within the current intent . To do this the affinity engine looks at a combination of factors including the overall usefulness of a piece of content as represented by the activeness vector learned affinities between terms and content as represented by term doc matrices and predicted navigational patterns as represented by next step matrices associated with those peers the factors are weighted according to their peer weight and used in aggregate to compute those documents with highest affinity to the current intent. These documents become the unfiltered recommendations . Filtering described in greater detail below may now be applied to limit or augment those community recommendations.

Step 5 Now that the recommendations have been identified and appropriately filtered a final optional step is to ask the affinity engine for community information on each of the recommendations . This information can be combined with other asset information such as title size or price and displayed to the user to help them understand the community wisdom underlying a recommendation. Many aspects of the community wisdom associated with a document in the affinity engine can be exposed but in common implementations we expose the number of users in total who found value in the document the number of peers who associated the document with the current context intent and the terms phrases the peer community has associated with the document.

This latter piece i.e. terms associated to the document is called a virtual folksonomy because it represents terms that the community has associated to the document but unlike a traditional folksonomy discussed above where users must explicitly make the associates the virtual folksonomy is created automatically by the affinity engine based on the implicit actions of the user community. Terms that are searched or clicked as navigation links and lead within one or more steps to useful content as determined by behavioral fingerprinting become automatically associated. These term doc connections are both fundamental to the process of providing recommendations as well as providing useful feedback to users on the topics associated with each recommendation. When these terms are displayed to a user they can be made clickable so that the user can click them to provide further input regarding their current intent. This information then becomes part of their context vector the whole process repeats and new recommendations may be provided based on this new information. provides a conceptual diagram of a virtual folksonomy contrasted against traditional approaches for connecting terms to content.

Time factors in heavily to all computations in moving from context to recommendation. First all information collected by the affinity engine is subject to time decay. This means that for example information from last month has less of an influence on the calculations than information from today. This is important because the site may change e.g. new content added or removed people may individually change e.g. their interests may change and the community as whole may shift interest e.g. new fads. Although there is a default decay rate for all information some information may decay away more rapidly if it is determined that this is necessary. For example fad behavior such as interest in Christmas products that come and go quickly may need to be decayed more quickly to prevent recommending Christmas products too long after Christmas has passed. To prevent this the system runs a trend detection system across all content on a periodic basis e.g. once a day or every five minutes for very time sensitive sites. Trend detection can be done in a number of ways but one way is to use the Mann Kendall algorithm. Other ways include various regression techniques that employ least squares fit. If a strong negative trend is detected for a given piece of content the information associated with that content in the affinity engine is decayed at a more rapid rate. The result is that the likelihood of the affinity engine recommending this piece of content is reduced.

Another algorithm for fad detection that is run on a periodic basis is cyclical fad detection. If a piece of content shows a strong positive trend and that same trend can be found at regular intervals in the past then that piece of content can be automatically boosted in importance in anticipation of the coming trend.

In a traditional Web site even where the site proprietor is trying to understand customer behaviors techniques that are typically used survey the customer and analyze the customer responses to produce a report. By the time the report is analyzed the results of the report are out of phase with the actual situation at the time the report is being reviewed. For example in a traditional system a commerce site might collect feedback during the Christmas season and redesign their site in February in response to that information. In effect the commerce site is trying to sell Christmas products in February. Alternatively analytics reports may be created offline and analyzed by a team of specialists to infer trends and determine appropriate actions a process which may take weeks or month and again lead to out of date site modifications. In this way the automatic nature of the invention herein allows a Web site to adapt in real time to discovered trends and fads providing recommendations to users that fit the current context and time.

In addition to contributing to the process of automatic content recommendation however the trend and fad detection algorithms used by the invention can also be exposed to owners of the content system e.g. the Web site through reports within a customer portal. In the preferred embodiment such reports on community trends and fads can for example be used by merchants to promote certain products or content at the right time making such promotions more effective. Given the example of products that are sold during the Christmas season if such sales were to die out after Christmas then the proprietor of the commerce site would be able to follow the sales curve based on community interaction with the Web site. If on the other hand an emergent news story drives demand for a product quickly the invention allows the merchant to watch the curve of demand and respond in real time. Thus if people are all of a sudden dramatically interested in a particular piece of content or product the proprietor of the Web site can provide that content or product more quickly. If the demand dies out quickly for example the interest was based on a fad then the proprietor of the site can adjust to that fact as well.

As already mentioned and as discussed in greater detail in U.S. patent application Ser. No. 11 319 928 filed Dec. 27 2005 the affinity engine learns connections affinities between terms documents terms terms terms users documents documents documents users and users users. Note the terms document content and asset are used interchangeably throughout this document. In all cases these terms refer to any type of medium that provides information or services including for example but not limited to webpages word documents pdfs video audio files widgets and or products . In the preferred embodiment all affinities are stored as sparse matrices and vectors. However there are many alternative ways of storing such information known to those skilled in the art. Although there is ultimately a single number that can be calculated to represent the affinity between any two entities e.g. a document and term there are usually several sub affinities that are combined in a weighted sum to arrive at that single number. The weights on that sum may be dependent on context. For example documents have at least three kinds of affinities to other documents similarity in virtual folksonomy i.e. terms the community has associated to the documents similarity in user groups i.e. how many users have used both documents and similarity in navigational patterns usage e.g. do users show a pattern of finding value in one document after using the other. A variety of mathematical techniques are available to be employed as appropriate to each kind of data although a combination of vector space models and custom probabilistic techniques are currently used by the preferred embodiment of the invention. See U.S. patent application Ser. No. 11 319 928 filed Dec. 27 2005 for more discussion.

One important aspect of the invention is that all sub affinities are always computed bi directionally. To illustrate this bidirectionality we describe its application to the term document sub affinity derived from the virtual folksonomy. is an architectural diagram showing the aspects of the invention that involve topic attraction and topic match the two dimensions that make up the bidirectional virtual folksonomy connection between terms and documents as well as general activeness. In the example of a query is provided and a percentage of respondents who ultimate find value in one of three example documents are indicated. The query involves a particular Nikon camera and it can be seen that 20 of the respondents find value in a Web page focused on high resolution cameras 78 found value in the overview page for the specific camera and only 2 of users making this query found value in the detailed specification sheet for the camera . As a side note remember that value is determined by the full spectrum of behaviors exhibited on a given page by the user described in detail later .

There are two ways to consider the virtual folksonomy connection between the query and each example document. Topic attraction starts from the query and considers the proportion of users who searched for Nikon 580X and subsequently found value in each document. In this example the Overview page has highest topic attraction with 78 of users finding value there followed by the High resolution camera page with 20 and then the spec sheet with only 2 . Note that this example is simplified for the purposes of illustration. In reality we also break down the query into its component parts and consider the affinities of these to each document as well as consider affinities between other terms that the affinity engine has learned have similar meaning to this one. In this way topic attraction can be thought of as a problem of predicting the probability that a user finds value in a given document given the intent represented by the query. In the preferred embodiment arriving at this probability is accomplished using a combination of probabilistic techniques including Bayesian inference.

Topic match looks at it from the other direction starting from each document and considering all of the other terms that have been associated with it through the behaviors of other users. So for example the High resolution camera page may have many other camera names connected to it and thus the degree of focus on the particular query entered is lower than both of the other two pages whose term document connections are more highly concentrated around Nikon 580X specifically. In fact the Spec sheet may turn out to be the document with term connections most focused around Nikon 580X and thus have the highest topic match for this query even though it has the lowest topic attraction. As with topic attraction in reality topic match also breaks down the query into its component parts and takes into account affinities between other terms not in the query. In this way topic match can be seen as finding the document with the best overall match in topic to the intent represented by the query. In the preferred embodiment a vector space model and modified cosine similarity technique similar to that used by traditional full text search is used to determine the degree of matching.

These two directions of considering the affinity of each document to the query i.e. topic attraction and topic match are combined in a non linear weighted sum along with the final factor of overall activeness i.e. usefulness to arrive at a value for the virtual folksonomy sub affinity. This sub affinity is then combined with other sub affinities such as those based on navigational patterns and filtered through the lens of peer groups to arrive at an ultimate ranking of documents against the query called UseRank. Similar bidirectional techniques are used to compute UseRank when providing recommendations on a particular Web page and when considering the full context and intent vectors.

There are several methods for combining the various sub affinities and the bidirectional dimensions therein to arrive at the ultimate UseRank of documents for a particular user and context. In the preferred embodiment one or more thresholds are applied to each dimension including absolute and relative thresholds before they are combined together based on hybrid arithmetic geometric weighted average. Each resulting sub affinity is then similarity subjected to a thresholding before being combined with other sub affinities again based on hybrid arithmetic geometric weighted average. The values for the thresholds are generally fixed however the individual weights for the weighted averages can also be adjusted and learned based on the success of the result set returned by the affinity engine.

As discussed herein a key feature of the invention is the processing and analysis of implicit observations that are made during the actual use of a Web site for example versus traditional approaches in technology which use explicit feedback. One key advantage of the invention as confirmed through scientific studies and as understood from human psychology and sociology is that humans are very bad at giving feedback particularly if the feedback must be given explicitly. If a person is being surveyed the person does not have an incentive to give actual accuracy in the form of feedback. One aspect of the invention eliminates such survey bias by using the implicit behaviors observed during use of particular materials on the Web by individuals within a community. Thus the invention trusts what people do but does not watch what they say. The invention watches people s behavior through their actual action implicit behaviors and can accurately interpret what their true intent is and whether or not they dislike or like something. Thus the invention observes behavior versus click actions.

Current technologies focus on clicks such as Web link clicks. If a person clicks on a link that click is reported. The click may be reported as having resulted in a viewed Web page even though not much time is spent on the page on which the person clicked. Thus this known approach is not a good indication one way or another if the page is good or bad. If a link is put in a prominent position on a Web page then people are likely to click on it. However when people get to the location indicated by the link they may immediately leave the site. This is why the number one used button on the browser is the BACK button. The use of the BACK button could indicate like or dislike of a site. Accordingly the invention recognizes that it is the action of the user after the click that matters and not the click itself. The invention tracks behaviors beyond the click to determine whether a page is good or bad. Thus if a person backs out of the page it is considered negative feedback i.e. the person did not like it. In this way clicks can identify a very negative reaction in the invention. If a person goes to a link follows the link down spends time there and does other things that behavior is tracked as well. If a peer group validates the behavior as consistent e.g. a significant number of the group members exhibit the same behavior when reacting with the page then the page is considered to be a good page.

An embodiment of the invention goes one step farther. Not only does the invention determine which assets are useful based on behavioral fingerprinting but it also learns the context associated with the usefulness. For example the invention can learn that a particular camera page is very useful for users that show interest and intent in high definition cameras but not if the intent is compact cameras. In this way the affinity engine is able to distinguish the usefulness of assets based on the context and intent expressed by a user through their implicit actions.

The primary input to the affinity engine which drives all of the learned associations affinities is the behaviors of users on the site. In the preferred embodiment all behaviors are captured by the ObserverTag i.e. a piece of HTML JavaScript embedded within the Web site typically in a header or footer template. Although this is the preferred method for capturing user behaviors it is also possible to capture user behaviors using a browser plug in or lower level network traffic analyzer. The behaviors captured include 

The captured information is send back to the affinity engine for processing. shows the process for analyzing these behaviors or other implicit data captured from various user interface devices. The input to this process is the user trail which includes all assets visited and the implicit and explicit actions observed on those assets. There are two main steps in processing the behaviors that combine to provide an understanding of what content is useful and in what contexts. The first step is determining whether a user is finding value usefulness in a particular piece of content. Conceptually the more time spent on the page in think mode i.e. user is processing the information the higher the likelihood that it is useful. Think mode can be approximated by non scrolling time on a page where some scrolling or mouse movement has been detected with a specified time range. Repeat visits and percentage of page seen are also generally good indicators of liking or usefulness. For each user and piece of content we can create a behavior vector with each entry in the vector representing one of the features listed above or a predefined combination of features. However not all content is created equal nor are all users created equal. We normalize the behavior vectors in a several ways. First we normalize by user to make behavior vectors comparable to the rest of the user population . For example some users may read slower than others affecting their mean time spent on a page. Some users may use the mouse more than others. One way to accomplish this normalization is by translating all entries into z scores which adjusts for means and standard deviations specific to that user.

A second normalization is to normalize based on the content. This is done based on inherent or specified properties of the content. For example 30 seconds spent on a one paragraph document likely has a different meaning than 30 seconds spent on a ten page document. Dwell time can thus be normalized based on page length or number of words. Similarly 30 seconds spent viewing a 30 second video has a different meaning than 30 seconds spent viewing a five minute video. In many cases the Observer Tag is capable of capturing these page characteristics through information within the DOM Document Object Model . However when not available in the DOM the invention provides other mechanisms for collecting the needed information from offline catalogs e.g. a product or media catalog or allowing the Web site designer to add explicit information in the page itself e.g. as meta tags or added JavaScript variables. For example on an e commerce site certain pages may be defined as information versus product pages. The system can normalize the behavior vectors for each of these content groups independently. In this way the dwell time necessary to indicate liking can be adaptable to the type of content. All behaviors features can be made adaptable in this way. In the current implementation these normalization strategies are hard coded. However the invention allows the possibility for plugging in various machine learning techniques to learn the appropriate normalizations.

We now have a behavior vector normalized to the user and the content . In the current system a predefined set of rules are applied to determine whether this behavior vector represents liking usefulness. Each of the normalized features in turn is considered to determine whether it meets the pre specified thresholds for indicating usefulness. Each passing feature increases the probability of usefulness . In some cases the thresholds for a given feature are pre specified and tuned by the person implementing the system based on previous experience. In other cases these thresholds are dynamically determined by the system. For example certain features are known to exhibit a bimodal distribution and the threshold can be dynamically determined to lie between the two modes. The invention also provides a mechanism for plugging in various machine learning techniques to learn which features are most important and thus dynamically learn the rules for converting from the behavior vector to the probability of usefulness.

Once usefulness or probability of usefulness has been determined the second step in the analysis of behaviors is to understand the entire context surrounding the use of that piece of content including searches done prior to the use links clicked prior and pages used prior. All of this information combines with information about the user to influence affinities affinities can be learned and stored in a number of ways described in more detail in U.S. patent application Ser. No. 11 319 928 filed Dec. 27 2005 . Before doing so however there is one final step validation. Validation is a form of noise filtering wherein an affinity connection is established e.g. between a document and term if and only if enough similar users have through their behaviors confirmed this connection. One user making the connection is not enough for an affinity to emerge. A minimum number of users in the same context peers must have exhibited similar behaviors and connections for that affinity to be validated. We call this peer validated behavior.

UseRank the ultimate ranking of the usefulness of content based on a user s context and intent based on learned affinities and full spectrum behavioral fingerprinting can be compared to the very popular PageRank strategy made famous by Google. In PageRank each Web page is given a value based on the number of other pages linking to it. In addition links coming from pages that are themselves of high value raise the PageRank even more. This democratic strategy is quite effective in getting users to useful Web sites based on their Google query but generally breaks down once the user begins to look for further information within the Web site itself. The main reason for this is that pages within a Web site particularly those down into the long tail of content are not heavily linked to either externally or internally. Internally the linkages are determined solely by the structure of the Web site and more links does not necessarily mean more value. In addition many of the pages on such sites are in formats other than HTML e.g. PDFs Word Docs or videos and do not link to other content at all.

The UseRank methodology based on user behaviors on the Web site alleviates all of these problems. Instead of relying on the linking of documents by Web designers it relies on usage of documents by Web users who are a rich source of information on any Web site. Usage is a truly democratic way of learning what content is most valuable. In fact the designers of PageRank recognized their approach as an approximation of user activity. Google s choice to approximate user value based on PageRank makes sense given the privacy concerns associated with tracking a user s behavior across the entire Web. In the preferred embodiment of UseRank we track only the behaviors of user s on sites instrumented with the Observer Tag and always in an anonymous fashion unless otherwise configured by the Web site deploying the Observer Tag.

Another important aspect of the invention emerges from the context driven approach underlying the affinity engine. Many previous systems that are content centric or user centric suffer from a problem where only the most popular product content is ever recommended. Because these systems lack a deep understanding of the user s current context intent this is the best that they can do. The invention provides a context centric approach that allows the affinity engine to narrow its focus to the subset of users peers and content that match the current user s context intent even if that intent is not particularly popular in the grand scheme of things. This is what is known as the long tail of the products i.e. those products that may not be the most popular overall but are extremely important because they strongly meet the need of a small but important subset of the community see C. Anderson The Long Tail Why the Future of Business Is Selling Less of More Hyperion Press 2006 . The invention is capable of identifying that long tail interest along with the associated peer group and related content and can thus recommend those important long tail products content at the appropriate time see . This is critical for product content providers because it is often these long tail products that are most useful to the community and often lead to the highest margins or benefits for the company itself.

Thus the invention uses like minded peers as an enabler to target small target segments along the long tail instead of using individual problem personalization and historical interest as known in the prior art. The invention provides contextual targeting. People s needs within a given context are typically similar among like minded people. An insight of the invention is that people have hundreds of profiles if not thousands of profiles. These profiles have no cross connections to reality so it is difficult to predict an individual s use level without context. Once the individual is in a particular context i.e. among like minded peers the individual behaves similarly to everybody else within the group context. Thus if a person is visiting a golf equipment site to buy a golf driver or clubs the person behaves in a manner similar to other golfers. It does not matter what the person s political bias is or their cultural background. These aspects of the individual have no relevancy because the person s present context which is based upon group membership is more relevant.

The invention represents a new approach to leveraging implicit community wisdom to create adaptive Web sites and other information portals. In his book James Surowiecki explains how the collective intelligence of a large group of average individuals almost always outweighs the intelligence of experts. To illustrate the concept he uses an example from a county fair where a group of fair attendees attempted to guess the weight of a cow. A group of so called experts e.g. butchers dairy farmers etc. also made their guess. In the end the experts were all off on their guesses by a large amount. The average weight guessed by the crowd of non experts however came within one pound of the actual weight of the cow. Surowiecki goes on to show how a similar phenomenon can be seen in everything from stock market prediction to democratic governance. This notion that groups of individual actors can collectively exhibit a level of collective intelligence going beyond even the sum of the individual actors themselves has been known in the fields of biology and artificial intelligence as emergent behavior and collective intelligence. Numerous examples exist in nature where very simplistic individual animals such as ants or bees in collection exhibit extraordinary intelligence and resourcefulness in meeting the needs of group such as finding food or building nests.

In the context of Web site design Surowiecki s experts are the Web designers. These experts attempt to make correct decisions on which Web pages should be linked to what others such that visitors are best able to find what they are looking for. Web designers may also spend a lot of time an effort tuning search results to match the expected needs of their visitors. The crowd in this context is the large group of Web site visitors who come to the site. Generally the crowd has no direct impact on the organization of the Web site. They remain silent.

The invention gives this crowd a voice enabling their collective actions i.e. a form of expressed opinions to be collected and automatically drive decisions on Web site organization through their impact on recommended content. The invention thus taps into the wisdom of the crowd for the purposes of creating useful Web sites. Although in the preferred embodiment the impact of crowd wisdom is sectioned off into specific regions of recommendations and social search within the Web site it is easy to imagine an extended implementation where the entire organization of the Web site all of its links and menus for example are ultimately driven by and adapted to the behaviors of the Web site visitors. In this way the entire Internet made up of multiple interconnected sites could begin to evolve and adapt into a form that best meets the needs of Internet users.

The actions of the Affinity Engine can also be considered as a form of Memory Prediction Machine. Recent learnings from cognitive science teach that the brain is structured to capture and encode associations between and among objects and concepts in targeted regions of memory and organize those associations in a hierarchy moving from abstract to detailed. New stimuli are responded to and their consequences predicted based off of the memories created from previous stimuli and learned associations thus encoded. The Affinity Engine similarly learns associations between users objects and contexts by observing interactions between them in the environment of Web sites and remembers those patterns in its memory stored hierarchically. When a user exhibits a context that has been previously learned by the Affinity Engine it lights up the appropriate associations within memory which triggers the prediction of those objects that best meet the needs of the current user and context. is an architectural schematic of the Affinity Engine in its function as a memory prediction machine.

An important aspect of the invention is its ability to integrate seamlessly with other recommendation systems policies and information sources. Some examples are shown in .

When a user performs a search on a site implemented with the invention the search terms are sent to the affinity engine for processing. The affinity engine determines the set of content that is most useful given the context of that search based on learned affinities from past community behaviors. However the affinity can also take into account opinions from external sources such as a search engine. The search engine has its own set of recommendations which the affinity engine can accept. The relevance ratings from the search engine can be combined with the information embodied in the affinity engine to produce a single unified set of recommendations discussed more fully in U.S. patent application Ser. No. 11 319 928 filed Dec. 27 2005 . An XML feed is a typical way for the system to interface with a search engine.

The invention also provides a mechanism for automatically importing product or media catalogs . The preferred means for doing so is using an XML feed that is accessible to the system. There are three ways in which the catalog can be used 

1 Information from the catalog may be displayed along with the recommendations e.g. summary or price 

2 Attributes from the catalog may be used to filter the result set e.g. the Web site designer may want to restrict recommendations to products only or PDFs only a particular section of the Web site 

3 Content categorizations can be used to group content for the purposes of normalization in the behavioral fingerprinting process described above.

Another aspect of the invention is that it is content agnostic. The system can observe behaviors on and recommend Web pages pictures videos documents blogs downloads and even ads. In the case of ad recommendations it is often necessary to integrate with an ad server. Ad servers can provide functionality similar to both search engines and catalog systems as described above. The invention interfaces with ad servers in the same way as it does to these typically through XML.

Merchandizing systems in the general case provide a mechanism for the Web site owners to influence what products content are recommended to users. In the invention it is preferred that the community be the primary driver of recommendations. However there are times when the owners have specific needs that must be met independent from the community s expressed interest. For example the owner may choose always to recommend a particular product or piece of content first in a particular context for example there may be a promotion going for a product. Owners may also wish to influence the community to purchase products with higher margins or prevent a certain piece of content from being recommended. There are two ways in which the invention can honor such influence. The first is through a custom rule interface that is one component of the invention. Here Web site owners can log in and choose from a variety of predefined rule types including pinning i.e. forcing a particular recommendation to show up blacklisting i.e. preventing a given recommendation or class of recommendations from showing up and boosting i.e. increasing the chances that a recommendation or class of recommendations show up but still honor the community wisdom. All of these rule types can be applied globally or only in a given context e.g. on a particular page for a given search term for a given class of users in a particular time range. Another type of rule allows owners to honor fully the community wisdom but influence the goal of the recommendations. For example rather than focusing on recommending useful products the affinity engine can be told to recommend products that a user is most likely to purchase. As another example the affinity engine can be asked to recommend content that most likely leads users to a particular set of target pages. In addition to the custom rule interface the invention provides a mechanism for importing rules in XML form from an external merchandising application. When rule types exist in the external system that do not already exist within the invention a custom rule plug in can be designed for the invented system which matches the desired behavior of the external rule type.

A further type of system that the affinity engine can integrate with is user profiling systems . In its simplest form a profiling system is a set of attributes e.g. demographic attributes associated with a user. These attributes can be passed along to the affinity engine on a recommendation request. As discussed earlier the affinity engine can take these attributes into account when identifying a peer group which then influences the recommendations that are returned. More sophisticated profiling systems require XML integration similar to other external components. Here the affinity engine may have to dynamically contact the external profiling system during the peer identification process.

In the preferred embodiment of the invention a Recommendation System is implemented in a Software as a Service SaaS Model to provide automatic suggestions that help Web site visitors find products or content they like or need. is an architectural schematic diagram of a method and apparatus for context based content recommendation. In interaction of users with a Web site produces implicit emergent behaviors. Such user interaction includes page referrals links entry trails queries page sizes mouse movements peers negative experiences virtual bookmarks time spent virtual printing exit trails and the like. Such behaviors are processed in a recommendation affinity engine according to the invention resulting in automatic content and product recommendations in the form of social search and navigation guides as well as providing real time feedback to a merchant such as regarding visitor clubs and identifying content gaps each of which is discussed in greater detail below . The following key processes comprise the recommendation system 

1 Comparative Products also known as Similar or Competitive Products are shown for purpose of comparison shopping and up selling based on affinity of like minded peers on their product considerations i.e. observed full spectrum behavioral finger prints discussed earlier. Consideration based peer recommendations promote higher end niche products and yield more revenues and profits for a site. 2 Affiliated Products are also shown for cross selling related non competitive products such as accessories to increase overall order size. 3 The invention also provides a showing of most popular products such as products with greatest appeal across the whole site or within a category. 4 The invention also generates intent driven landing pages also known as AdGuide and Site Concierge with recommendations based on what a visitor searched on standard Web search engines such as Google Yahoo or MSN. If a visitor search Viking Cooktop on Google and landed on a customer site AdGuide serves the best Viking cooktop recommendations to the visitor on the landing page dynamically instead showing him something irrelevant. 5 The invention also provides a community filtered search on products based on the foregoing.

For peer driven recommendations the invention automatically identifies small and large population segments having a unique interest and guides individuals to popular competing and accessory products. The invention provides a user intent and product mapping that translates shoppers intent into peer validated products and brands. The inventors have found a 700 percent conversion power based on independent studies. The invention provides fad and trend detection and recognizes and adapts to seasonal promotional and other shifts in shopper interest in real time. The invention also allows for product gap detection where product gaps are detected and site owners are helped to introduce new products or content in those areas where the gaps are detected. The invention assists in merchandising with real time customer feedback. It works with existing merchandising promotions segmentation and search and magnifies products or product families. The invention provides built in concurrent A B measurement and reports in real time the net revenue generated by the invisible crowds.

1 Aspects of this feature of the invention include a social search which involves UseRank and implicit learning. This approach is intent driven adaptive and makes use of an implicit folksonomy community terms such as link text they use and search queries they enter as discussed above. The invention supports audio video binary code and all other content types because it does not have to parse the content itself whereas a typical search engine has to therefore is limited to text content and metadata only. 2 The invention also provides Related or Similar Content based on the implicit behavioral feedback of like minded users. The invention ranks the recommendations by usefulness and on target ness based on the context and intent. 3 The invention also provides a most popular category which provides content with the greatest value across the whole site or sub portion of the site. This promotion of information leads to business target conversion increases such as trial download registration conversions and may be provided at a site or topic level. 4 A Next Step feature is provided that concerns common next steps that lead to customer conversions. The next step may be any next step in a business process or a natural next set of content to read in connection with the process. 5 A similar Intent Driven Landing Page as with the e commerce application discussed above is available for a marketing Web site.

There are many ways to leverage the community wisdom distilled by the Recommendation Engine in addition to social search and onsite content product recommendations. Many applications can be built or modified to directly tap into this wisdom through APIs to the Recommendation Engine. is an architectural diagram showing the invention s role as a community wisdom platform. The community wisdom platform can also include such items as contextual email marketing mobile applications IPTV systems SEO SEM applications and mashups in custom applications. Within these systems input from the Recommendation Engine can be used to dynamically determine the most appropriate information and or organization of information to present to users. Such a platform is built upon Web services that include such features as AJAX and REST discussed below . Thus the invention is built upon new Web 2.0 fundamentals which include a true understanding of invisible crowds gathering information from like minded peers making content or product recommendations providing onsite social search implicit community based reports and the like. The following descriptions give details on how they are deployed 

Email Recommendations Marketing uses massive emails to communicate with customers and prospects. But the content of the email campaign is typically determined by the marketing staff. The invention enhances the quality of email campaigns by crowd sourcing the like minded peers. The content of the emails are determined by what s popular or similar to what the like minded readers of the emails are interested in. Marketers simply facilitate the process of letting one customer promoting content to another customer implicitly.

Mobile Recommendations The quality demand of mobile recommendations is even higher than the Web due to the small footprint of mobile devices such as cell phones and personal devices. The invention s like minded peer and intent driven approach is better than traditional approaches based on collaborative filtering page views and demographic or purchase data. Intent context driven peers predict people s need far better than the competing approaches.

IPTV Recommendations Similar recommendations are effective for using Digital TV HDTV to watch internet delivered movies and video programs. With thousands of movies and TV program selections features like You may also like these movies are extremely important for up sell and cross sell movies and TV programs. Context driven peer recommendations are the most effective way for the viewers to like the recommendations.

Live Connect shows an architectural view of the Live Connect system. Because the invention can identity like minded peers in addition to harvest their collective wisdom implicitly and to make peer recommendations to individuals the invention can connect the individuals with the like minded peer group and have them exchange information knowledge and experience. This is similar to the real world experience of like minded peers who may not know each other. Imagine you are in Best Buy shopping for a TV you see other like minded peers also shopping for TVs you can ask them questions for their experience of certain products.

SEO SEM Recommendations The invention is also used for guiding Web site visitors at the moment of landing on the site. The features are called AdGuide Site Concierge and Site Maitre d. This application examines the queries that people have searched on Google Yahoo MSN ASK and other major Web search engines then uses them as the proxy of user intent to display the right product or content for visitors when they land on the site either on the pre designed landing pages SEM or natural search result pages SEO . For example if a visitor search Viking Cooktop on Google and landed on a Baynote customer site AdGuide serves the best Viking cooktop recommendations based on collective interests of like minded peers to the visitor on the landing page dynamically instead showing him something irrelevant or less important by simply matching keywords.

Based on the affinities learned by the affinity engine the invention is also able to provide a Keyword Recommendation system that provides suggestions to marketers on which keywords should be purchased on external search engines as part of their SEO SEM efforts. For example the invention can suggest those terms that the community uses most often to describe content on a Web site as well as those terms which are most likely to lead to useful content within the site. Also given a particular collection of keyword already purchased the invention can suggest other words used by the community that have high affinity to the existing set. These recommendations can be provided to SEO SEM merchandisers either through reports within the Customer Portal or can be directly integrated with external SEO SEM systems to bid on the recommend keywords dynamically and automatically.

Blogs Forums and Discussion Threads Recommendations The invention is used by community generated content such as blogs forums discussions pod casts video content etc. Because of the volume of content involved surfacing the right content in the right order is even more important than expert generated Web sites. Using like minded peers for recommendations and social search is extremely important and effective. These Web sites can be public facing blog sites or login required sites such as partner portals or patient physician portals or developer s networks or intranets. Features such as Similar content Accessory products Next steps Most popular and social search are very useful and effective.

Insights Visitor Clubs Content Gaps etc. Insights the Customer Portal has already been discussed in part earlier in this application. Although a primary use of the invention is as an automatic recommendation system there are times when site administrators may want to modify the behavior of the recommendation system or see reports based on the knowledge learned by the affinity engine. Insights currently offers three main categories of functionality though its UI configuration management and reports. Configuration enables site administrators to configure fully all aspects of the recommendation engine required for full functionality and deployment including integration with external sources of information such as full text search engines and ad servers. Management enables site administrators to modify the behaviors of recommendations and social search in a holistic manner or specific to defined contexts e.g. a particular search term or page location or user. For example administrators can create product content promotions to override community wisdom for set periods of time blacklist particular products content from being recommended artificially magnify or boost defined classes of products content or set up business rules such as limiting the classes of products recommendations acceptable for recommendation in particular contexts. Reports offer various views on the community wisdom distilled by the affinity engine. Although the reports include a subset of the click based and purchase based information found in traditional analytics they go beyond these in providing many details on the community wisdom distilled through full spectrum behavioral fingerprinting and the connections learned by the affinity engine. Example reports are discussed below.

The Content Gap report uses the affinity engine to analyze those user interests that are not being met by the existing content on the site. Using the full spectrum behavioral fingerprinting technology and by analyzing community behaviors as discussed earlier in this application the affinity engine can identify community interests. The affinity engine can then analyze all cases in which a particular interest was expressed by a user and the subsequent behaviors of that user including use of content and level of value extracted as well as subsequent searches or exits. If a large portion of the user population with a particular interest is unable to find useful content then that interest is designated a content gap. The degree to which the gap is suspected by the affinity engine can also be reported. The administrator can then take necessary actions to remedy the gap by adding new content and then continue to use the Content Gap report to see the effect on the presence of the gap.

The Visitor Clubs report provides the most comprehensive view on the affinities learned by the affinity engine. Visitors to the site are implicitly grouped based on shared interest. Each interest group or visitor club is displayed in the interface along with the number of visitors in the club their overall activity level the virtually folksonomy of terms that describe the club and the content products which are most useful to that club in the context of the club s interest. In addition all club characteristics can be charted over time to gain an understanding of how the interest is trending over time. Clubs can also be compared to one another for overlap in membership. This report provides administrator with the ability to explore the learned associations between users content products and terms contexts. Such associations can then drive promotions or other merchandizing activities within the Management section of Insights. The visitor club information can also be used to drive various business decisions outside of the context of the invented system.

The architecture consists of a server farm a customer enterprise and a user browser . The user browser is instrumented with an extension and accesses both customer servers at the customer enterprise and the server farm via a load balancer . Communication with the server farm is currently effected using the HTTPS protocol. User access to the customer server is in accordance with the enterprise protocols. The browser extension is discussed in greater detail below.

Of note in connection with the invention is the provision of a failsafe. The extension as well as the enterprise extension are constructed such that if the server farm does not respond in a successful fashion the extension is shut down and the enterprise and browser interact in a normal manner. The features of the invention are only provided in the event that the server is active and performing its operations correctly. Therefore failure of the server does not in any way impair operation of the enterprise for users of the enterprise.

As discussed above an extension is also provided for the enterprise which communicates with the load balancer at the server farm via the HTTPS protocol.

The enterprise also includes a helper which communicates with the server farm via an agency using the HTTPS protocol. The agency retrieves log information from the enterprise and provides it to log analyzers which produce a result that is presented to the usage repository . The agency also communicates with any data sources accessible to the enterprise for the purposes of loading information into the affinity engine . Such information includes catalog database information use profile data authentication data as well as any other form of data that may be useful for the purposes of determining or filtering recommendation and search results.

Information is exchanged between the affinity engine and the browser and enterprise via various dispatchers . The browser itself provides observations to the server and receives displays in response to search or recommendation requests therefrom. Alternatively the customer server can directly request search or recommendation results directly from the affinity engine through the Baynote extension using REST.

A key feature of the invention is the affinity engine which comprises a plurality of processors and a configuration administration facility . During operation of the invention a form of information also referred to as wisdom is collected in a wisdom database .

The preferred embodiment of the Baynote extension uses a custom build tag based AJAX architecture for collecting Website behaviors from the client as well as serving recommendations to the client. provides code snippets for JavaScript integration with a client. is a block schematic diagram showing an AJAX tag platform according to the invention. In a Web page is displayed on a client and includes a particular script. In this example all files and results from the system are dynamically injected into the DOMS as elements. A coordinator on a trusted Web server provides such things as a failsafe mechanism common code a policy handlers and communication facilities. A system server provides such elements as a heartbeat common JavaScript policies handlers such as an observer and a guide and the affinity engine. In the coordinator is a piece of code that sits on the trusted Web site. In the presently preferred embodiment the coordinator resides on a customer s Web site. Thus there is a file that serves as coordinator role and that resides on a particular customer Web site although it could reside on any other trusted Web site. This aspect of the invention concerns the notion of a trusted Web server for failsafe reasons. It is important to be certain that if the invented system crashes that it does not crash or hold up the customer Web site. Thus a coordinator code is provided as a failsafe procedure. It goes through a failsafe procedure to ensure proper recovery after a crash of the server s Web site.

The architecture of the code is broken down such that common code is used. Common code or a piece of the code is used for all types of recommendations. Thus a tag might require common code. The policy may be customer specific or it can be user specific. This concerns how recommendations may be made. The policy information is loaded separately. Then the code needed for specific tags is loaded as well. For example if there is a tag for the most popular recommendation and another one which is the recommendation then these tags are loaded. The system then loads the code that is necessary for performing the particular tagging operations. A further type of tag or code is the observer that observes what users do as discussed above. Users do not see the tag but it is still a tag. The tags must also be able to communicate back to the system. Each of these functions is performed by a piece of code for example JavaScript is loaded into the Web page. The JavaScript is loaded into the page through a dynamic script injection.

This aspect of the invention recognizes that a Web page can become more dynamic. For example there are things such as dynamic HTML and JavaScript which cause things to be added to a page dynamically. If a script tag needs a particular piece of information then that code is injected into the page. For example if a policy is needed then a JavaScript tag is injected into the page that accesses the policy. The policy may require further injection of information such as handlers. Therefore the handlers are now dynamically injected as pieces of JavaScript into the page. Communications are handled in a similar matter.

One advantage of the foregoing approach is to provide the option of using first party cookies or third party cookies. The main Web page that the customer accesses may be the Web site of a merchant. The merchant communicates with the system to get the information with regard to recommendations and the like.

In this case the merchant is the first party and the system is a third party. As a result of dynamic script injection the invention allows the setting of cookies in both the first party domain and in the third party domain. The ability to set first and third party cookies also allows the invention to operate in a cross domain.

Although the invention is described herein with reference to the preferred embodiment one skilled in the art will readily appreciate that other applications may be substituted for those set forth herein without departing from the spirit and scope of the present invention. Accordingly the invention should only be limited by the Claims included below.

