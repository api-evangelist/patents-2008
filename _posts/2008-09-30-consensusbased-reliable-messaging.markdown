---

title: Consensus-based reliable messaging
abstract: A system includes a leader server capable of communicating with a plurality of follower servers and a network. The leader server is configured to determine when a message received from a client connected to the network has been committed to main memory in a majority of the leader and follower servers. The leader server and each of the follower servers that committed the message to main memory retain the message in main memory until requested to remove the message from main memory by a delivery agent or an archiver.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08005917&OS=08005917&RS=08005917
owner: Yahoo! Inc.
number: 08005917
owner_city: Sunnyvale
owner_country: US
publication_date: 20080930
---
The disclosed system and method relate to messaging in a network. More specifically the disclosed system and method relate to consensus based messaging in a network.

In a publish subscribe system senders of messages publishers are characterized into classes without the knowledge of how many if any receivers subscribers there may be. Subscribers express interest in one or more classes of messages without knowing how many publishers there are. The subscribers will only receive messages that are of interest to them.

In conventional publish subscribe systems a publisher connected to subscribers through an intermediary or broker . In these systems publishers post messages to the broker and subscribers register subscriptions with the broker. The broker receives messages from the publishers filters the messages to identify the topic or content and then forwards the messages to interested subscribers.

In a typical system messages were required to be delivered to all live subscribers in the order in which the messages were published. The requirement for messages to be delivered in order was to be maintained even despite failures in the system. To protect against failures some conventional systems employ network attached storage that internally replicate the messages using a redundant array of inexpensive disks RAID system or other complex data replication techniques. These conventional systems are big expensive and are difficult to setup.

In one embodiment a system includes a leader server capable of communicating with a plurality of follower servers and a network. The leader server is configured to determine when a message received from a client connected to the network has been committed to main memory in a majority of the leader and follower servers. The leader server and each of the follower servers that committed the message to main memory retain the message in main memory until requested to remove the message from main memory by a delivery agent or an archiver.

In some embodiments a method comprises requesting a plurality of follower servers to commit a message to main memory determining that the message has been published when a majority of the leader and follower servers have committed the message to main memory and maintaining the message in a main memory until receiving a signal from a delivery agent or an archiver to remove the message from the main memory.

In some embodiments a machine readable storage medium is encoded program code. When the program code is executed by a processor the processor performs a method comprising requesting a plurality of follower servers to commit a message to main memory determining that the message has been published when a majority of the leader and follower servers have committed the message to main memory and maintaining the message in a main memory until receiving a signal from a delivery agent or an archiver to remove the message from the main memory.

A processor in a multi processor system receives a message from a client. Each respective processor of the system is coupled to a respective main memory. Multiple processor main memories store copies of the message. As a copy of the message is archived from a main memory the copy of the message stored in that archiving storage is removed from such memory. A data structure is produced that keeps track of the current memory locations of copies of the messages in main memory and archive storage. The message is delivered to a multiplicity of subscribers. Upon completion of delivery of the message to the multiplicity of subscribers the copies of the message are removed from memory and storage locations that currently store the message. The data structure is referenced in the course of such latter removal to determine the memory and storage locations that currently contain the message.

Thus the message is stored in multiple memory locations to provide redundancy in case of failure of one or more processors or other system components. The messages are stored initially in multiple main memories. As copies of the message are archived a data structure is created and is modified to keep track of main memory locations and archive storage locations that currently contain copies of the message. Thus redundancy is achieved through storage of copies of the message in a combination of the main memory locations and archive storage locations. After the message has been delivered over the network to subscribers copies of the message are removed from current memory and storage locations indicated by the data structure.

Moreover in some embodiments the data structure is accessible to a delivery agent. The delivery agent accesses the data structure in the course of removal of the message following completion of delivery to the multiplicity of subscribers. Therefore the delivery agent can effect timely removal of messages that have been stored redundantly but that are no longer required to be stored.

An improved publish subscribe system and method are now described. illustrates one example of a topology of a publish subscribe system . As shown in system includes clients and that communicate with consensus system through an application programming interface API . Consensus system is connected to backing stores through archiving services . Consensus system communicates with subscribers and through a delivery agent .

Clients and subscribers may be computers servers mobile phones personal digital assistants PDAs or other devices that electronically transmit messages. Although only four clients and subscribers are shown in more or fewer clients and subscribers may be implemented. Clients and subscribers may be connected to consensus service through a local area network LAN wide area network WAN or the Internet.

Publish API may be any API including but not limited to Windows API Java API and the like. In one embodiment the publish API is implemented through the Tomcat servlet engine available from the Apache Software Foundation. Clients publish messages to the consensus system through the publish API .

In one embodiment the consensus system is the ZooKeeper service available from Yahoo Inc. of Sunnyvale Calif. However other consensus services including but not limited to Chubby Distributed Lock Service available from Google Inc. of Mountain View Calif. As shown in consensus system includes a plurality of severs and . Note that although three servers are shown in fewer or more servers may be implemented. However the more servers included in consensus system the more fault tolerant the messaging system will be as described below.

An example architecture of a server is illustrated in . As shown in server may include one or more processors which may be connected to a wired or wireless communication infrastructure e.g. a communications bus cross over bar local area network LAN or wide area network WAN . Server may include a main memory e.g. a local or working memory such as a random access memory RAM . Server may also include a secondary memory such as for example a hard disk drive and or removable storage drive representing a floppy disk drive a magnetic tape drive an optical disk drive or the like. Secondary memory is a more persistent memory than main memory and may be used to archive messages or data. The removable storage drive may read from and or write to a removable storage unit . Removable storage unit may be a floppy disk magnetic tape CD ROM DVD ROM optical disk ZIP disk or the like which may written to and or read by removable storage drive . Removable storage unit may include a machine readable storage medium having stored therein computer software and or data.

In some embodiments secondary memory may include other similar devices for allowing computer programs or other instructions to be loaded into server such as a removable storage unit and an interface . An example of such a device and socket includes but is not limited to a USB flash drive and associated USB port respectively. Other removable storage units and interfaces that allow software and data to be transferred from the removable storage unit to server may be used such as SIM cards or MP3 players.

Server may also include a communications interface . Communications interface allows software and data to be transferred between server and external devices such as clients and subscriber shown in . Examples of communications interface may include a modem a network interface such as an Ethernet card a communications port a Personal Computer Memory Card International Association PCMCIA slot and card an IEEE 1394 port or the like. Software and data transferred via communications interface are in the form of signals which may be electronic electromagnetic optical or any other signal capable of being received by communications interface . These signals are provided to communications interface via a communications path or channel. The path or channel that carries the signals may be implemented using wire or cable fiber optics a telephone line a cellular link a radio frequency RF link and the like.

Each of the servers is in communication with the other servers of the consensus system . Servers may be connected via a wired or wireless connection. At startup the servers will elect a leader e.g. server of the consensus system . The remaining servers e.g. servers are known as follower servers. Note that the leader server may change should a fault to the leader server occur or a transmission connection between the leader server and follower servers be disrupted. Each of the servers may maintain a data structure in main memory or secondary memory that identifies the current leader server. In some embodiments the servers comprising consensus system service the clients by establishing connections using a connection oriented protocol such as transmission control protocol TCP with clients . In other embodiments other control protocols including but not limited to connectionless protocols such as real time transfer protocol RTP and user datagram protocol UDP may be used to communicate with clients .

Consensus system may use a Paxos algorithm or like protocol to determine whether a message has been published e.g. the message has been received by the consensus system . For example a follower server may establish a connection with client which may then send a message to follower server . Examples of messages may include but are not limited to sports scores news updates stock market update and like messages that may be communicated from a publisher to a plurality of subscribers. Upon receipt of the message from client follower server forwards the message to leader server which then forwards the message to each of the servers of the consensus system asking them to commit the message to memory. When a follower server commits the message to main memory it sends a message to leader server identifying that the message has been committed to memory. When leader server determines that a majority of the follower servers have persisted a change update the leader server will log the change update as having been persisted and transmit a message to the follower server that received the message from client . In some embodiments the leader server may determine that the message has been published when a predetermined number of servers which may be less than a majority acknowledge that the message has been stored in memory. The predetermined number of servers may be selected to ensure an adequate redundancy of the message has been stored. The message from leader server to follower server instructs follower server to inform client that the message has been published. It is in this manner that consensus system decides in a fault tolerant manner whether a message has been published.

Each of the servers may store data such as copies of the received messages and metadata about the messages in a plurality of data registers not shown as data structures. Additionally each server may maintain a data tree image and transaction logs in main memory . By maintaining the data in main memory consensus system provides a high throughput and low latency of messages compared to conventional systems which store messages to hard disks. Each data structure may include version numbers for data changes access control list ACL changes and timestamps to allow cache revalidations and coordination updates enabling consensus system to maintain the messages received from clients in a consistent order.

Each server maintains current versions of the data structure by communicating an update to the leader server which then advises the follower servers of the update to the data structure. In some embodiments the manner in which an update to a data structure is performed is the same manner a message is published. For example the leader server may receive an update to a data structure which it distributes to each of the follower servers which then signal the leader server when the data structure has been updated. The leader server may identify a data structure as having been updated when all a majority or some predetermined number of the servers acknowledge that the data structure has been updated.

In some embodiments each of the messages received by the consensus system are stored as a data structure underneath a topic data structure. For example a topic FOO may have its own data structure which may include an additional data structure for each message for the topic as illustrated below 

Referring again to archiving services are configured to archive older messages stored on main memory of servers to backing stores . Note that although two archiving processes are shown fewer or more archiving processes may be implemented. In some embodiments the archiving processes are configured to run on the same servers or a subset of the servers as the consensus system . For example a server may have two hard disk drives in a RAID configuration. In some embodiments the archiving processes run on servers or machines that are separate from the servers of the consensus system .

Because messages received by the consensus system are stored in the main memory of servers these messages must be archived to free up space on main memory to ensure that these messages are available to be delivered to slow subscribers. The result is that messages are always stored on multiple nodes either multiple consensus servers multiple archive nodes such as backing stores possibly both or on some combination of backing stores and servers of consensus system . Storing the messages on multiple nodes ensures high reliability and fault tolerance. Additionally increasing the number of archiving processes increases the redundancy of the archived messages enabling the system to tolerate more failures without losing messages.

Backing stores are databases configured to store the messages archived from the consensus system . In one embodiment backing stores are configured with a relational database management system RDBMS . Examples of an RDBMS include but are not limited to MySQL owned by My SQL AB of Sweden and PostgreSQL developed by PostgreSQL Global and available from http www.postgresql.org . In some embodiments backing stores may be a file system. Example file systems include but are not limited to Unix file systems Windows file systems and the like. Each backing store may asynchronously read through the sequence of messages and write them to a local log stored in backing stores . In some embodiments the local log may be managed as a MySQL table using group commit for throughput.

When the message is successfully archived in a backing store a flag may be written into the message in the data structure stored in consensus system . For example assume that backing store denoted as A and backing store denoted as B are each assigned to archive the Topic FOO. The data structure stored in the consensus system may be as follows 

In the example shown above Msg is archived in backing stores A and B Msg is archived in backing store B and Msg is not archived and thus remains stored in the main memory of multiple servers of consensus system . If a message is archived in multiple backing stores such as Msg then it may be removed from the consensus system . For example if archiving service archives Msg to backing store it will update the metadata of Msg stored in consensus system . If archiving service determines that archiving service has already archived a copy of the message to backing store while it is updating the metadata then archiving service may send a message to consensus system to remove the message from main memory . The hierarchy in the consensus system may then be represented as follows 

At this point Msg is guaranteed to be in both backing stores unless there is a failure to either of the backing stores . Metadata about a topic such as the desired redundancy level can also be stored in the topic data structure. The identities of the backing stores can be stored as a set of ephemeral data structures under the topic data structure so that the failure of a backing store can be detected and another backing store can be started to replace the failed backing store . Thus the schema for topic FOO will be 

Delivery agent is configured to read messages from the consensus system and backing stores and deliver them to one or more of the subscribers and . In some embodiments there is only one active delivery agent . In some embodiments there may be multiple delivery agents . However if multiple delivery agents are implemented multiple delivery agents should not be assigned to the same topic or topics. If a delivery agent for a topic starts and another delivery agent is already active for the topic in the consensus system then the new agent should be terminated.

The delivery agent may be a form of code stored in a computer readable storage medium on one or more of the servers of the consensus system or on another server computer or machine that is separate from consensus system . If delivery agent is stored on a separate machine it may gain access to the consensus system by establishing a connection in the same manner in which a client connects to consensus system . For example delivery agent may use a TCP connection through the consensus system API . Once the connection is established delivery agent may request a read or write of data to or from a server of the consensus system . In this manner delivery agent may maintain access to a data structure stored in consensus system .

The consensus system may be configured to store metadata about the delivery agent . For example the top level schema in the consensus system may be as follows 

For each of the subscribers the delivery agent maintains a copy of a delivery pointer not shown and a copy of a consume pointer not shown . The delivery and consume pointers may be a main memory or a secondary memory of one or all of the servers or in the server computer or machine in which the delivery agent is stored. In some embodiments the delivery agent maintains a hash table that identifies all subscribers and lists each of the messages that each of the subscriber has acknowledged receiving. The hash table may be archived to a backing store . The delivery pointer identifies the next message that the delivery agent is to deliver to subscribers and the consumer pointer identifies the last message that was acknowledged by a subscriber for a particular topic. In some embodiments the consume pointer can be stored in the MySQL database and be replicated using lazy MySQL replication.

Delivery agent reads the messages from either the consensus system or the backing stores and sends the messages to each of the subscribers . In some embodiments a server may output a message stored in main memory at which point the delivery agent directs the message to the appropriate subscribers. If a message has been archived then a backing store may output the message from a secondary memory to main memory where it is then output for delivery by delivery agent which directs the message to the appropriate subscribers. After a subscriber receives the message it will send an acknowledgment or consume message to delivery agent at which point delivery agent will update the consume pointer for the client. When the consume pointer for all of the clients for a topic identify that a message has been consumed or acknowledged as having been received by all subscribers for a topic then the delivery agent may send a message requesting that the message be removed from the backing stores the consensus system or both. In some embodiments the delivery agent requests the message be removed from the consensus system and the backing stores . To remove the message from consensus system delivery agent may send a message to one or more of the servers of the consensus system. For example delivery agent may send a message to remove the message from memory to the leader server which will then direct the follower servers to remover the message. In some embodiments delivery agent may send a message to a follower server which forwards the message to leader server which directs each of the servers of the consensus system to remove the message from memory. To remove the message from a backing store delivery agent may send a message to each of the archivers requesting each to remove the message from secondary storage .

With reference to an improved method of consensus based fault tolerant messaging is now described. At block a connection between a client and the consensus system is established. As described above the connection may be a TCP connection between one or more of the clients and a server of consensus system .

At block the consensus system receives a topic based or content based message from a client through the publish API . The server that receives the message from the client forwards the message to the leader server at block . Leader server forwards the received message to each of the follower servers asking each to commit the message to memory.

Each of the servers will store the received message in main memory and report to the leader server when the message has been stored. At block the leader server receives message from a majority of the follower servers that the message has been successfully stored in main memory . When the majority of the follower servers report that the message has been stored leader server instructs the server in communication with the client to inform the client that the message has been published at block .

At block the messages stored in servers of consensus system are archived to backing stores by the archiving services . As described above each backing store may asynchronously read through the sequence of messages and write them to a local log. When the message is successfully archived a flag may be written into the message in the data structure.

At block the delivery agent reads the stored messages from either the consensus system or from one or more of the backing stores . For example delivery agent may first check to see if the message is stored in consensus system . If the message is still stored in consensus system then the delivery agent will read the message from a main memory of one or more of the servers of the consensus system . Alternatively if the message is no longer stored in the consensus system then it will be stored in at least one of the backing stored so the delivery agent may read the message from backing store or backing store

At block delivery agent delivers the message to one or more subscribers . Subscribers may send an acknowledgment or consume message back to delivery agent in response to receiving the message. Once the delivery agent receives the message it may send a message to the consensus system and or archivers identifying that the message has been delivered. The consensus system and backing stores may delete the message from memory when they receive a signal that the message has been received by the one or more subscribers 

In addition to the above described embodiments the disclosed method and system may be embodied in the form of computer implemented processes and apparatus for practicing those processes. The present disclosed method and apparatus may also be embodied in the form of computer program code embodied in tangible storage media such as floppy diskettes read only memories ROMs CD ROMs hard drives ZIP high density disk drives DVD ROMs flash memory drives or any other computer readable storage medium wherein when the computer program code is loaded into and executed by a computer the computer becomes an apparatus for practicing the disclosed method and system. The present disclosed method and apparatus may also be embodied in the form of computer program code for example whether stored in a storage medium loaded into and or executed by a computer wherein when the computer program code is loaded into and executed by a computer the computer becomes a special purpose apparatus for practicing the disclosed method. When implemented on a general purpose processor the computer program code segments configure the processor to create specific logic circuits.

Although the invention has been described in terms of exemplary embodiments it is not limited thereto. Rather the appended claims should be construed broadly to include other variants and embodiments of the invention which may be made by those skilled in the art without departing from the scope and range of equivalents of the invention.

