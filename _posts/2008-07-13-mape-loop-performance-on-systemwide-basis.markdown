---

title: MAPE loop performance on system-wide basis
abstract: A manage-analyze-plan-execute (MAPE) loop is performed on a system-wide basis in relation to subsystems of a computing system based on one or more parameters. Performing the MAPE loop results in a performance level at which each subsystem is to operate to conserve energy utilization on the system-wide basis such that the computing system still satisfies the parameters. The subsystem the performance level at which each subsystem is to operate is communicated to the subsystem. Each subsystem operates at the performance level communicated to the subsystem.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08407711&OS=08407711&RS=08407711
owner: International Business Machines Corporation
number: 08407711
owner_city: Armonk
owner_country: US
publication_date: 20080713
---
The present invention relates generally to performing a manage analyze plan execute MAPE loop and more particularly to performing such a MAPE loop on a system wide basis in relation to a number of subsystems of a computing system specifically for energy conservation purposes.

An autonomic computing system is a computing system that senses its operating environment models its behavior within that environment and takes actions to change the environment or its own behavior. Autonomic computing systems are typically self configuring self healing self optimizing and self protecting. They are self configuring in that such systems have characteristics that enable them to adapt to changing conditions by changing their own configurations and have functionality that permits the addition and removal of components or resources within the system without service disruption.

Autonomic computing systems are self healing in that they have the capacity to recognize and diagnose deviations from normal conditions and to take actions to normalize the conditions as well as the capability to proactively circumvent issues that could cause service disruptions. Such computing systems are self optimizing in that they have the ability to monitor their state and performance and proactively tune themselves to respond to environmental stimuli. Autonomic computing systems are self protecting in that they incorporate intelligence to recognize and circumvent security threats and have the facility to protect themselves from physical harm such as excessive heat or motion.

The monitoring includes receiving one or more events from or regarding the managed element . The analysis includes correlating and analyzing these events to determine if a specific known situation exists. The planning includes determining how if such a situation does exist or is otherwise detecting the situation should be processed or handled via determining one or more actions. The execution thus includes the managed element performing these actions that have been determined.

Such individual performance of the MAPE loop on a subsystem by subsystem basis is problematic however. Individual MAPE loops may make wrong decisions as to what actions for the subsystems to take because they are not informed by events occurring at all the other subsystems . For example a MAPE loop for a processor subsystem may determine that the subsystem should step down its performance level to save energy even though other subsystems have determined that there are processes which will soon require significant processing time by the processor subsystem. Thus as soon as the processor subsystem steps down its performance level it may then be requested to immediately perform processing activity such that the subsystem may immediately have to step back up its performance level to handle these requests. This can be problematic because it may take time for the processor subsystem to step back up its performance level once the performance level has been stepped down.

Furthermore individual performance of the MAPE loop on a subsystem by subsystem basis can result in undesirable latency effects that cause performance degradation and energy conservation degradation as to the computing system a whole. For example all the subsystems may have stepped their performance level down due to minimal activity being performed by the system . Requests that will result in significant activity to be performed by the system as a whole may then be received by a front tier subsystem. Ideally once the front tier subsystem receives these requests all the subsystems immediately step up their performance levels in anticipation of the requests travelling from the front tier subsystem to the middle tier subsystems and finally to the back end subsystems.

However where the MAPE loop is individually performed on a subsystem by subsystem basis this cannot occur. Rather the front tier subsystem first steps up its performance level in response to its own MAPE loop being performed which takes time before the front tier subsystem can adequately process the requests. When the requests are transferred to each tier subsystem down the line this process is repeated with the tier subsystems having to step up their performance levels sequentially until they are able to adequately process the requests. Where each of N subsystems has the same latency n to step its performance level back up after performance of its MAPE loop this means that processing a request after all the subsystems previously have had their performance levels stepped down results in a delay of N times n which can be significant. That is the latency n ripples across the N subsystems sequentially. The ideal case by comparison is that all the relevant subsystems are stepped up at the same time so that just a latency n occurs overall.

The same latency ripping occurs in the reverse situation within the prior art as well. When all the subsystems are operating at stepped up performance levels the back tier subsystem may first step its performance level down in response to less activity being performed as detected by the MAPE loop . Responsive to this each middle tier subsystem in sequence may then step its performance level down until finally the front tier subsystem steps its performance level down. As before where each of N subsystems has the same latency n to step its performance level back down after performance of its MAPE loop this means that reaching an energy efficient state for the computing system as a whole results in a delay of N times n because the latency n ripples across the N subsystems sequentially. The ideal case is again that all the relevant subsystems are stepped down at the same time so that just a latency n occurs overall.

The present invention relates to performing a manage analyze plan execute MAPE loop on a system wide basis. In a method of an embodiment of the invention a MAPE loop is performed on a system wide basis in relation to a number of subsystems of a computing system based on one or more parameters. Performing the MAPE loop results in a performance level at which each subsystem is to operate to conserve energy utilization on a system wide basis such that the computing system still satisfies the parameters. The performance level at which each subsystem is to operate is communicated to the subsystem. Each subsystem operates at the performance level communicated to the subsystem.

A computing system of an embodiment of the invention includes a number of subsystems and a manger component. At least one of the subsystems is implemented in hardware. The manager component is to perform a MAPE loop on a system wide basis in relation to the subsystems based on one or more parameters to result in a performance level at which each subsystem is to operate to conserve energy utilization on the system wide basis such that the computing system still satisfies the parameters. The manager component is to communicate to each subsystem the performance level at which the subsystem is to operate. Each subsystem is then to operate at the performance level communicated to the subsystem by the manager component.

A computing device of an embodiment of the invention includes hardware and a manager component implemented within the hardware. The manager component is to perform a MAPE loop on a system wide basis in relation to a number of subsystems of a computing system based on one or more parameters. This results in a performance level at which each subsystem is to operate to conserve energy utilization on the system wide basis such that the computing system still satisfies the parameters.

An article of manufacture of an embodiment of the invention includes a tangible computer readable medium such as a recordable data storage medium and means in the medium such as one or more computer programs that can be executed by one or more processors of one or more computing devices. The means is for performing a MAPE loop on a system wide basis in relation to a number of subsystems of a computing system based on one or more parameters. Performing the MAPE loop results in a performance level at which each subsystem is to operate to conserve energy utilization on the system wide basis such that the computing system still satisfies the parameters.

In the following detailed description of exemplary embodiments of the invention reference is made to the accompanying drawings that form a part hereof and in which is shown by way of illustration specific exemplary embodiments in which the invention may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention. Other embodiments may be utilized and logical mechanical and other changes may be made without departing from the spirit or scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined only by the appended claims.

There is a manager component which may be implemented in software hardware or a combination of software and hardware. The manager component is depicted in as being separate from the computing system but in another embodiment the component may be part of the computing system . The manager component performs a monitor analyze plan execute MAPE loop . Unlike within the prior art however in which a MAPE loop is performed for each subsystem on a subsystem by subsystem basis as has been described in the background section the MAPE loop performed by the manager component is performed on a system wide basis for all the subsystems of the computing system . In one embodiment the MAPE loop is performed on a system wide basis in the same manner as it is performed on a subsystem by subsystem basis as described in the background section except for the fact that the MAPE loop takes into account all the subsystems as a whole and at least substantially at the same time.

The MAPE loop is performed on the basis of knowledge and more specifically one or more parameters regarding the computing system as a whole and or regarding the individual subsystems of the system . For example these parameters can include the characteristics of a workload running on the computing system and or characteristics of an application computer program running on the system . Other parameters include a required service level agreement SLA of the computing system where the system is managed by one party for another party such that the former party guarantees certain performance levels of the system within such an SLA. Still other parameters include the topology of the computing system e.g. its individual constituent subsystems and how they are connected to one another the historical usage pattern of the system e.g. the system may not be used as frequently at night as compared to during the day workload priority as well as any network of the system . The parameters may be user specified.

Performance of the MAPE loop results in a performance level at which each subsystem is to operate to conserve energy utilization on a system wide basis of the computing system while still satisfying all the parameters in question. For example if performance of the MAPE loop determines that processing centric activities are currently being performed with little storage device access then processing subsystems may operate at 100 performance while storage subsystems may operate at 50 performance while still satisfying an SLA in relation to which the system is operated. In general the performance level at which a subsystem should operate can be expressed in terms of a percentage of its highest performance capability such as 0 25 50 75 100 and so on. Furthermore in general higher performance levels typically equate to higher energy usage such that to maintain energy efficiency the subsystems are directed to operate at performance levels commensurate with the activities that they are currently performing or will soon have to perform.

Because the MAPE loop is performed as to all the subsystems of the computing system on a system wide basis more intelligent decisions as what the performance levels of the individual subsystems should be can be made in comparison to the prior art without latency ripple effects. For example a processing subsystem may not currently be processing any requests but a front tier subsystem may have received a large number of requests that the processing subsystem may have to soon process. Within the prior art a MAPE loop performed in relation to just the processing subsystem itself on a subsystem by subsystem basis may result in the processing subsystem being put in a low performance level state to conserve energy because the processing subsystem is unaware that it will soon be asked to process a large number of requests.

By comparison within the computing system the MAPE loop performed in relation to all the subsystems identifies that the processing subsystem will soon have to process a large number of requests even though it is not currently performing much processing activity. Therefore the result of the MAPE loop is that the processing subsystem is not put into a low performance level state because the processing subsystem will soon be processing a large number of requests. Because the MAPE loop is performed on a system wide basis in other words it is aware of all the activities currently occurring at all the subsystems and can specify performance levels for the individual subsystems by taking into account this information. This is advantageous as it ensures that performance degradation of the computing system will not result by placing a subsystem into a low power performance state only to immediately have to take the subsystem out of that state.

Furthermore performance of the MAPE loop on a system wide basis precludes latency ripple effects. For example if the subsystems are all currently operating at all low performance levels and a front tier subsystem receives a large number of requests a latency ripple sequentially occurring throughout the subsystems does not occur in the computing system as in the prior art. This is because when the front tier subsystem receives a large number of requests performance of the system wide MAPE loop can identify that all the subsystems may have to soon operate at a high performance level. Therefore all the subsystems can be caused to operate at high performance levels at the same time. Rather than having a latency of N subsystems times n latency for each subsystem the result is there is just a latency of n to bring all the subsystems back to the high performance level.

After the manager component performs the single system wide MAPE loop for all the subsystems as opposed to individual MAPE loops for the subsystems as in the prior art the component communicates to each subsystem the performance level at which it should operate as indicated by the dotted arrows . Each subsystem then operates at the performance level that has been communicated to it by the manager component . Two different ways by which the performance levels can be communicated by the manager component to the subsystems are now described in exemplary relation to the subsystems A and N.

First as to the subsystem A the manager component has a plug in installed for the subsystem A. A plug in can generally be considered as an auxiliary computer program that works with a primary computer program to enhance its capability. In this case the plug in permits the manager component to communicate the performance level at which the subsystem A should operate directly to the subsystem A. Thus because the subsystem A has a plug in installed for the subsystem A at the manager component the component directly communicates the performance level at which the subsystem A should operate to the subsystem A via the plug in as indicated by the arrow .

Second as to the subsystem N the manager component communicates the performance level at which the subsystem N is to operate to an operating system of the computing system . An operating system can be considered the master control computer program of the computing system and sets the standards for all application computer programs that run on the system permitting the application computer programs to interface with the underlying subsystems . An example of an operating system is the LINUX operating system for instance.

Installed at the operating system is an application programming interface API for the subsystem N by which the operating system is able to specify the performance level at which the subsystem N is to operate. An API can be considered a language and message format used by one entity such as the operating system to communicate with another entity such as the subsystem N. An API may be implemented via a function call. Thus the operating system takes the performance level at which the subsystem N is to operated as communicated to it by the manager component as indicated by the arrow and calls the API to pass this performance level to the subsystem N as indicated by the arrow .

There may be certain situations in which application computer programs such as the application computer program may wish to override the performance levels of the subsystems dictated by the manager component . For example even though stepping down and stepping up performance levels of the subsystems via performance of the system wide MAPE loop is achieved intelligently and with a minimum latency n the application computer program may decide that its execution is so important that even this minimum latency is too much to incur. Furthermore the application computer program may not be able to communicate this information to the manager component .

Therefore in exemplary relation to the subsystem B the application computer program may call an API of the subsystem B to override the performance level of the subsystem B dictated by the manager component . The API may be the same API or a different API than the API that the operating system may use to communicate the performance level at which the subsystem B is to operate from the manager component to the subsystem B. When the application computer program overrides the performance level of the subsystem B dictated by the manager component the subsystem B may modify its performance level as suggested by the manager component until the application computer program itself tells the subsystem B otherwise.

The manager component performs the MAPE loop on a system wide basis in relation to the subsystems of the computing system based on one or more parameters as has been described. Performance of the MAPE loop results in a performance level at which each subsystem is to operate to conserve energy utilization on the system wide basis such that the computing system as a whole is still able to satisfy the parameters in question. For instance those subsystems that can operate at lower performance levels while not unduly affecting performance of the computing system may be operated at such lower performance levels to conserve energy utilization. By comparison other subsystems that have to operate at higher performance levels to guarantee a minimum performance of the system may not be operated at lower performance levels.

The manager component communicates to each subsystem the performance level at which the subsystem is to operate . In one embodiment parts and can be performed to achieve such communication whereas in another embodiment part can be performed to achieve such communication. In the former case the performance level at which a given subsystem is to operate is communicated to the operating system which communicates this performance level to the subsystem in question via an API for the subsystem . In the latter case the performance level at which a given subsystem is to operate is directly communicated by the manager component to the subsystem in question via a plug in for the subsystem .

In some embodiments an application computer program can call an API of a given subsystem to override the performance level at which the subsystem in question is to operate as communicated by the manger component indirectly via parts and or directly via part . Ultimately then each subsystem operates at the performance level that has been communicated to it either by the manager component indirectly or directly or as overridden by the application computer program . The method can then be repeated one or more times beginning at part as indicated by the arrow .

As has been described embodiments of the invention can be performed in relation to computing systems having different types of subsystems . One type of computing system may have for instance a collection of server devices storage subsystems implemented on separate or a collection of computing devices and so on. In such instance each server device and each subsystem may be considered one of the subsystems . In another embodiment a computing system may be a single computing device having a number of components such as processors memory storage devices like hard disk drives and so on. In such instance each component may be considered one of the subsystems .

As to the former case shows a topology for a representative computing system according to an embodiment of the invention. The subsystems in are each a server device. The server devices are organized in tiers from a first or front tier subsystem A to a second tier subsystem B and ultimately to a last or back tier subsystem N. In this embodiment requests are received from outside the computing system by the first tier subsystem A and travel to the last tier subsystem N for ultimate processing to generate responds which travel back up to the first tier subsystem A for communication back outside the system . The embodiment of is thus presented to show that the computing system can be made up of a number of separate server devices as the subsystems and organized over a number of tiers.

As to the latter case shows a topology for a representative computing system according to another embodiment of the invention. The computing system in is itself a single computing device. The computing device includes processors memory storage devices as well as other components. Thus the subsystems in include a processor subsystem A a memory subsystem B a storage device subsystem C as well as subsystems N for other components. The embodiment of is presented to show that the computing system can be a single computing device having a number of components that correspond to the subsystems of the computing system .

Furthermore shows a representative rudimentary computing device to specify how the manager component can be implemented according to an embodiment of the invention. The computing device includes hardware such as a tangible computer readable medium like a recordable data storage medium. Examples of recordable data storage media include hard disk drives and semiconductor memories for instance. The manager component is thus implemented as one or more computer programs stored on the computer readable medium . The programs as stored on the medium of the hardware can be considered the means that implements the functionality that has been ascribed to the component .

It is finally noted that although specific embodiments have been illustrated and described herein it will be appreciated by those of ordinary skill in the art that any arrangement calculated to achieve the same purpose may be substituted for the specific embodiments shown. This application is intended to cover any adaptations or variations of embodiments of the present invention. Therefore it is manifestly intended that this invention be limited only by the claims and equivalents thereof.

