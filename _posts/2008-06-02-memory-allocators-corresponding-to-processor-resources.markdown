---

title: Memory allocators corresponding to processor resources
abstract: A memory allocator is provided for each processor resource in a process of a computer system. Each memory allocator includes a set of pages, a locally freed list of objects, and a remotely freed list of objects. Each memory allocator requests the pages from an operating system and allocates objects to all execution contexts executing on a corresponding processing resource. Each memory allocator attempts to allocate an object from the locally freed list before allocating an object from the remotely freed list or an allocated page.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08291426&OS=08291426&RS=08291426
owner: Microsoft Corporation
number: 08291426
owner_city: Redmond
owner_country: US
publication_date: 20080602
---
Processes that execute in a computer system typically request memory allocations to perform work using the memory. These processes may execute in computer systems with more than one processing resource where one or more memory allocators may be provided to process the memory requests. Each process may be configured to taking advantage of available concurrency on a computer system by allowing different parts of the process to be executed on different processing resources simultaneously. In a process that uses such concurrency managing the locality of memory accesses may be of importance to the performance of the process. In addition accessing one or more memory allocators from multiple processing resources typically involves including synchronization mechanisms which adds significant overhead to the memory allocation process.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

A memory allocator is provided for each processor resource in a process of a computer system. Each memory allocator includes a set of pages a locally freed list of objects and a remotely freed list of objects. Each memory allocator requests the pages from an operating system and allocates objects to all execution contexts executing on a corresponding processing resource. Each memory allocator attempts to allocate an object from the locally freed list before allocating an object from the remotely freed list or an allocated page.

A processing resource of a locally allocated object may cause the object to be freed by adding the object to the locally freed list of the memory allocator corresponding to the processing resource. A processing resource of a remotely allocated object may cause the object to be freed by adding the object to the remotely freed list of the memory allocator that allocated the object i.e. the memory allocator corresponding to another processing resource .

In the following Detailed Description reference is made to the accompanying drawings which form a part hereof and in which is shown by way of illustration specific embodiments in which the invention may be practiced. In this regard directional terminology such as top bottom front back leading trailing etc. is used with reference to the orientation of the Figure s being described. Because components of embodiments can be positioned in a number of different orientations the directional terminology is used for purposes of illustration and is in no way limiting. It is to be understood that other embodiments may be utilized and structural or logical changes may be made without departing from the scope of the present invention. The following detailed description therefore is not to be taken in a limiting sense and the scope of the present invention is defined by the appended claims.

It is to be understood that the features of the various exemplary embodiments described herein may be combined with each other unless specifically noted otherwise.

Runtime environment represents a runtime mode of operation in a computer system such as embodiments A and B of a computer system shown in and described in additional detail below where the computer system is executing instructions. The computer system generates runtime environment from a runtime platform such as a runtime platform shown in and described in additional detail below.

Runtime environment includes an least one invoked process an operating system OS a set of hardware threads M where M is an integer that is greater than or equal to one and denotes the Mth hardware thread M a resource management layer and a memory system . Runtime environment allows tasks from process to be executed along with tasks from any other processes that co exist with process not shown using OS resource management layer and hardware threads M . Runtime environment operates in conjunction with OS and or resource management layer to allow process to obtain processor and other resources of the computer system e.g. hardware threads M .

Runtime environment includes a scheduler function that generates scheduler . In one embodiment the scheduler function is implemented as a scheduler application programming interface API . In other embodiments the scheduler function may be implemented using other suitable programming constructs. When invoked the scheduler function creates scheduler in process where scheduler operates to schedule tasks of process for execution by one or more hardware threads M . Runtime environment may exploit fine grained concurrency that application or library developers express in their programs e.g. process using accompanying tools that are aware of the facilities that the scheduler function provides.

Process includes an allocation of processing and other resources that host one or more execution contexts viz. threads . Process obtains access to the processing and other resources in the computer system e.g. hardware threads M from OS and or resource management layer . Process causes tasks to be executed using the processing and other resources.

Process generates work in tasks of variable length where each task is associated with an execution context in scheduler . Each task includes a sequence of instructions that perform a unit of work when executed by the computer system. Each execution context forms a thread or analogous OS concept such as child process that executes associated tasks on allocated processing resources. Each execution context includes program state and machine state information. Execution contexts may terminate when there are no more tasks left to execute. For each task runtime environment and or process either assign the task to scheduler to be scheduled for execution or otherwise cause the task to be executed without using scheduler .

Process may be configured to operate in a computer system based on any suitable execution model such as a stack model or an interpreter model and may represent any suitable type of code such as an application a library function or an operating system service. Process has a program state and machine state associated with a set of allocated resources that include a defined memory address space. Process executes autonomously or substantially autonomously from any co existing processes in runtime environment . Accordingly process does not adversely alter the program state of co existing processes or the machine state of any resources allocated to co existing processes. Similarly co existing processes do not adversely alter the program state of process or the machine state of any resources allocated to process .

OS manages processing and other resources of the computer system and provides a set of functions that allow process and other processes in the computer system to access and use the components. In addition OS offers execution contexts to scheduler and process and allocates pages of memory system to scheduler and process in conjunction with memory allocators . OS may allocate pages of memory system in any suitable fixed or variable sizes e.g. pages of 4 kilobytes KB to 64 KB .

Hardware threads reside in execution cores of a set or one or more processor packages e.g. processor packages shown in and described in additional detail below of the computer system. Each hardware thread is configured to execute instructions independently or substantially independently from the other execution cores and includes a machine state. Hardware threads may be included in a single processor package or may be distributed across multiple processor packages. Each execution core in a processor package may include one or more hardware threads .

Resource management layer allocates processing resources to process by assigning one or more hardware threads to process . Resource management layer exists separately from OS in the embodiment of . In other embodiments resource management layer or some or all of the functions thereof may be included in OS .

Memory system includes any suitable type number and configuration of volatile or non volatile storage devices configured to store instructions and data. The storage devices of memory system represent computer readable storage media that store computer executable instructions including process OS and resource management layer . The instructions are executable by computer system to perform the functions and methods of process OS and resource management layer described herein. Examples of storage devices in memory system include hard disk drives random access memory RAM read only memory ROM flash memory drives and cards and magnetic and optical disks.

Process implicitly or explicitly causes scheduler to be created via the scheduler function provided by runtime environment . Scheduler instance may be implicitly created when process uses APIs available in the computer system or programming language features. In response to the API or programming language features runtime environment creates scheduler with a default policy. To explicitly create a scheduler process may invoke the scheduler function provided by runtime environment and specifies a policy for scheduler .

Scheduler interacts with resource management layer to negotiate processing and other resources of the computer system in a manner that is transparent to process . Resource management layer allocates hardware threads to scheduler based on supply and demand and any policies of scheduler .

In the embodiment shown in scheduler manages the processing resources by creating virtual processors that form an abstraction of underlying hardware threads . Scheduler includes the set of virtual processors N . Scheduler multiplexes virtual processors onto hardware threads by mapping each virtual processor to a hardware thread . Scheduler may map more than one virtual processor onto a particular hardware thread but maps only one hardware thread to each virtual processor . In other embodiments scheduler manages processing resources in other suitable ways to cause instructions of process to be executed by hardware threads .

Prior to executing tasks scheduler obtains execution contexts and from runtime environment or operating system . Available virtual processors locate and execute execution contexts and to begin executing tasks. The set of execution contexts in scheduler includes a set of execution contexts N with respective associated tasks N that are being executed by respective virtual processors N a set of zero or more runnable execution contexts and a set of zero or more blocked i.e. wait dependent execution contexts . Each execution context and includes state information that indicates whether an execution context and is executing runnable e.g. in response to becoming unblocked or added to scheduler or blocked. Execution contexts that are executing have been attached to a virtual processor and are currently executing. Execution contexts that are runnable include an associated task and are ready to be executed by an available virtual processor . Execution contexts that are blocked include an associated task and are waiting for data a message or an event that is being generated or will be generated by another execution context or .

Each execution context executing on a virtual processor may generate in the course of its execution additional tasks which are organized in any suitable way e.g. added to work queues not shown in . Work may be created by using either application programming interfaces APIs provided by runtime environment or programming language features and corresponding tools in one embodiment. When processing resources are available to scheduler tasks are assigned to execution contexts or that execute them to completion or a blocking point e.g. waiting for a message or a stolen child task to complete on virtual processors before picking up new tasks. When a task unblocks the task is re scheduled to execute on an available virtual processor possibly with priority given to choosing a virtual processor on the hardware thread where it executed before blocking in the hope that the memory hierarchy viz. cache hierarchy already contains data that can be optimally reused. An execution context executing on a virtual processor may also unblock other execution contexts by generating data a message or an event that will be used by another execution context .

Each task in scheduler may be realized e.g. realized tasks and which indicates that an execution context or has been or will be attached to the task and the task is ready to execute. Realized tasks typically include unblocked execution contexts and scheduled agents. A task that is not realized is termed unrealized. Unrealized tasks e.g. tasks may be created as child tasks generated by the execution of parent tasks and may be generated by parallel constructs e.g. parallel parallel for begin and finish . Scheduler may be organized into a synchronized collection e.g. a stack and or a queue for logically independent tasks with execution contexts i.e. realized tasks along with a list of workstealing queues for dependent tasks i.e. unrealized tasks as illustrated in the embodiment of described below.

Upon completion blocking or other interruption e.g. explicit yielding or forced preemption of a task associated with an execution context running on a virtual processor the virtual processor becomes available to execute another realized task or unrealized task . Scheduler searches for a runnable execution context or an unrealized task to attach to the available virtual processor for execution in any suitable way. For example scheduler may first search for a runnable execution context to execute before searching for an unrealized task to execute. Scheduler continues attaching execution contexts to available virtual processors for execution until all execution contexts of scheduler have been executed.

Scheduler causes memory to be allocated to tasks of execution contexts executing on virtual processors using memory allocators . In response to detecting memory allocation requests virtual processors of scheduler initiate corresponding memory allocators . Each memory allocator allocates all memory to tasks of all execution contexts that execute on a corresponding virtual processor . Accordingly each memory allocator may allocate memory to the same or different tasks of the same or different execution contexts at different times.

Each memory allocator includes an array of one or more pools of memory pages . As shown in each pool includes one or more memory pages allocated from OS a locally freed list and a remotely freed list . Each pool includes a set of allocated unallocated and freed objects . Each object includes a header and allocated memory . Each header includes information that identifies the virtual processor and or memory allocator that allocated the corresponding allocated memory .

Locally freed list identifies objects of pages in a pool that have been freed locally by a corresponding virtual processor i.e. the virtual processor that that corresponds to the memory allocator that includes the pool . Each locally freed list is only accessible by the corresponding virtual processor . Because each virtual processor executes only one execution context at any given time the execution context being executed is the only execution context that may cause locally freed list to be accessed i.e. two execution contexts cannot access locally freed list simultaneously . Accordingly a virtual processor accesses locally freed list without using synchronization mechanisms.

Remotely freed list identifies objects of pages in a pool that have been freed remotely by a non corresponding virtual processor i.e. a virtual processor that that does not correspond to the memory allocator that includes the pool . Each remotely freed list is accessible by all non corresponding virtual processors . Accordingly accesses to a remotely freed list use synchronization mechanisms such as atomic swapping of remotely freed list with locally freed list to prevent a corresponding and a non corresponding virtual processor from accessing remotely freed list simultaneously.

In one embodiment each pool of the array is used for a fixed size of allocated objects shown in . Array of pools is indexed by the size of objects allocated from each page . Accordingly locally freed list and remotely freed list do not need to be ordered according to size in this in this embodiment. In other embodiments array of pools may be organized into any another suitable configuration.

Memory allocators request and receive pages from OS independently from one another and separately allocate objects to all tasks of all execution contexts executing on corresponding virtual processors . OS may provide pages in predefined sizes of memory such as page sizes of 4 kilobytes KB to 64 KB to memory allocators . Memory allocators allocate objects from pages for internal data structures not shown of scheduler and tasks associated with execution contexts executing on virtual processors . Each memory allocator attempts to allocate objects from a corresponding locally freed list before allocating objects from a corresponding remotely freed list or unallocated objects . By doing so memory allocators may enhance memory locality effects.

In one embodiment memory allocators may allocate objects in accordance with object sizes requested for use by internal data structures of scheduler not shown and tasks . In other embodiments memory allocators may allocate objects in other suitable ways. For object sizes larger than a full page memory allocators may cause memory to be allocated in one or more whole pages from OS .

Respective virtual processors N own respective memory allocators N as indicated by respective arrows N . Each memory allocator N only allocates objects to tasks associated with execution contexts executing on respective virtual processors N . Memory allocator only allocates objects to tasks associated with execution contexts executing on virtual processor memory allocator only allocates objects to tasks associated with execution contexts executing on virtual processor and so on. Accordingly respective memory allocators N are associated exclusively with respective virtual processors N .

When a task frees an allocated object the virtual processor executing the task determines whether the memory allocator corresponding to the virtual processor allocated the object . In one embodiment the virtual processor makes this determination using header . In another embodiment the virtual processor makes this determination using an indirect association with the information of header to minimize storage of header or to optimize cache locality of header . If the memory allocator corresponding to the virtual processor allocated the object i.e. the object was allocated locally then the virtual processor causes the allocated object to be freed by adding the object to the locally freed list of the corresponding memory allocator . If the memory allocator corresponding to the virtual processor did not allocate the object i.e. the object was allocated remotely then the virtual processor causes the allocated object to be freed by adding the object to the remotely freed list of the memory allocator that allocated the object . Freed objects may be later reallocated by corresponding memory allocators .

Additional details of allocating and freeing objects will now be described with reference to the embodiments of respectively.

Once a request is detected a determination is made by memory allocator as to whether a locally freed object is available as indicated in a block . Memory allocator accesses locally freed list and determines whether an object that is suitable for the allocation request e.g. is of a suitable size appears on locally freed list . If a locally freed object is available then memory allocator allocates a locally freed object from locally freed list as indicated in a block .

If no objects are present in locally freed list i.e. locally freed list is empty then a determination is made by memory allocator as to whether a remotely freed object is available as indicated in a block . Memory allocator accesses remotely freed list and determines whether an object that is suitable for the allocation request e.g. is of a suitable size appears on remotely freed list . If a remotely freed object is available then memory allocator atomically swaps remotely freed list with locally freed list as indicated in a block and allocates a remotely freed object from the locally freed list as indicated in a block . Memory allocator atomically swaps the empty locally freed list for the given size of object with the remotely freed list for the same size of object .

If no objects are present in locally freed list i.e. locally freed list is empty or remotely freed list i.e. remotely freed list is empty a determination is made by memory allocator as to whether an object is available in an allocated page as indicated in a block . Memory allocator determines whether an object that is suitable for the allocation request e.g. is of a suitable size appears on an allocated page . If an object is available in an allocated page then memory allocator allocates an object from the allocated page as indicated in a block .

If an object is not available in an allocated page then memory allocator requests a new page from OS as indicated in a block and allocates an object from the newly allocated page as indicated in a block .

If the object was allocated remotely then the owning memory allocator is identified as indicated in a block . An object is allocated remotely if the virtual processor executing the task that is attempting to free the object does not correspond to the memory allocator that allocated the object . Virtual processor examines the header of the object to identify the virtual processor and or memory allocator that owns the object . The object is added to the remotely freed list of the owning memory allocator as indicated in a block .

In the above embodiments scheduler may operate as a cooperative scheduler where process and other processes are associated with virtual processors in a controlled way. In other embodiments scheduler may operate as another type of scheduler such as a preemptive scheduler.

Although one instance of scheduler was shown in the embodiment of other embodiments may include other instances of scheduler that each includes a different memory allocator for each virtual processor .

The above embodiments may advantageously provide scalability increase memory locality minimize allocator overhead including synchronization overhead and limit the inefficiencies caused by the number of memory allocators in a process.

When compared to methods that associate a memory allocator with each thread the number of memory allocators may be reduced while still realizing the reduction in synchronization overhead that such methods may provide. The number of memory allocators in the above embodiments correspond to the number of virtual processors and the number of virtual processors is typically bound by the number of hardware threads in a computer system. If the average inefficiency factor of a given memory allocator implementation is C then the overall inefficiency of a process is P n n C where n is the number of allocators in the process. With one allocator per thread there is no upper bound on P since there is no upper bound on the number of threads. With one allocator per processing resource however an upper bound is placed on Pby limiting n to the number of available processing resources multiplied by the number of scheduler instances which is typically one or another small number. Thus the above embodiments may result in a reduction in program wide inefficiency.

In one embodiment process organizes tasks into one or more schedule groups and presents schedule groups to scheduler . is a block diagram illustrating an embodiment of a schedule group for use in a scheduler .

Schedule group includes a runnables collection a realized task collection a work collection and a set of zero or more workstealing queues . Runnables collection contains a list of unblocked execution contexts . Scheduler adds an execution context to runnables collections when an execution context becomes unblocked. Realized task collection contains a list of realized tasks e.g. unstarted agents that may or may not have associated execution contexts . Scheduler adds a realized task to realized task collection when a new unstarted task is presented to scheduler by process . Work collection contains a list of workstealing queues as indicated by an arrow and tracks the execution contexts that are executing tasks from the workstealing queues . Each workstealing queue includes one or more unrealized tasks .

Using the embodiment of scheduler may first search for unblocked execution contexts in the runnables collection of each schedule group in scheduler . Scheduler may then search for realized tasks in the realized task collection of all schedule groups before searching for unrealized tasks in the workstealing queues of the schedule groups .

In one embodiment a virtual processor that becomes available may attempt to locate a runnable execution context in the runnables collection or a realized task in the realized task collection in the schedule group from which the available virtual processor most recently obtained a runnable execution context i.e. the current schedule group . The available virtual processor may then attempt to locate a runnable execution context in the runnables collections or a realized task in the realized task collection in the remaining schedule groups of scheduler in a round robin or other suitable order. If no runnable execution context is found then the available virtual processor may then attempt to locate an unrealized task in the workstealing queues of the current schedule group before searching the workstealing queues in the remaining schedule groups in a round robin or other suitable order.

As shown in computer system A includes one or more processor packages memory system also shown in zero or more input output devices zero or more display devices zero or more peripheral devices and zero or more network devices . Processor packages memory system input output devices display devices peripheral devices and network devices communicate using a set of interconnections that includes any suitable type number and configuration of controllers buses interfaces and or other wired or wireless connections.

Computer system A represents any suitable processing device configured for a general purpose or a specific purpose. Examples of computer system A include a server a personal computer a laptop computer a tablet computer a personal digital assistant PDA a mobile telephone and an audio video device. The components of computer system A i.e. processor packages memory system input output devices display devices peripheral devices network devices and interconnections may be contained in a common housing not shown or in any suitable number of separate housings not shown .

Processor packages include hardware threads M . Each hardware thread in processor packages is configured to access and execute instructions stored in memory system . The instructions may include a basic input output system BIOS or firmware not shown OS also shown in a runtime platform applications and resource management layer also shown in . Each hardware thread may execute the instructions in conjunction with or in response to information received from input output devices display devices peripheral devices and or network devices .

Computer system A boots and executes OS . OS includes instructions executable by hardware threads to manage the components of computer system A and provide a set of functions that allow applications to access and use the components. In one embodiment OS is the Windows operating system. In other embodiments OS is another operating system suitable for use with computer system A.

Resource management layer includes instructions that are executable in conjunction with OS to allocate resources of computer system A including hardware threads as described above with reference to . Resource management layer may be included in computer system A as a library of functions available to one or more applications or as an integrated part of OS .

Runtime platform includes instructions that are executable in conjunction with OS and resource management layer to generate runtime environment and provide runtime functions to applications . These runtime functions include a scheduler function as described in additional detail above with reference to . The runtime functions may be included in computer system A as part of an application as a library of functions available to one or more applications or as an integrated part of OS and or resource management layer .

Each application includes instructions that are executable in conjunction with OS resource management layer and or runtime platform to cause desired operations to be performed by computer system A. Each application represents one or more processes such as process as described above that may execute with scheduler that uses local collections N as provided by runtime platform .

As noted above memory system includes any suitable type number and configuration of volatile or non volatile storage devices configured to store instructions and data. The storage devices of memory system represent computer readable storage media that store computer executable instructions including OS resource management layer runtime platform and applications .

Memory system stores instructions and data received from processor packages input output devices display devices peripheral devices and network devices . Memory system provides stored instructions and data to processor packages input output devices display devices peripheral devices and network devices .

Input output devices include any suitable type number and configuration of input output devices configured to input instructions or data from a user to computer system A and output instructions or data from computer system A to the user. Examples of input output devices include a keyboard a mouse a touchpad a touchscreen buttons dials knobs and switches.

Display devices include any suitable type number and configuration of display devices configured to output textual and or graphical information to a user of computer system A. Examples of display devices include a monitor a display screen and a projector.

Peripheral devices include any suitable type number and configuration of peripheral devices configured to operate with one or more other components in computer system A to perform general or specific processing functions.

Network devices include any suitable type number and configuration of network devices configured to allow computer system A to communicate across one or more networks not shown . Network devices may operate according to any suitable networking protocol and or configuration to allow information to be transmitted by computer system A to a network or received by computer system A from a network.

In the embodiment of each processor package R and respective memory device R form a node. The nodes are interconnected with any suitable type number and or combination of node interconnections .

Each processor package includes a set of hardware threads where each hardware thread includes an L1 level one cache not shown . Each processor package also includes a set of L2 level two caches that correspond to respective hardware threads . Each processor package further includes an L3 level three cache available to the set of hardware threads a system resource interface a crossbar switch a memory controller and a node interface . System resource interface provides access to node resources not shown . Crossbar switch interconnects system resource interface with memory controller and node interface . Memory controller connects to a memory device . Node interface connects to one or more node interconnections .

In other embodiments each processor package may include other configurations and or numbers of caches. For example each hardware thread may include two or more L1 caches in other embodiments and the L2 and or L3 caches may or may not be shared in other embodiments. As another example other embodiments may include additional caches e.g. a level four L4 cache or fewer or no caches.

Although specific embodiments have been illustrated and described herein it will be appreciated by those of ordinary skill in the art that a variety of alternate and or equivalent implementations may be substituted for the specific embodiments shown and described without departing from the scope of the present invention. This application is intended to cover any adaptations or variations of the specific embodiments discussed herein. Therefore it is intended that this invention be limited only by the claims and the equivalents thereof.

