---

title: Optimized database appliance
abstract: A system and method from processing database queries allows for cost and locale based distribution for execution of database queries. The database queries are executed on execution engines that provide flexible configuration and overlapping functionality. The system reduces various costs, including elapsed time, required to perform database queries. The method provides processing of a database query using a database catalog comprising database table locality information, record locality information and execution engine information. A query optimizer receives the query and accesses the catalog to create a query execution plan comprising locality-based database operations. A central database operation processor providing a first execution engine executes the query execution plan by performing at least a portion of the locality-based database operations and distributing at least a portion of the locality-based database operations as a subplan. A second database operation processor providing a second execution engine executes the subplan received from the central processor.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07921130&OS=07921130&RS=07921130
owner: Netezza Corporation
number: 07921130
owner_city: Framingham
owner_country: US
publication_date: 20081015
---
This application is a continuation of U.S. application Ser. No. 11 332 704 filed Jan. 13 2006 now U.S. Pat. No. 7 464 106 which is a continuation of U.S. application Ser. No. 10 145 571 filed May 13 2002 now U.S. Pat. No. 7 010 521.

This invention relates generally to systems for processing database queries and more specifically to systems for processing database queries by creating a distributed locality cost based execution plan.

Database management systems DBMS manage data records stored on storage devices. The data is often distributed across multiple storage devices organized by a server and accessible over a network. A client presents a query the DBMS processes the query and returns results to the client. These systems suffer from performance problems related to processor speed memory speed disk access time and network bandwidth.

Attempts to solve the performance problem of query processing in a distributed database environment can be generally characterized by three approaches fat pipe clustering and active disk.

The fat pipe solutions simply attempt to increase overall query processing performance by increasing the network bandwidth. No modification is made to the query processing itself data and instructions just travel over a faster communications medium. Storage systems manufactured by EMC Corporation of Hopkinton Mass. exemplify this approach.

Clustering of computers to perform database queries allows for parallel processing to occur. Parallel computer architectures generally fall into two categories symmetric multiprocessing SMP and massively parallel processing MPP . SMP systems contain multiple processors which share the same memory containing one copy of the computer application and data. SMP systems reduce transaction time by dividing computer operations into tasks which execute on assigned processors. MPP systems contain many processors each with their own memory containing a copy of a portion or all of the computer application and or data. Each processor can then work independently on its portion of the data to reduce overall transaction time. In database systems data records can be striped across multiple disk drives connected to these clusters of computers. Each computer in a cluster can process parts of the database query on a group of data records that it has access to. Where fat pipe solutions attack the database query performance problem by increasing network bandwidth clustering solutions attack the database query performance problem by increasing processing capacity. Oracle Corporation of Redwood Shores Calif. provides an SMP clustering solution in its Oracle 8i database product.

A third approach to improving database query processing performance is active disk technology. Active disks combine computer processing with physically linked disk drives to provide parallel processing of database queries that also reduces the load on the communications network. This processing approach may be used to provide database management functionality on a processor closely linked to the database storage device. Conventional and proposed active disk systems only provide fixed functionality query processors and do not provide overlapping query processor functionality. Systems exemplifying this approach are described below.

A thesis by Eric Reidel entitled Active Disks Remote Execution for Network Attached Storage Technical Report CMU CS 99 177 Pittsburgh Pa. November 1999 describes disks with programmable processors. The thesis evaluates scan based algorithms that allow for parallel processing of database queries. Reidel proposes a massively parallel processing MPP system using identical processors.

Another active disk system is described in a paper entitled Active Disks Programming Model Algorithm and Evaluation by Acharya University of California Santa Barbara Uysal University of Maryland College Park and Saltz University of Maryland College Park Proceedings of the 8th International Conference on Architectural Support for Programming Languages and Operating Systems ASPLOS VIII October 1998 . The paper by Acharya et. al. describes a concept of moving processing closer to disks but lacks a teaching of an overall database architecture outlining where and how all database operations are organized and optimally executed.

The 1999 doctoral dissertation by Kimberly Keeton of the University of California Berkeley entitled Computer Architecture Support for Database Applications describes the concept of intelligent disks or idisks as an array of intelligent disk controllers. The idisks are user programmable processors and memory packaged on a disk storage device coupled to a high speed communications link. The idisks in the array can work together to provide parallel processing of database queries. Keeton only describes a single type of processor with non overlapping functionality. Alternative future architectures are hinted at but not enabled. Also at the University of California Berkeley research on introspective storage or istore has been done by David Patterson http istore.cs.berkeley.edu . Istore proposes the concept of intelligent disk bricks configured on a chassis having redundant components for self healing . Each brick comprises a CPU memory redundant network interfaces and a disk drive. The istore architecture is similar to Keeton s idisk architecture and suffers from many of the same deficiencies.

Commercial attempts to provide improved speed for processing database queries include a system sold by Teradata a division of NCR Corporation. Teradata provides classic MPP database query processing systems having two classes of fixed function processors. The first class of processors are Access Module Processors AMP which execute queries in parallel. The AMPs access data on directly attached disk drives. The second class of processors are InterFace Processors IFP . The IFPs parse optimize and direct execution of the database queries executed on the AMPs. IFPs and AMPs are connected using a redundant tree shaped interconnect. The AMP IFP architecture provides only one type of execution processor without overlapping functionality capability and no multiple locale optimization or execution options. Teradata has evolved the hardware based AMP IFP architecture into a hardware software architecture with nodes running software versions of the AMP and IFP processors but the AMP and IFP processes still are restricted to the fixed functionality of the hardware based architecture. The software based architecture also does not provide multiple locale optimization or execution options.

The present invention provides a system for processing database queries that allows for cost and locale based distributed execution of database queries on a parallel multi locale processing system. The database queries are processed on execution engines that provide flexible processor configuration and overlapping processor functionality. The system reduces various costs including elapsed time required to perform database queries.

The present invention provides a system for processing a query on a database having a database catalog comprising database table locality information record locality information execution engine information and characteristics of the various system components such as processing capabilities and performance. A query optimizer receives the query and accesses the database catalog to create a query execution plan comprising locality based database operations. A first execution engine associated with a central database operation processor executes the query execution plan by performing at least a portion of the locality based database operations and distributing at least a portion of the locality based database operations to a second database operation processor as a subplan. A second execution engine associated with a second database operation processor executes the subplan received from the central database operation processor. At least one of the database operations can be executed on either the central execution engine or the second execution engine. Data is stored on a storage unit connected to the second database operation processor which stores at least a portion of database tables and records. A portion of the database tables and records may also be stored on a storage unit connected to the central database operation processor. A data communications network connects the central database processor to the second database operation processor.

Configurations of the present invention allow for the execution engines to process data received from the storage unit or the data communications network as a data stream. An execution engine can comprise any combination of processing devices and any number of processing devices. In one embodiment of the present invention an execution engine comprises a general purpose CPU and a Field Programmable Gate Array FPGA . Alternate configurations of the execution engines comprise various combinations of one or more of a general purpose CPU an FPGA an Application Specific Integrated Circuit ASIC a Digital Signal Processor DSP micro controller or other similar device. The processing devices can be arranged in a symmetric multiprocessing SMP massively parallel processing MPP or other configuration.

The present invention provides for many dynamic operations. In one particular embodiment a query optimizer tracks performance results from queries and reallocates the database tables and records in order to create improved query execution. The execution engines dynamically track performance results from queries and modify the query execution plan to improve performance. The execution engines can also be dynamically reconfigured to process different database operations.

The present invention provides for storage units to be divided into sections and having locality information comprising a section indicator indicating which section stores a database table and a distribution indicator indicating whether the database table is striped or broadcast to the section.

In one embodiment each Snippet Processing Unit SPU includes a respective Snippet Processing Unit Controller and a respective storage unit . In an alternative embodiment each Snippet Processing Unit Controller may be associated with more than one storage unit . Each SPU is coupled to the central database operation processor through a data communication network . This array of SPUs can be considered an MPP configuration.

The central database operation processor manages tables for the database stored in the plurality of storage units. Routines for managing and accessing records stored in the database are stored in central database operation processor memory and portions of the database can be copied from the storage units and stored in central database operation processor memory or local storage. The central database operation processor receives database queries from the client transmitted over a data communications network . A network interface component in the central database operation processor receives the database queries. A network interface component may be a network interface card switch or router Fibre Channel transceiver InfiniBand enabled device or other device programmed to transmit and receive messages according to standardized data network protocols.

A Central Processing Unit CPU in the central database operation processor processes a received database query and forwards pieces of a query execution plan through a network interface component over the data communications network to the SPU storing the requested record. In an alternative embodiment there are multiple CPUs configured as an SMP array. The piece of the query execution plan forwarded to either the central database operation processor execution engine or SPU for processing is referred as to a snippet . The snippet can include database operations such as join sort aggregate restrict reject expression evaluation statistical analysis or other operations. Database requests can be processed more efficiently by off loading some of the processing from the central database operation processor to the SPUs.

Data records from one logical database table may be stored across multiple data storage units using various distribution techniques. Storage units may be grouped into sections each section consisting of one or more storage units. Sectioning is a logical grouping that may vary from table to table i.e. the sectioning scheme associated with a given table may differ from the sectioning scheme associated with another table. The data records may be replicated to all data storage units in a broadcast fashion. Alternately certain data records may be stored on specific data storage units or sections in a striping fashion. The stored data records may also be mirrored across multiple data storage units or sections for improved reliability and performance. A portion of the database tables and records may also be stored on a storage unit connected to the central database operation processor. The distribution technique may vary from table to table.

A central database operation processor is connected to multiple SPUs through data communications network . There can be multiple central database operation processors and data communication networks in a database appliance .

A parser validator parses and checks the syntax of the database query . If the syntax is correct a query tree is produced otherwise an error is generated. A query optimizer takes the query tree as input and generates an execution plan using metadata from the primary database catalog . A system optimizer monitors data storage and execution plan history in order to update the primary database catalog . Parts or snippets of the execution plan are distributed and executed on various execution engines e.g. central database operation processor execution engine SPU execution engine and programmable execution engine within database appliance . The results of executing the snippets are aggregated on a central database operation processor execution engine e.g. central database operation processor execution engine and made available for further processing e.g. printing .

Query optimizer scheduler creates an optimal execution plan by combining possible ways of scanning and joining tables that appear in the database query to achieve the desired result. The execution plan comprises basic database processing steps including scan data project restrict sort hash group join distribute return broadcast etc. Each step can be comprised of multiple substeps. Each step is associated with a locale i.e. an execution engine . The query optimizer scheduler then chooses the optimal execution plan based on the processing capabilities performance characteristics and current workloads of various components and the locality of the data records within the database appliance . Query optimizer scheduler can alter the query tree both in form adding deleting rearranging database operations and in content modifying database operations in addition to assigning operations to various execution engines. Query optimizer scheduler calculates the optimal query plan based on the lowest cost by considering possible steps possible locales and data table access variants for each step of the execution plan.

Primary database catalog stores the performance characteristics of the various components and the locality of the data records within the database appliance . Performance characteristics include for example availability of specific database operations on the execution engine execution engine processor speed execution engine processor cache configuration data disk speed data disk controller speed data disk controller interface bandwidth memory availability memory speed network interface controller speed and network interface bandwidth. Locality information includes for example database table definitions storage methods and database record count statistics. Using these characteristics and locality information query optimizer can analyze the costs associated with various execution plans in order to choose the optimal plan. The optimal execution plan is generally considered to be the execution plan that will take the least elapsed time to complete. Other definitions of optimal e.g. minimize I O seeks or fabric traffic are possible and the query optimizer can be configured to create execution plans based on alternative definitions of optimal.

Execution plan is comprised of various parts snippets . Snippets represent one or more database operations e.g. scan restrict project join etc. and a locality designation e.g. central database operation processor SPU etc. . The snippets are distributed to the various execution engines for processing and the results are aggregated at a designated central database operation processor execution engine. The ability to distribute the snippets to different execution engines provides for parallel processing of database query . Parallel processing reduces the elapsed time to process the database query .

System optimizer monitors data storage usage and requirements. As execution plans are executed statistics are gathered and maintained in primary database catalog . System optimizer maintains various usage statistics in order that query optimizer can adjust execution plans for optimal execution. System optimizer is also used to detect reallocation thresholds and participates in planning for reallocation of data when certain predefined thresholds are met.

System components can also dynamically reconfigure the execution plan created by the system optimizer and redirect operations to other components as needed. For example if an SPU cannot perform an assigned operation for any reason such as running out of memory the SPU can change the assigned locality e.g. to the central database operation processor of that operation and any dependent subsequent operations in the execution plan. Similarly a central database operation processor could direct operations to SPUs.

Central database operation processor execution engine provides processing for database operations including the aggregation of data from other database operations. Central database operation processor is connected to the SPU through data communication network .

SPU receives a SPU execution plan containing database operations to be performed on data records stored on data storage units . SPU execution engine executes the database operations and or distributes them to a programmable execution engine . SPU execution engine accesses a supplemental database catalog to determine optimal distribution in the same fashion the query optimizer accesses primary database catalog . The resulting programmable engine execution plan is then executed on programmable execution engine .

In one particular embodiment the programmable execution engine is a field programmable gate array FPGA configured specifically to perform certain database operations. Using input parameters the FPGA can be reconfigured to perform various database operations selected from a set of preprogrammed operations. In another embodiment programmable execution engine is an application specific integrated circuit ASIC specifically configured to perform various database operations. Both the FPGA and ASIC implementations allow for increased speed in the execution of database operations as actual database instructions can be implemented in hardware firmware. In other embodiments various combinations of general purpose CPU FGPA and ASIC as well as other processing devices can be used to implement the SPU execution engine and programmable execution engine without departing from the scope of the present invention.

The various execution engines within database appliance provide for distributing database functionality in a flexible manner. Each execution engine can execute many different database operations. The configuration of the database engines is maintained in the database catalogs thereby allowing the query optimizer to adjust the generation of execution plans as the configuration of the execution engines changes. This flexibility allows for the dynamic reconfiguration of database records and database operations to suit specific performance needs without affecting the underlying operation of query optimizer . In this way database operations are executed by the most appropriate execution engine. For example for a given query record scanning may be done on SPU execution engine restriction operations may be done on programmable execution engine and join operations may be done on central database operation processor execution engine .

The present invention provides overlapping functionality of database operations as the various execution engines central database operation processor execution engine SPU execution engine programmable execution engine provide a subset of database operations. Because database appliance provides for flexible distribution of database operation execution multiple execution engines can provide overlapping functionality that is invoked based on external factors e.g. data record layout number of data records performance characteristics of the execution engines etc. .

As an aid in the illustration of a distribution of a basic execution plan a store sales database example will be presented that defines a Sales data table and a SalesDetail data table as follows 

A sample query on the store sales database may be show me the total units and dollar amount of rain gear sold in North Carolina in 2000 by month. This can be translated into the SQL statement 

The output from the sample query showing the total units and dollar amount of rain gear sold in North Carolina in 2000 by month can be shown in tabular format 

In a particular embodiment the database appliance can be configured to provide for the distribution of database operation execution for this query as follows 

While this invention has been particularly shown and described with references to preferred embodiments thereof it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the scope of the invention encompassed by the appended claims. A database query is meant to encompass all data manipulation e.g. select insert delete and update and data definition e.g. create table drop table instructions.

