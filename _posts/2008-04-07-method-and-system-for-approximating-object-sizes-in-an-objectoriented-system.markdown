---

title: Method and system for approximating object sizes in an object-oriented system
abstract: A method and system for increasing a system's performance and achieving improved memory utilization by approximating the memory sizes that will be required for data objects that can be deserialized and constructed in a memory cache. The method and system may use accurate calculations or measurements of similar objects to calibrate the approximate memory sizes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08095766&OS=08095766&RS=08095766
owner: International Business Machines Corporation
number: 08095766
owner_city: Armonk
owner_country: US
publication_date: 20080407
---
This invention is related to object oriented programming environments and more particularly to the calculation of the memory size required by an object.

An object oriented programming OOP environment is often described as a collection of cooperating objects as opposed to a traditional programming environment in which a program may be seen as a group of tasks to compute subroutines or functions . In OOP a class is an abstract construct that provides a blueprint defining the nature of an object that can be created. Objects are therefore created based on the structure and definitions of a particular class. In object oriented programming an object is an instance or instantiation of a class and multiple objects instances can be created based on the same class. The class object contains a combination of data and the instructions that operate on that data making the object capable of receiving messages processing data and sending messages to other objects. A cluster is essentially a group of classes or objects.

In the context of data storage and transmission serialization is the process of converting an object into a binary form that can be saved on a storage medium such as disk storage device or transmitted across a network. The series of bytes in the serialized binary form of the object can be used to re create an object that is identical in its internal state to the original object actually a clone . The opposite of serialization known as deserialization is the process by which the serialized binary form can be constructed into an instance of the object.

Each object created in memory requires a portion of the available memory capacity. Some of that memory portion may be comprised of information imposed by the programming environment overhead and some of which may be consumed by data contained in the object. Many related objects with references between each other are often used to represent a complex piece of data and are known as clusters. Each constituent object in the cluster can require its own portion of available memory.

The performance cost of reading data from disk storage devices such as hard disk drives is typically much higher than accessing the same data in random access memory RAM . This is generally contributed to the slow mechanical nature of magnetic disk drives and the slower data transmission paths from a hard disk drive to the memory and microprocessor components. For this reason higher performance memory such as RAM is typically used as a memory cache or data buffer for data that is stored on the hard disk drive. This technique increases performance when the same data is accessed repeatedly and minimizes performance delays caused by retrieving data directly from the hard disk drive when it is required for a processing task. There is an added delay when retrieving objects from the hard disk drive because they are stored in a serialized binary form and must first be deserialized i.e. constructed into instantiated objects before processing. Processing performance is therefore significantly increased when run time instances of deserialized objects are cached in memory before they are needed as opposed to being stored only on the hard disk drive in a serialized representation of object clusters.

However storage capacity on a hard disk drive is typically much cheaper and far greater than that of memory. There is therefore a limited amount of high performance memory available to be used as a memory cache or data buffer. In determining how to best allocate available memory to maximize performance it is generally necessary to first determine how much memory will be required by objects under consideration for being deserialized and instantiated in memory.

There is therefore a need to increase performance by deserializing and creating run time objects in the memory cache but in order to optimize utilization of the limited memory capacity there is a need to first determine how much memory will be required and used by the objects before any such creation takes place.

One simple solution is to calculate accurately the exact amount of memory required for each and every object but the calculations and processing required to analyze each and every object to make such a determination can prove excessively expensive in performance terms and thus detracts from the benefits of any subsequent performance gains.

By way of example an Extensible Markup Language XML document that has been parsed can be represented as a cluster of related objects. The Document Object Model DOM application programming interface allows for navigation of such XML documents wherein the tree of node objects represent the document s content. However DOM implementations tend to be memory intensive because the entire document must be loaded i.e. deserialized into memory as a tree of objects before access is allowed. Thus the exact amount of memory required for the tree cluster of objects cannot be determined without deserializing it into run time objects in memory at least temporarily. This is therefore an expensive operation in terms of performance.

Another possible solution is to cache the object representations in memory in their serialized format creating the run time object instances only when they are needed and discarding them from memory as soon as they are no longer required. This strategy is essentially a half way approach because by copying only the serialized object representations to memory no calculations of the size of actual run time instances of the objects are required. However performance is only partially improved because each time objects need to be processed the serialized object representations must first be deserialized into the actual run time instances of objects which decreases performance. Thus while the calculation of the required memory cache for serialized objects is simple and this technique improves performance as opposed not having objects in the memory cache altogether it will not perform as well as situations where the actual run time instances of objects are created in the memory cache.

It is therefore the case that where the size of a piece of data on the hard drive disk is known i.e. the serialized representation of an object but the size of its representation in memory is much harder to compute i.e. the run time instance of the object it becomes very hard to optimize the use of memory cache where there is a fixed maximum capacity of cache. There is a need for a method that increases performance and achieves an optimal utilization of available memory by deserializing actual run time instances of objects into memory but without having to first calculate the exact amount of memory required for each and every object.

The invention provides a new method and system for calculating memory size requirements of an object comprising having a new object that is to be stored in a memory cache or a data buffer further having a known memory size of a similar object with at least one fixed size component using the known memory size of the similar object to approximate a memory size required by the new object by determining that at least one fixed size component of the new object is equivalent in size to a respective fixed size component of the similar object calculating an actual size of any variable sized components of the new object approximating the total size of the new object wherein if there is space in the memory cache or data buffer available for the new object storing the new object in the memory cache or the data buffer.

Additional features and advantages are realized through the techniques of the present invention. Other embodiments and aspects of the invention may be recognized from the description and the drawings.

The detailed description explains embodiments of the invention together with the advantages and features by way of example with reference to the drawings which are not necessarily drawn to scale.

Typically the in memory size is some function of the complexity of the data. For example in a trivial case the size of a string of characters can be calculated from the length of the string multiplied by the size of a character in addition to the overhead of the string itself. For example a six character string like Hello where each character is represented by an 8 bit value will simply be 6 multiplied by 8 for a total of 48 bits plus the overhead. In a less trivial case the size of a table of integers can be calculated if you know the number of integers. For even more complex cases the principle still holds that there is a viable approximation function calculated based on the content of the data and its structure in memory in addition to any overhead.

There are known algorithms which calculate with reasonable accuracy but subject to certain limitations the memory size of objects in Java based objects. For example see Vladimir Roubtsov Sizeof for Java Object sizing revisited JavaWorld.com Dec. 26 2003 . The claimed advantage of the embodiment of the invention is an algorithm that performs better than the known algorithms by using approximations based on specific measurements. In particular these measurements can be obtained through the use of an instrumented Java Virtual Machine JVM .

The embodiment of the invention depends upon being able to calculate the approximate size of a composite object in a relatively accurate fashion. The more accurate the calculation the better the anticipated increase in performance and optimal memory utilization.

An embodiment of the invention can be applied to an object such as a Java Message Service JMS message as illustrated in . While applicable to many other types of data sought to be placed in memory cache an embodiment of the invention is clearly suitable to object oriented data such as that used by a JMS provider that is executed within a JVM. In basic terms the Java Message Service API is a messaging standard that allows application components based on the Java 2 Platform Enterprise Edition J2EE to create send receive and read messages. It enables distributed communication that is loosely coupled reliable and asynchronous. A complete description of JMS is available on the Sun Microsystems website. 

An empty JMS message is quite complex but its accurate size in memory can be established by measurement or calculation. This can be viewed as a base value for the approximate size of any JMS message so the approximate size of any JMS message will typically be related to this accurate size of an empty message. For any JMS message which contains only a single string the relationship between the empty message and the string message is very simple so it is both accurate and cheap to calculate the approximate size of a string message. The memory consumed by metadata associated with the message such as message properties and header values can be accounted for in a similar way.

For example the approximation calculation for a JMS TextMessage object of unknown size can recognize that the object is comprised of a properties portion and the data payload portion . A typical JMS TextMessage object might be known to have a properties portion consisting of a fixed size portion A plus Z elements of a fixed size B plus a variable length string C and another variable length string E and if the payload itself is a character string of length E. From examination of the structure of similar JMS objects a formula may be derived such as JMS TextMessage object size size of size of fixed overhead of string length of string fixed overhead of string length of string fixed overhead of string length of string .

A modified version of the JVM with instrumentation for memory allocation techniques can contain an embodiment of the invention to establish the values of the fixed parts of the JMS TextMessage . This can simplify the approximation formula to JMS TextMessage size size of size of fixed overhead of string length of string fixed overhead of string length of string fixed overhead of string length of string constant size length of strings and

Having obtained these measurements it is possible to know the memory allocation required for all fixed parts of the JMS TextMessage before run time and the only related task needed at run time can simply be querying the lengths of strings C D and E and inputting those values into the above formula . This last run time calculation is quick simple and has little impact on performance.

A similar technique can be used with regard to many other types of objects. As in JMS messages even complex objects can be broken down into their fixed and variable portions and approximation calculations can be used to determine the size of a significant part of the object limiting the analysis of the object itself to only the remaining parts. The embodiment thereby provides a method and system for optimizing memory cache utilization which in turn increases system performance while minimizing the performance costs associated with calculating the actual memory size required for copying or creating particular data or objects in memory.

The computing device can comprise any general purpose computing article of manufacture capable of executing computer program code installed thereon e.g. a personal computer server handheld device etc. . However it is understood that the computing device is only representative of various possible equivalent computing devices that may perform the processes described herein. Similarly the computer infrastructure is only illustrative of various types of computer infrastructures for implementing the invention. For example in one embodiment the computer infrastructure comprises two or more computing devices e.g. a server cluster that communicate over any type of communications link such as a network a shared memory or the like to perform the process described herein.

The memory allocation approximations described in the embodiment can be executed by a computing device wherein a serialized representation of the object can be stored in the storage system or received from other storage mediums or from a network via I O interface which is connected to the I O device . Processor can analyze the representation of the object to determine if the object can or should be deserialized and placed into a memory cache or data buffer in memory . An embodiment of the invention as described can be implemented to approximate the size calculations. This can increase overall system performance by decreasing the complexity and scope of the object analysis required by the processor while retaining an optimal allocation of available memory .

