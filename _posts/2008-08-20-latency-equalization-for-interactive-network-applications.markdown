---

title: Latency equalization for interactive network applications
abstract: A network configuration that supports latency-equalization (LEQ) routing by effectively “storing” packets on communication links, rather than at end points. A suitable network configuration is found by (i) identifying a candidate pool of routers through which the participating client terminals and application servers can exchange packets intended for LEQ routing and (ii) analyzing the delay inventory corresponding to the network paths connecting the client terminals and application servers, through those routers. Based on the analysis, M routers from the candidate pool are selected to serve as hub nodes. Each participating client terminal is assigned m of these M hub nodes and, thereafter, directs and receives its packets intended for LEQ routing through one of these m hub nodes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07885271&OS=07885271&RS=07885271
owner: Alcatel-Lucent USA Inc.
number: 07885271
owner_city: Murray Hill
owner_country: US
publication_date: 20080820
---
The present invention relates to networks and more specifically to communication protocols for controlling latency differences for network clients.

This section introduces aspects that may help facilitate a better understanding of the invention s . Accordingly the statements of this section are to be read in this light and are not to be understood as admissions about what is in the prior art or what is not in the prior art.

The increasing availability of broadband access to consumers and the ease with which network based applications can be defined spawned a new generation of network users. Today the end user expects the network to serve as an interactive medium for multimedia communications and or entertainment. These expectations fuel the development of new network applications in the entertainment and business sectors. In the entertainment sector such new applications involve multiple users participating in a single interactive session such as on line gaming or on line music playing in a virtual distributed orchestra. The commercial sector offers new interactive services such as telepresence and live bidding in e commerce.

One requirement that an interactive application might have for the underlying network is latency bounds. However when the application has multiple users mere latency bounds no longer suffice because the user experience can be severely impacted by latency differences among the interacting users. For example in online gaming differences in lag experienced by different players can significantly reduce the entertainment value of the game and game servers often enable the participants to vote out and exclude the players having relatively high lag times. In e commerce latency differences between different pairs of shopping and pricing agents can result in price oscillations leading to an unfair advantage to those pairs of agents whose latency differences are relatively small.

A typical prior art approach to latency equalization is to perform it an end point e.g. at the client or at the application server. Latency equalization at the client is based on hardware and software enhancements that speed up the processing of event updates and application rendering. However these techniques are unable to appropriately compensate for the network based delay differences. Latency equalization by the application server requires estimating the state of the network. However such estimation has limitations because the application server usually infers the state of the network from the behavior of the applications being run even if the network related issues are not the dominating factor in that behavior. Also the overhead of network measurements and latency equalization processing tend to impose a significant additional burden on the server s CPU which might unacceptably reduce the maximum number of users that the server can accommodate at the same time.

Problems in the prior art are addressed by a network configuration that supports latency equalization LEQ routing by effectively storing packets on communication links rather than at end points. A suitable network configuration is found by i identifying a candidate pool of routers through which the participating client terminals and application servers can exchange packets intended for LEQ routing and ii analyzing the delay inventory corresponding to the network paths connecting the client terminals and application servers through those routers. Based on the analysis M routers from the candidate pool are selected to serve as hub nodes. Each participating client terminal is assigned m of these M hub nodes and thereafter directs and receives its packets intended for LEQ routing through one of these m hub nodes. In one embodiment the analysis employs a greedy heuristic each step of which identifies a router that can provide a maximum number of acceptable LEQ routing paths for the eligible client terminals. Advantageously LEQ routing with just a single hub node per client terminal m 1 is capable of reducing the average delay difference with respect to that achieved under prior art routing by as much as about 80 . Furthermore the LEQ routing with m 2 or 3 is advantageously capable of substantially avoiding spikes in the delay differences induced by transient congestion of pertinent transmission links.

According to one embodiment a method of routing packets between a set of two or more client terminals and an application server all connected to a network comprises the steps of A selecting M hub nodes from a pool of candidate hub nodes each candidate hub node being a router of said network where M is a positive integer B for each client terminal in said set assigning m of the M hub nodes for LEQ routing where m is a positive integer and m M C instructing an edge router connected to a client terminal belonging to said set to direct packets designated for LEQ routing from the client terminal to the application server through at least one of the m hub nodes assigned to the client terminal and D instructing an edge router connected to the application server to direct packets designated for LEQ routing from the application server to the client terminal through at least one of the m hub nodes assigned to the client terminal.

According to another embodiment a network comprises a network management server and a plurality of interconnected routers adapted to route packets between a set of two or more client terminals and an application server all connected to the network. The network management server is adapted to A select M hub nodes from a pool of candidate hub nodes each candidate hub node being a router of said network where M is a positive integer and B for each client terminal in said set assign m of the M hub nodes for LEQ routing where m is a positive integer and m M C instruct an edge router connected to a client terminal from said set to direct packets designated for LEQ routing from the client terminal to the application server through at least one of the m hub nodes assigned to the client terminal and D instruct an edge router connected to the application server to direct packets designated for LEQ routing from the application server to the client terminal through at least one of the m hub nodes assigned to the client terminal.

Through experimentation and simulation we have determined that it is beneficial to enable the network as opposed to just the end nodes to handle application delay imbalances for different users. For example according to one embodiment of our latency equalization LEQ method the network configures a small number e.g. two to six of its nodes to serve as hub nodes. A hub node can be an enhanced router capable of providing network services pertinent to the application in question. Alternatively a hub node can be a stand alone network appliance. The network selects and assigns a subset of the hub nodes to each client running a latency imbalance sensitive application. The network then steers the packets corresponding to different clients through the respective assigned hub nodes thereby reducing delay differences among the clients. In one configuration the hub nodes are used to steer packets away from congested links. Advantageously our LEQ routing architecture can coexist with and leverage prior art latency capping network functions.

In the configuration of network uses an Open Shortest Path First OSPF routing protocol which directs traffic from client terminals and to server through low latency paths. Thus packets from client terminal are directed through the following path R R R R which has a cumulative delay of 3 ms. Similarly packets from client terminal are directed through the following path R R R R which has a cumulative delay of 10 ms. As a result there is a 7 ms delay difference between client terminals and . As already explained above this delay difference can be a significant problem for both the application server and the client terminals running real time interactive applications. More details on the OSPF routing protocol can be found e.g. in a book by John T. Moy entitled OSPF Anatomy of an Internet Routing Protocol published by Addison Wesley in 1998 which is incorporated herein by reference in its entirety.

In the routing configuration shown in latency equalization is achieved through intelligent selection and assignment of hub nodes which is described in more detail below e.g. in reference to . As a result edge router R has two paths to edge router R R R R and R R R each of which has a cumulative delay of 10 ms. Similarly edge router R has two paths to edge router R R R R and R R R each of which also has a cumulative delay of 10 ms. As can be seen in network uses routers R and R to direct packets corresponding to client terminal through a longer path than the OSPF path of thereby equalizing the cumulative propagation delays for the two client terminals.

There are three hub nodes i.e. R R and R in the routing configuration of . Each of client terminals and is assigned two of these hub nodes. One reason for assigning multiple hub nodes to a client terminal is that the overall network delay experienced by a client has at least two parts propagation delay and queuing delay. If pertinent links of network are not significantly congested then the overall delay is dominated by the propagation delay. However if some links become heavily congested then the queuing delay on those links might become a significant factor in the overall delay. Having two or more hub nodes assigned to each client terminal enables the network to direct packets through an alternative less congested path thereby substantially taking the queuing delay out of the picture.

To be able to bypass congestion an edge router first identifies application packets based on the port number and the IP addresses of the application server s . The edge router then duplicates the packets in accordance with the number of hub nodes assigned to the client by the LEQ routing protocol and sends the duplicate packet copies through tunnels to the corresponding hub nodes. The hub nodes then tunnel the copies to the destination edge router. The destination edge router uses the first received copy and discards the subsequently received copies. Application identification and packet tunneling are known in the art and are normally supported by conventional network routers.

Yet another part of the delay experienced by a client is the network access delay. The term network access delay refers to the delay on the link between a network client e.g. one of client terminals and and server and the corresponding edge router e.g. R R or R . The network access delay generally depends on the access technology employed by the client and the traffic load on the corresponding link. For example for dial up cable and asymmetric digital subscriber line ADSL representative network access delays are 182 ms 19 ms and 15 ms respectively. The network access delay is generally negligible for the fiber optic service FIOS technology. In one embodiment the LEQ routing protocol takes into account the disparity in the network access delays by grouping clients into latency subgroups and providing latency equalization within each of those subgroups independently.

Each routing protocol installs in data plane a corresponding forwarding table also often referred to as the forwarding information base FIB that caters to the routing requirements of the application. For example each of LEQ routing protocols can have a different target delay value. Note that each of forwarding tables receives route computations both from the corresponding LEQ routing protocol and from the IPv4.

A packet classifier is used to identify different packet types within each application to enable hub node to apply customized routing to the packets. Packets from different applications are identified by the source and destination IP addresses. Within an application packets are classified based on the port number or application tag. For example packets in a gaming application can be classified into game setup packets and interactive event packets based on their tags. In the initial game setup e.g. when the player downloads a map from the gaming server the corresponding setup packets are preferably routed using shortest path e.g. OSPF routing to reduce the download time. However during the interactive portion of the game interactive event packets are routed using the LEQ routing.

Data plane can optionally use application level packet processing . For example for online gaming packet processing can be used to aggregate event updates and inspect packets for virus signatures. For a distributed live concert packet processing can be used for sound amplification echo cancellation and distortion compensation. Packet processing can further be used to provide the applications with an opportunity to reconsider their packet destinations on the fly. For example in peer to peer P2P games hub node can perform load balancing among multiple application servers and aid in server migration by doing a soft hand off between the servers wherein packets are sent to both the old server and the new server until the migration process is completed.

For at least some of the following reasons hub node enables LEQ routing to be deployed relatively easily and inexpensively 1 deployment of only a few hub nodes is sufficient to support LEQ routing which minimizes the cost of rollover from non LEQ to LEQ routing 2 incremental deployment of hub nodes is permissible because deployment of a single hub node can significantly reduce the delay disparity see and 3 no modification of the underlying routing protocols is necessary because the LEQ function can be implemented as an overlay on substrate routers.

In the description that follows we first formulate in mathematical terms the problem of selecting and assigning hub nodes for the purpose of LEQ routing. We then describe two representative methods algorithms for solving this mathematical problem. Each of the two methods can be implemented in a network analogous to network .

In the problem formulation we assume that each client terminal and application server is connected to the network through its corresponding edge router e.g. as shown in . The connection can be through regular Internet routing or through tunneling. Each edge router is adapted to measure or infer the delay between itself and each of its clients. For example in network edge router R knows the delay between itself and client terminal . Similarly edge router R knows the delay between itself and client terminal and edge router R knows the delay between itself and server . If an edge router has more than one client e.g. more than one client terminal or more than one application server or a combination of client terminals and application servers then the edge router can sort its clients into groups based on their access delay. For example an edge router can sort its clients into four groups corresponding to the following access delay intervals expressed in ms 0 30 30 60 60 100 100 . For the purpose of LEQ routing access delay a p for client p is then defined as the median delay of the corresponding client group.

Without loss of generality we consider each client group to be a single client of the corresponding edge router. We denote d u v u v V as the propagation delay of the underlying network path between routers u and v. Note that d u v is defined over router set V which includes at least three subsets 1 subset Vof application server edge routers 2 subset Vof client terminal edge routers and 3 subset Vof routers that are candidate hub nodes. It is assumed that d u v d v u .

Suppose that there are a total of Napplication servers on the network. For each client terminal p these Nservers are sorted to select r servers having the r shortest propagation delays with respect to the client terminal. The r selected servers form a group denoted S.

Ddenotes a maximum delay that each client terminal can tolerate on its paths to and from the servers in group S. M denotes the number of routers that can serve as hub nodes which number is preferably greater than one. We require that each LEQ client be connected to m hub nodes where 1 m M.

Given M m r and D the LEQ algorithm is supposed to find for each client terminal p a corresponding set Hof m hub nodes that provide propagation paths characterized by minimum delay differences with respect to the propagation paths found for other participating client terminals. Eq. 1 defines a parameter that can be used to quantify this LEQ search 

Constraint 2 states that the total number of hub nodes cannot exceed M. Constraint 3 states that each client terminal can only select its hub nodes from subset V. Constraint 4 states that each client terminal is assigned at least m hub nodes. Constraint 5 states that the maximum delay cannot exceed D. Constraint 6 specifies that pair wise delay differences cannot exceed . Constraint 7 indicates that yand xare binary variables. Note that constraint 6 is meaningful only if x 1 and x 1. Otherwise it is trivially true.

Method begins at step in which a table hereafter referred to as the delay bound catalog cataloging the minimum and maximum delays for each of the candidate hub nodes is compiled. It is relatively straightforward for the network management server to create a delay bound catalog e.g. by inspecting for each of the candidate hub nodes delay values corresponding to the routes from different client terminals through that particular candidate hub node to the application server. In one embodiment the delay bound catalog has three columns 1 network ID of the candidate hub node denoted hid 2 minimum delay over the set of participating client terminals denoted mnd and 3 maximum delay over the set of participating client terminals denoted mxd.

At step the delay bound catalog is sorted to create two sorted copies. The first sorted copy B is a copy sorted by the values of mnd in the ascending order. The second sorted copy B is a copy sorted by the values of mxd also in the ascending order.

At step the i th row of table Bis selected. Then starting from the top row of table Band going down the first M entries whose mnd is greater than the mnd of the i th entry in table Bare determined. These M entries form a list denoted L.

At step it is determined whether the last row of table Bhas been processed. If not then the processing of method is returned to step . Otherwise the processing continues on to step .

At step an array of values produced in different instances of step is inspected to determine the index i corresponding to the smallest in the array. Candidate hub nodes listed in list Lrepresent a solution to our problem and as such are selected as the M hub nodes for LEQ routing.

At step the edge routers corresponding to the participating client terminals are notified by the network management server about the selection. Thereafter when a client s packet arrives at the corresponding edge router the edge router will analyze the packet s header to determine whether the packet is intended for LEQ routing. If it is then the edge router will direct the packet to the corresponding application server through one of the hub nodes. As already indicated above traffic load and link congestion considerations might be factored in while selecting a particular one of the M hub nodes for handling the packet.

One skilled in the art will appreciate that method employs a greedy heuristic designed to select hub nodes from the available pool of candidate hub nodes so that each participating client terminal is appropriately covered at least m times. As used herein the term cover means a network path from a client terminal through a hub node to the application server that satisfies the LEQ routing constraints such as those expressed by Eqs. 2 7 . Although greedy algorithms often fail to find the globally optimal solution due to their tendency to make commitments to certain choices too soon it is generally known that they work reasonably well in the field of network routing and are capable of finding good approximations to the globally optimal solution.

At step of method for each client terminal a corresponding delay table is compiled. The delay table lists delays corresponding to the network paths from the client terminal to the application server s through different candidate hub nodes. If there are q participating client terminals then q delay tables are compiled. For example in reference to the delay table corresponding to client terminal might contain three entries with delays corresponding to the network paths from edge router R to edge router R through candidate hub nodes R R and R respectively. Similarly the delay table corresponding to client terminal might contain three entries with delays corresponding to the network paths from edge router R to edge router R through candidate hub nodes R R and R respectively. Since there are two participating client terminals i.e. client terminals and a total of two delay tables are compiled.

At step the delay tables corresponding to different client terminals are merged together to form a delay inventory based on which a suitable LEQ routing configuration can be selected. The delay inventory is then sorted by the delay values in the ascending order. The sorted delay inventory is truncated to remove entries with delay values exceeding D. Recall that Dis the maximum delay that can be tolerated by the client terminals on their connections to the application server s . The truncated inventory is denoted A.

At step the i th entry of the truncated delay inventory is selected. Then for each of the other entries of the truncated delay inventory the delay difference between the i th entry and that other entry is calculated. The calculated delay differences are sorted to determine the minimum delay difference .

At step a greedy cover selection algorithm is executed to compile a list denoted L. Each entry in list Lidentifies a different one of the candidate hub nodes selected from the pool of available candidate hub nodes. The greedy cover selection algorithm compiles list Lby adding one entry at a time using the values of D and m as parameters.

To determine the first entry for list L the algorithm finds a candidate hub node that provides for the participating client terminals a maximum number of covers whose pair wise delay differences do not exceed D 2. In this context a cover is a network path from a participating client terminal through the candidate hub node to the application server. Each cover is counted toward the goal of finding m covers for the corresponding client terminal. The covers corresponding to the first entry of list Lare evaluated to determine 1 the minimum delay D and 2 the maximum delay difference among them. The candidate hub node selected to be the first entry in list Lis removed from consideration in the subsequent steps of the algorithm.

To determine the second entry for list L the algorithm finds among the remaining candidate hub nodes a candidate hub node that provides for the not yet fully covered client terminals a maximum number of covers for which the minimum delay is not greater than Dand the pair wise delay differences do not exceed . A client terminal is considered to be fully covered if and only if it has at least m valid covers provided by the previously entered into list Lcandidate hub nodes. The candidate hub node selected to be the second entry for list Lis removed from consideration in the subsequent steps of the algorithm. Each cover corresponding to the second entry is counted toward the goal of finding m covers for the corresponding client terminal.

To determine each subsequent entry for list L the algorithm substantially repeats the processing corresponding to the second entry albeit with some modifications. For example one modification is that the pool of available candidate hub nodes is shrinking due to the removal from consideration of the previously listed candidate hub nodes. Another modification is that some of the client terminals might become fully covered and as such are no longer considered when the covers provided by the next selected candidate hub node are counted.

At step it is determined whether the last entry of truncated inventory Ahas been processed. If not then the processing of method is returned to step . Otherwise the processing continues on to step .

At step an array of values produced in different instances of step is inspected to determine the index i corresponding to the smallest in the array. Candidate hub nodes listed in list Lrepresent an acceptable approximate solution to our problem provided that the number of entries in list Ldoes not exceed M. If there are several lists Lcharacterized by the same smallest then the list characterized by the smallest Dis selected from those several lists.

At step the edge routers corresponding to the participating client terminals are notified by the network management server about the selection of hub nodes. More specifically each edge router receives a hub node assignment listing the m hub nodes that provide the m covers for the corresponding client terminal in list L. Thereafter when a client s packet arrives at the corresponding edge router the edge router will analyze the packet s header to determine whether the packet is intended for LEQ routing. If it is then the edge router will direct the packet to the corresponding application server through one of the m assigned hub nodes. The edge router s corresponding to the application server s are similarly notified about the hub node assignments and thereafter direct packets intended for LEQ routing through the hub nodes specified in the hub node assignments.

As can be seen in LEQ routing with a single hub node per client terminal m 1 is capable of reducing the average delay difference with respect to that achieved with OSPF routing by as much as about 80 . It is also clear that the best performance for LEQ routing is achieved when the number of hub nodes per client terminal is set to one m 1 . As the number of hub nodes per client terminal increases the average delay difference increases which is due to the increased path diversity. However even with three hub nodes per client terminal the LEQ routing performs significantly better than the OSPF routing in terms of the average delay differences.

Further analysis of our simulation results revealed that LEQ routing achieves smaller delay differences at the expense of increasing the average delay for the terminals to which relatively short paths are available in the network. However for many applications the absolute delay is not as critical as the delay difference as long as the absolute delay is capped by a relatively short value of D. This observation highlights the major concept underlying LEQ routing LEQ routing effectively stores packets on network paths rather than buffering them at various end points with the lengths of packet storing paths selected so as to achieve relatively small delay differences among the participating client terminals.

For typical traffic load conditions network links operate at lower than about 50 utilization. However it has been observed that in the presence of bursty or long range dependent traffic average link utilization can spike up to about 95 of the link s capacity. For such spikes packet queuing times significantly contribute to the overall network delay.

Background traffic matrices for the simulation were generated by analyzing packet traces of Abilene Netflow data and scaling the volume down by 1000 times to match the 10 Mbps link capacity because in reality the Abilene link bandwidth is about 10 Gbps. Extra traffic was added as necessary to the link between Denver and Kansas City to create a traffic bottleneck at that link.

The various curves shown in depict the maximum delay difference for the OSPF and LEQ routing when there is a traffic load spike on the link between Denver and Kansas City. The results show that when the amount of traffic begins to approach the link s capacity both the OSPF routing and the LEQ routing with m 1 experience significant increases in the delay disparity due to the increased queuing delay. However the LEQ routing with m 2 or 3 is advantageously capable of substantially avoiding such an increase due to the path diversity afforded by multiple hub nodes.

As used in this specification and the appended claims the term LEQ routing refers to a routing method aimed at reducing routing delay disparities among the participating client terminals. Depending on the topology of the network the location of candidate hub nodes client terminals and application servers connected to the network and the values of m and M methods of the invention may or may not be able to fully equalize the routing delays. However the spread of delay values obtained under an LEQ routing protocol of the invention at least approximates the optimal minimum spread value that is theoretically attainable for the network wherein the term optimal minimum spread value refers to the global theoretical minimum of Eq. 1 over a set of conditions defined by Eqs. 2 7 . One skilled in the art will appreciate that method see is capable of finding an LEQ routing configuration whose delay spread is substantially equal to the optimal minimum spread value. Method see is capable of finding an LEQ routing configuration whose delay spread might be greater than the optimal minimum spread value but smaller than the spread value corresponding to OSPF routing. In one of the worst case scenarios i.e. when the optimal minimum spread value is zero the difference between the optimal minimum spread value and the delay spread for the LEQ routing configuration does not exceed 100 of the latter.

While this invention has been described with reference to illustrative embodiments this description is not intended to be construed in a limiting sense. For example a network in which various embodiments of the invention can be practiced may contain routers capable of supporting wireline and or wireless communication links. Methods of the invention can be appropriately modified to handle LEQ routing for multiple application servers. Various modifications of the described embodiments as well as other embodiments of the invention which are apparent to persons skilled in the art to which the invention pertains are deemed to lie within the principle and scope of the invention as expressed in the following claims.

The present invention can be embodied in the form of methods and apparatuses for practicing those methods. The present invention can also be embodied in the form of program code embodied in tangible media such as floppy diskettes CD ROMs hard drives or any other machine readable storage medium wherein when the program code is loaded into and executed by a machine such as a router or server the machine becomes an apparatus for practicing the invention. The present invention can also be embodied in the form of program code for example whether stored in a storage medium loaded into and or executed by a machine or transmitted over some transmission medium or carrier such as over electrical wiring or cabling through fiber optics or via electromagnetic radiation wherein when the program code is loaded into and executed by a machine such as a computer the machine becomes an apparatus for practicing the invention. When implemented on a general purpose processor the program code segments combine with the processor to provide a unique device that operates analogously to specific logic circuits.

Unless explicitly stated otherwise each numerical value and range should be interpreted as being approximate as if the word about or approximately preceded the value of the value or range.

It should be understood that the steps of the exemplary methods set forth herein are not necessarily required to be performed in the order described and the order of the steps of such methods should be understood to be merely exemplary. Likewise additional steps may be included in such methods and certain steps may be omitted or combined in methods consistent with various embodiments of the present invention.

Reference herein to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment can be included in at least one embodiment of the invention. The appearances of the phrase in one embodiment in various places in the specification are not necessarily all referring to the same embodiment nor are separate or alternative embodiments necessarily mutually exclusive of other embodiments. The same applies to the term implementation. 

Also for purposes of this description the terms couple coupling coupled connect connecting or connected refer to any manner known in the art or later developed in which energy is allowed to be transferred between two or more elements and the interposition of one or more additional elements is contemplated although not required. Conversely the terms directly coupled directly connected etc. imply the absence of such additional elements.

