---

title: Persistent local storage for processor resources
abstract: Local storage may be allocated for each processing resource in a process of a computer system. Each processing resource may be virtualized and may have a one-to-one or a many-to-one correspondence with with physical processors. The contents of each local storage persist across various execution contexts that are executed by a corresponding processing resource. Each local storage may be accessed without synchronization (e.g., locks) by each execution context that is executed on a corresponding processing resource. The local storages provide the ability to segment data and store and access the data without synchronization. The local storages may be used to implement lock-free techniques such as a generalized reduction where a set of values is combined through an associative operator.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08887162&OS=08887162&RS=08887162
owner: Microsoft Corporation
number: 08887162
owner_city: Redmond
owner_country: US
publication_date: 20081217
---
In concurrent programming viz. interaction between multiple execution contexts such as threads fibers i.e. lightweight threads and child processes shared data is typically synchronized. When an execution context accesses data it generally invokes a lock or other synchronization technique to ensure that no other execution context performs a conflicting access to the data. The synchronization prevents data from being corrupted but adds processing overhead to each data access. Perhaps more importantly the synchronization often serializes the access to the data by different execution contexts. This serialization may inhibit the performance and scalability of a process particularly where there are many independent processing resources that execute execution contexts.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Local storage may be allocated for each processing resource in a process of a computer system. Each processing resource may be virtualized and may have a one to one or a many to one correspondence with with physical processors. The contents of each local storage persist across various execution contexts that are executed by a corresponding processing resource. Each local storage may be accessed without synchronization e.g. locks by each execution context that is executed on a corresponding processing resource. The local storages provide the ability to segment data and store and access the data without synchronization. The local storages may be used to implement lock free techniques such as a generalized reduction where a set of values is combined through an associative operator.

In the following Detailed Description reference is made to the accompanying drawings which form a part hereof and in which is shown by way of illustration specific embodiments in which the invention may be practiced. In this regard directional terminology such as top bottom front back leading trailing etc. is used with reference to the orientation of the Figure s being described. Because components of embodiments can be positioned in a number of different orientations the directional terminology is used for purposes of illustration and is in no way limiting. It is to be understood that other embodiments may be utilized and structural or logical changes may be made without departing from the scope of the present invention. The following detailed description therefore is not to be taken in a limiting sense and the scope of the present invention is defined by the appended claims.

It is to be understood that the features of the various exemplary embodiments described herein may be combined with each other unless specifically noted otherwise.

Runtime environment represents a runtime mode of operation in a computer system such as a computer system shown in and described in additional detail below where the computer system is executing instructions. The computer system generates runtime environment from a runtime platform such as a runtime platform shown in and described in additional detail below.

Runtime environment includes an least one invoked process an operating system OS a set of hardware threads M where M is an integer that is greater than or equal to one and denotes the Mth hardware thread M a resource management layer and a memory system . Runtime environment allows tasks from process to be executed along with tasks from any other processes that co exist with process not shown using OS resource management layer and hardware threads M . Runtime environment operates in conjunction with OS and or resource management layer to allow process to obtain processor and other resources of the computer system e.g. hardware threads M .

Runtime environment includes a scheduler function that generates scheduler . In one embodiment the scheduler function is implemented as a scheduler application programming interface API . In other embodiments the scheduler function may be implemented using other suitable programming constructs. When invoked the scheduler function creates scheduler in process where scheduler operates to schedule tasks of process for execution by one or more hardware threads M . Runtime environment may exploit fine grained concurrency that application or library developers express in their programs e.g. process using accompanying tools that are aware of the facilities that the scheduler function provides.

Process includes an allocation of processing and other resources that host one or more execution contexts viz. threads fibers i.e. lightweight threads or child processes . Process obtains access to the processing and other resources in the computer system e.g. hardware threads M and memory from OS and or resource management layer . Process causes tasks to be executed using the processing and other resources.

Process generates work in tasks of variable length where each task is associated with an execution context in scheduler . More than one task may be associated with a given execution context. Each task includes a sequence of instructions that perform a unit of work when executed by the computer system. Each execution context forms a thread or analogous OS concept such as child process that executes associated tasks on allocated processing resources. Each execution context includes program state and machine state information. Execution contexts may terminate when there are no more tasks left to execute. For each task runtime environment and or process either assign the task to scheduler to be scheduled for execution or otherwise cause the task to be executed without using scheduler .

Process may be configured to operate in a computer system based on any suitable execution model such as a stack model or an interpreter model and may represent any suitable type of code such as an application a library function or an operating system service. Process has a program state and machine state associated with a set of allocated resources that include a defined memory address space. Process executes autonomously or substantially autonomously from any co existing processes in runtime environment . Accordingly process does not adversely alter the program state of co existing processes or the machine state of any resources allocated to co existing processes. Similarly co existing processes do not adversely alter the program state of process or the machine state of any resources allocated to process .

OS manages processing and other resources of the computer system and provides a set of functions that allow process and other processes in the computer system to access and use the components. In addition OS offers execution contexts to scheduler and process and allocates memory from memory system to scheduler and process . OS may allocate memory from memory system in any suitable fixed or variable sizes e.g. pages of 4 kilobytes KB to 64 KB .

Hardware threads reside in execution cores of a set or one or more processor packages e.g. processor packages shown in and described in additional detail below of the computer system. Each hardware thread is configured to execute instructions independently or substantially independently from the other execution cores and includes a machine state. Hardware threads may be included in a single processor package or may be distributed across multiple processor packages. Each execution core in a processor package may include one or more hardware threads .

Resource management layer allocates processing resources to process by assigning one or more hardware threads to process . Resource management layer exists separately from OS in the embodiment of . In other embodiments resource management layer or some or all of the functions thereof may be included in OS .

Memory system includes any suitable type number and configuration of volatile or non volatile storage devices configured to store instructions and data. The storage devices of memory system represent computer readable storage media that store computer executable instructions including process OS and resource management layer . The instructions are executable by a computer system to perform the functions and methods of process OS and resource management layer described herein. Examples of storage devices in memory system include hard disk drives random access memory RAM read only memory ROM flash memory drives and cards and magnetic and optical disks.

Process implicitly or explicitly causes scheduler to be created via the scheduler function provided by runtime environment . Scheduler instance may be implicitly created when process uses APIs available in the computer system or programming language features. In response to the API or programming language features runtime environment creates scheduler with a default policy. To explicitly create a scheduler process may invoke the scheduler function provided by runtime environment and specifies a policy for scheduler .

Scheduler interacts with OS and resource management layer to negotiate processing and other resources of the computer system in a manner that is transparent to process . OS allocates memory to scheduler in response to requests from virtual processors . Resource management layer allocates hardware threads to scheduler based on supply and demand and any policies of scheduler .

In the embodiment shown in scheduler manages the processing resources by creating virtual processors that form an abstraction of underlying hardware threads . Scheduler multiplexes virtual processors onto hardware threads by mapping each virtual processor to a hardware thread . Scheduler may map more than one virtual processor onto a particular hardware thread but maps only one hardware thread to each virtual processor . In other embodiments scheduler manages processing resources in other suitable ways to cause instructions of process to be executed by hardware threads .

Prior to executing tasks scheduler obtains execution contexts and from runtime environment or operating system . Available virtual processors locate and execute execution contexts and to begin executing tasks. The set of execution contexts in scheduler includes a set of execution contexts N with respective associated tasks N that are being executed by respective virtual processors N a set of zero or more runnable execution contexts and a set of zero or more blocked i.e. wait dependent execution contexts . Each execution context and includes state information that indicates whether an execution context and is executing runnable e.g. in response to becoming unblocked or added to scheduler or blocked. Execution contexts that are executing have been attached to a virtual processor and are currently executing. Execution contexts that are runnable include an associated task and are ready to be executed by an available virtual processor . Execution contexts that are blocked include an associated task and are waiting for data a message or an event that is being generated or will be generated by another execution context or .

Each execution context executing on a virtual processor may generate in the course of its execution additional tasks which are organized in any suitable way e.g. added to work queues not shown in . Work may be created by using either application programming interfaces APIs provided by runtime environment or programming language features and corresponding tools in one embodiment. When processing resources are available to scheduler tasks are assigned to execution contexts or that execute them to completion or a blocking point e.g. waiting for a message or a stolen child task to complete on virtual processors before picking up new tasks. An execution context executing on a virtual processor may also unblock other execution contexts by generating data a message or an event that will be used by another execution context .

Each task in scheduler may be realized e.g. realized tasks and which indicates that an execution context or has been or will be attached to the task and the task is ready to execute. Realized tasks typically include light weight tasks and agents and may be associated with an execution context or just before executing or in advance of execution. A task that is not realized is termed unrealized. Unrealized tasks e.g. tasks may be created as child tasks generated by the execution of parent tasks and may be generated by parallel constructs e.g. parallel parallel for begin and finish . Scheduler may be organized into a synchronized collection e.g. a stack and or a queue for logically independent tasks with execution contexts i.e. realized tasks along with a list of workstealing queues for dependent tasks i.e. unrealized tasks as illustrated in the embodiment of described below.

Upon completion blocking or other interruption e.g. explicit yielding or forced preemption of a task associated with an execution context running on a virtual processor the virtual processor becomes available to execute another realized task or unrealized task . Scheduler searches for a runnable execution context a realized task or an unrealized task to attach to the available virtual processor for execution in any suitable way. For example scheduler may first search for a runnable execution context to execute before searching for a realized task or an unrealized task to execute. Scheduler continues attaching execution contexts to available virtual processors for execution until all execution contexts of scheduler have been executed. In other embodiments runnable execution contexts and realized tasks may be merged into single concept from the perspective of schedulers .

Scheduler includes one or more memory allocators not shown that cause memory to be allocated for internal data structures of scheduler not shown and tasks of execution contexts executing on virtual processors . The memory allocators request and receive access to pages of memory from OS and allocate objects or other suitable portions of memory from the pages to tasks executing on virtual processors . OS may provide pages in predefined sizes of memory such as page sizes of 4 kilobytes KB to 64 KB to the memory allocators.

The memory allocated to tasks may include thread or context local storage TLS or CLS not shown . With thread and context local storage the allocated memory corresponds to an execution context that is currently being executed by a virtual processor . This memory is saved along with the program state and machine state information of an execution context when the execution context blocks or is otherwise interrupted so that the memory can be restored when the corresponding thread or context resumes. The thread or context local storage may be moved to a new virtual processor along with the execution context when the execution context is picked up by the new virtual processor for execution. As a result thread and context local storage is only available to tasks that are executed on an execution context that corresponds to the thread or context local storage. Thread and context local storage does not persist across execution contexts and the contents of thread and context local storage are not maintained across execution contexts.

The memory allocated to tasks also includes virtual processor local storages . Virtual processor local storages are allocated in response to one or more requests from one or more tasks . Each virtual processor local storage corresponds to a different virtual processor . Each virtual processor local storage persists across all execution contexts that execute on a corresponding virtual processor . Accordingly the contents of virtual processor local storages are maintained when execution contexts complete block or are otherwise interrupted on virtual processors . Such contents are not saved with the program state and machine state information of an execution context when the execution context blocks or is otherwise interrupted.

Subsequent execution contexts executed by available virtual processors may access modify and or overwrite the data in corresponding virtual processor local storages . Because each virtual processor may execute only one execution context at any given time the execution context executing on a given virtual processor may access the virtual processor local storage corresponding to the virtual processor without synchronization. As a result each virtual processor allows different tasks that execute on different execution contexts to access the same data in the virtual processor local storage corresponding to the virtual processor at different times without synchronization i.e. without using locks or other synchronization techniques on the data . In addition an element of data inserted into a virtual processor local storage by a given execution context may have no correlation to an element of data later retrieved by the same execution context . For example a first execution context executing on a given virtual processor stores the value 5 in virtual processor local storage of the virtual processor and subsequently blocks. A second execution context is then scheduled on the virtual processor and overwrites the value 5 in virtual processor local storage of the virtual processor with the value 99. If the first execution context is subsequently unblocked and rescheduled on the virtual processor then the first execution context will read the value in virtual processor local storage as 99 not the value of 5 that the first execution context stored.

An execution context on one virtual processor may also access the virtual processor local storage of another virtual processor . In one embodiment such an execution context employs locks or other synchronization techniques when accessing the virtual processor local storage of another virtual processor to prevent conflicting accesses of the virtual processor local storage . Such an embodiment is described in additional detail below with reference to . In other embodiments the virtual processor local storage may be structured to inherently prevent conflicting accesses to allow access by an execution context of another virtual processor without synchronization. Such an embodiment is described in additional detail below with reference to .

Virtual processor local storage may be allocated for each virtual processor in process . is a flow chart illustrating an embodiment of a method for accessing virtual processor local storage of a virtual processor . The method of may be performed by each virtual processor in scheduler where corresponding virtual processor local storage is or will be allocated according to one embodiment.

In a virtual processor executes a next task in an execution context as indicated in a block . If the task accesses local storage of the virtual processor executing the task as indicated in a block then the execution context executing on the virtual processor accesses the corresponding local storage without synchronization as indicated in a block . The virtual processor may allocate the corresponding local storage if not present in response to the task requesting and or accessing the corresponding local storage .

If the task accesses local storage of another virtual processor as indicated in a block then the execution context executing on the virtual processor accesses the local storage of the other virtual processor with or without synchronization as appropriate as indicated in a block . An embodiment of local storage that involves synchronization when accessed by a non corresponding virtual processor is described with reference to below and an embodiment of local storage that does not involve synchronization when accessed by a non corresponding virtual processor is described with reference to below.

Task repeats the functions of blocks through until task blocks is interrupted or completes. When task becomes blocked or interrupted as indicated in a block the virtual processor saves the state of the task and execution context without saving the contents of the corresponding local storage as indicated in a block . To do so virtual processor stores the state of the task and execution context in memory to allow the state to be retrieved from memory when the task and execution context resume. When task becomes blocked or interrupted or completes as indicated in a block the virtual processor maintains the corresponding local storage as indicated in a block . As described above the corresponding local storage persists across tasks and execution contexts such that subsequent tasks and execution contexts may access modify and or overwrite data stored in the local storage by previous tasks and execution contexts .

Subsequent to a task blocking being interrupted or completing the virtual processor accesses a next task for execution. If the previous task blocked or was otherwise interrupted then the virtual processor executes the next task on a next execution context . If the previous task completed then the virtual processor may execute the next task as a continuation on the same execution context or may execute the next task on a next execution context .

Because virtual processor local storages allow data to be segmented stored and accessed without synchronization virtual processor local storages may be used to implement lock free techniques such as a generalized reduction where a set of values is combined through an associative operator. Virtual processor local storages may be thought of as highly efficient storage for accumulators of data. For example a set of data S may include elements X through X n where n 2 as shown in Equation I. 1 . . . Equation I

With the set of data S and the operator op a reduction may be determined using Equation III. 1 2 3 Equation III For example the set of data S may be a set of integer numbers and the operator op may be integerpoint addition. As another example the set of data S may be a set of n n matrices and the operator op may be matrix multiplication. Other examples may use floating point math even though strictly speaking floating point math is not associative. But this limitation may be obviated through viewing floating point numbers as equivalence classes modulo a relative epsilon error viz. e f iff abs e f 

When computing the reduction in parallel in scheduler the set of data S may be partitioned and every virtual processor may be given a subset of S on which to apply the operator op. The accumulator for the operator op is kept in virtual processor local storage on each virtual processor and when all virtual processors have finished their work the accumulators stored locally in virtual processor local storages are combined to achieve the result of the reduction. Some synchronization may be performed in the final combine step in accessing the virtual processor local storages . No synchronization is performed however when storing or reading from a given virtual processor local storage where the access is made from the corresponding virtual processor .

At some point task may block e.g. subsequent to computing a part of the reduction and while waiting for the remaining tasks P to be executed as shown in . When task blocks virtual processor saves the state of execution context and task and maintains the data in slot . Virtual processor may then execute a new execution context N 1 which may execute task . Task computes its part of the reduction by applying the operator to any data stored in slot and its subset of the data and storing the intermediate result in slot .

Other virtual processors also execute the remaining tasks P . For example virtual processor N executes task P as a continuation on execution context N subsequent to task N completing as shown in . Task P causes slot N to be allocated in virtual processor local storage N if not already allocated and computes its part of the reduction by applying the operator to any data stored in slot N and its subset of the data and storing the intermediate result in slot N . Other virtual processors may execute any number including zero of the tasks P 1 and allocate corresponding slots for storing intermediate results.

Subsequent to all tasks P being executed task becomes unblocked and resumes execution on an available virtual processor . In the example of virtual processor resumes task and execution context to complete the reduction. To do so task accesses all data in slots N possibly with synchronization for slots N and applies the operator to the data from slots N to generate the final result of the reduction. Task stores the final value in slot or another suitable location.

The remaining virtual processors continue execution of tasks as described above with reference to . For example virtual processor N executes task N 1 as a continuation on execution context N subsequent to task P completing as shown in .

Virtual processor local storages provide two aspects of usage. First virtual processor local storages provide an accumulator combinable aspect which forms an unstructured reduction implementation. In this aspect elements of data inserted into virtual processor local storages are not retrieved but instead accumulate to a value that is later retrieved across all virtual processor local storages . Second virtual processor local storages provide a reuse pooling reusable aspect. In this aspect elements of data inserted into virtual processor local storages are interchangeable so that when an element is retrieved by a virtual processor it is immaterial which element it is.

To implement virtual processor local storages B a workstealing deque may be allocated on each virtual processor and an array may be formed of all of the workstealing deques. The array may be used to form self load balancing workqueues for scheduler .

In the above embodiments scheduler may operate as a cooperative scheduler where process and other processes are associated with virtual processors in a controlled way. In other embodiments scheduler may operate as another type of scheduler such as a preemptive scheduler.

Although one instance of scheduler was shown in the embodiment of other embodiments may include other instances of scheduler where each instance includes virtual processor local storage for each virtual processor .

In one embodiment process shown in organizes tasks into one or more schedule groups shown in and presents schedule groups to scheduler as shown in . In other embodiments process organizes tasks into collections for each virtual processor of scheduler in other suitable ways.

Using the embodiment of scheduler may first search for unblocked execution contexts in the runnables collection of each schedule group in scheduler . Scheduler may then search for realized tasks in the realized task collection of all schedule groups before searching for unrealized tasks in the workstealing queues of the schedule groups .

In one embodiment a virtual processor that becomes available may attempt to locate a runnable execution context in the runnables collection or a realized task in the realized task collection in the schedule group from which the available virtual processor most recently obtained a runnable execution context i.e. the current schedule group . The available virtual processor may then attempt to locate a runnable execution context in the runnables collections or a realized task in the realized task collection in the remaining schedule groups of scheduler in a round robin or other suitable order. If no runnable execution context is found then the available virtual processor may then attempt to locate an unrealized task in the workstealing queues of the current schedule group before searching the workstealing queues in the remaining schedule groups in a round robin or other suitable order.

In other embodiments schedule groups contain other suitable numbers types and or configurations of task collections.

Computer system includes one or more processor packages memory system also shown in zero or more input output devices zero or more display devices zero or more peripheral devices and zero or more network devices . Processor packages memory system input output devices display devices peripheral devices and network devices communicate using a set of interconnections that includes any suitable type number and configuration of controllers buses interfaces and or other wired or wireless connections.

Computer system represents any suitable processing device configured for a general purpose or a specific purpose. Examples of computer system include a server a personal computer a laptop computer a tablet computer a personal digital assistant PDA a mobile telephone and an audio video device. The components of computer system i.e. processor packages memory system input output devices display devices peripheral devices network devices and interconnections may be contained in a common housing not shown or in any suitable number of separate housings not shown .

Processor packages include hardware threads M . Each hardware thread in processor packages is configured to access and execute instructions stored in memory system . The instructions may include a basic input output system BIOS or firmware not shown OS also shown in a runtime platform applications and resource management layer also shown in . Each hardware thread may execute the instructions in conjunction with or in response to information received from input output devices display devices peripheral devices and or network devices .

Computer system boots and executes OS . OS includes instructions executable by hardware threads to manage the components of computer system and provide a set of functions that allow applications to access and use the components. In one embodiment OS is the Windows operating system. In other embodiments OS is another operating system suitable for use with computer system .

Resource management layer includes instructions that are executable in conjunction with OS to allocate resources of computer system including hardware threads as described above with reference to . Resource management layer may be included in computer system as a library of functions available to one or more applications or as an integrated part of OS .

Runtime platform includes instructions that are executable in conjunction with OS and resource management layer to generate runtime environment and provide runtime functions to applications . These runtime functions include a scheduler function as described in additional detail above with reference to . The runtime functions may be included in computer system as part of an application as a library of functions available to one or more applications or as an integrated part of OS and or resource management layer .

Each application includes instructions that are executable in conjunction with OS resource management layer and or runtime platform to cause desired operations to be performed by computer system . Each application represents one or more processes such as process as described above that may execute with one or more schedulers as provided by runtime platform .

As noted above memory system includes any suitable type number and configuration of volatile or non volatile storage devices configured to store instructions and data. The storage devices of memory system represent computer readable storage media that store computer executable instructions including OS resource management layer runtime platform and applications .

Memory system stores instructions and data received from processor packages input output devices display devices peripheral devices and network devices . Memory system provides stored instructions and data to processor packages input output devices display devices peripheral devices and network devices .

Input output devices include any suitable type number and configuration of input output devices configured to input instructions or data from a user to computer system and output instructions or data from computer system to the user. Examples of input output devices include a keyboard a mouse a touchpad a touchscreen buttons dials knobs and switches.

Display devices include any suitable type number and configuration of display devices configured to output textual and or graphical information to a user of computer system . Examples of display devices include a monitor a display screen and a projector.

Peripheral devices include any suitable type number and configuration of peripheral devices configured to operate with one or more other components in computer system to perform general or specific processing functions.

Network devices include any suitable type number and configuration of network devices configured to allow computer system to communicate across one or more networks not shown . Network devices may operate according to any suitable networking protocol and or configuration to allow information to be transmitted by computer system to a network or received by computer system from a network.

Although specific embodiments have been illustrated and described herein it will be appreciated by those of ordinary skill in the art that a variety of alternate and or equivalent implementations may be substituted for the specific embodiments shown and described without departing from the scope of the present invention. This application is intended to cover any adaptations or variations of the specific embodiments discussed herein. Therefore it is intended that this invention be limited only by the claims and the equivalents thereof.

