---

title: System and method for providing timer affinity through engine polling within a session-based server deployment
abstract: The session-based server deployment is comprised of an engine tier that is designed for high throughput processing of communications and a state tier that maintains state information for access by the engine tier. The engine tier includes a plurality of engine nodes that process incoming messages. The state nodes contain call state, including various timers. When processing a call, an engine sets various timers in the state tier. The engine then periodically polls the state tier for any timers that have expired. Upon expiration of a timer, the timer is assigned to the corresponding engine node which handled previous communications for the call. This affinity between the timer and its respective engine provides numerous benefits, including reduced latency and more localized data access.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08179912&OS=08179912&RS=08179912
owner: Oracle International Corporation
number: 08179912
owner_city: Redwood Shores
owner_country: US
publication_date: 20080926
---
The following commonly owned co pending United States patent applications are related to the present application and are incorporated by reference herein in their entireties 

U.S. patent application Ser. No. 11 748 791 entitled ENGINE NEAR CACHE FOR REDUCING LATENCY IN A TELECOMMUNICATIONS ENVIRONMENT by Anno R. Langen et al. filed on May 15 2007 and

U.S. patent application Ser. No. 12 260 846 entitled SYSTEM AND METHOD FOR PROVIDING TIMER AFFINITY THROUGH NOTIFICATIONS WITHIN A SESSION BASED SERVER DEPLOYMENT by Ioannis Cosmadopoulos et al. filed on Oct. 29 2008.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

The current invention relates generally to managing telecommunications and timers and more particularly to providing affinity for session based timers within a cluster network deployment.

With the advent and the ever increasing popularity of the World Wide Web more and more previously unrelated technologies are becoming integrated with the enormous network of information and functionality that the Internet provides. Everything from television and radio to telephony and video conferencing are now available online amongst a wide variety of other technologies. One such area of technology that has seen a recent explosion in growth is telecommunications and related services.

Conventionally telecommunications and network infrastructure providers have relied on often decades old switching technology for providing routing for network traffic. Businesses and consumers however are driving industry transformation by demanding new converged voice data and video services. The ability to meet these demands often can be limited by existing IT and network infrastructures that are closed proprietary and too rigid to support these next generation services. As a result telecommunications companies have been transitioning from traditional circuit switched Public Switched Telephone Networks PSTN the common wired telephone system used around the world to connect any one telephone to another telephone to Voice Over Internet Protocol VoIP networks. VoIP technologies enable voice communication over vanilla IP networks such as the public Internet. Additionally a steady decline in voice revenues has resulted in heightened competitive pressures as carriers vie to grow data service revenues and reduce churn through the delivery of these more sophisticated data services. Increased federal regulation security and privacy issues as well as newly emerging standards can further compound these pressures.

Delivering more sophisticated data services has proved to be more difficult than first imagined. Existing information technology IT and network infrastructures closed proprietary network based switching fabrics and the like have proved to be complex and rigid in allowing the creation and deployment of new service offerings.

In recent times specialized telecommunication application servers have emerged to enable simpler ways for developers to include telephony based functionality in their software applications as well as to provide increased security and stability. Nevertheless these specialized solutions continue to need improvements in performance generally required by network operators and demanded by their subscribers.

One such area for improvement involves the performance in processing of various communications at the server level. The telephony protocols are typically extremely time sensitive more so than standard Web and HTTP transmissions. Accordingly it would be desirable to optimize the processing of various telecommunications so as to minimize latency and improve performance. In addition it would be advantageous to provide these improvements in distributed and highly scalable deployments accessible via voice over internet protocols VOIP .

The invention is illustrated by way of example and not by way of limitation in the figures of the accompanying drawings in which like references indicate similar elements. References to embodiments in this disclosure are not necessarily to the same embodiment and such references mean at least one. While specific implementations are discussed it is understood that this is done for illustrative purposes only. A person skilled in the relevant art will recognize that other components and configurations may be used without departing from the scope and spirit of the invention.

In the following description numerous specific details are set forth to provide a thorough description of the invention. However it will be apparent to those skilled in the art that the invention may be practiced without these specific details. In other instances well known features have not been described in detail so as not to obscure the invention.

Although a diagram may depict components as logically separate such depiction is merely for illustrative purposes. It can be apparent to those skilled in the art that the components portrayed can be combined or divided into separate software firmware and or hardware components. For example one or more of the embodiments described herein can be implemented in a network accessible device or appliance. Furthermore it can also be apparent to those skilled in the art that such components regardless of how they are combined or divided can execute on the same computing device or can be distributed among different computing devices connected by one or more networks or other suitable communication means.

In accordance with embodiments there are described systems and methods for providing timer affinity in session initiation protocol SIP based server deployments. The session initiation protocol SIP is a well known and widely used signaling protocol used to create modify and terminate sessions between multiple participants during telecommunications across the internet or other networks. These sessions can include internet telephone calls video conferencing multimedia interactions and the like. The SIP application server is thus used to deploy and manage software applications which implement the SIP protocol in some way. In various embodiments the SIP server delivers a highly available service convergence platform for network operators network equipment providers systems integrators and independent software vendors ISVs that seek to develop integrate and operate real time multimedia communication services.

Generally the SIP protocol includes a number of protocol timers which are used to control the interactions between the multiple participants in a session. For example in SIP when something does not occur within a certain specified time period in a session a timer will fire causing some corrective or other action to be taken. An illustration of this can be a cellular telephone client trying to establish a connection with another client via an INVITE message. If one fails to respond within a certain time period a timer will fire and an action will be taken to address the situation. These SIP protocol timers are specified in the SIP protocol and are generally used to standardize communications.

In addition a SIP application server can also include application timers which are implemented by the software application deployed thereon. For example a video conferencing application may implement timers for various reasons unrelated to the SIP protocol such as to notify the participants of a particular event.

In the various embodiments these and other timers are processed by a SIP server engine during the course of the various sessions. In one embodiment the SIP server is deployed as two separate tiers in order to gain the optimal performance and minimum latency. An engine tier is designed for handling high throughput processing of SIP communications. The engine tier remains mostly stateless and is designed to process the various communications with as little latency as possible. The state tier on the other hand maintains state information for access by the engine tier. The state tier is an in memory RAM data store for maintaining the data required during the engine tier processing. In one embodiment when processing a request the engine node will retrieve the needed state information from a state node and when finished update the state information thereon. This multi tier type of deployment can significantly improve performance such as by reducing latency during garbage collection in the state tier as well as providing numerous other benefits.

In various embodiments the timers are part of the state information contained in the state tier of the SIP server deployment. While the engine nodes process the timers the state nodes will typically designate which engine node will process the particular timers. In one implementation the timers can be distributed evenly across the multiple engine nodes so as not to overload any particular engine in the deployment. However because there is often an association that exists between a call and a particular engine it may be desirable to create affinity between the timers associated with a particular call session and the engine that handled the last message for that call session. For example with sticky load balancing deployments the load balancer will distribute the incoming message for a particular call to the engine node that handled the last message for that call. This is desirable because the engine may be able to avoid having to retrieve the state information for the particular call from the state tier since that state information may already be residing there from previous processing. For example in various embodiments the engines implement a near cache which maintains a small portion of state information associated with certain a session s on the assumption that the engine will need that state information again in the very near future because it will likely receive another message associated with the particular session that it is currently processing. The engine near cache is described in more detail in the U.S. patent application Ser. No. 11 748 791 entitled ENGINE NEAR CACHE FOR REDUCING LATENCY IN A TELECOMMUNICATIONS ENVIRONMENT which is incorporated herein in its entirety.

Accordingly if timers can be fired on the same engine that processed the last message for the call there is a higher likelihood that this engine will have the needed state information available in the near cache thereby eliminating the need to contact the state tier. This can reduce network bandwidth and improve performance. Furthermore any other information unrelated to SIP protocol state e.g. media server data may be local to the engine node and timer affinity would avoid having to retrieve it again when timers are fired.

Another possible use case of timer affinity can be seen in a hot standby deployment where two engines are running simultaneously but only one engine is actually processing any calls. In such a deployment the secondary engine does not process any messages and is essentially used for redundancy purposes in order to take over processing calls in the event of primary engine failure. Because the primary engine is the only node that is processing calls allocating the timers evenly across both engines would be undesirable since that may cause both engines to process messages creating possible errors in IP addresses and other undesirable issues. Creating some correlation between the timer for a particular session and the engine that processed the last message for that session can remove these unnecessary burdens and resolve these inconveniences.

The various embodiments are described in more detail with reference to the figures in the paragraphs below.

In the embodiment illustrated the SIP Server is a carrier class Java Enterprise Edition J2EE application server that has been extended with support for the Session Initiation Protocol SIP and other operational enhancements that allow it to meet the demanding requirements of next generation internet protocol IP based communications networks. The SIP Server can be used to create deploy and manage various real time communications services and applications by telecom operators who wish to provide mobile and fixed multimedia services. As with any Java application server the SIP server can take advantage of the J2EE Platform Kernel and Services in order to compile execute and optimize the performance of various programs and applications. In one embodiment the SIP server is also extended with support for a multitude of standards and protocols such as SIP Diameter Hyper Text Transfer Protocol HTTP Lightweight Directory Access Protocol LDAP Simple Network Management Protocol SNMP Simple Object Access Protocol SOAP Java Database Connectivity JDBC and others.

As stated previously the SIP Server is enabled to support session initiation protocol SIP . SIP is a protocol used primarily for creating and terminating sessions with one or more participants such as setting up or tearing down voice or video calls. SIP is described in more detail in RFC 3261 of the IETF SIP Working Group which is incorporated herein by reference.

The SIP protocol specification defines different types of high level SIP roles namely user agents UA which include UA clients UA servers and Back to Back user agents B2BUA . The SIP protocol also defines the roles of Proxies Registrars and Redirect Servers. Accordingly the SIP Servlet API of the SIP server allows any of these roles to be coded as a SIP Servlet Application. Furthermore because SIP is an extensible protocol the API is also designed to allow developers to easily extend functionality. This can be accomplished by dividing up the SIP processing between the container functions and the applications. Most of the base protocol can be performed by the container leaving the higher level tasks for the applications to perform. This division of processing can lead to a great amount of flexibility to the SIP Servlet API.

As further illustrated in the SIP Server can include an Enterprise Java Bean EJB container an HTTP Servlet container and a SIP Servlet container . Each of these containers can provide an environment that supports the execution of applications developed using its corresponding technology. For example the EJB container manages enterprise beans contained within it which in turn provide the business logic for a J2EE application. This management can encompass services such as registering creating and destroying objects and their instances providing remote interfaces to objects managing the state of objects maintaining security and coordinating distributed transactions. Similarly the HTTP container and the SIP Servlet container can be responsible for managing HTTP and SIP servlets respectively.

The SIP stack of the SIP Server can be fully integrated into the SIP Servlet container and can be easier to use than a traditional protocol stack. For example the higher level abstraction of the SIP Servlet API can free the developer from the mechanics of handling of transaction timers syntactic evaluation of received requests generation of non application related responses generation of fully formed SIP requests from request objects which may involve correct preparation of system headers and generation of syntactically correct SIP messages and handling of lower layer transport protocols such as Transport Control Protocol TCP User Datagram Protocol UDP and Stream Control Transmission Protocol SCTP .

In one embodiment the Servlet container can provide a Shared Session Context and session application programming interface API in order to maintain awareness of the state of the larger converged SIP and HTTP application session. There are many use cases where a converged application using both SIP and HTTP functions is desirable. Some examples of these applications include conferencing and click to call applications as well as Presence and User Agent Configuration Management applications. The converged applications can also use other protocols e.g. Diameter to perform more advanced functions such as modifying subscriber profile data. Furthermore the container can provide a whole host of other services including distributing request and response objects to components in a structured way as well as managing the end to end object lifecycle including resource transaction and session state management.

In the embodiment illustrated the SIP server along with the various applications hosted thereon e.g. and can be used as a back to back user agent B2BUA in a typical telecommunications environment. A B2BUA can take the place of an intermediary between communications by user agents which may include various cellular phones wireless devices laptops computers applications and other components capable of communicating with one another electronically. The B2BUA can provide multiple advantages such as controlling the flow of communication between user agents enabling different types of user agents to communicate with one another e.g. a web application can communicate with a cellular phone as well as various security advantages. As one illustration the user agents can transmit to the SIP server instead of communicating directly to each other and thus malicious users can be prevented from sending spam and viruses hacking into other user agent devices and otherwise compromising security. It should be noted that the SIP Server need not necessarily take the role of a B2BUA as illustrated in but can also be used as a proxy a redirect server or some other role defined by the SIP protocol.

As illustrated messages such as phone call initiation requests or other transfers of data associated with the SIP protocol can come into the cluster from the internet such as over VoIP phone or some other type of outside network . This message can be received and handled by a load balancer which can be responsible distributing message traffic across the engines i.e. engine engine engine and engine in the cluster which handle the processing of the message traffic. The load balancer can be a standard load balancing appliance hardware device and it is not necessary that it be SIP aware. Alternatively the load balancer can be implemented as software that distributes the messages to the various engines. In certain embodiments it is preferable that the load balancer be sticky meaning that the load balancer distributes the messages associated with the same session or call to the same engine that handled the last message for the same session. This can provide performance advantages in that certain call state information may be readily available in the engine that handled the previous message. In the various embodiments the primary goal of the load balancer is to provide a single public address that distributes incoming SIP requests to available servers in the SIP server engine tier cluster . Such distribution of requests can ensure that the SIP server engines are fully utilized. The load balancer can also be used for performing maintenance activities such as upgrading individual servers or applications without disrupting existing SIP clients.

In the embodiment illustrated the SIP server can provide a two tier cluster architecture model to handle the incoming messages. In this two tier model a stateless engine tier cluster can process all signaling traffic and can also replicate transaction and session state to the state tier cluster which in turn can be divided into multiple partitions. Each partition can consist of any number of nodes replicas distributed across any number of hosts such as host and host which can be implemented as computers linked in a cluster type network environment. For example partition can include state replica and a state replica which can maintain copies of the call state information of the partition. The state tier cluster can be an n way peer replicated Random Access Memory RAM store that maintains various data objects which can be accessed by the engine nodes in the engine tier. In this manner engines can be provided a dual advantage of faster access to the data objects than retrieving data from a database while at the same time engines can be freed up from having to store the data onto the engine tier itself. This type of separation can offer various performance improvements. The state tier can also function as a lock manager where call state access follows a simple library book model i.e. a call state can be checked out by one SIP engine at a time .

The engine tier cluster can be implemented as a cluster of SIP server instances that hosts the SIP servlets which provide various features to SIP clients. In one embodiment the engine tier is principally stateless meaning that the majority of SIP session state information is not persisted in the engine tier but is obtained by querying the state tier cluster which can in turn provide replication and failover services for SIP session data. However in many embodiments the engine tier still maintains a small portion of call state such as in the engine near cache.

In various embodiments the Java Virtual Machine JVM garbage collection algorithms can slow down the throughput processing and cause latency when removing stateful long lived objects from memory. These stateful objects can be thought of as being more global e.g. referenced by various threads and entities than other localized stateless objects and as such the garbage collector would typically stop all thread execution in order to clean them up. In some cases this can introduce latency since the execution of various threads needs to be halted for a period of time while the garbage collector removes the unused stateful objects. While in typical web server environments this processing pause may be tolerated the SIP server environment is generally highly sensitive to any latency and as such this form of garbage collection pausing can be undesirable.

Accordingly the call state which may include such stateful objects can be maintained in memory on the state tier of the SIP server deployment. The engine tier on the other hand can be generally stateless so as not to become significantly affected by the various JVM garbage collection and heap clearing processes. Thus the primary goal of the engine tier can be to provide maximum throughput combined with low response time to SIP clients. As the number of calls or their duration increases more server instances can be added to the engine tier to manage the additional load. It should be noted however that although the engine tier may include many such server instances it can be managed as a single logical entity. For example the SIP servlets can be deployed uniformly to all server instances by targeting the cluster itself and the load balancer need not maintain affinity between SIP clients and individual servers in the engine tier.

In various embodiments the state tier cluster can be implemented as a cluster of SIP server instances that provides a high performance highly available in memory store e.g. RAM for maintaining and retrieving session state data for SIP servlets. This session data may be required by SIP applications in the SIP server engine tier in order to process incoming messages. Within the state tier session data can be managed in one or more partitions e.g. partition and partition where each partition manages a fixed portion of the concurrent call state. For example in a system that uses two partitions the first partition could manage one half of the concurrent call state e.g. A M and the second partition can manage the other half e.g. N Z . With three partitions not shown each can manage a third of the call state and so on. Additional partitions can be added as needed to manage large number of concurrent calls or incoming messages.

In one embodiment within each partition multiple state servers can be added to provide redundancy and failover should the other servers in the partition fail. When multiple servers participate in the same partition those servers can be referred to as replicas because each server maintains a duplicate copy of the partition s call state. For example partition can maintain its state information in replica and replica . In some embodiments the replicas can be distributed over multiple hosts e.g. host and host in order to provide host to host failover services in cases where a computer crashes. Furthermore to increase the capacity of the state tier the data can be split evenly across a set of partitions as previously discussed. The number of replicas in the partition can be called the replication factor since it determines the level of redundancy and strength of failover that it provides. For example if one node goes down or becomes disconnected from the network any available replica can automatically provide call state data to the engine tier.

Replicas can join and leave the associated partition and each replica can serve as exactly one partition at a time. Thus in one embodiment the total available call state storage capacity of the cluster is a summation of the capacities of each partition.

In one embodiment each partition can peer replicated meaning that clients perform all operations reads writes to all replicas in the partition wherein the current set of replicas in the partition is called the partition view . This can provide improved latency advantages over more traditional synchronous primary secondary architecture wherein one store acts as a primary and the other nodes serve as secondaries. Latency is reduced because there is no wait for the second hop of primary secondary systems. The peer replicated scheme can provide better host to host failover characteristics as well since there does not need to be change propagation delay.

In one embodiment the engine nodes and which are distributed over multiple hosts can be responsible for executing the call processing. Each call can have a call state associated with it. This call state can contain various information associated with the call such as the ids of the caller callee where the caller is what application is running on the callee as well as any timer objects that may need to fire in order to process the call flow. The state for each call can be contained in the state tier . The engine tier on the other hand could be stateless in order to achieve the maximum performance. In alternative embodiments the engine tier can have small amounts of state data stored thereon at selected periods of time.

In one embodiment a typical message processing flow can involve locking getting the call state processing the message and putting unlocking the call state. The operations supported by the replicas for normal operations can include 

lock and get call state where the engines request state data from the state replicas in the partition and the primary replica locks that state data for processing across all the other replicas in the same partition.

put and unlock call state where the engines update the state data to the state tier and the data is unlocked for access by other engines.

lock and get call states with expired timers where session state data that has expired can be retrieved from the state tier.

As previously discussed the state tier can maintain call state in various data objects residing in the random access memory RAM of a computer. This can provide significant access speed advantages to the engine tier . The SIP server can also provide a way for efficiently persisting long lived state information to a database disk storage in order to avoid unnecessary consumption of cluster resources. Since RAM is generally significantly more expensive than database memory it may be desirable to reduce the number of replicas in the state tier by storing at least some of the session state information to the database. In many cases database access to data is slower than RAM based replica access. However because some SIP communications are not as latency sensitive as others these communications can be persisted in the database in order to save the amount of random access memory required by the SIP server deployment. For example a standard telephone call can be viewed as having three stages a call setup stage an active call stage and the call teardown stage hanging up the call . The call setup stage is typically the most latency sensitive since users tend to expect immediate results from the server after pressing the call button. However the call teardown stage may not be as sensitive to latency because after the handset disconnects it may not matter from the user s perspective how long it will take for the server to complete call termination. As such session state for call termination can be maintained in a database. Similarly the active call stage may also be not as latency sensitive as the call setup stage since it mostly involves communication of voice bits between media servers. It should be noted that this example of a telephone call is provided purely for purposes of illustration and is not intended to limit the invention.

In the various embodiments affinity can be provided between the timer and the engine node that is processing the call. In order to process a call state information call state is usually needed and this call state is stored and retrieved from the replicas in the state tier. Timers comprise a part of this call state information. In various embodiments there can be several ways to implement the timer affinity feature as discussed in further detail below.

In one implementation in order to accomplish timer affinity the engines can be set to repeatedly poll the state tier for expired timers. Once a timer fires the state node can assign the expired timer to its engine owner at the next poll. For example if the engine is set to poll the replica every 100 milliseconds ms and a timer expires between the polling interval the replica can wait until the engine returns for the next poll before assigning it the expired timer for processing. In one embodiment the polling interval can be determined based on a service level agreement SLA contained in an extensible markup language XML file. In some embodiments the polling interval can be calculated by the data tier so as to ensure the SLA while at the same time minimizing the number of polls in order to reduce network traffic. This minimizing feature is described in more detail in .

In an alternative implementation there can be a thread deployed and running on each state replica. This thread can periodically look for timers that have expired and then notify the engine that had initially set the corresponding expired timer. The engine in turn can process the expired timer. Since the notified engine is the last engine that processed a message for the session affinity is created between the engine and the timer.

Additionally there can be other possible implementations for enabling timer affinity within the scope of the present embodiments as will be evident to one of ordinary skill in the art based on this disclosure. It should be noted that the embodiments are not limited to one specific implementation and that any one all or some of the implementations can be implemented as desired.

In one embodiment there can be a partition client data structure on each state replica. The partition client can monitor and provide information regarding engine health for a particular engine. For example it can keep track of the last time that the state tier has heard from the engine the last time that the engine has accessed the state tier . If the time period reaches a specified length e.g. 1 second the partition client can ping the engine to check whether it has failed. If the engine fails to respond it can be declared dead by the partition client. In one embodiment part of declaring the engine dead can include reassigning the timers associated with that engine to one or more other engines in the cluster. Since the engine is assumed to have failed the timers can be considered un owned and thus they can be reassigned to the next engine that polls the state tier. Alternatively the un owned timers can be distributed among several engines to reduce any possible processing overload.

During regular call processing messages are received by the load balancer. The load balancer can be a sticky load balancer meaning that it can distribute the message to the engine that handled the last message for the particular call. Thus once a message is received it is distributed to the engine which can in turn retrieve and set the call state in the state tier. For example the engine can retrieve state information process the message set the call state and cache some or all of the state information in a near cache at the engine. In one embodiment the engine node has a particular identifier ID associated with it and this information can also be maintained in the state replica. Accordingly the state replica can contain information regarding which engine has set the particular call state.

Part of setting the call state includes setting appropriate timers in the state tier. These timers would fire in cases where the time period specified by the timer lapses before the next message is received or some other specific event occurs. For example when an INVITE message is received a timer is set. If no response is received to the INVITE message within a certain time period the timer fires.

When timers fire they can be assigned to the particular engine based on the engine ID information as previously mentioned. In order to process the timer the engine can lock the call state in the state tier and process the timer by performing the specified action e.g. retransmitting a message etc. . In some cases the engine can already have the needed state information available locally in the near cache and it may not need to retrieve the call state again.

As illustrated messages from the network can be received to the SIP server deployment at a sticky load balancer . The load balancer can in turn distribute the messages to engine nodes in the engine tier for processing. In one embodiment the load balancer is sticky i.e. it tries to distribute all messages for one call leg to the same engine node for efficiency purposes.

The engine nodes can process the messages by retrieving and setting any necessary call state in the state replica nodes at the state tier . The state nodes can maintain the call state for the calls which can be locked read and updated by the engine nodes.

As illustrated the engine node can receive an incoming message associated with session A and process it. As part of processing the message the engine node may need to set a timer on state node . As such the timer is also associated with session A . Once initiated the timer gradually counts down until its specified time period is expired and then is fired. In one embodiment when the timer expires it is determined which engine node had set the timer and the timer is assigned to that engine node for processing. In this illustration engine node had originally initiated the timer and thus the state node will assign timer to engine node for processing. As part of processing the timer the engine node may perform corrective actions retransmit messages set other timers and execute other functionality.

In various embodiments each engine node can implement a near cache to store a portion of the call state. When such a near cache is used an engine tier server can first check its local cache for existing call state data when processing various messages. In one embodiment if the cache contains the data needed by the engine server and if the local copy is up to date when compared to the state tier copy the engine node can lock the call state in the state tier but read directly from its cache. This can improve response time performance for the request because the engine does not have to retrieve the call state data from a data tier server.

Retrieving call state from the state tier can involve various costs. One such cost is the time duration of the communication and transporting the state data between the engine node and a state replica. Another such cost is the time for serialization and de serialization of the call state. In modern systems serialization is used to transmit an object or data over the network as a series of bytes. De serialization involves using these bytes on the receiving end in order to re construct the object or a copy thereof . As an illustration the Java programming language provides automatic serialization and may require that the object be marked by implementing the java.io.Serializable interface. Java can then handle the serialization internally.

In various embodiments such serialization and de serialization can introduce latency which in certain cases may be undesirable. For example during an initial call set up the SIP server can receive a flurry of initial messages from several SIP clients. It may be advantageous to maintain a local copy of the state on the engine server while handling this flurry of messages instead of repeatedly accessing the state tier upon every message. Maintaining such a local copy can prevent the need to serialize and de serialize the state data each time since it does not need to be transported across the network.

Call state can be moved into an engine s local cache as needed to respond to SIP client requests or to refresh out of date state data. If the cache is full when a new call state should be written to the cache the least used call state entry can be removed from the cache and the new entry written.

In various embodiments the local cache can be more beneficial when a SIP aware or sticky load balancer is used to manage SIP client requests. With a sticky load balancer all of the requests for one call leg or session can be directed to the same engine tier node which can improve the effectiveness of the cache. For example if the load balancer is not SIP aware subsequent messages requests for the same call could be distributed to different engine tier servers which may have different cache contents and thus the performance benefit of the near cache can be inhibited. Even in such embodiments however some performance improvements can be realized as there should be at least some cache hits. On the other hand when messages for the same call leg are distributed to the same engine node it is more likely that the engine node has the state needed for the message stored locally in the near cache. In this manner latency can be further improved.

In some embodiments there may be a tension between using a near cache that is too large and reducing latency caused by garbage collection. Since objects in the near cache can be complex or long lived objects it may be more difficult for the garbage collector to remove them in order to clean up the memory. This can introduce latency as previously discussed. On the other hand using the near cache can reduce the time costs of communicating transporting serializing and deserializing data. Therefore it may be preferable to tune various factors such as the size of the cache the JVM and its garbage collection to a proper balance in order to achieve maximum performance output.

In various embodiments during the processing of messages the engine node can store state information into the near cache as needed. With sticky load balancers the near cache is used on the assumption that the engine that is handling the current message for a particular call will also need that state information for the next message associated with that call. During times when numerous messages are expected for a session the near cache can be used to load the call state therein. When timer affinity is used the near cache can provide further performance advantages because the state information needed to process a timer is more likely to be in the near cache of an engine that set the timer. This is because the engine that originally set the timer had also processed a message associated with the same call or session. Thus if the timer is fired on the engine that originally set the timer it is more likely that that engine would not need to re obtain the call state from the state tier in order to process the timer.

As shown in step an engine node can be deployed for processing signaling messages. The engine node can reside in a distributed cluster network of multiple engine nodes called an engine tier. In step the engine node can process an incoming message from the network. The incoming message can be associated with a session between two or more participants such as mobile phones PDAs or other devices.

In step during the processing of the message the engine initiates a timer associated with the session. The timer is gradually counting down until a time period expires. In various embodiments the timers are maintained at a state node located remotely from the engine node. In step the state node can determine that the timer associated with the session has expired. When this occurs the state node can determine which engine node in the distributed cluster network processed the last incoming message associated with the session as shown in step . Once that is determined the timer can be executed on the engine node that processed the last incoming message for the call as shown in step .

As shown in step the engine nodes can be processing various SIP messages. During the processing of these messages the engine can read and write state information to the state tier. In addition each engine periodically poll the state tier for any expired timers that belong to that particular engine as shown in step . When polled the state node can determine if there are any expired timers that belong to the polling engine step and if there are the state node can assign the timer to the engine that owns it as shown in step . In addition if there are any un owned timers e.g. as in the event of some engine failure these un owned timers can also be assigned to the next polling engine. If there are no expired timers the state node need not take any action and can wait until the next poll. This type of implementation removes some of the processing load from the state replicas and allows them to function as more of a storage unit.

In step if the engine is assigned timers which have affinity to it the engine can process the timers by performing the appropriate corrective action as specified by the SIP protocol. If the timer is an application timer the engine can perform an action that is custom according to the application.

As shown in step once deployed the engine nodes can continuously process messages and set timers on state nodes. When a timer expires step the state node can determine which engine node had set the timer as shown in step . Once that determination is made a notification can be transmitted to the engine node as shown in step . The engine node can thus receive the notification and process the expired timer as shown in step .

In certain cases the second implementation may add some processing to the state replica nodes since they need to notify the appropriate engine when a timer has expired. This may or may not be desirable depending on context. Either of the implementations can be used and alternatively the selection of the implementation may be made configurable.

As shown in step the engine node can set a timer on a partition that can include one or more state tier replicas. As shown in step the timers have a unique identifier that associates them to a particular engine. In step the engines periodically poll the state tier replicas for expired timers. In response the state tier can return all expired owned and or orphaned timers to the engine and lock the respective timers in the state tier. At this point if state information is in the engine near cache step there is no need to deserialize the timers step before processing them step . In certain embodiments it may also be desirable to check whether the state information is up to date when compared to the state tier as other engine nodes may have changed the call state since it was stored in the near cache.

If the state information is not available in the near cache the engine node may need to de serialize the state information retrieved from the state tier as shown in step . However even in these instances timer affinity can provide certain benefits. For example other data which is unrelated to the SIP protocol may be locally available or cached at the server hosting the engine node. Furthermore hot standby type of deployments can be enabled such that there can be engines which do not process any timers. Additional performance enhancements can also be enabled as will be apparent to one of ordinary skill in the art based on this disclosure.

When using the polling implementation each engine can periodically poll the state replicas for expired timers. In order to ensure the service level agreement SLA the engine may need to maintain a required accuracy of firing and thus may be required to poll every specified interval of time. In some embodiments it may be advantageous to reduce or minimize the number of polls so as to reduce the overall network traffic. Thus in one embodiment the engine poll interval is calculated by the data tier so as to ensure the SLA while at the same time minimizing polls.

As illustrated the SLA can be adjusted by a timer increment variable e.g. 100 milliseconds . This can be a configurable variable in the SIP server which specifies the maximum period of time that the engine can wait before processing the timer once fired. In one embodiment each engine can poll the state replicas periodically upon each interval to check whether any timers that belong to that engine have fired. However since timers generally fire infrequently this may lead to excessive and unnecessary polling. Thus in one embodiment engine polling can be optimized according to whether there are any expired timers that need to be processed.

In order to minimize the polling a hint can be provided by the state replicas regarding whether the next poll is optional for the engine. This can be done by setting an optional poll flag to true during the standard state read write operations. In one embodiment if the flag is set to optional true the engine will not poll the state tier. Instead the engine will reset the flag to false so that the engine would poll the state tier upon next time interval in the future.

As illustrated in in order to guarantee the SLA the engine can be required to poll the state tier at certain periods. In one embodiment these periods and can be designated as must poll regardless of any hinting. For example assuming the poll interval is set to 100 milliseconds and assuming there are 4 engine nodes in the cluster each engine node can be required to poll the state replicas every 400 4 100 milliseconds . In one embodiment this is done in order to stagger the engines such that there is at least one engine polling the state tier every SLA interval e.g. 100 ms .

In addition there can be other intervals which are designated as optional polling intervals for the particular engine. At these optional intervals the engine will poll unless the optional flag is set to true. Throughout regular system processing each engine performs many reads writes to the state tier . During these state read write operations if there are no expired timers that belong to the engine the state replica can set the optional flag to true . In that case when the engine reaches the next optional polling interval the engine would read the flag and choose not to poll the state tier. Instead the engine would reset the optional flag to false default value for the next time interval. Thus in a typical busy system since there are numerous state write operations being performed continuously and since timers fire only infrequently most engines would not poll the state tier during the optional time intervals. In an idle system on the other hand the engines would likely poll the state tier very often since there would not be many read write operations and thus not many hints via optional flag provided by the state tier. However this is acceptable since in an idle system there isn t a large processing load of client communications to handle for the engines.

In various embodiments the SIP server can have two modes of operation. In the first mode the SIP server can function without any timer affinity and the timers can be distributed across the engines so as to minimize timer polling while still ensuring the SLA. In the second mode the SIP server can function with timer affinity and timers would be fired on the engine that last accessed the call. With the timer affinity mode the state replica can request that each engine in the cluster return for a poll after the specified time period. In order to reduce unnecessary polling the polling strategy can be further optimized by allowing the engine to decide that the next poll is optional as described in .

Various embodiments previously described include a computer program product which is a storage medium media having instructions stored thereon in which can be used to program a general purpose or specialized computing processor s device s to perform any of the features presented herein. The storage medium can include but is not limited to one or more of the following any type of physical media including floppy disks optical discs DVDs CD ROMs micro drives magneto optical disks holographic storage ROMs RAMs PRAMS EPROMs EEPROMs DRAMs VRAMs flash memory devices magnetic or optical cards nanosystems including molecular memory ICs paper or paper based media and any type of media or device suitable for storing instructions and or information.

Various embodiments include a computer program product that can be transmitted in whole or in parts and over one or more public and or private networks wherein the transmission includes instructions which can be used by one or more processors to perform any of the features presented herein. In various embodiments the transmission may include a plurality of separate transmissions.

Stored one or more of the computer readable medium media the present disclosure includes software for controlling both the hardware of general purpose specialized computer s and or processor s and for enabling the computer s and or processor s to interact with a human user or other mechanism utilizing the results of the present invention. Such software may include but is not limited to device drivers operating systems execution environments and containers as well as user interfaces and applications.

The foregoing description of the preferred embodiments of the present invention has been provided for purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations can be apparent to the practitioner skilled in the art. Embodiments were chosen and described in order to best explain the principles of the invention and its practical application thereby enabling others skilled in the relevant art to understand the invention. It is intended that the scope of the invention be defined by the following claims and their equivalents.

