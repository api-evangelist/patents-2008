---

title: Linear laplacian discrimination for feature extraction
abstract: An exemplary method for extracting discriminant feature of samples includes providing data for samples in a multidimensional space; based on the data, computing local similarities for the samples; mapping the local similarities to weights; based on the mapping, formulating an inter-class scatter matrix and an intra-class scatter matrix; and based on the matrices, maximizing the ratio of inter-class scatter to intra-class scatter for the samples to provide discriminate features of the samples. Such a method may be used for classifying samples, recognizing patterns, or other tasks. Various other methods, devices, system, etc., are also disclosed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08218880&OS=08218880&RS=08218880
owner: Microsoft Corporation
number: 08218880
owner_city: Redmond
owner_country: US
publication_date: 20080529
---
Discriminant feature extraction plays a central role in recognition and classification. Principal component analysis PCA is a classic linear method for unsupervised feature extraction. PCA learns a kind of subspaces where the maximum covariance of all training samples is preserved. More specifically PCA is mathematically defined as an orthogonal linear transformation that transforms given data to a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate called the first principal component the second greatest variance on the second coordinate and so on. PCA is theoretically the optimum transform for given data in least square terms.

To facilitate explanation of various techniques consider face recognition where data are presented in the form of image data. The ability to perform face recognition can be tested according to standards of the Face Recognition Grand Challenge FRGC . For example a FRGC version 2.0 test consists of three components i a data set of images of a person i.e. a face ii a Biometric Experimentation Environment BEE distribution that includes all the data sets for performing and scoring trials and iii a set of baseline algorithms for performing trials. With all three components it is possible to run trials by processing raw images to producing Receiver Operating Characteristics ROCs where performance can be judged based on ROCs.

A conventional approach involves so called eigenfaces which are a set of eigenvectors used in the computer vision problem of human face recognition. To explain an eigenvector consider that a linear transformation may operate on a vector to change it for example by changing its magnitude and its direction. An eigenvector of a given linear transformation is a non zero vector which is multiplied by a constant called the eigenvalue as a result of that transformation. The direction of the eigenvector is either unchanged by that transformation for positive eigenvalues or reversed for negative eigenvalues . In general linear transformations of a vector space such as rotation reflection stretching compression shear or any combination of these may be visualized by the effect they produce on vectors. In other words linear transformations are linear vector functions. Eigenfaces which are a set of eigenvectors are derived from the covariance matrix of a probability distribution of a high dimensional vector space of possible faces of human beings.

To generate a set of eigenfaces a large set of digitized images of human faces taken under similar lighting conditions can be normalized to line up the eyes and mouths. The images can then be resampled at the same pixel resolution. Eigenfaces can be extracted out of the image data by PCA. For example the following steps can convert an image of a face into eigenfaces i prepare a training set T ii subtract the mean where the average matrix A is calculated and subtracted from the original in T and the results stored in variable S iii calculate the covariance matrix iv calculate the eigenvectors and eigenvalues of the covariance matrix and v choose the principal components.

In step iv there will be a large number of eigenfaces and in general far fewer are needed. To reduce the number one can select those that have the largest eigenvalues. For instance a set of 100 pixel by 100 pixel images will create 10 000 eigenvectors. Since most individuals can be identified using a database with a size between 100 and 150 most of the 10 000 eigenvectors can be discarded.

In a typical example the eigenfaces created will appear as light and dark areas that are arranged in a specific pattern. This pattern represents how different features of a face can be singled out to be evaluated and scored. Often patterns exist to evaluate symmetry style of facial hair hairline position nose size or mouth size. Other eigenfaces can have patterns that are less simple to identify and the image of the eigenface may look very little like a face.

Techniques used in creating eigenfaces may find use outside the realm of facial recognition. For example the foregoing technique has also been used for handwriting analysis lip reading voice recognition sign language hand gestures and medical imaging. Therefore some prefer use of eigenimage instead of eigenfaces.

As mentioned the so called eigenfaces method for face recognition applies PCA to learn an optimal linear subspace of facial structures. PCA also plays a fundamental role in face sketch recognition. Locality Preserving Projections LPP is another typical approach for un supervised feature extraction. LPP is the linearization of Laplacian Eigenmaps which can find underlying clusters of samples. LPP shows superiority in terms of image indexing and face recognition.

The Laplacian faces face recognition method is based on the combination of PCA and LPP in the sense that LPP is performed in the PCA transformed feature space. However un supervised learning cannot properly model underlying structures and characteristics of different classes.

Discriminant features are often obtained by class supervised learning. Linear discriminant analysis LDA is the traditional approach to learning discriminant subspaces where the between class scatter of samples is maximized and the within class scatter is minimized at the same time. The so called Fisherfaces algorithm and many variants of LDA have shown good performance in face recognition in complex scenarios.

By defining representations of intra personal and extra personal differences Bayesian face recognition proposes another way to explore discriminant features via probabilistic similarity measure. In one study the inherent connection between LDA and Bayesian faces was unified in a more general form.

LDA algorithm has the advantages of being reasonable in principle and simple in form. The conventional LDA algorithm is formulated by the ratio of between class scatter and the within class scatter which are represented by norms measured with Euclidean metrics. So there is an underlying assumption behind LDA that it works in Euclidean spaces. However there are many scenarios where sample spaces are non Euclidean in computer vision. For instance distances between feature vectors yielded by histograms cannot be measured by Euclidean norms. In this case some non Euclidean measures are usually applied such as the Chi squares statistic the log likelihood statistic and the histogram intersection. The primary formulation of LDA does not hold in non Euclidean spaces. As a consequence LDA fails to find the optimal discriminant subspace.

As described herein various exemplary techniques can be applied to high dimensional spaces that may have non Euclidean metrics. While the foregoing discussion mentions face recognition various exemplary techniques can be applied in areas other than face recognition and in areas where data are other than image data.

An exemplary method for extracting discriminant feature of samples includes providing data for samples in a multidimensional space based on the data computing local similarities for the samples mapping the local similarities to weights based on the mapping formulating an intra class scatter matrix and an inter class scatter matrix and based on the matrices maximizing the ratio of inter class scatter to intra class scatter for the samples to provide discriminate features of the samples. Such a method may be used for classifying samples recognizing patterns or other tasks. Various other methods devices system etc. are also disclosed.

Discriminant feature extraction plays a fundamental role in pattern recognition. As described herein various exemplary methods employ a Linear Laplacian Discrimination LLD algorithm for discriminant feature extraction.

In various examples LLD is presented as an extension to Linear Discriminant Analysis LDA . As noted in the Background section LDA does not work well in cases where sample spaces are non Euclidean. To handle non Euclidean spaces an exemplary LLD approach defines within class scatter and between class scatter using similarities which are based on pair wise distances in sample spaces. In this approach structural information of classes is contained in the within class and the between class Laplacian matrices are free from metrics of sample spaces. In turn the optimal discriminant subspace can be derived by controlling the structural evolution of the Laplacian matrices.

Trials results are presented further below where data were selected from the facial database for Facial Recognition Grand Challenge FRGC version 2. Trial results show that LLD is effective in extracting discriminant features. While such trial data pertain to images various exemplary techniques described herein can be applied to any of a variety of data. In other words various exemplary techniques can be applied to data other than image data.

An exemplary LLD approach formulates within class scatter and between class scatter by means of similarity weighted criteria. These criteria benefit from the advantages of Laplacian Eigenmaps and LPP. In this exemplary LLD approach similarities can be computed from an exponential function of pair wise distances in original sample spaces which can be measured by either Euclidean or non Euclidean metrics. Consequently this LLD approach can be applied to any linear space for classification. The structural information of classes is governed by the within class Laplacian matrix and the between class Laplacian matrix. These two matrices evolve with time which is a free parameter in the similarity measure. From this viewpoint LDA is exactly a special case when the time approaches the positive infinity. Therefore LLD not only overcomes the problems of non Euclidean metrics but also presents an alternative way to find better discriminant subspaces.

As mentioned trials were performed for face identification on a subset of facial database for FRGC version 2. Trial results are compared for the LLD method with PCA LPP LBP and the traditional LDA.

In the trials discriminant features were extracted on PCA and LBP expressive features implying that LLD LPP and LDA are performed in the PCA and LBP transformed spaces respectively. The PCA expressive features can be viewed as Euclidean whereas the LBP expressive features are non Euclidean. Trial results show that an exemplary LLD approach outperforms various conventional methods in terms of discrimination power.

As mentioned the conventional LDA approach to discrimination uses Euclidean distance as a metric measure whereas an exemplary approach uses similarity as the inherent characteristic of pair wise points instead of Euclidean distance. In Laplacian Eigenmaps for manifold learning and its linearization LPP for clustering and recognition geometric distances between mapped points that lie on an underlying manifold can be controlled by similarities between corresponding points in the original space while underlying clusters will appear automatically after non linear maps. Linearization of such criteria can yield good performance in image indexing e.g. for image searches clustering and face recognition. As described herein an exemplary linear Laplacian algorithm allows for use of similarity to perform discrimination e.g. Linear Laplacian Discrimination LLD . For example discrimination can be used to perform tasks such as pattern recognition and classification.

In the example of the mapping block can map the computed local similarities to weights using a non linear function. For example an exponential function may be used that ranges from 0 to 1 where a weight of 0 a minimum corresponds to no similarity and a weight of 1 a maximum corresponds to infinite similarity. In contrast to LDA where weights are always 1 in this example LLD can use weights that vary from a minimum of 0 to a maximum of 1 where a higher weight means that a data sample is of higher importance. The mapping block thus provides weights for similarity weighted discrimination criteria.

In the example of the formulation block uses the similarity weighted discrimination criteria to define a within class Laplacian matrix and a between class Laplacian matrix which are referred to collectively as scatter matrices. In general for purposes of discrimination samples in the same class should be very close to each other and samples in different classes should be far away from each other.

As mentioned a conventional LDA algorithm relies on the ratio of between class scatter and within class scatter which are represented by norms measured with Euclidean metrics i.e. Euclidean distance based metrics . In contrast the maximization block of maximizes the ratio of the scatter by maximizing the ratio of the between class scatter inter class and far away to the within class scatter intra class and close based on similarity e.g. similarity weighted discrimination criteria . In turn the maximization block can find the optimal projection.

The method of can be applied using Euclidean spaces or non Euclidean spaces for purposes of computing local similarities per computation block . In other words for local similarities Euclidean distance may be guaranteed to some extent locally which is not necessarily true globally. Hence the computation block can use Euclidean distance for computation of local similarities.

An exemplary method for extracting discriminant feature of samples can include providing data for samples in a multidimensional space see e.g. block based on the data computing local similarities for the samples see e.g. block mapping the local similarities to weights see e.g. block based on the mapping formulating an inter class scatter matrix and an intra class scatter matrix see e.g. block and based on the matrices maximizing the ratio of inter class scatter to intra class scatter for the samples see e.g. block to for example provide discriminate features of the samples.

An exemplary method for extracting discriminant feature of samples may rely on a conventional approach to reduce dimensionality of a space. For example an exemplary method may include providing data for samples in a multidimensional Euclidean space reducing the dimension of the Euclidean space using principle component analysis PCA reducing the dimension of the PCA reduced space by computing similarities for the samples and mapping the similarities to weights using a non linear function to thereby remove dependence on the Euclidean metric formulating an inter class scatter matrix and an intra class scatter matrix and based on the matrices maximizing the ratio of inter class scatter to intra class scatter for the samples to provide discriminate features of the samples.

As described herein an exemplary method considers similarity as the inherent characteristics of pair wise points e.g. data samples as opposed to conventional approaches that rely on Euclidean distance as the inherent characteristics of pair wise points e.g. data samples . Hence in such an exemplary method geometric distances between mapped points that lie on an underlying manifold can be controlled by similarities between corresponding points in the original space. Further underlying clusters can be uncovered via non linear mapping. Such a method may be applied to tasks such as indexing clustering classifying and recognizing patterns.

With respect to discriminant scatters let xdenote the i th sample in the s th class where x Mand Mis the D dimensional sample space. One can obtain the associated discriminant feature yof xby projection according to the following equation Eqn. 1 where the d columns of the projection matrix U are the orthogonal bases of discriminant subspace. Let X x x . . . x denote all original samples where n is the number of all samples. Then we have Y UX where Y y . . . y . Given two points xand x the Euclidean distance between them is defined by the following equation Eqn. 2 

Let denote the within class scatter of class s and define it according to the following equation Eqn. 3 

Here t is the time variable and exp denotes the exponential function. It suffices to note that the distance between yand are measured by the Euclidean norm and the distance between xand are measured by the norm which depends on the metric of the original sample space. The space may be Euclidean or non Euclidean. To obtain a compact expression of Eqn. 3 let W diag w w . . . w be a diagonal matrix and Y y y . . . y . Besides let edenote the all one column vector of length c. Then 1 c Ye. Rewriting Eqn. 3 provides the following series of equations Eqns. 5 10 

With Eqns. 5 10 one may obtain the following equation Eqn. 11 where the following equation provides for L Eqn. 12 

By letting denote the total within class scatter of all samples the following equation results Eqn. 13 

In this analysis there is a 0 1 indicator matrix Ssatisfying Y YS. Each column of Srecords the class information which is known for supervised learning. Substituting the expression of Yinto Eqn. 13 gives the following equation Eqn. 14 

Plugging the expression of Y into Eqn. 14 one arrives at the final form of the total within class scatter per the following equation Eqn. 16 where D XLXis the within class scatter matrix.

Next the between class scatter of all classes can be defined according to the following equation Eqn. 17 

Let . . . denote the matrix consisting of all center vectors of classes and W diag w w . . . w . Following similar formulations from Eqn. 5 to Eqn. 12 one can rewrite Eqn. 17 as the following equation Eqn. 19 where Lcan be presented as the following equation Eqn. 20 

With respect to finding the optimal projection see e.g. the maximization block of to make projected samples favor of classification in feature space it is expected that samples within the same classes cluster as close as possible and samples between classes separate as far as possible.

An examination of the formulations of the within class scatter Eqn. 3 and the between class scatter Eqn. 17 shows that the smaller the distance between xand is the larger the similarity wis. If the within class scatter remains constant from Eqn. 3 it is known that y will be small if the weight wis large implying that ywill be close to its center . So ywill approach its center as approaches the minimum. Therefore the expectation on within class samples will be fulfilled if the total within class scatter is minimized.

By the similar analysis the expectation on between class samples being far apart will be realized if the between class scatter is maximized. To summarize the following dual objective optimization model results Eqn. 22 

To simplify the optimization the following Fisher criterion may be constructed according to the following equation Eqn. 23 

To solve for U the above optimization can be performed on Grassmann manifolds where U is viewed as a point on geodesic flows. As described in this exemplary analysis an approach used in conventional LDA may be taken to solve the above optimization problem. For example take the d eigenvectors from the following generalized eigen analysis according to the following equation Eqn. 25 that are associated with the d largest eigen values i 1 . . . d.

Like LDA LLD encounters some computational issues when D is singular. D is not invertible when L is not of full rank. Such cases can occur frequently in computer vision since images may have large dimensions whereas the number of classes is usually small. However the generalized eigen analysis of Eqn. 25 needs a positive definite D. Several strategies exist to address the issue consider the following two approaches.

When the original sample space is Euclidean discriminant features can be extracted from expressive features yielded by PCA. Namely LLD can be performed in the PCA transformed space. Specifically let Udenote the matrix whose columns are a set of orthogonal base of the principal subspace. First project tilde over D and tilde over D into the PCA transformed space to give tilde over D U tilde over D Uand tilde over D U tilde over D U. Then one can perform the generalized eigen analysis of Eqn. 25 using tilde over D and tilde over D . By letting Udenote the discriminant subspace one can then represent the final transformation as UU.

In this approach let the eigen decomposition of D be D V V where V is the eigen vector matrix and is the diagonal eigenvalue matrix. Suppose V is split into V V V where Vconsists of eigenvectors corresponding to the r non zeros eigenvalues and Vconsists of eigenvectors associated with the d zero eigenvalues where r is the rank of D. In a dual subspace a goal is to project Dinto Vand I VVrespectively then perform eigen analysis on the projected between class scatter matrices which can be viewed as projecting the center of each class in the two spaces and performing PCA respectively. Next one can compute D VDVand D VDV. Now let Qand Qdenote the principal eigenvector matrices of Dand D respectively. Then one arrives at two dual projection matrices W VQand W VQ.

Given two samples xand x the distance between their feature vectors yand yis determined by the following equation Eqn. 26 Note that for LDA projecting samples only on the subspace spanned by Wis essentially akin to tackling the singular problem of the within class scatter matrix by simultaneous diagonalization. As presented the dual LLD and the dual LDA means that LLD and LDA are performed by dual subspaces. Comparisons to Other Approaches

An exemplary LLD approach becomes like LDA as t approaches positive infinity in the similarity functions of Eqns. 4 and 18. So the discriminant subspace of LDA is a stable state of the evolution of that of LLD with respect to the time t. Therefore LLD is a more general version of LDA as such an exemplary LLD approach inherits the strengths of an LDA approach.

The LLD method also has some relationship to LDE and MFA approaches. Overall the LDE and MFA approaches can be viewed as specific forms of graph embedding. However in principle they are essentially different. LDE and MFA are more complicated as they take advantage of the partial structural information of classes and neighborhoods of samples at the same time while LDA and LLD purely explore the information of classes for discrimination.

As mentioned trials focused on the problem of face identification. Given a novel face the identification problem is that the system is asked to find the identity of the person in a gallery where the portrait of the person is presented. The motivation of this task comes from the current trends of performing face recognition or retrieval based on the facial images on the web or photos in digital family albums. In such cases one is usually interested in finding the most similar faces of a given sample which can be converted to be the face identification problem.

In the trials related experiments were performed on a subset of facial data in experiment 4 of FRGC version 2. The query set for experiment 4 in this database consists of single uncontrolled still images which contains all the diverse factors of quality presented in the preceding subsection. There are 8014 images of 466 subjects in the set. However there are only two facial images available for some persons. To help guarantee meaningful results for the tasks given above a subset in the query set was selected for performing trials.

To ensure reproduction of the trials procedures were performed as follows. First all images of each person in the set were searched and the first ten facial images taken if the number of facial images was not less than ten. This resulted in 3160 facial images of 316 subjects. Then the 316 subjects were divided into three subsets. The first 200 subjects were used as a gallery and probe set and the remaining 116 subjects were used as a training set. Second the first five facial images of each person were taken as the gallery set and the remaining five images as the probe set. Therefore the set of persons for training is disjoint with that of persons for the gallery and the probe.

Table 1 contains the information of facial data for experiments. The facial images were aligned according to the positions of eyes and mouth of each person. Each facial image was cropped to a size of 64 72.

Disciminant feature extraction was performed on the expressive features yielded by PCA and LBP respectively. This means that LLD LPP and LDA were performed in the PCA and LBP transformed spaces respectively. As mentioned PCA is the classic and well recognized method for expressive feature extraction while LBP is a newer approach which has proven effective for un supervised feature extraction. The PCA feature space is Euclidean. The distances in this space are measured by the Euclidean norm Eqn. 2 . The LBP feature space is non Euclidean. A distance measure in such a space is the Chi square defined by the following equation Eqn. 27 

For the conventional PCA based two step strategy the number of principal components is a free parameter. The dimension of principal subspaces significantly affects the performance of recognition for the PCA plus LDA strategy. Studies confirmed by experiments that the optimal number lies in the interval of 50 to 200. Based on such studies for a baseline a search for the optimal number of principal components in this interval found that PCA performs best when the dimension of feature vectors is 190. Hence 190 was taken as the number of principal components.

An exemplary approach was named Laplacian Fisherfaces L Fisherfaces noting that it is Laplacian kernelized and formulated by Fisher criterion. As shown in a plot of L Fisherfaces converge to Fisherfaces with a fast speed within t 1 as shown by recognition rates based on PCA features. In the plot Eigenfaces EF is the baseline and Eigenfaces EF PCA Laplacianfaces LF PCA plus LPP Fisherfaces FF PCA plus LDA and Lapalcian Fisherfaces L Fisherfaces or L F PCA plus LLD were tested. As mentioned 190 principal components were taken in the PCA step due to that PCA performs best in this dimension the plot ranges from a dimension of 50 to 190 and t 0.01 t 0.1 t 1 t 10 and t 100. Laplacian Fisherfaces L F converge to Fisherfaces FF with a fast speed in the Euclidean feature space.

The best performance of LLD was shown as approximately achieved when LLD arrived at its stable state where each Wwas essentially the identity matrix when t 100. This result means that the principal subspace of LLD yields the best discriminant performance in the Euclidean feature space when it approaches the stable state. The trials also examined visually the various eigenfaces and their evolution across time.

Trials were also performed with LBP for each facial image which were then sub divided by 7 7 grids. Histograms with 59 bins were performed on each sub block. A LBP feature vector was obtained by concatenating the feature vectors on subblocks. For the trials 58 uniform patterns for LBP were used where each uniform pattern accounted for a bin. The remaining 198 binary patterns were put in another bin resulting in a 59 bin histogram. So the number of tuples in a LBP feature vector was 59 7 7 2891. The parameters of 8 2 for LBP were adopted. Namely the number of circular neighbors for each pixel was 8 and the radius of the circle was 2.

As described herein an exemplary technique uses LLD for pattern classification and discriminant feature extraction. This technique uses similarity weighted discriminant criteria to define a within class Laplacian matrix and a between class Laplacian matrix. This LLD approach has the flexibility of finding optimal discriminant subspaces.

Trials performed on a subset in FRGC version 2 demonstrated that an exemplary LLD approach is at least equivalent to a conventional LDA approach when the feature space is Euclidean and is superior to the conventional LDA approach when the feature space is non Euclidean. In addition an exemplary LLD approach can significantly improve discriminant performance of expressive facial features yielded by PCA and LBP. These trial results indicate that discriminant criterions formulated in an exemplary LLD approach are more suitable for discriminant feature extraction than various other conventional techniques.

As discussed herein whether a sample space is Euclidean or non Euclidean an exemplary LLD approach is capable of capturing discriminant characteristics of samples.

As described herein an exemplary method may be implemented in the form of processor or computer executable instructions. For example a computing device may include instructions and associated circuitry to perform an exemplary LLD method. shows various exemplary modules that include such instructions. One or more of the modules may be used in a single device or in multiple devices to form a system. Some examples are shown as a portable device a personal computer a server with a datastore and a networked system e.g. where the network may be an intranet or the Internet . The devices may interact or operate as part of a system in an application space . For example the application space shows a security system lab equipment and machine vision for quality assurance .

The modules include a data acquisition module to acquire data a similarity computation module to compute similarities a weighting module to weight similarities a matrix formulation module to formulate matrices e.g. scatter matrices an optimization module to perform optimizations e.g. based on ratio of intra to inter class scatter and a results reporting module to report results or take further action.

In a particular example a security system may rely on biometrics e.g. facial recognition . Such a system may include a video or still picture camera. A computing device may include the acquisition module to acquire information from the video or still picture camera. Data acquisition may also prepare data for analysis for example as described with respect to the trials. The computing device may report results to the security system according to the reporting module to cause an action to occur e.g. open a door a gate a vault etc. .

In another example lab equipment may rely on spectroscopic data or other data that is expected to include some inter class and intra class relationships. Such a system may acquire information in a multidimensional manner e.g. two dimensional or more . For example spectroscopy equipment may acquire data that can be presented in a format similar to an image. In other instances a microscope may capture images of cells crystals etc. A computing device may include the acquisition module to acquire information from the lab equipment. Data acquisition may also prepare data for analysis for example as described with respect to the trials. The computing device may report results to the lab equipment or to a lab technician according to the reporting module to cause an action to occur e.g. to raise temperature of a lab sample add a chemical to a mixture etc. .

In yet another example machine vision equipment may rely on visual data that is expected to include some inter class and intra class relationships. Such a system may acquire information in a multidimensional manner e.g. two dimensional or more . For example a camera on an assembly line may acquire data of assemblies for purposes of quality control. In other instances a video camera may track motion. A computing device may include the acquisition module to acquire information from the machine vision equipment. Data acquisition may also prepare data for analysis for example as described with respect to the trials. The computing device may report results to the machine vision equipment or to a technician according to the reporting module to cause an action to occur e.g. to stop an assembly line to adjust an assembly or manufacturing process etc. .

In a very basic configuration computing device typically includes at least one processing unit and system memory . Depending on the exact configuration and type of computing device system memory may be volatile such as RAM non volatile such as ROM flash memory etc. or some combination of the two. System memory typically includes an operating system one or more program modules and may include program data . The operating system include a component based framework that supports components including properties and events objects inheritance polymorphism reflection and provides an object oriented component based application programming interface API such as that of the .NET Framework manufactured by Microsoft Corporation Redmond Wash. The device is of a very basic configuration demarcated by a dashed line . Again a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.

Computing device may have additional features or functionality. For example computing device may also include additional data storage devices removable and or non removable such as for example magnetic disks optical disks or tape. Such additional storage is illustrated in by removable storage and non removable storage . Computer storage media may include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. System memory removable storage and non removable storage are all examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of device . Computing device may also have input device s such as keyboard mouse pen voice input device touch input device etc. Output device s such as a display speakers printer etc. may also be included. These devices are well known in the art and need not be discussed at length here.

Computing device may also contain communication connections that allow the device to communicate with other computing devices such as over a network e.g. consider the aforementioned network of . Communication connections are one example of communication media. Communication media may typically be embodied by computer readable instructions data structures program modules etc.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

