---

title: Multi-layer hardware-based service acceleration (MHSA)
abstract: Methods and apparatus for intelligent sharing and tighter integration between a service engine (SE) for network communication and a high-speed forwarding device, such that certain network flows may be offloaded from the SE to benefit from the high-speed forwarding capacity of such a device are provided. To accomplish the integration, an application binary interface (ABI) may be employed as an in-band high-priority communication protocol between the data planes of the SE and the high-speed forwarding device, and an application programming interface (API) may be utilized to leverage the ABI and any in-band or out-of-band channel to allow the master SE to control the high-speed slave device. Such integration techniques are not limited to a few specialized hardware components, but may also be applied to other types of hardware resources, such as flow tables, quality of service (QoS) tables, access control list (ACL) tables for security, forwarding and adjacency tables, etc.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08327014&OS=08327014&RS=08327014
owner: Cisco Technology, Inc.
number: 08327014
owner_city: San Jose
owner_country: US
publication_date: 20080630
---
Embodiments of the present invention generally relate to the field of network communication and more particularly to combining the benefits of a specialized service engine SE and the raw power of a high speed forwarding device with intelligent sharing for tighter device integration.

Networking devices for routing network traffic may comprise a service engine or appliance and one or more switch modules. While the service engine may contain a sophisticated processor for managing numerous tasks including handling new network connections and applying certain network policies the switch modules are typically designed with one goal in mind to route network traffic quickly and efficiently. Despite the increases in switch performance over the years with application specific integrated circuits ASICs geared towards these forwarding devices specialty the high speed forwarding capacity of many switch modules remains largely untapped for applying SE supported network services e.g. policies at much higher performance levels.

Accordingly techniques for increased forwarding performance and network policy enforcement performance are needed.

Embodiments of the present invention generally relate to network traffic processing acceleration by intelligent sharing and tighter integration between a service engine SE and specialized hardware components of a networking device. For example some embodiments may relate to offloading certain network flows from the SE of a networking device to a high speed forwarding device for increased device performance. Embodiments of the present invention generally provide methods and apparatus for the discrete SE and forwarding devices in the networking device to work as a unified high performance networking device to apply forwarding and network policies both at control and data planes and for the SE to use the forwarding device hardware resources as virtual hardware resources to be used at one of the switching layers.

One embodiment of the present invention provides an apparatus. The apparatus generally includes a device for network communication. The device generally includes a high speed forwarding device and a service engine coupled to the high speed forwarding device and configured to handle network flows wherein the high speed forwarding device is capable of higher speed forwarding than the service engine and wherein the service engine offloads a portion of the network flows to be handled by the high speed forwarding device.

Another embodiment of the present invention provides a method. The method generally includes using a slow path of a service engine for network flow of a new network connection making a forwarding decision to use an ultra fast path of a high speed forwarding device coupled to the service engine instead of the slow path for the network flow if the new network connection meets one or more criteria or to use a fast path of the service engine instead of the slow path if the new network connection does not meet the criteria and using the ultra fast path or the fast path for the network flow according to the forwarding decision.

Yet another embodiment of the present invention provides a computer readable medium containing a program for determining a forwarding path for a new network connection which when executed by a processor performs certain operations. The operations generally include using a slow path of an SE for network flow of the new network connection making a forwarding decision to use an ultra fast path of a high speed forwarding device coupled to the service engine instead of the slow path for the network flow if the new network connection meets one or more criteria or to use a fast path of the service engine instead of the slow path if the new network connection does not meet the criteria and using the ultra fast path or the fast path for the network flow according to the forwarding decision.

Embodiments of the present invention provide networking apparatus and techniques for intelligent sharing and tighter integration between a service engine SE for network communication and a high speed forwarding device such that certain network flows may be offloaded from the SE to benefit from the high speed forwarding capacity of such a device thereby increasing the performance of the networking apparatus. To accomplish the integration for some embodiments an application binary interface ABI may be employed as an in band high priority communication protocol between the data planes of the SE and the high speed forwarding device and an application programming interface API may be utilized to leverage the ABI and any in band or out of band channel to allow the master SE to control the high speed slave device. In general such integration techniques are not limited to a few specialized hardware components e.g. high speed switch hardware since they may also be applied to numerous common types of hardware resources such as flow tables quality of service QoS tables access control list ACL tables for security forwarding and adjacency tables buffer memories general purpose registers etc.

As used herein a service engine SE generally refers to a specialized device for use on a network dedicated to performing certain applications as opposed to a general purpose computer. For example an SE may provide load balancing intrusion prevention advanced QoS and or firewall capabilities. An example of an SE includes the PIX firewall a security appliance designed to protect IP Internet Protocol networks from unwanted inbound traffic. An SE may also be known as a service device a service appliance a server appliance a network appliance an Internet appliance a service blade a service card an embedded service ASIC application specific integrated circuit or an embedded service processor. Hereinafter these shall be referred to as a service engine.

Also as used herein a high speed forwarding device generally refers to specialized hardware intended to route network traffic at typically faster speeds than other forwarding capable hardware. Whereas an SE s embedded service processors may have limited per connection forwarding capacity a high speed forward device may possess one or more switching ASICs designed for much more forwarding capacity. A high speed forwarding device may also be known as a switching node switch hardware a switch linecard or blade a switch module and a forwarding node. Hereinafter these shall be referred to as a high speed forwarding device.

According to MHSA as described in greater detail below an SE may function as a master device taking ownership of portions of the high speed forwarding device which functions as a slave entity in this case. The high speed forwarding device may most likely contain efficient message based fully automated forwarding table programming and purging logic perhaps in an ASIC or a field programmable gate array FPGA . Network flow entries may be stored in the forwarding table logic such as in the NetFlow table on the PFC Policy Feature Card of the Catalyst 6500. To facilitate high speed lookups for flow entries a special piece of high speed lookup memory called ternary content addressable memory TCAM may be used. The SE may elect to offload only certain flows of network traffic to the high speed forwarding device in a direction . The SE s decision may be based on pre configured criteria e.g. protocol and or transaction type and related configurable thresholds such that short and or tough to handle flows may remain with the SE whereas long relatively simple and or bandwidth consuming flows may be fully or partially offloaded to be hardware switched by the high speed forwarding device .

Referring now to MHSA may also be used in the opposite direction in an effort to export hardware information e.g. statistics from the hardware tables of the high speed forwarding device and direct it to the SE to be reconciled with other SE data e.g. statistics of non offloaded packets .

The MHSA approach is very generic and may be applied to various types of SEs functioning as master devices such as service blades appliances and embedded service ASICs and their associated control planes . The MHSA technique may also be applied to numerous types of hardware resources slaves such as NetFlow and other types of flow tables QoS tables ACL tables for security packet based or byte based policer tables adjacency tables service modules e.g. for server load balancing frame managing or fire walling services etc.

Conventionally in a service engine SE new network connections are handled at high speeds in the flow setup path the so called slow path relatively speaking of the SE. Once a forwarding decision is made for a particular connection a shortcut is typically created in the SE s so called fast path a less complex data path for applying repetitive high speed operations. Subsequent packets for that connection then take the shortcut through the fast path. However the overall system throughput remains limited by the maximum performance of the SE. Usually this forwarding limit is significantly smaller than the aggregate capacity of the one or more high speed forwarding devices in a networking device.

Therefore in accordance with the MHSA scheme illustrates a block diagram of an SE tightly integrated with a high speed forwarding device in both the data and control planes in an effort to promote intelligent offloading of certain network flows from the SE to the high speed forwarding device among other features. To benefit from various efficient message based fully automated hardware programming logic mechanisms of the high speed forwarding device the MHSA architecture may utilize a hardware abstraction layer HAL in an effort to abstract the specifics of each high speed forwarding device from the high level software control logic.

High speed hardware programming may be accomplished by employing an in band high priority communication protocol with a programmatic application binary interface ABI for example between any high level entity e.g. a central processing unit or a service processor in the SE and the efficient message based fully automated forwarding table logic. In the ABI is illustrated as providing efficient direct communication between the fast forwarding path in the data plane of the SE and an ultra fast forwarding path i.e. a wire speed path in the data plane of the high speed forwarding device . In conventional network devices the SE processor does not directly interact with or send messages to the ultra fast forwarding path .

The ABI and any in band or out of band communication channel may be leveraged by an application programming interface API in an effort to communicate directly and efficiently between the high speed switch forwarding device and the SE . In the API is illustrated as providing direct communication between the SE processor in the control plane of the SE and the forwarding device processor in the control plane of the high speed forwarding device . The communication protocol the ABI API approach relies upon may support addressing of each internal entity so as to allow for efficient direct communication. In this case each software or hardware component may be addressable with a special unicast address whereas certain categories of components may be addressable as a whole through a multicast address. For some embodiments generic notifications may also be sent as broadcast messages to all addressable entities.

With the architecture of the SE may function as a master with the high speed forwarding device acting as a slave device. New network connections may be initially handled in the slow forwarding path of the SE data plane as described above. For a given new connection a forwarding decision may be made whether to create a shortcut in the fast forwarding path of the SE data plane or to offload traffic for the new network connection to the ultra fast forwarding path of the forwarding device FD data plane according to MHSA. The forwarding decision may be based on one or more criteria such as protocol or transaction type of the network flows and on related configurable thresholds. In this manner shorter and or more complex network flows may most likely be handled by the SE in the slow or the fast forwarding paths while longer simpler and or more bandwidth consuming flows may be fully or partially offloaded to the high speed forwarding device utilizing the ultra fast forwarding path .

In this manner the hierarchy of forwarding hardware e.g. ASICs and processors may be realized as a multi tier architecture according to an MHSA approach with one tier e.g. the slow forwarding path for handling ultra complex forwarding a second tier comprising a number of fast devices e.g. the fast forwarding path to handle high to medium complexity forwarding and a third tier e.g. the ultra fast forwarding path which is capable of handling medium to low complexity forwarding. With the MHSA scheme the overall system throughput is no longer limited by the maximum performance of the SE and can take advantage of the forwarding capacity of the high speed forwarding device s and the SE control plane may also be tightly integrated with the forwarding device control plane. Furthermore the use of a hardware abstraction layer HAL and the control plane integration API to exchange messages between the master and slave entities may allow the master entity code and the resource manager code on the slave entity to reside on physically different devices. The MHSA approach encourages the implementation of direct communication protocols natively into the hardware so that messages may be directly exchanged between the high speed forwarding devices and any higher level entity such as the SE processor .

Moreover hardware information e.g. statistics from the hardware tables of the high speed forwarding device may be exported to the SE processor using a corresponding ABI such that this information may be reconciled with other SE data e.g. statistics of non offloaded packets . The information may be exported inside the payload of in band packets generated by the high speed forwarding device .

For some embodiments the service engine may not reside within a chassis of the networking device housing the high speed forwarding device s . For example the API ABI approach may be utilized to enable integrated communication between an external dedicated firewall appliance such as the PIX Private Internet Exchange from Cisco Systems Inc. and a high speed forwarding device.

One possible embodiment of the MHSA scheme may be realized on the Catalyst 6500 or on a similar networking device. For such embodiments a service module such as the Firewall Services Module FWSM or the Application Control Engine ACE service module may gain direct control over the switch hardware according to the MHSA approach including the ABI API scheme and the messaging technique. The ABI API scheme may allow the SE s control plane to partially integrate with the FD control plane while the messaging technique may allow the SE s processors to send direct messages to the high speed forwarding device. Additionally the API ABI would allow an SE to control the export of the statistics information from the hardware to the blade under the supervision of the switch control plane.

In the Catalyst 6500 implementation above the FD control plane may act as a relay and translation agent whenever the SE and the hardware cannot speak to each other directly. The FD control plane may also function as a resource manager and consistency enforcer. However for other embodiments the MHSA idea may comprise the implementation of direct communication protocols natively into the hardware so that messages may be directly exchanged between the high speed forwarding device and any higher level entity such as the SE processor . In general depending on the specific capabilities of the hardware direct SE to FD communication may be possible or a message translation and relay service may be provided by the FD control plane.

Such integration techniques are not limited to high speed switch hardware but may also be applied to other types of hardware resources such as quality of service QoS tables NetFlow tables for collecting statistics on network traffic that flows through a forwarding device access control list ACL tables for security adjacency tables packet based or byte based policer tables fault management FM modules service modules and policy feature cards PFCs .

As an example of implementing the MHSA approach illustrates a block diagram of a supervisor communicating with a service module through in band and out of band channels. The control plane of the supervisor may comprise a route processor RP and a switch processor SP . The RP may provide Layer 3 functionality and control the forwarding table e.g. the Cisco Express Forwarding or CEF table such that all routing takes place in hardware. The RP may comprise one or more service modules such as a frame management FM and Quality of Service QoS module and a service acceleration API SAAPI agent . The SAAPI agent may contain the ABI for efficient communication between the hardware devices as described above. The SP may control system operation port management and services such as spanning tree protocol STP virtual local area networks VLANs VLAN trunking protocol VTP and Internet Group Management Protocol IGMP . The SP may contain TCAM manager logic for TCAM programming with high speed lookups for flow entries. The supervisor may also contain a forwarding engine for routing network traffic.

The service module may comprise a management CPU or other suitable processor as part of the service module s control plane. The management CPU may comprise a SAAPI agent corresponding to the RP s SAAPI agent . The SAAPI agents may communicate using an Ethernet out of band control EOBC channel which may be a bus for some embodiments. TCAM programming may also be performed over an EOBC channel from the management CPU to the TCAM manager logic . Furthermore the service module may comprise a data plane which may use an in band channel such as a data bus to send certain control packets and offload certain network flows to the supervisor for the forwarding engine to handle.

The supervisor and the service module may both be master devices for different types of traffic portions. For example network traffic as well as the corresponding forwarding and processing resources that is to be just on Layer 2 L2 the data link layer of the Open Systems Interconnection Reference Model or OSI model or Layer 3 L3 the network layer switched may be controlled by the supervisor s processor control plane. In contrast network traffic to which a certain service is to be applied along with the corresponding forwarding and processing resources may be controlled by the control plane e.g. the management CPU of the service module .

According to MHSA two hardware devices such as the supervisor and the service module may possess a means to negotiate who is to be the master and then to partition the hardware s forwarding and processing resources accordingly. Therefore mastership may be a dynamic and negotiable property according to MHSA and traffic forwarding speed may be affected by which resources are allocated by the corresponding master to perform a certain operation. For some embodiments ownership may be determined by checking whether the traffic is to be L2 L3 switched or if the traffic indicates higher level and more complex services than L2 L3 switching.

Once a new network connection is recognized at step the flow setup path also known as the slow path relatively speaking of the SE may be used initially at step in an effort to handle network flow through the new connection. At step the SE may determine whether the network flow for the new connection is complicated. If the network flow is complicated then a forwarding decision may be made to create a shortcut in the SE s fast path and use the fast path for subsequent network flow for this particular connection at step .

In contrast if the network is not complicated as determined at step then at step the SE may determine whether the new connection suggests using a large bandwidth. If a large bandwidth is not indicated at step then a forwarding decision may be made to create a shortcut in the SE s fast path and use the fast path for subsequent network flow for this particular connection at step . If the new connection suggests using a large bandwidth at step then a forwarding decision may be made to offload the network flow for this particular connection to the high speed forwarding device and use the ultra fast path for subsequent network flow thereby boosting performance for the networking device.

The MHSA approach described above is a novel mechanism that may be implemented with API ABI messaging techniques in an effort to significantly boost the performance of a networking device by offloading certain traffic processing e.g. forwarding accounting encryption traffic rewriting traffic dropping traffic buffering etc. to a high speed slave device under full control of the SE control plane thereby circumventing the SE s performance limitations in an effort to accelerate the traffic processing. Network equipment vendors selling service enabled devices may use this idea to achieve higher performance and tighter integration between dumb forwarding devices and smart service aware engines. While the ABI represents the communication language between the SE and the high speed forwarding device the API represents the management glue necessary to ensure control plane integration between such heterogeneous devices.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims that follow.

