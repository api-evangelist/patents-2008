---

title: Encoder with multiple re-entry and exit points
abstract: An encoder is disclosed that is partitioned into discrete hardware modules. The discrete modules include multiple re-entry and exit points that allow enhanced control by software. The software can control the discrete modules during the encoding process and make adjustments according to CPU bandwidth and/or user requirements allowing for enhanced quality control and seamless hardware/software operations. In one embodiment, a media stream is received into an encoder that includes a pipeline of multiple hardware stages for encoding. An intermediate result is provided from at least one of the hardware stages to an encoding control module that processes the intermediate result to determine configuration instructions for a next hardware stage in the pipeline. Thus, the encoding process can be modified dynamically through hardware and software interactions as the media stream progresses through the pipeline of the encoder.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08320448&OS=08320448&RS=08320448
owner: Microsoft Corporation
number: 08320448
owner_city: Redmond
owner_country: US
publication_date: 20081128
---
Companies and consumers increasingly depend on computers to process distribute and play back high quality video content. Engineers use compression also called source coding or source encoding to modify the bit rate of digital video. Compression decreases the cost of storing and transmitting video information by converting the information into a lower bit rate form. Decompression also called decoding reconstructs a version of the original information from the compressed form. A codec is an encoder decoder system.

Compression can be lossless in which the quality of the video does not suffer but decreases in bit rates are limited by the inherent amount of variability sometimes called source entropy of the input video data. Or compression can be lossy in which the quality of the video suffers and the lost quality cannot be completely recovered but achievable decreases in bit rate are more dramatic. Lossy compression is often used in conjunction with lossless compression lossy compression establishes an approximation of information and the lossless compression is applied to represent the approximation.

A basic goal of lossy compression is to provide good rate distortion performance. So for a particular bit rate an encoder attempts to provide the highest quality of video. Or for a particular level of quality fidelity to the original video an encoder attempts to provide the lowest bit rate encoded video. In practice considerations such as encoding time encoding complexity encoding resources decoding time decoding complexity decoding resources overall delay and or smoothness in quality bit rate changes also affect decisions made in codec design as well as decisions made during actual encoding.

In general video compression techniques include intra picture compression and inter picture compression. Intra picture compression techniques compress a picture with reference to information within the picture and inter picture compression techniques compress a picture with reference to a preceding and or following picture often called a reference or anchor picture or pictures.

For intra picture compression for example an encoder splits a picture into 8 8 blocks of samples where a sample is a number that represents the intensity of brightness or the intensity of a color component for a small elementary region of the picture and the samples of the picture are organized as arrays or planes. The encoder applies a frequency transform to individual blocks. The frequency transform converts an 8 8 block of samples into an 8 8 block of transform coefficients. The encoder quantizes the transform coefficients which may result in lossy compression. For lossless compression the encoder entropy codes the quantized transform coefficients.

Inter picture compression techniques often use motion estimation and motion compensation to reduce bit rate by exploiting temporal redundancy in a video sequence. Motion estimation is a process for estimating motion between pictures. For example for an 8 8 block of samples or other unit of the current picture the encoder attempts to find a match of the same size in a search area in another picture the reference picture. Within the search area the encoder compares the current unit to various candidates in order to find a candidate that is a good match. When the encoder finds an exact or close enough match the encoder parameterizes the change in position between the current and candidate units as motion data such as a motion vector MV . In general motion compensation is a process of reconstructing pictures from reference picture s using motion data.

The example encoder also computes the sample by sample difference between the original current unit and its motion compensated prediction to determine a residual also called a prediction residual or error signal . The encoder then applies a frequency transform to the residual resulting in transform coefficients. The encoder quantizes the transform coefficients and entropy codes the quantized transform coefficients.

If an intra compressed picture or motion predicted picture is used as a reference picture for subsequent motion compensation the encoder reconstructs the picture. A decoder also reconstructs pictures during decoding and it uses some of the reconstructed pictures as reference pictures in motion compensation. For example for an 8 8 block of samples of an intra compressed picture an example decoder reconstructs a block of quantized transform coefficients. The example decoder and encoder perform inverse quantization and an inverse frequency transform to produce a reconstructed version of the original 8 8 block of samples.

As another example the example decoder or encoder reconstructs an 8 8 block from a prediction residual for the block. The decoder decodes entropy coded information representing the prediction residual. The decoder encoder inverse quantizes and inverse frequency transforms the data resulting in a reconstructed residual. In a separate motion compensation path the decoder encoder computes an 8 8 predicted block using motion vector information for displacement from a reference picture. The decoder encoder then combines the predicted block with the reconstructed residual to form the reconstructed 8 8 block.

Over the last two decades various video coding and decoding standards have been adopted including the H.261 H.262 MPEG 2 and H.263 series of standards and the MPEG 1 and MPEG 4 series of standards. More recently the H.264 standard sometimes referred to as AVC or JVT and VC 1 standard have been adopted. For additional details see representative versions of the respective standards.

Such a standard typically defines options for the syntax of an encoded video bit stream according to the standard detailing the parameters that must be in the bit stream for a video sequence picture block etc. when particular features are used in encoding and decoding. The standards also define how a decoder conforming to the standard should interpret the bit stream parameters the bit stream semantics. In many cases the standards provide details of the decoding operations the decoder should perform to achieve correct results. Often however the low level implementation details of the operations are not specified or the decoder is able to vary certain implementation details to improve performance so long as the correct decoding results are still achieved.

During development of a standard engineers may concurrently generate reference software sometimes called verification model software or JM software to demonstrate rate distortion performance advantages of the various features of the standard. Typical reference software provides a proof of concept implementation that is not algorithmically optimized or optimized for a particular hardware platform. Moreover typical reference software does not address multithreading implementation decisions instead assuming a single threaded implementation for the sake of simplicity.

While some video decoding and encoding operations are relatively simple others are computationally complex. For example inverse frequency transforms fractional sample interpolation operations for motion compensation in loop deblock filtering post processing filtering color conversion and video re sizing can require extensive computation. This computational complexity can be problematic in various scenarios such as decoding of high quality high bit rate video e.g. compressed high definition video . In particular decoding tasks according to more recent standards such as H.264 and VC 1 can be computationally intensive and consume significant memory resources.

Some decoders use video acceleration to offload selected computationally intensive operations to a graphics processor. For example in some configurations a computer system includes a primary central processing unit CPU as well as a graphics processing unit GPU or other hardware specially adapted for graphics processing. A decoder uses the primary CPU as a host to control overall decoding and uses the GPU to perform simple operations that collectively require extensive computation accomplishing video acceleration.

In a typical software architecture for video acceleration during video decoding a video decoder controls overall decoding and performs some decoding operations using a host CPU. The decoder signals control information e.g. picture parameters macroblock parameters and other information to a device driver for a video accelerator e.g. with GPU across an acceleration interface.

The acceleration interface is exposed to the decoder as an application programming interface API . The device driver associated with the video accelerator is exposed through a device driver interface DDI . In an example interaction the decoder fills a buffer with instructions and information then calls a method of an interface to alert the device driver through the operating system. The buffered instructions and information opaque to the operating system are passed to the device driver by reference and video information is transferred to GPU memory if appropriate. While a particular implementation of the API and DDI may be tailored to a particular operating system or platform in some cases the API and or DDI can be implemented for multiple different operating systems or platforms.

In some cases the data structures and protocol used to parameterize acceleration information are conceptually separate from the mechanisms used to convey the information. In order to impose consistency in the format organization and timing of the information passed between the decoder and device driver an interface specification can define a protocol for instructions and information for decoding according to a particular video decoding standard or product. The decoder follows specified conventions when putting instructions and information in a buffer. The device driver retrieves the buffered instructions and information according to the specified conventions and performs decoding appropriate to the standard or product. An interface specification for a specific standard or product is adapted to the particular bit stream syntax and semantics of the standard product.

A conventional hardware encoder allows one acceleration entry point and exit point. For example a main software module can activate at the entry point a motion estimation accelerator to provide a motion vector field at the exit point. Alternatively the main software module can activate at the entry point a fully accelerated video encoder to encode one picture and provide bit streams at the exit point. Thus the encoder allows for little control so that the input follows a single predetermined path to the output.

Unfortunately video encoding process can involve very complex operations with strong data dependencies. As a result a predetermined encoder accelerator should not be expected to deliver very high compression quality.

An encoder is disclosed that is partitioned into discrete hardware modules. The discrete modules include multiple re entry and exit points that allow enhanced control by software. The software can control the discrete modules during the encoding process and make adjustments according to CPU bandwidth and or user requirements allowing for enhanced quality control and seamless hardware software operations.

In one embodiment a media stream is received into an encoder that includes a pipeline of multiple hardware stages for encoding. An intermediate result is provided from at least one of the hardware stages to an encoding control module that processes the intermediate result to determine configuration instructions for a next hardware stage in the pipeline. The configuration instructions can be injected back into the encoder through one of the intermediate re entry points. Thus the encoding process can be modified dynamically through hardware and software interactions as the media stream progresses through the pipeline of the encoder.

In another embodiment the encoding control module can decide to bypass a hardware stage based on CPU bandwidth and or user requirements.

In yet another embodiment the encoder can be part of a transcoder that includes a decoder and a digital signal processor. The decoder decodes an input media stream. The digital signal processor modifies the media stream by performing at least one or more of the following operations changing color spaces smoothing locating scene changes etc. The encoder can then encode the modified media stream by using entropy encoding. The resultant encoded media stream can have a different bit rate than the input media stream so that it may be used on a consumer that desires a specified bit rate.

The foregoing features and advantages of the invention will become more apparent from the following detailed description which proceeds with reference to the accompanying figures.

With reference to the computing environment includes at least one CPU and associated memory as well as at least one GPU or other co processing unit and associated memory used for video acceleration. In this basic configuration is included within a dashed line. The processing unit executes computer executable instructions and may be a real or a virtual processor. In a multi processing system multiple processing units execute computer executable instructions to increase processing power. A host encoder or decoder process offloads certain computationally intensive operations e.g. fractional sample interpolation for motion compensation in loop deblock filtering to the GPU . The memory may be volatile memory e.g. registers cache RAM non volatile memory e.g. ROM EEPROM flash memory etc. or some combination of the two. The memory stores software for an encoder environment implementing one or more of the encoder innovations described herein. As shown the software can be stored in any one of a variety of memory locations or can distributed across memory locations depending on the design.

A computing environment may have additional features. For example the computing environment includes storage one or more input devices one or more output devices and one or more communication connections . An interconnection mechanism not shown such as a bus controller or network interconnects the components of the computing environment . Typically operating system software not shown provides an operating environment for other software executing in the computing environment and coordinates activities of the components of the computing environment .

The storage may be removable or non removable and includes magnetic disks magnetic tapes or cassettes CD ROMs DVDs or any other medium which can be used to store information and which can be accessed within the computing environment . The storage stores instructions for the software .

The input device s may be a touch input device such as a keyboard mouse pen or trackball a voice input device a scanning device or another device that provides input to the computing environment . For audio or video encoding the input device s may be a sound card video card TV tuner card or similar device that accepts audio or video input in analog or digital form or a CD ROM or CD RW that reads audio or video samples into the computing environment . The output device s may be a display printer speaker CD writer or another device that provides output from the computing environment .

The communication connection s enable communication over a communication medium to another computing entity. The communication medium conveys information such as computer executable instructions audio or video input or output or other data in a modulated data signal. A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media include wired or wireless techniques implemented with an electrical optical RF infrared acoustic or other carrier.

The techniques and tools can be described in the general context of computer readable media. Computer readable media are any available media that can be accessed within a computing environment. By way of example and not limitation with the computing environment computer readable media can include memory storage communication media and combinations of any of the above.

The techniques and tools can be described in the general context of computer executable instructions such as those included in program modules being executed in a computing environment on a target real or virtual processor. Generally program modules include routines programs libraries objects classes components data structures etc. that perform particular tasks or implement particular abstract data types. The functionality of the program modules may be combined or split between program modules as desired in various embodiments. Computer executable instructions for program modules may be executed within a local or distributed computing environment.

For the sake of presentation the detailed description uses terms like decide make and get to describe computer operations in a computing environment. These terms are high level abstractions for operations performed by a computer and should not be confused with acts performed by a human being. The actual computer operations corresponding to these terms vary depending on implementation.

In the proposed encoder architecture it is optionally possible to turn on and off entry and exit points based on hardware capability and preferred system settings. Additionally the encoding control module can decide to bypass certain hardware modules in the pipeline and operate in a software mode for corresponding tasks. In this way the encoder can take full advantage of hardware acceleration modules while maintaining the best quality through close software control. Such a structure allows flexible software control of the encoder to maximize the encoding procedure.

The transcoder includes a decoder a digital signal processor DSP and an encoder . One function of the transcoder is to dynamically change the bit rate of the input stream received through a primary input in order to meet with constraints of a consumer. The decoder decodes the input stream so that it can be manipulated and later encoded. Decoders are well understood in the art and need not be further described. The decoder may generate optional metadata shown at and that can be provided to the DSP and the encoder respectively. Such metadata can be used to assist the DSP or the encoder to perform more efficiently. For example the decoder may have information that can be useful to the encoding process such as search information that the encoder can use for encoding frames related to motion. The DSP is coupled to the output of the decoder and can perform a wide variety of functions as is well understood in the art. Some example functions of the DSP include changing color spaces smoothing finding scene changes etc. The DSP can also inform the encoder of various scene changes frame modes etc.

The encoder generally performs entropy encoding which can include variable length encoding arithmetic encoding etc. The internals of the encoder are shown in detail at box and include multiple hardware stages coupled in series in a pipeline fashion. The various hardware stages are only for purposes of illustration and some of the stages may be eliminated or other stages added. Nonetheless the stages include a preprocessor a motion estimation 1 ME 1 module a motion estimation 2 ME 2 module a motion estimation 3 ME 3 module a mode decision module a transform and quantization reconstruction module and an entropy encoding module shown as a variable length encoding module . The first module in the pipeline is the preprocessor module that receives input from the DSP and the optional metadata from the decoder . The preprocessor generally decides the picture type through analysis of the frames determines scene changes and bit rate allocation. The primary input and exit point for the preprocessor are shown at and respectively. The preprocessor also includes secondary re entry and exit points at and that allow for dynamic control by the encoding control module . The multiple re entry and exit points allow for communication lines to be coupled in parallel to the encoding control module . As further described below multiple of the hardware stages have re entry and exit communication lines coupled in parallel to the encoding control module as indicated at . Each re entry and exit point of the hardware stage is not individually numbered for ease of illustration. The preprocessor output is coupled in series to the ME 1 module . The ME 1 module identifies image motions between frames so that coding redundancy can be reduced. Generally ME 1 performs a high level rough estimation of the coding redundancy whereas later ME stages are for further refining the coding redundancy. In prior encoders the preprocessor results would automatically be passed to the ME 1 stage and no other options were provided. However shows that the intermediate re entry and exit points can be used by the encoding control module to further manipulate the preprocessing step and provide further instructions to the ME 1 stage . For example the encoding control module can instruct the ME 1 hardware stage to act on the data from the preprocessor in a certain manner such as directing the ME 1 module to perform a motion search with a large search window. Alternatively the ME 1 module can perform a motion search with a small area of the frame for low motion frames. Furthermore the encoding control module can instruct the ME 1 module when to terminate the search based on the encoding quality the CPU bandwidth user requirements etc.

Thus the multiple re entry and exit point communication lines of the motion estimation module shown generally at allow the encoding control module to determine how to proceed with the next frame in the media stream based on the appropriate budget. For example the encoding control module can adjust the time of search in order to increase the speed of the overall encoding process. Alternatively the encoding control module can increase the search time in order to improve quality of the overall encoding. The raw data from the ME 1 module is passed to the ME 2 module . As previously described the ME 2 module operates on motion vectors on a finer scale and with more accuracy. Consequently the software in the encoding control module operates on ME 2 differently than ME 1 because the goal of ME 1 is to find motion in a frame whereas the goal of ME 2 is to find a pixel in a region where there is motion. ME 2 is shown with intermediate re entry and exit points shown generally at that allow the encoding control module to further manipulate and instruct the ME 2 module how to behave. The output of ME 2 is coupled in series to ME 3 shown at .

ME 3 is generally used for one quarter pixel resolution and can be bypassed if the encoding control module so desires. Thus the encoding control module can instruct the ME 3 that the data provided from ME 2 is to be passed directly to the mode decision stage . Such a decision can be made by the encoding control module based on whether the resolution is satisfactory in view of the CPU budget and user requirements. Additionally communication between the ME 3 module and the encoding control module occurs using the intermediate entry and exit point communication lines . In mode decision stage each frame is divided into blocks that include a 16 16 pixel window although other block sizes can be used. A motion search is performed in each block using a reference frame. A motion vector is thereby generated and a coding mode is determined in order to minimize the bits used and to maximize the coding quality. The mode decision stage can be performed in hardware as shown at box or the encoding control module can decide to perform the mode decision on its own based on the current state of the processing. Communication between the mode decision stage and the encoding control module occurs using the intermediate entry and exit communication lines . In any event the mode decision stage has an output coupled to the T Q recon stage . The T Q recon stage performs transformation quantization and reconstruction of frames as is well understood in the art. The transformation can perform a Fast Fourier Transform FFT or a Discrete Cosine Transform DCT on the various pixels depending on the particular type of encoding performed. In the quantization phase of the T Q recon a weighted average is calculated by using parameters provided from the encoding control module provided via the entry and exit points of the T Q recon module. Thus quantization parameters are passed from the encoding control module to the T Q recon module depending on the desired accuracy quality bandwidth and user input. The final hardware stage is labeled as the variable length encoding module but any desired entropy encoding module can be used such as arithmetic encoding. Again through the secondary and intermediate entry and exit point communication lines the encoding control module can manipulate the encoding performed in the VLE hardware stage . The final stage then delivers on the primary output the output stream shown at that is provided to the consumer .

In view of the many possible embodiments to which the principles of the disclosed invention may be applied it should be recognized that the illustrated embodiments are only preferred examples of the invention and should not be taken as limiting the scope of the invention. Rather the scope of the invention is defined by the following claims. We therefore claim as our invention all that comes within the scope of these claims.

