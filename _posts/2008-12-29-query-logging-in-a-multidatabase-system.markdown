---

title: Query logging in a multi-database system
abstract: Query processing statistics are logged in a multi-database system containing a plurality of system databases. A plurality of query log entries generated using information regarding the execution of a plurality of queries are temporarily stored in a database query log (DBQL). The information is received from a plurality of system databases. Each query log entry is generated using information regarding execution of one query by the plurality of system databases. Each query log entry includes a field identifying a system database that was the source of the information in that entry. An express request is created upon occurrence of a triggering event. The express request contains a subset of the query log entries temporarily stored in the DBQL cache. The created express request is transmitted to a plurality of system databases and the contents of the transmitted express request are stored in each of plurality of system databases to which it was transmitted.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08311989&OS=08311989&RS=08311989
owner: Teradata US, Inc.
number: 08311989
owner_city: Dayton
owner_country: US
publication_date: 20081229
---
Conventional database systems often execute a large number of queries for users. The number of queries processed by typical database systems has been increasing as the database systems continue to grow larger and capable of storing more information. Conventional database systems also typically monitor the queries executed by the database systems. For example a database system may log each query executed by the system and information about each query such as the amount of system resources used during execution of the query.

In general in one aspect the invention features a method for logging query processing statistics in a multi database system containing a plurality of system databases. The method includes temporarily storing in a database query log DBQL a plurality of query log entries generated using information regarding the execution of a plurality of queries. The information is received from a plurality of system databases. Each query log entry is generated using information regarding execution of one query by the plurality of system databases. Each query log entry includes a field identifying a system database that was the source of the information in that entry. The method further includes creating an express request upon occurrence of a triggering event. The express request contains a subset of the query log entries temporarily stored in the DBQL cache. The method further includes transmitting the created express request to a plurality of system databases and storing the contents of the transmitted express request in each of plurality of system databases to which it was transmitted.

Implementations of the invention may include one or more of the following. Temporarily storing query log entries generated using information regarding the execution of the plurality of queries from the plurality of system databases in the DBQL cache may include aggregating the information regarding execution of the query from the plurality of system databases summarizing the aggregated information and temporarily storing the aggregated information in the DBQL cache. Temporarily storing query log entries generated using information regarding the execution of the plurality of queries from the plurality of system databases in the DBQL cache may include aggregating the information regarding execution of the plurality of queries from the plurality of system databases and determining that a portion of the aggregated information for a query is less than a threshold and in response incrementing a counter. Temporarily storing query log entries generated using information regarding the execution of the plurality of queries from the plurality of databases in the DBQL cache may include aggregating the information regarding execution of the plurality of queries from the plurality of system databases and determining that a portion of the aggregated information for a query is greater than a threshold and in response storing the aggregated information for the query in the DBQL cache. Creating the plurality of express requests may include selecting a query log entry to be included in one of the plurality of express requests based on applying a hash function to a plurality of fields in the query log entry the plurality of fields including the field identifying the system database that was the source of the information in that entry. Transmitting one of the created express requests to a plurality of system databases may include transmitting the one created express request from a parsing engine responsible for maintaining the DBQL cache and for issuing commands to a plurality of subordinate parsing engines to one of a plurality of parsing engines associated with one of the plurality of system databases to which the created express request is to be transmitted. Storing the contents of the transmitted express request may include determining where in one of the plurality of databases to which the transmitted express request was transmitted to store a query log entry in the transmitted express request based on applying a hash function to a plurality of fields in the query log entry the plurality of fields including the field identifying the system database that was the source of the information in that entry.

In general in another aspect the invention features a multi database system including a virtual regulator to issue messages. The multi database system further includes a plurality of parsing engines to receive and process the messages issued by the virtual regulator. The multi database system further includes a DBQL cache to temporarily store query log entries generated by the virtual regulator from information sent from the plurality of parsing engines regarding the execution of a plurality of queries by the plurality of parsing engines. The multi database system further includes a plurality of query logs to store a query log entry retrieved by the virtual regulator from the DBQL cache and transmitted to two or more of the plurality of parsing engines by the virtual regulator in the form of a message upon the occurrence of a triggering event.

Implementations of the invention may include one or more of the following. The multi database system may include a plurality of system databases each system database being managed by one or more of the plurality of parsing engines. The virtual regulator may include a hash function operator to determine which of two or more of the plurality of parsing engines to receive the message containing the query log entry based on applying a hash function to a portion of the query log entry. The multi database system may further include database cache for storing the message transmitted from the virtual regulator to two or more of the plurality of parsing engines until a database triggering event occurs.

In general in another aspect the invention features a computer program stored in a tangible medium for logging query processing statistics in a multi database system containing a plurality of system databases. The program includes executable instructions that cause a computer to temporarily store in a database query log DBQL a plurality of query log entries generated using information regarding the execution of a plurality of queries. The information is received from a plurality of system databases. Each query log entry is generated using information regarding execution of one query by the plurality of system databases and each query log entry including a field identifying a system database that was the source of the information in that entry. The program further includes executable instructions that cause the computer to create an express request upon occurrence of a triggering event the express request containing a subset of the query log entries temporarily stored in the DBQL cache. The program further includes executable instructions that cause the computer to transmit the created express request to a plurality of system databases. The program further includes executable instructions that cause the computer to store the contents of the transmitted express request in each of plurality of system databases to which it was transmitted.

The multi database query logging technique disclosed herein has particular application but is not limited to multi database systems that contain a plurality of databases. Each database might contain many millions or billions of records managed by a database system DBS such as a TERADATA ACTIVE DATA WAREHOUSING System available from Teradata Corporation. shows a sample architecture for one node of the DBS . The DBS node includes one or more processing modules connected by a network that manage the storage and retrieval of data in data storage facilities . Each of the processing modules may be one or more physical processors or each may be a virtual processor with one or more virtual processors running on one or more physical processors.

For the case in which one or more virtual processors are running on a single physical processor the single physical processor swaps between the set of N virtual processors.

For the case in which N virtual processors are running on an M processor node the node s operating system schedules the N virtual processors to run on its set of M physical processors. If there are 4 virtual processors and 4 physical processors then typically each virtual processor would run on its own physical processor. If there are 8 virtual processors and 4 physical processors the operating system would schedule the 8 virtual processors against the 4 physical processors in which case swapping of the virtual processors would occur.

Each of the processing modules manages a portion of a database that is stored in a corresponding one of the data storage facilities . Each of the data storage facilities includes one or more disk drives. The DBS may include multiple nodes in addition to the illustrated node connected by extending the network .

The system stores data in one or more tables in the data storage facilities . The rows of the tables are stored across multiple data storage facilities to ensure that the system workload is distributed evenly across the processing modules . A parsing engine organizes the storage of data and the distribution of table rows among the processing modules . The parsing engine also coordinates the retrieval of data from the data storage facilities in response to queries received from a user at a mainframe or a client computer . The DBS usually receives queries and commands to build tables in a standard format such as SQL.

In one implementation the rows are distributed across the data storage facilities by the parsing engine in accordance with their primary index. The primary index defines the columns of the rows that are used for calculating a hash value. The function that produces the hash value from the values in the columns specified by the primary index is called the hash function. Some portion possibly the entirety of the hash value is designated a hash bucket . The hash buckets are assigned to data storage facilities and associated processing modules by a hash bucket map. The characteristics of the columns chosen for the primary index determine how evenly the rows are distributed.

In one example system the parsing engine is made up of three components a session control a parser and a dispatcher as shown in . The session control provides the logon and logoff function. It accepts a request for authorization to access the database verifies it and then either allows or disallows the access.

Once the session control allows a session to begin a user may submit a SQL request which is routed to the parser . As illustrated in the parser interprets the SQL request block checks it for proper SQL syntax block evaluates it semantically block and consults a data dictionary to ensure that all of the objects specified in the SQL request actually exist and that the user has the authority to perform the request block . Finally the parser runs an optimizer block which develops the least expensive plan to perform the request.

A memory arrangement illustrated in supports the logging of database queries in DBS . In this example parsing engine has access to a cache . Cache represents any suitable memory or memories that can be used by parsing engine to store and retrieve information. In the illustrated example cache includes query data and a query rules table . Query data represents information about queries executed by one of nodes see . For example query data could represent summarized query data and query data could represent full query data. The query data in cache is used to update one or more query logs in data storage facilities . Log could represent a summarized query log and log could represent a full query log. A full query log stores specific information associated with one or more queries such as the user and user account that submitted a query the times at which various events occurred during execution of the query and the total processing time and total number of input output I O operations associated with the query. A summarized query log contains summarized information about one or more queries. As a particular example summarized query log may identify the number of queries having execution times that fall within a particular range of execution times. One example of a summarized query log is shown in which is described below. One example of a full query log is shown in which is also described below.

In one aspect of operation queries in DBS are logged using a threshold option. Dispatcher monitors the execution time of a query and compares the execution time to a threshold time. If the execution time falls below the threshold time dispatcher increments a counter. The value of the counter may be stored in cache as summarized query data . If the execution time of a query is greater than the threshold time dispatcher writes information about the query to cache as full query data . Depending on the implementation information about a query having an execution time equal to the threshold could be stored as summarized query data or as full query data . For longer term storage the summarized query data may then be written to summarized query log and the full query data may be written to full query log . In this way dispatcher may not need to log every single query in log

In another aspect of operation queries in DBS are logged using a time bucket option. One or more time buckets are associated with one or more counters and each time bucket is also associated with a range of execution times. Dispatcher monitors the execution time of a query and determines which time bucket has a range of execution times that includes the identified execution time. Dispatcher then increments the counter associated with the identified time bucket. The values of the counter may be stored in cache as summarized query data . The summarized query data may then be written to summarized query log . In this example dispatcher summarizes all queries in log and need not fully log any queries.

Query rules table identifies how dispatcher logs queries submitted by particular users in DBS . For example query rules table could identify whether a user wishes to use the threshold option the time bucket option or neither option to log queries submitted by that user. If the user wishes to use the threshold option query rules table identifies the threshold time to be used by dispatcher . If the user wishes to use the time bucket option query rules table identifies a range of execution times for each of the one or more time buckets. One example of a query rules table is shown in which is described below.

As described above dispatcher uses the query data in cache to update one or more logs in data storage facilities . In this example cache represents a temporary storage area for the query data and logs in data storage facilities represent a longer term storage area for the query data . To update a log dispatcher communicates at least a portion of the query data in cache to one or more processing modules .

In an example system dispatcher communicates query data to processing modules in response to a triggering event. The triggering event could for example represent a specified amount of time such as a ten minute period elapsing query data reaching a specified size or specified percentage of the capacity of cache a command being received from a user or any other suitable trigger. The triggering event causes dispatcher to communicate query data to one or more processing modules .

Processing modules receive the query data and store the query data in one or more logs . In an example system dispatcher stores query data in cache in a file system format. The file system format is also used by processing modules to store information in data storage facilities . By writing the query data in cache in a file system format processing modules need not convert the query data before storing it in data storage facilities . This may help to reduce the load placed on processing modules . In a particular system the full query data is written in file system format while the summarized query data is not.

To help further reduce the load placed on node an express request can be used by dispatcher to communicate query data to processing modules . In an example system when parsing engine receives queries or other commands parsing engine executes a step building process to divide the command into one or more steps. Dispatcher then communicates the various steps to processing modules for execution. An express request is a message sent from parsing engine to processing modules without requiring parsing engine to perform the step building process. One example of an express request is shown in which is described below.

In an example system parsing engine communicates with processing modules over network . Network carries messages between parsing engine and processing modules . Network could for example carry broadcast messages point to point messages and point to multipoint messages between parsing engine and processing modules . Network could also merge information from multiple processing modules and deliver a consolidated set of information to parsing engine such as when multiple processing modules retrieve information from data storage facilities in response to a query. Network could further provide automatic load balancing and automatic reconfiguration after detection of a fault in one or more processing modules . In a particular system network represents a dual redundant bi directional interconnect network. In this particular system node may support a parallel database extension PDE to enable multiple processing modules to execute on the same node and communicate over network .

While dispatcher has been and may be described as logging database queries dispatcher could also log individual query steps. In this example a query is divided into one or multiple steps and the execution time of each step is identified. Dispatcher may then increment a counter and or fully log an individual step based on the identified execution time of the step.

Although illustrates one example of a memory arrangement for logging database queries various changes may be made to memory arrangement . For example additional divisions among query data and or logs could be used in node .

A summarized query log illustrated in includes one or more entries . Each entry includes summarized information about zero or more queries executed by DBS . The queries could be summarized using the threshold option and or the time bucket option described above. The fields shown in are for illustration only. Other or additional fields can be used to describe queries executed by DBS .

Processor identifier identifies the parsing engine that processes the queries being summarized in an entry . Collection timestamp identifies the time when the information in entry was sent from parsing engine to processing modules for storage in log . User identifier identifies the user who submits the queries being summarized in an entry . In an example system each entry is associated with queries submitted by one user. Account name identifies the account used by a user to submit the queries summarized in an entry . In an example system a user may use multiple accounts to submit queries to node and each entry is associated with one account name . Session identifier identifies the session used by a user to communicate with node . In an example system a user may establish multiple sessions to communicate with node and each entry is associated with one session identifier .

Query count identifies the number of queries summarized by an entry . Query seconds identifies the amount of processing time needed by node to execute the queries summarized by an entry . Low histogram value identifies the lowest execution time in a range of execution times associated with an entry . High histogram value identifies the highest execution time in a range of execution times associated with an entry . Taken together low histogram value and high histogram value identify the range of execution times associated with an entry . The queries that are summarized in an entry have execution times that fall within the range of execution times defined by low histogram value and high histogram value .

In one aspect of operation a time bucket option is used to summarize queries executed by node . In an example system a set of one or more entries denoted as is associated with this option one entry for each time bucket. The low histogram value and high histogram value in an entry define the size of the time bucket associated with that entry . As a particular example the first four entries in log define time buckets used to summarize queries. The first time bucket represents queries having an execution time of between zero and five seconds the second time bucket represents queries having an execution time of between five and ten seconds and the third time bucket represents queries having an execution time of between ten and fifteen seconds. The fourth time bucket represents queries having an execution time greater than fifteen seconds where the value 65 535 represents a value used to represent infinity in a particular system. In an example system the low histogram values and high histogram values used to establish the time buckets are provided by a user when a logging feature in node is activated. In a particular system the user provides three values X Y and Z when activating the logging feature and these values are used to create four time buckets for execution times ranging from zero to X X to Y Y to Z and greater than Z.

When logging is invoked with the time bucket option dispatcher initializes a set of counters one for each time bucket. Dispatcher also initializes a running execution time value for each bucket. The running execution time value identifies the total execution time for all queries counted by one of the counters. Dispatcher identifies the execution time of each query executed by node and identifies the time bucket that includes the identified execution time. Dispatcher increments the counter associated with the identified time bucket and adds the execution time of the query to the running execution time value associated with the identified time bucket. The counter values and the running execution time values may also be stored in cache as summarized query data . As a particular example non zero counter values and non zero running execution time values are stored in cache .

When a triggering event is detected node stores the summarized query data in log . For example one or more processing modules may receive the data from parsing engine and generate one or more entries in log . In an example system a processing module generates a new entry in log for each counter value received from parsing engine . As a particular example the processing modules may generate entries and insert information into fields of the newly created entries . This may include inserting the value of a counter into an entry as query count and inserting the running execution time value associated with the counter into entry as query seconds . As shown in if one of the counters has a value of zero no corresponding entry needs to be created in log . This is illustrated in in the second set of entries where no entry is shown for execution times between five seconds and ten seconds. This may help to avoid consuming space in data storage facility to store entries that summarize no queries.

In another aspect of operation a threshold option is used to summarize queries executed by node . In an example system one entry denoted as is associated with this option. The low histogram value identifies the threshold time used to summarize the queries. The high histogram value may have a value of zero null or any other suitable value. As a particular example the fifth entry in log defines a threshold time used to summarize queries.

When logging is invoked with the threshold option dispatcher may initialize and use one counter and one running execution time value. Dispatcher then identifies the execution time of each query executed by node . When a query has an execution time less than or equal to the threshold time dispatcher increments the counter and adds the execution time to the running execution time value. The counter value and the running execution time value may be stored in cache as summarized query data . When a query has an execution time greater than the threshold time dispatcher stores information about the query in cache as full query data

In response to a triggering event dispatcher communicates at least a portion of query data in cache to one or more processing modules and processing modules update logs using query data . For example a processing module could generate a new entry in log and insert the counter value and running execution time value into the new entry . The triggering event could for example represent query data reaching eighty percent of the capacity of cache .

In an example system each processing module has access to a portion of log . In a particular system dispatcher uses a processor identifier and a collection timestamp to perform a hash function. The hash function identifies the processing module to receive a portion of the query data to be stored in an entry or a set of entries in log . In this way the storage of query data in log can be divided among the processing modules which may help to reduce the load placed on any one processing module . Query data could also be provided to multiple processing modules in any other suitable manner or to a single processing module .

Although illustrates one example of a summarized query log various changes may be made to . For example log could store other or additional information about queries executed by node .

A full query log illustrated in includes one or more entries . Each entry includes information associated with a query executed by node . The fields shown in are for illustration only. Other or additional fields can be used to describe queries executed by DBS . Also the same or similar structure could be used in cache to store full query data

Processor identifier identifies the parsing engine that processes the query associated with an entry . Collection timestamp identifies the time when the cache was initialized to store the information contained in entry . Query identifier identifies the query associated with an entry . User identifier identifies the user who submits the query associated with an entry . Account name identifies the account used by a user to submit the query associated with an entry . Session identifier identifies the session used by a user to communicate with node and submit the query associated with an entry . Logical host identifier identifies the host machine executing the session used by the user to communicate with node .

Start time identifies the time that execution of the query associated with an entry begins at parsing engine . As described above parsing engine executes a step building process to divide a query into one or more steps and each step is sent to processing modules by dispatcher for execution. First step time identifies the time that the first step associated with the query is dispatched to processing modules . First response time identifies the time that the first response is sent to the host identified by logical host identifier . Last response time identifies the time that the last response is sent to the host identified by logical host identifier . Number of steps value identifies the total number of steps used to execute a query associated with an entry . Total CPU time value identifies the execution time of a query or the total amount of processing time used by one or more processing modules to execute the query. Total I O count value identifies the total number of I O operations performed by one or more processing modules to execute the query associated with entry . A method for calculating the total CPU time value and total I O count value is shown in which is described below.

In one aspect of operation queries for a user account session combination may be logged using a time bucket option and or a threshold option. When the time bucket option is used dispatcher need not log any of the queries in log . When the threshold option is used node logs a query in log when the execution time of the query exceeds a threshold time. Because it may use less resources to log queries in summarized query log than to log queries in full query log this may help to reduce the overhead imposed on DBS by the logging feature.

Although illustrates one example of a full query log various changes may be made to . For example log could store other or additional information about queries executed by node .

A query rules table illustrated in includes one or more entries . Each entry includes information identifying whether and how queries are to be summarized in DBS . The fields shown in are for illustration only. Other or additional fields can be used in table .

User identifier identifies a user who may submit queries to node . Account name identifies the account used by a user to submit queries to node . In a particular system one entry is associated with a particular user account combination. Also the entry may be created when the user initiates query logging in DBS . In a particular system the entry may be created even when the user does not invoke query logging with the summary or threshold options.

Time bucket flag identifies whether the logging feature of node has been activated for the user account combination using the time bucket option. Threshold flag identifies whether the logging feature of node has been activated for the user account combination using the threshold option. In a particular system the time bucket option and the threshold option are mutually exclusive and cannot be used at the same time for a user account combination. In another example system both options can be used at the same time for a user account combination. For example multiple time buckets could be established and any query having an execution time that does not fall within one of the time buckets is fully logged.

If the time bucket flag is active entry identifies a low value a middle value and a high value . These values define the various time buckets used to summarize queries in node . For example one time bucket may encompass execution times ranging from zero to the low value . Another time bucket may encompass execution times ranging from the low value to the middle value . A third time bucket may encompass execution times ranging from the middle value to the high value . A fourth time bucket may encompass execution times that are greater than the high value . Values could for example be used as low histogram values and high histogram values in log . If the threshold flag is active entry identifies a threshold value used to determine whether a query is counted in summarized query data or fully logged in full query data

Although illustrates one example of a query rules table various changes may be made to table . For example table could store other or additional information about users submitting queries to node . Also while table is shown as including null values other suitable values could be used. In addition both flags could have values of zero indicating that queries are not summarized for a user account combination.

Header represents a standard message header. Header could for example contain a destination of request such as the identity of one or more processing modules . Header could also include error information used by processing module to ensure that request is received correctly. Express header identifies request as an express request. Express header could for example identify the log to be updated using the information in request . Row of data includes information for one entry to be created in a log . Buffer of rows includes information for zero or more additional entries to be created in a log .

In an example system parsing engine generates express request without performing the step building process that normally occurs for queries and other commands executed by parsing engine . By eliminating the step building process parsing engine may be able to generate express requests faster. This helps to reduce the load placed on parsing engine .

When a processing module receives express request processing module identifies the log to be updated. Processing module also extracts the information contained in express request generates one or more entries in log and stores the extracted information in the one or more new entries . In an example system logs may each be formed from one or more data blocks. In a particular system processing module writes information to an existing data block in a single write operation. In this system processing module also writes a new data block to data storage facility during a single write operation.

Although illustrates one example of an express request used to update a log various changes may be made to request . For example row of data and buffer of rows could be consolidated into a single element in request . Also other or additional elements can be added to express request .

Parsing engine initiates a query log at block . This may include for example a user initiating logging using a command such as with BEGIN QUERY LOGGING. The user could initiate logging with a time bucket option such as with the command BEGIN QUERY LOGGING WITH SUMMARY ON USER1. The user could also initiate logging with a threshold option such as the command BEGIN QUERY LOGGING WITH THRESHOLD ON USER1. This may also include dispatcher initiating one or more counters.

Parsing engine receives multiple queries from the user at block . This may include for example dispatcher receiving queries from a mainframe or a client . This may also include parsing engine performing a step building process and dispatcher communicating the steps to processing modules for execution. Parsing engine identifies the execution time for each query at block . This may include for example processing modules using the method shown in to measure the processing time used to execute each query.

Parsing engine counts rather than fully logging at least some of the queries at block . This may include for example dispatcher incrementing counters for one or more time buckets when the time bucket option is used. This may also include dispatcher incrementing a counter if the execution time of a query falls below a threshold time when the threshold option is used. Parsing engine stores the counters in a database log at block . This may include for example dispatcher storing the values from one or more counters in cache as summarized query data . This may also include dispatcher communicating summarized query data to one or more processing modules . This may further include processing modules storing the summarized query data in log

Although illustrates one example of a method for logging database queries various changes may be made to method . For example parsing engine need not store the values of the counters in a log depending on particular needs such as when a counter has a value of zero.

Parsing engine initiates logging with a time bucket option at block . This may include for example a user initiating logging using a command such as BEGIN QUERY LOGGING WITH SUMMARY X Y Z ON USER1. Parsing engine establishes one or more time buckets at block . This may include for example dispatcher initializes one or more counters and one or more running execution time values. As a particular example counters could be associated with time windows defined using the X Y and Z parameters supplied in the BEGIN QUERY LOGGING command.

Parsing engine executes a query at block . This may include parsing engine performing a step building process and dispatcher communicating the steps to processing modules for execution. Parsing engine identifies the execution time for the query at block . This may include for example dispatcher identifying the processing time used by processing modules to execute the query.

Parsing engine increments a counter in the time bucket associated with the identified execution time at block . This may include for example dispatcher incrementing the counter associated with the time window in which the execution time falls. This may also include dispatcher adding the execution time of the query to the running execution time value. The counter value and the running execution time could be stored in cache as summarized query data .

Parsing engine determines whether a triggering event is detected at block . This may include for example dispatcher detecting that a specified amount of time has elapsed the query data in cache has reached a specified size or percentage of capacity a command has been received from a user or any other suitable trigger. If a triggering event is not detected parsing engine returns to block to execute and log another query.

If a triggering event is detected parsing engine selects one or more processing modules at block . This may include for example dispatcher performing a hash using a processor identifier and a collection timestamp . Parsing engine communicates at least a portion of the query data to the selected processing modules at block . This may include for example dispatcher generating an express request containing the query data .

Although illustrates one example of a method for summarizing database queries using time buckets various changes may be made to method . For example parsing engine could always communicate query data to the same processing module or modules in response to the triggering event so parsing engine would not need to select one or more processing modules at block .

Parsing engine initiates logging with a threshold option at block . This may include for example a user initiating logging using a command such as BEGIN QUERY LOGGING WITH THRESHOLD X ON USER1. Parsing engine identifies a threshold time at block . This may include for example dispatcher identifying a threshold time using the X parameter supplied in the BEGIN QUERY LOGGING command. Parsing engine establishes one or more buckets at block . This may include for example dispatcher initializing a counter to count all queries having an execution time below the threshold. The counter could also have an associated running execution time value.

Parsing engine executes a query at block . This may include for example parsing engine performing a step building process and dispatcher communicating the steps to processing modules for execution. Parsing engine identifies the execution time for the query at block . This may include for example dispatcher identifying the processing time used by processing modules to execute the query.

Parsing engine compares the execution time of the query to the threshold time at block . If the execution time exceeds the threshold time parsing engine fully logs the query at block . This may include for example dispatcher storing information about the query as full query data in cache . If the execution time equals or falls below the threshold time parsing engine increments a counter in one of the buckets at block . If multiple buckets are used this may include dispatcher selecting one of the buckets such as by using the execution time of the query to select a bucket and incrementing the counter associated with that bucket.

Parsing engine determines whether a triggering event is detected at block . If a triggering event is not detected parsing engine returns to block to execute and log another query. Otherwise parsing engine selects one or more processing modules at block . Parsing engine communicates at least a portion of the query data in cache to the selected processing modules at block .

Although illustrates one example of a method for summarizing database queries using a time threshold various changes may be made to method . For example parsing engine could always communicate query data to the same processing module or modules in response to the triggering event so parsing engine would not need to select one or more processing modules at block .

A processing module receives a step of a query being executed from dispatcher at block . This may include for example processing module receiving the step over network . In an example system multiple processing modules can receive the same step over network . Processing module executes the step of the query at block . This may include for example processing module accessing data storage facility . This may also include processing module retrieving data from data storage facility . This could further include processing module combining the retrieved data with other data retrieved from data storage facility . Processing module completes execution of the step of the query at block . This may include for example processing module identifying the amount of processing time needed to fully execute the step of the query at processing module and the number of I O operations performed by processing module during the execution of the step.

Processing module informs a channel subsystem that the step has been completed and reports step statistics to the channel subsystem at block . The channel subsystem may for example represent part of the parallel database extension used to facilitate communication by multiple processing modules over network . The step statistics reported to the channel subsystem may include the identified processing time and the number of I O operations. Other or additional step statistics could also be reported.

Processing module determines whether it was the last processing module to complete execution of the step at block . This may include for example the channel subsystem informing the processing module if it was the last to complete execution of the step. If processing module was not the last to complete execution of the step method ends. Otherwise processing module was the last to complete execution of the step and processing module collects the step statistics from the channel subsystem at block . This may include for example processing module receiving the step statistics produced by all of the processing modules . Processing module aggregates the step statistics at block . This may include for example processing module identifying the total amount of processing time needed by all processing modules to fully execute the step and the total number of I O operations performed by all processing modules during execution of the step. Processing module communicates the aggregated statistics to parsing engine at block . This may include for example processing module communicating the total processing time and total number of I O operations to dispatcher .

At this point the parsing engine can use the step statistics to log database queries. For example dispatcher could use the step statistics to determine whether to summarize or fully log the query step or the entire query. As particular examples dispatcher could use the total processing time to determine which time bucket the query is associated with or to determine whether the execution time of the query exceeds a threshold.

Although illustrates one example of a method for collecting query statistics various changes may be made to method . For example other techniques for aggregating step statistics could be used in DBS .

In one embodiment the virtual parsing engines PE perform the functions of the parsing engine described above. In one embodiment however the virtual parsing engines are not fixedly assigned to a set of processing modules . Instead the mapping between virtual parsing engines and processing modules is variable depending on the current needs of the system. In one embodiment one of the virtual parsing engines serves as a virtual regulator providing the functions described in co pending U.S. patent application Ser. No. 11 891 919 entitled Dynamic Query Optimization Between Systems Based On System Conditions incorporated by reference.

In one embodiment Access Module Processors AMPs which are generally equivalent to the processing modules shown in are grouped as shown by the dashed boxes in . In one embodiment each group is a DBS or system database . In one embodiment each system database is assigned one or more virtual PEs . In the example shown in virtual PE is assigned to system database as indicated by the dashed box enclosing that item. Further virtual PEs and are assigned to system database virtual PEs and are assigned to system database and virtual PEs and are assigned to system database . Virtual PE is not assigned to any system database and is being held in reserve. In one embodiment hash maps identify which system database and AMP is to receive a message directed to one of the system databases . For example if a message is directed to system database the virtual PE that is assigned to system database will use hash map to determine if the message is to be delivered to AMP or AMP . Some of the AMPs in such as AMP are represented as overlapping circles indicating that AMP is a plurality of AMPs. Generally in one embodiment the groups can contain any number of AMPs. Each system database includes a replication service group RSG that coordinates applying changes made to data in one system database to the same data replicated in another system database.

In one embodiment such as that shown in PEs are used to manage workloads on an individual system database basis. A virtual regulator comprises a modified regulator as that term is defined in U.S. patent application Ser. No. 10 915 609 incorporated by reference implemented to enhance a closed loop system management CLSM architecture in a multi database system . That is by extending the functionality of the regulator components complex workloads are manageable across the multi database system .

The function of the virtual regulator is to control and manage workloads across all DBS in a multi database system . The functionality of the virtual regulator extends the existing goal oriented workload management infrastructure which is capable of managing various types of workloads encountered during processing.

In one embodiment the virtual regulator is not just a PE but includes all the other elements of one of the system databases such as system database . In that case the virtual regulator includes AMPs and which provide the virtual regulator with a persistence layer for data protection a hash map and an RSG .

In one embodiment the virtual regulator includes a thin version of a DBS where the thin DBS is a DBS executing in an emulation mode such as described in U.S. Pat. Nos. 6 738 756 7 155 428 6 801 903 and 7089258 all of which are incorporated by reference herein. A query optimizer function of the thin DBS allows the virtual regulator to classify received queries into who what where classification criteria and allows a workload query manager see application Ser. No. 11 891 919 referenced above of the thin DBS to perform the actual routing of the queries among multiple DBS in the multi database system . In addition the use of the thin DBS in the virtual regulator provides a scalable architecture open application programming interfaces APIs external stored procedures XSPs user defined functions UDFs message queuing logging capabilities rules engines etc.

The virtual regulator also includes a set of open APIs known as Traffic Cop APIs that provide the virtual regulator with the ability to monitor DBS states to obtain DBS status and conditions to activate inactive DBS to deactivate active DBS to set workload groups to delay queries i.e. to control or throttle throughput to reject queries i.e. to filter queries to summarize data and statistics and to create dynamic operating rules. The Traffic Cop APIs are also made available to all of the regulators for each DBS thereby allowing the PEs for each DBS and the virtual regulator for the multi database system to communicate this information between themselves.

In some exemplary environments one or more backup virtual regulators illustrated in by the boxes stacked behind the virtual regulator are also provided for circumstances where the primary virtual regulator malfunctions or is otherwise unavailable. Such backup virtual regulators may be active at all times or may remain dormant until needed.

In some embodiments each PE communicates its system conditions and operating environment events directly to the virtual regulator . In other embodiments each PE may have superordinate and or subordinate PEs. For example in PE has superordinate PE and subordinate PEs and . In such embodiments each PE gathers information related to its own system conditions and operating environment events as well as that of its children regulators and reports the aggregated information to its parent regulator or the virtual regulator at the highest level of the multi database system .

In one embodiment query statistics at the system database level are gathered. In one embodiment each system database s channel subsystem reports the step statistics to a multi system channel subsystem which facilitates communications by the multiple system databases over the network . In one embodiment the multi channel subsystem compiles statistics including the total amount of processing time needed by all system databases in the multi system database to fully execute each query step and the total number of I O operations performed by all system databases during execution of the query step. The multi channel subsystem communicates the aggregated statistics to the virtual regulator which stores the statistics in the DBQL cache .

In one embodiment the virtual regulator compiles the information reported by the subordinate PEs and the information reported by the multi system channel subsystem adds multi database or additional system level information to the extent there is any into log entries. The log entries are temporarily stored in a Database Query Log DBQL cache . In one embodiment the log entries are in the form of a multi database system summarized log or a multi database system full log shown in which are similar to the summarized log and the full log respectively except that a new System ID column and is added. The new column identifies the system database associated with the PE that reported the statistics being logged. In one embodiment the System ID column is part of the primary index.

In one embodiment the virtual regulator applies SUMMARY and THRESHOLD mechanisms similar to those described above when compiling the log entries. In one embodiment when the SUMMARY mechanism is invoked the virtual regulator will summarize the queries using the threshold option and or the time bucket option described above. In some cases a single query may be processed by more than one system database which in one embodiment will result in one update to the summarized log for each system database that processed the query. For example if a single query is processed in its entirety by system database and by system database for example so a DBA can compare the processing of the same query by the two system databases the virtual regulator will update log entries for both of those system databases. The System ID column for the two entries will be different. One will indicate that the entry is for system database and the other will indicate that the entry is for system database . In this scenario the Processor ID column will not have any meaning. In one embodiment a separate logging mode is provided in which logging at the Processor ID level is provided. In that mode a separate record will be logged for each processor at each system database that worked on the query. In those records the System ID and Processor ID columns will identify the processor that executed the query.

Further in one embodiment if the virtual regulator divides responsibility for executing a query between two or more system databases the virtual regulator will initiate step logging for that query. For example assume the virtual regulator divides responsibility for executing Query A between system database and system database . The virtual regulator will initiate step logging for Query A. Thereafter system database and system database will report statistics to the virtual regulator on a step by step basis. The virtual regulator will record the statistics in the DBQL on the same basis and record a designator for the system that reported each set of statistics in the System ID column. In one embodiment in a processor logging mode the statistics are reported on a processor by processor basis similar to that described above. Thus if a system database divides responsibility for executing a step among two or more processes the system database will report statistics for each of the processors that worked on the step to the virtual regulator . Using this technique the virtual regulator will be able to compile and report the performance of the two system databases in performing the query.

In one embodiment when the THRESHOLD mechanism is invoked the virtual regulator simply counts queries with an elapsed time or CPU time less than a threshold and fully logs those queries in which the elapsed time or CPU time exceeds the time threshold. In one embodiment since execution of a query in the multi database system can be performed by multiple system databases in the multi database system the elapsed time or CPU time for a query is computed by summing the statistics reported for that query from all system databases in the multi database system . In one embodiment if the summed elapsed time or CPU time for a query is greater than the time threshold the virtual regulator will and fully log the statistics from each of the system databases that reported statistics for that query. For example if a single query is processed by system database and by system database and the sum of the processing times for the two databases exceeds the threshold the virtual regulator will aggregate the statistics for the two system databases for that query and fully log the aggregated statistics.

In one embodiment the virtual regulator logs query steps in addition to or instead of queries. In one embodiment a separate data maintenance and analysis database system not shown performs the logging function rather than the virtual regulator

In one embodiment the virtual regulator maintains the only DBQL cache and the other PEs do not have such caches. In one embodiment the PEs maintain separate DBQL caches and perform their own query logging separate from that done at the multi database system level.

In one embodiment the log entries stored in the DBQL cache are distributed among some or all of the system databases for storage. In one embodiment the distribution occurs upon a triggering event such as one of those described above.

In one embodiment illustrated in upon the occurrence of the triggering event the virtual regulator accesses the DBQL cache to construct a series of DBQL express requests. In particular in one embodiment the dispatcher within the virtual regulator applies a hashing function to the System ID processor ID and collection timestamp columns see and for each entry in the DBQL cache and combines entries with common hash results into a DBQL express request. In one embodiment the virtual regulator which is in one embodiment a special case of a PE generates the DBQL express request without performing the step building process that normally occurs for queries and other commands executed by a PE. In one embodiment each resulting DBQL express request is then sent from the virtual regulator to one or more PEs . As an example shows the virtual regulator sending a single express request to a PE which in this example is assigned to a system database

In one embodiment a dispatcher within the subordinate PE that received the DBQL express request i.e. PE in forwards the DBQL express request to the AMPs associated with that database i.e. AMPs in . Each AMP uses a hash function to identify entries in the DBQL express request that it is assigned to store and writes the identified entries into DBQL log tables in data storage facilities under its control. The AMPs perform the writes in a single write operation.

In one embodiment the hashing function used by the virtual regulator to determine the distribution of the DBQL express requests is designed to cause DBQL entries to be stored in more than one location. For example the hash function may be designed to cause a single entry from the DBQL cache to be sent to more than one system database for storage. This results in beneficial duplication in the storage of log entries across the multi database system . That is using this technique the query log can be automatically and systematically backed up across the multi database system . In one embodiment in a situation such as that shown in in which the virtual regulator has its own AMPs and the virtual regulator sends the express request to its own AMPs and for storage.

In one embodiment the subordinate PE that receives a DBQL express request from the virtual regulator stores the DBQL express request before distributing it to the AMPs. For example in PE stores the DBQL express request in an optional cache upon receiving it. Then upon occurrence of a triggering event such as one of those described above the PE will distribute the DBQL express request to the AMPs for processing as described above.

The text above described one or more specific embodiments of a broader invention. The invention also is carried out in a variety of alternative embodiments and thus is not limited to those described here. For example while the invention has been described here in terms of a DBMS that uses a massively parallel processing MPP architecture other types of database systems including those that use a symmetric multiprocessing SMP architecture are also useful in carrying out the invention. The foregoing description of the preferred embodiment of the invention has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. It is intended that the scope of the invention be limited not by this detailed description but rather by the claims appended hereto.

