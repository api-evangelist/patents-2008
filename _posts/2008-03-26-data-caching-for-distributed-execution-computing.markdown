---

title: Data caching for distributed execution computing
abstract: Embodiments for caching and accessing Directed Acyclic Graph (DAG) data to and from a computing device of a DAG distributed execution engine during the processing of an iterative algorithm. In accordance with one embodiment, a method includes processing a first subgraph of the plurality of subgraphs from the distributed storage system in the computing device. The first subgraph being processed with associated input values in the computing device to generate first output values in an iteration. The method further includes storing a second subgraph in a cache of the device. The second subgraph being a duplicate of the first subgraph. Moreover, the method also includes processing the second subgraph with the first output values to generate second output values if the device is to process the first subgraph in each of one or more subsequent iterations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08229968&OS=08229968&RS=08229968
owner: Microsoft Corporation
number: 08229968
owner_city: Redmond
owner_country: US
publication_date: 20080326
---
Distributed computing is the use of networked computing devices to process data. In one implementation the data processed by a distributed computing scheme may be organized into a data structure known as a Directed Acyclic Graph DAG . In such a scheme a node of the DAG represents a computational task and each edge of the DAG represent a specific flow of a data stream. Each computational task involves the use of data inputs and disk outputs. In most instances the distributed computing scheme also involves the use of iterative algorithms to process data. The combination data input output during each task and the use of iterative processing may result in the transfer of large amounts of DAG data via data pipelines at each iterative step of the distributed computing scheme.

This Summary is provided to introduce a selection of concepts in a simplified form that is further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Described herein are embodiments of various technologies for implementing of a memory caching service into a Directed Acyclic Graph DAG distributed execution engine. The memory caching service may enhance efficiency by reduce or eliminate the need to transfer DAG data via data pipelines at each iterative data processing step of an iterative algorithm that is implemented on the DAG distributed execution engine. In one embodiment a method includes storing subgraphs of a directed acyclic graph DAG in a distributed storage system. The method also includes processing a first subgraph of the plurality of subgraphs from the distributed storage system in a device of the DAG distributed execution engine. The first subgraph being processed with associated input values in the device to generate first output values in a first iteration. Additionally the method includes storing a second subgraph in a cache of the device. The second subgraph being a duplicate of the first subgraph. The method further includes processing the second subgraph with the first output values to generate a second output values if the device is to process the first subgraph in a second iteration. The second iteration being an iteration that occurs after the first iteration.

Other embodiments will become more apparent from the following detailed description when taken in conjunction with the accompanying drawings.

This disclosure is directed to embodiments that facilitate the storage and access of Directed Acyclic Graph DAG data to and from a cache during the process of the graph data by a DAG distributed execution engine. The embodiments described herein are directed to the caching of previously processed subgraphs of a DAG in the local cache of a computing device during the processing of a particular iteration of an iterative algorithm. Additionally each of the subgraphs are accessed from the local cache of the computing device and further processed if the computing device is assigned the processing of the same subgraph in one or more subsequent iterations. In this way embodiments of the present disclosure may eliminate the need to transfer a subgraph which may encompass large amounts of data between computing devices for successive iterations of the iterative algorithm. Various examples of storing and accessing the previously processed subgraphs in the cache of a computing device for subsequent processing by the same computing device during the execution of an iterative algorithm are described below with reference to .

The DAG may be divided into discretion portions of data for processing which are referred to as subgraphs. Exemplary subgraphs are shown in . Moreover the subgraphs may be processed on a distributed execution engine via an iterative algorithm. In one embodiment the distributed execution engine is comprised of a plurality of networked computing devices such as a computing device . Each computing device is configured to process a particular subgraph in one iteration of the iteration algorithm.

For example during a first iteration such as iteration a first set of inputs and a subgraph are processed together to generate outputs. In a succeeding iteration such as iteration the outputs are used as its input and process with a subgraph to produce additional outputs. In a subsequent iteration such as iteration these additional outputs are once again used as inputs and process with another subgraph to generate subsequent outputs. In this way the iterations of the iterative algorithm may be repeated until the desired final outputs are generated.

As further shown in the subgraph processed during each iteration may be cached on a local data cache . The local cache is included in the computing device . For instance during iteration the computing device may be assigned by the iteration algorithm to process subgraph Accordingly the computing device may retrieve the subgraph from a global data storage and process the subgraph. The computing device may also store the subgraph in the local data cache . Subsequently the computing device may be assigned by the iteration algorithm to execute iteration using the subgraph Since the subgraph is already stored in the local data cache the computing device may access the subgraph from the cache rather than the global data source to execute the iteration

Next the computing device may be assigned by the iterative algorithm to process subgraph during iteration Accordingly the computing device may retrieve the subgraph from a global data storage and process the subgraph. The computing device may further store the subgraph in the local data cache . During the iteration the computing device may perform similar retrieval and processing of subgraph as well as the storage of subgraph into local data cache . In the iteration the iterative algorithm may be assigned to repeat the processing of subgraph Thus the computing device may once again access the stored subgraph from the local data cache . It will be appreciated that the iterations are illustrative and the actual number of iterations may vary. Further while only the access of subgraph from the local data cache is illustrated in any subgraph stored in the cache may be accessed for repeated processing in a subsequent iteration.

In essence the local data cache of the computing device functions to store subgraphs that have been processed in previous iterations of the iterative algorithm. The computing device may further retrieve a stored subgraph in an iteration where the stored subgraph is to be processed. In this way the need to retrieve previously processed subgraphs from the networked global data storage may be eliminated. As further described below the use of the local data cache to store previously processed subgraphs confers several benefits.

As shown in a first iteration is implemented by vertex propagation algorithms and Each of the vertex propagation algorithms and may be carried out by a corresponding computing device that make up the DAG distributed execution engine. For example the DAG distributed execution engine may include computing devices and Accordingly the vertex propagation algorithm may be executed by the computing device and the vertex propagation algorithm may be executed by the computing device Similarly the vertex propagation algorithm may be carried out by the computing device

Additionally each of the vertex propagation algorithms and is assigned to process a certain partition or one of the subgraphs of a DAG. The partition of the DAG into subgraphs may facilitate processing. For example the distributed execution engine may be used to process a page rank algorithm that calculates web page rankings. Typically a page rank algorithm is designed to assign weight to each of a plurality of web pages with the purpose of ranking the web pages according to the relative importance of each web page. In such an instance the DAG may represent a plurality of web pages and include a plurality of data records that indicate the number of page links present in each web page. An exemplary DAG file with exemplary data records in sparse matrix mode is shown in Table I. The exemplary data records are designed to represent the relationship between pages as well as the number of page links in each webpage.

As shown each line of Table I represents a record in the DAG graph file. Each number in the table represents an assigned index number of a web page. For example as shown in the second row of the table a web page designated with the index number 1 Source Page has links to web pages assigned with index numbers 2 3 100 and 10 000 respectively Destination Page Set . Likewise a webpage designated with the index number 2 Source Page has links to web pages assigned with index numbers 3 100 200 respectively Destination Page Set and so forth. Moreover the plurality of indexed data records may be partitioned according the ranges of the indices. For example the subgraph may include data records having indices between and while the subgraph may include data records having indices between and and so on.

In various embodiments each of the vertex propagation algorithms and may be implemented using a one of the subgraphs and a set of corresponding input values. For example vertex propagation algorithm may be implemented using subgraph and the set of input values . Likewise vertex propagation algorithm may be implemented using subgraph and the set of input values and vertex propagation algorithm may be implemented using subgraph and the set of input values . Accordingly each of the computing devices and may access a subgraph that correspond to each set of input values from a global data storage . It will be appreciated that when the first iteration is an initial iteration executed by the DAG distributed execution engine and the iterative algorithm includes an Eigenvalue algorithm the input values and may be arbitrarily assigned.

During the implementation of the vertex propagation algorithms and each of the algorithms may generate a set of temporary values. For instance in the page rank example described above a set of temporary values may include the rank values of the web pages in the subgraph. In other words each web page in the subgraph is assigned a temporary rank value based on the number of links in the page. Subsequently the sets of temporary values from the vertex propagation algorithms such as vertex propagation algorithms and are further consolidated at each of the computing devices and These consolidations may be carried out at each of the computing devices using vertices aggregation algorithms and respectively. As shown in the vertices aggregation algorithm and enable the calculation of output values and .

Referring again to the instance where in the distributed execution engine is used to calculate web page rankings a vertices aggregative algorithm such as vertices aggregation algorithm may consolidate the rank values for a particular webpage as calculated by the vertex propagation algorithms on each computing device. For example as illustrated in Table I if records are partitioned into the subgraph and records are partitioned into the subgraph rank values for web page 3 may be obtained from both vertex propagation algorithms and as web page 3 is present in the destination sets of subgraphs and Accordingly the aggregation algorithm may consolidate the rank values for web page 3 from propagation algorithms and to derive a combined rank value. The aggregation algorithm on each computing device may perform such a consolidation of rank values for each webpage that is present in a plurality of subgraphs. Therefore each of the vertices aggregation algorithms and may provide sets of output values and respectively in this manner. Once the vertices aggregation algorithms have provided the output values the first iteration may terminate.

Prior to the execution of a second iteration by each of the computing devices a data manager may store the subgraph processed by each computing device in a respective cache of the computing device. Continuing with the example described above since vertex propagation algorithm is implemented using the subgraph the subgraph may be stored in a cache of the computing device Moreover since vertex propagation algorithms and are respectively implemented using the subgraph and the subgraph may be stored in a cache of the computing device Similarly the subgraph may be stored in a cache of the computing device According to various embodiments the transfer of the subgraphs into the respective caches of the computing devices may occur at any point prior to the execution of the second iteration . For example the transfers may occur before after and or simultaneous with the execution of one or more of the vertex propagation algorithm and the vertices aggregation algorithm on a computing device.

Subsequently the DAG distributed execution engine may execute a second iteration . Like the first iteration the second iteration is performed using the same computing devices of the DAG distributed execution engine. Once again each computing device performs its processing using a set of input values and an associated subgraph. Thus the second iteration may be executed using the output values and as input values for the vertex propagation algorithms and respectively.

During the second iteration the DAG distributed execution engine may use the output values from the computing device as input values to re execute the vertex propagation algorithm on the computing device Accordingly since the output values are associated with subgraph the computing device may access the stored subgraph from the data cache of the computing device The access of the subgraph from the data cache may advantageously eliminate a second download of subgraph from the global data storage . Thus the use of data transfer bandwidth may be reduced. Additionally since the data cache is local to the computing device computing device may be ready to execute the second iteration without being hindered by data transfer lag.

However for the sake of performance stability the computing devices included in the DAG distributed execution engine may be called upon to propagate and aggregate different vertices nodes in the second iteration . In other words the DAG distributed execution engine may switch the subgraphs and associated values process by each computing device. For example rather than remaining with computing device the output values which is derived from the subgraph and input values may be reassigned to the computing device for further processing. Likewise the output values which derived from the subgraph and input values may be reassigned to the computing device for processing. As a result the computing device is reassigned to process the output values with subgraph during the second iteration . The computing device is reassigned to process the output values with subgraph during the second iteration .

As shown in following the execution of the first iteration the subgraph may be stored in the local cache of the computing device However the local cache does not contain the subgraph Thus in order to perform the second iteration the computing device may access the global data storage to retrieve the subgraph A similar situation may also exist for the computing device Since the computing device does not contain the subgraph the computing device may also access the global data storage to retrieve the subgraph so that the second iteration may be performed.

The second iteration is performed in a similar manner as the first iteration . First the vertex propagation algorithms and of the respective computing devices and may be implemented on the corresponding input values and subgraphs to generate temporary values. Subsequently the vertex aggregation algorithms and may be implemented on the sets of temporary values to generate the sets of final output values and . For example the sets of output values and may be web page rank scores. Once the vertices aggregation algorithms have provided the output values the second iteration terminates.

As shown in prior to the execution of a third iteration by each of the computing devices a data manager may store the subgraph processed by each computing device in the respective local cache of the computing device. However this storage process only occurs for each cache provided that the processed subgraph is not previously stored in the cache. For example since the subgraph is already stored in the local cache there is no need for the computing device to repeat the storage of subgraph However the subgraph as processed by the vertex propagation algorithms in the second iteration was not previously stored in the local cache of the computing device As a result the data manager may cause the computing device to store the subgraph in the local cache Likewise the subgraph as processed by vertex propagation algorithms in the second iteration was not previously stored in the local cache of the computing device Thus the data manager may cause the computing device to store the subgraph in the local cache

For the third iteration the DAG distributed execution engine may provide that each of the computing devices and process the same subgraphs and associated values as processed in the second iteration . Accordingly the computing device may access the stored subgraph on the data cache Likewise the computing devices may access the stored subgraph on the data cache and the computing device may access the stored subgraph on the data cache As described above the access of the subgraphs on the local caches on the computing devices may reduce data transfer lag. Once the subgraphs are accessible the third iteration may be performed on each computing device using one of the set of output values and as a set of input values. The third iteration may be implemented on each computing device using one of the respective vertex propagation algorithm and and the respective vertices aggregative algorithms and In this way additional iterations of the iteration algorithm may be performed with each computing device storing a newly encountered subgraph in its data cache for possible processing in another iteration. It will be appreciated that an iteration during which a computing device is assigned to access a subgraph that it has processed previously may or may not occur consecutively after the iteration during which the subgraph is stored. Indeed such an iteration may occur any number of iterations after the iteration during which the subgraph is stored. Moreover the subgraphs are not modified in each iteration. Thus in one embodiment the subgraphs may be read only data files.

While exemplary scheme is illustrated with respect to the computing devices and a limited number of subgraphs and a limited number of iterations a DAG distributed execution engine may include any number of computing devices. In turn DAG data for the distributed execution engine may be partitioned into any number of subgraphs and any number of iterations may be executed to obtain the desired final output values. For example the calculation of web page rankings may involve a larger DAG that includes linkage data for millions of web pages. Accordingly each subgraph may contain many gigabytes GB of data. Moreover the DAG may be processed by tens or hundreds of computing devices for hundreds of iterations.

Given this potential large scale the use of the global data storage and the data caches such as the data cache to store subgraphs may provide significant advantage over the handling of subgraphs by a conventional DAG distributed execution engine. In a conventional DAG distributed execution engine subgraphs are generally transferred from vertex to vertex between iterations via data pipelines . In other words in the scenario where the processing of the sets of output values and are switched between the computing devices and the computing device as part of a conventional DAG distributed execution engine needs transfer the subgraph along with the output values to the computing device Similarly the computing device needs to transfer the subgraph along with the output values to the computing device These transferred are generally accomplished via a network such as an Internet Protocol IP based network.

Thus in an implementation involving a large number of computing devices and a significant number of iterations a conventional DAG distributed execution engine may spend a considerable proportion of computing time transferring data between computing devices. The exemplary scheme may serve to decrease the need for data transfer between computing devices thereby increasing the efficiency of a DAG distributed execution engine that process an iterative algorithm.

During operation an exemplary data manager may receive a data request from a memory client . For example the memory client may be a vertex aggregation algorithm such as the vertex aggregation algorithm . Moreover the data may be a subgraph such as one of the subgraphs . Accordingly the data manager may obtain the requested data from a data source via the network if the requested data is not in a local cache.

Alternatively if the requested data is present in a local cache the data manager may access the data from a local cache. For instance one or more data files such as a data file may be located in the disk cache . The data file may include a subgraph. A memory map file is associated with the data file . The memory map file may be stored in a memory cache which may be located in a random access memory RAM of the computing device. The memory map file may act as a buffer that stores at least a portion of the data in the data file . Moreover the memory map file may be a virtual memory space that is readily accessible by the memory client . Accordingly by using a memory map file the data manager may enable the memory client to read and or write data from the data file without the need to retrieve and or duplicate the entire data file . Thus the speed and performance of file access may be enhanced. In one embodiment the memory client may pass a memory file request such as for the data file to a data manager . In turn the data manager may locate the memory map file in the memory cache and respond with the identity of the memory mapped file. The memory client which may be a vertex aggregation algorithm then accesses at least a portion of the data file via the memory map file . Nevertheless a subgraph may also be stored as a data file in the disk cache so that the data file is accessible by the memory client via read and or write operations. In another embodiment a data file that includes a subgraph and which is directly accessible by the memory client may be stored in the memory cache . It will be appreciated that the disk cache and the memory cache may be configured to store a plurality of data files that performs the same functions as each of the data file memory map file data file and data file .

The disk interface module is configured to enable the data manager to interact with a disk cache such as the disk cache . Likewise the memory interface module is employed to enable the data manager to interact with a memory cache such as the memory cache . The distributed data interface module is configured to enable the data manager to interact with a distributed execution data source that is external to the computing device such as the data source .

The memory client interface module is configured to interface with an application to store and access data files to a cache on the computing device. For example the memory client interface module may enable requests for subgraphs to be received from a memory client such as the memory client . Moreover once the data manager locates a particular subgraph the memory client interface module may enables the memory client to access the particular subgraph. For example the data manager my use the memory client interface module to pass the identity of the particular subgraph as a memory handle to the memory client .

The data locator module is configured to determine the location of data that is requested by the memory client. In one embodiment once the data manager receives a request for subgraph the data locator module may ascertain whether the subgraph is located one of the disk cache or the memory cache . For example the data manager may receive a subgraph identifier for a requested subgraph. The data locator module may compare the identity of the requested subgraph to the identities of the subgraphs stored in the database to determine whether the subgraph is located in one of the caches and .

The memory mapping module is configured to activate a memory mapping function of the computing device to create a memory map file for a data file. For example once a subgraph is stored in a disk cache the data mapping module may cause a memory map file such as memory map file to be created.

The data cache module is configured to store data files such as subgraphs into one of a disk cache and the memory cache of the computing device. In one embodiment when a memory client vertex propagation algorithm makes a request for a subgraph for processing the data cache module may activate the data locator module to compare the identity of the subgraph to the subgraphs stored in one of the caches and . If the requested subgraph is not located in one of the caches and the data cache module may retrieve the requested subgraph from a global data source such as the data source . Additionally the data cache module may store the graph in one of the caches or . Hence when a subgraph is stored in one of the caches and the data cache module may update the database with the identity and location of the subgraph.

Furthermore since memory input output I O is generally faster than disk I O the data cache module may include one or more algorithm that ensures the optimal use of the memory cache and the disk cache . For example the data cache module may include an algorithm that tracks the frequency at which each of a plurality subgraphs stored in the computing device is requested by a vertex aggregation algorithm. Accordingly one or more subgraphs that meet a first threshold of access frequency may be stored in the memory cache . One or more subgraphs that meet a second threshold of access frequency may be stored in a disk cache . However a memory map file such as memory map file may be created for each subgraph so that the vertex aggregation algorithm may process a particular subgraph in this category without the need to retrieve and or duplicate the subgraph. Likewise in an additional embodiment one or more subgraphs that meet a third threshold of access frequency may be stored in a disk cache without the creation of corresponding memory map files. Instead the vertex aggregation algorithm may access each subgraph in this category by performing a read operation from the disk cache . In one embodiment the first threshold access frequency is greater than the second frequency threshold. In turn the second frequency threshold is greater than the third frequency threshold.

Accordingly the data cache module may also keep track of the frequency that each subgraph is requested in the database . It will be appreciated that the frequency thresholds for the operation of the data cache module may be established in a variety of ways. For instance the frequency thresholds may be established based on a ratio between the times that a particular subgraph is processed and of total number subgraph processing iterations. To illustrate a particular subgraph may be deemed to have meet a particular frequency threshold if 10 iterations of the iteration algorithm are performed by the computing device and the particular subgraph was requested in 8 of those iterations. In another instance a frequency threshold may simply a predetermined number of times that a subgraph is processed. For example a frequency threshold may be established so that a subgraph is moved from the disk cache to the memory cache as long as it has been requested a predetermined number of times. Nevertheless it will be appreciated the frequency thresholds may be established using one or more of variety of criteria as long as these criteria serve to correlate the speed of the I O method with the frequency of subgraph request and serve to place some of the stored subgraphs to the disk cache and other stored subgraphs to the memory cache . The data cache module may also keep track of information regarding particular type of cache e.g. disk memory as well as whether a memory map is created for each stored subgraph.

Furthermore the data cache module may be provided with the ability to shuffle the subgraphs from between the disk cache and the memory cache . In other words the data cache module may move a subgraph from the disk cache to the memory caches and vice versa. In addition the disk cache module may also control the memory mapping module to ensure that the appropriate memory mapping files are created and retired as well possess the ability to purge the subgraphs from the caches and following the termination of an iterative algorithm.

At block a plurality of subgraphs of directed acyclic graph DAG are stored in a global storage system such as the global storage system . The global storage system may be a distributed data storage system that is configured to interface with the exemplary disk caching system illustrated in . The subgraphs are partitioned portions of the DAG. For example a DAG may be divided into hundreds or thousands of subgraphs. At block a computing device may process a subgraph such as one of the subgraphs that is stored on the global storage system. The computing device is a component of a DAG distributed execution engine that may include a plurality of computing devices.

In various embodiments a vertex propagation algorithm of an iterative algorithm running on the computing device may request a subgraph for processing. In one embodiment the iterative algorithm may be a web page ranking program. The computing device may use the data manager to access the subgraph from the global storage system via a network such as the network . The retrieved subgraph is processed the vertex propagation algorithm of the computing device.

At block a copy of the processed subgraph is stored in a cache of the computing device. For example the cache may include one of the disk cache and the memory cache . As described the memory cache may provide faster access than the disk cache . At decision block the iterative algorithm may make a determination as to whether another iteration of the iterative algorithm is to be performed. The iteration algorithm may call for the performance of iterations as long as the desired outputs have not been obtained. For example if the iteration algorithm is configured to calculate web page ranking for a plurality of web pages based on the links in each page the iteration algorithm may continue to perform iterations until final rankings for each web page are obtained. The performance of an additional iteration includes additional processing of a subgraph by the vertex propagation algorithm on the computing device.

If the iterative algorithm determines that another iteration of the iterative algorithm need not be performed the process may proceed to block . At block process may be terminated and the final results of the iterative algorithm outputted. However if it is determined that another iteration of the iterative algorithm is to be performed yes at decision block the process may proceed to decision block . At decision block the computing device may make a determination as to whether the subgraph to be process in this iteration has been previously processed by the computing device. In other words the computing device may ascertain whether the subgraph is stored in a local cache such as one of the disk cache and memory cache of the device.

If the computing device determines that the subgraph to be processed in the subsequent iteration is stored in the local cache of the computing device yes at the decision block the process may proceed to block . At block the computing device may enable the vertex propagation algorithm to access the subgraph from the local cache. In various instances the vertex propagation algorithm may access the stored subgraph via an I O operation a memory map file or the like. Accordingly the vertex propagation algorithm may carry out another processing of the stored subgraph. Once the processing of the stored subgraph is completed at block the process may loop back to decision block where the iteration algorithm may determine if an additional iteration is to be implemented.

However if the computing device determines that the subgraph to be processed in the subsequent iteration is not stored in the local cache of the computing device no at the decision block the process may proceed to block . At block the computing device may enable the vertex propagation algorithm to retrieve the subgraph to be processed from the global storage system . Once the subgraph is retrieved it is processed by the vertex propagation algorithm to implement the iteration. With the processing of the subgraph at block the process may loop back to block where a copy of the subgraph is stored in the local cache of the computing device. The process may then proceed further from block until all the iterations of the iteration algorithm are complete.

In a very basic configuration computing device typically includes at least one processing unit and system memory . Depending on the exact configuration and type of computing device system memory may be volatile such as RAM non volatile such as ROM flash memory etc. or some combination of the two. System memory typically includes an operating system one or more program modules and may include program data . The operating system include a component based framework that supports components including properties and events objects inheritance polymorphism reflection and provides an object oriented component based application programming interface API such as but by no means limited to that of the .NET Framework manufactured by Microsoft Corporation Redmond Wash. The device is of a very basic configuration demarcated by a dashed line . Again a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.

Computing device may have additional features or functionality. For example computing device may also include additional data storage devices removable and or non removable such as for example magnetic disks optical disks or tape. Such additional storage is illustrated in by removable storage and non removable storage . Computer storage media may include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. System memory removable storage and non removable storage are all examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of device . Computing device may also have input device s such as keyboard mouse pen voice input device touch input device etc. Output device s such as a display speakers printer etc. may also be included. These devices are well know in the art and are not discussed at length here.

Computing device may also contain communication connections that allow the device to communicate with other computing devices such as over a network. These networks may include wired networks as well as wireless networks. Communication connections are one example of communication media. Communication media may typically be embodied by computer readable instructions data structures program modules etc.

It is appreciated that the illustrated computing device is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well known computing devices systems environments and or configurations that may be suitable for use with the embodiments include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor base systems set top boxes game consoles programmable consumer electronics network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and or the like.

The caching of previously processed subgraphs by the computing devices of a DAG distributed execution engine may reduce the redundant retrieval and transfer of data for each iteration of an iterative algorithm. Thus embodiments in accordance with this disclosure may serve to enhance the speed and efficiency of iterative algorithms executed by a DAG distributed execution engine.

In closing although the various embodiments have been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended representations is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claimed subject matter.

