---

title: Storage systems using write off-loading
abstract: Improved storage systems which use write off-loading are described. When a request to store some data in a particular storage location is received, if the particular storage location is unavailable, the data is stored in an alternative location. In an embodiment, the particular storage location may be unavailable because it is powered down or because it is overloaded. The data stored in the alternative location may be subsequently recovered and written to the particular storage location once it becomes available.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08074014&OS=08074014&RS=08074014
owner: Microsoft Corporation
number: 08074014
owner_city: Redmond
owner_country: US
publication_date: 20080331
---
Enterprise data centers have a high power consumption which limits the density of servers and increases the total cost of ownership. Power management features have been introduced for server CPUs central processing units which provide low power states and dynamic clock and voltage scaling and reduce power consumption significantly during periods when the CPU is idle. There is no such advanced power management however for the storage sub system of a data center and as a result storage uses a significant fraction of the power budget. In an example a typical enterprise grade disk consumes 12 W even when idle compared to 24 W for a dual core processor. As servers typically have many disks the power consumption due to storage exceeds that of the processors.

The embodiments described below are not limited to implementations which solve any or all of the disadvantages of known data centers and their storage sub systems.

The following presents a simplified summary of the disclosure in order to provide a basic understanding to the reader. This summary is not an extensive overview of the disclosure and it does not identify key critical elements of the invention or delineate the scope of the invention. Its sole purpose is to present some concepts disclosed herein in a simplified form as a prelude to the more detailed description that is presented later.

Improved storage systems which use write off loading are described. When a request to store some data in a particular storage location is received if the particular storage location is unavailable the data is stored in an alternative location. In an embodiment the particular storage location may be unavailable because it is powered down or because it is overloaded. The data stored in the alternative location may be subsequently recovered and written to the particular storage location once it becomes available.

Many of the attendant features will be more readily appreciated as the same becomes better understood by reference to the following detailed description considered in connection with the accompanying drawings.

The detailed description provided below in connection with the appended drawings is intended as a description of the present examples and is not intended to represent the only forms in which the present example may be constructed or utilized. The description sets forth the functions of the example and the sequence of steps for constructing and operating the example. However the same or equivalent functions and sequences may be accomplished by different examples.

Saving power in storage systems is difficult. Simply buying fewer disks is usually not an option since this would reduce peak performance and or capacity. The alternative is to spin down disks when they are not in use but the traditional view is that idle periods in server workloads are too short for this to be effective.

Analysis of block level traces of storage volumes in an enterprise data center shows that significant idle periods do exist. Traces were gathered from servers providing typical enterprise services such as file servers web servers web caches etc. Experimental results in which the threshold as in block was set at 60 seconds demonstrated 28 36 saving in energy.

The method of operation as shown in may further comprise re directing writes which are received in relation to volumes which have been spun down to other storage elsewhere in the storage system such as another volume or a different type of memory or any other available storage in the storage system. In most examples the writes are re directed to other persistent storage.

When a write request is received for a volume block if the volume has been spun down in block resulting in Yes in block the data is written or off loaded to another storage location block . However if the volume has not been spun down No in block the data is written to the volume block . Where data is off loaded in block the data is subsequently recovered from where it was written and written to the correct volume block . The volume where data is originally intended to be stored as detailed in the request in block may be referred to as the home volume .

The volume s which have been powered down in block may be powered up periodically and or on receipt of a read request for data stored on the spun down volume. They may also be powered up i.e. spun up the disk example if they are unable to off load a write for any reason or if they have reached a limit set on the amount of off loaded data. Spinning up a volume takes time e.g. up to 10 15 seconds although the delay may differ between different storage technologies and this introduces a latency where it is performed in response to receipt of a read request however experimental results show that this occurs only rarely e.g. that the performance of all write requests and 99 of read requests are not degraded . For some applications a large delay e.g. 10 15 seconds may be unacceptable even if rare and in some examples write off loading may not be enabled on the volumes that these applications use.

Whilst main memory caches can be effective at absorbing reads they tend to be less effective at absorbing writes. Most workloads do not exhibit high rates of over writes in memory hence most writes have to go to disk. As a result there are periods in storage systems where all the traffic is write traffic. By using a method such as that shown in which allows blocks written to one volume to be redirected to other storage elsewhere in the data center or other storage system the periods during which disks can be spun down are increased significantly. Experimental results have shown that the method can cause volumes to be idle for 79 of the time on average and provide an energy saving of 45 60 .

Data is off loaded temporarily in block for a few minutes up to a few hours and can be reclaimed lazily in the background after the home volume s disks are spun up. The data reclaim process can be arranged to be performed when the home volume is idle but not spun down and also in some examples when the network load is low.

The method is applicable to various types of storage architectures including Direct Attached Storage DAS Network Attached Storage NAS and Storage Area Networks SANs . The method may be applied within a single machine or within a group of well connected machines e.g. within a rack within a data center or across multiple well connected data centers.

The method shown in may be referred to as write off loading . It uses periods of write dominated load to spin disks down and off load write requests reverting to normal operation during periods of read dominated load. When writes are being off loaded the write response times and throughput are comparable to using the home volume but the power consumption may be reduced significantly.

Example implementations of the system of and the method of are described in more detail below. Whilst the following description refers to off loading of data on a block level of granularity it will be appreciated that data may be off loaded at coarser or finer levels of granularity. Furthermore the following description uses volumes as the storage elements which are powered down. This is by way of example only and the methods are also applicable to different levels of granularity e.g. on a per disk basis and other types of memory. In most examples the memory referred to is persistent memory however in some examples volatile memory may be used.

The write off loading is implemented using two different components a manager and a logger as shown in the architecture diagram in . Each volume supporting off loading has a manager that is entirely responsible for the volume its home volume and decides when to spin the physical disks up or down and also when and where to off load writes. Off loaded blocks are only temporarily off loaded and the manager is also responsible for reclaiming blocks previously off loaded. To achieve all these tasks the manager intercepts all read and write requests to its home volume e.g. from application or other clients . A manager may be responsible for one or more volumes and therefore may have one or more home volumes. The manager may for example run on the server to which the storage is attached or may run within a storage device.

When a manager decides to off load a block it selects one or more loggers to store it temporarily. Each logger instance has a small area of storage which is used exclusively to store off loaded blocks and metadata until they are reclaimed by a manager or no longer required. In most examples the small area of storage is persistent storage. The following description refers to the use of persistent storage by way of example. The storage associated with a logger could be a disk NVRAM or solid state memory such as flash depending on what is available on each server the logger s data layout may be optimized for the particular type of storage.

In an example a logger may use a small partition at the end of an existing volume or a file on an existing volume disk NVRAM etc to persist data and metadata. This avoids the need to dedicate additional disks spindles for off loading. The remainder of the volume functions as before and could have an associated manager to enable off loading. In general a volume might host zero or more managers and zero or more loggers on the same set of physical disks. An example configuration for a data volume may be one manager and one logger with the latter using a small partition at the end.

The set of loggers that a manager uses is configurable and the loggers used by a manager may offer the same or better failure properties as the home volume. Whilst loggers could be selected which provide worse failure properties this may not be suitable for many applications where data integrity and reliability is important. In some examples a combination of loggers with the same or better failure properties and loggers with worse failure properties e.g. ones which use volatile storage may be used. In such an example the volatile storage may be used to store additional copies of data and provide better availability of data for read requests whilst the persistent storage provides the failure resilience.

The manager may also be configured so that it will for example only off load blocks to loggers residing on the same server as itself in the same rack or across the entire data center. Different managers may use different off loading strategies and the strategy used in any instance of off loading may be dependent on the application issuing the intercepted request.

The following description refers to two states for a manager or logger component active and standby. When a manager or logger component is referred to as being in standby the volume used by that component has transitioned to the standby state. When a manager goes into standby i.e. when it puts its home volume into standby it will force loggers sharing the same physical disks e.g. those loggers which use a small partition on the home volume to go into the standby state. The manager will then off load writes to loggers that are not in the standby state. Loggers using solid state memory or NVRAM do not enter the standby state as they are not located on a volume which may go into standby.

The following description provides more detailed example implementations of a logger and a manager. Loggers support the following remote operations as shown in write read invalidate and reclaim . A write consists of receiving blocks and metadata from a manager block and storing or persisting the provided blocks and metadata block . The metadata may comprise the source manager identity a range of logical block numbers LBNs and a version number. Additional data may also be stored either within the metadata or separately for each write request in block such as one or more epoch IDs as described below . Once the data is stored in block this is acknowledged to the manager block . A read returns the latest stored versions of the requested blocks blocks .

An invalidate request received in block specifies a set of blocks and versions that are no longer required. To ensure consistency the invalidate request explicitly includes version information and the logger durably marks the corresponding versions as invalid block and acknowledges this to the manager block . The logger can then lazily release the space used to store the invalidated data and metadata block . A reclaim request received in block is similar to a read except that no block range is specified the logger can return any valid block range it is holding for the requesting manager block . Invalidates and reclaims are non latency critical operations reads and writes are latency critical but reads are likely to be rare due to the use of techniques such as main memory caches. Hence loggers may be optimized for the performance of writes .

An example implementation uses a log based on disk layout for storing the off loaded writes and metadata. This gives writes a good locality both data and metadata are written with a single I O to the current head of the log. Log compaction and other maintenance tasks may be done in the background with low priority. Metadata about the valid blocks stored for each manager their versions and their location in the log may be cached in main memory for fast access.

The soft state which may be cached in main memory comprises details of the LBNs stored for each manager or home volume their location and version . Whilst the hard state may comprise multiple versions of the same block some of which are stale i.e. marked as invalid but not yet deleted the soft state does not include details of stale versions but comprises a list of the most recent version of any block stored for a manager. The soft state enables a logger to find blocks quickly within the hard state when a request is received from a manager. If the soft state is lost due to failure or shutdown it can be recreated from the hard state. In some embodiments only a hard state may be maintained at the logger however where a soft state is maintained read and reclaim requests can be serviced more quickly.

In an embodiment the hard state may comprise a circular on disk log as shown in . The log comprises a header block which may be at block . The header block stores a pointer indicated by arrow for the tail of the log. The tail marks the position where all preceding data in the log is stale. If the log is read going forward from the tail in direction of arrow until a null block is reached all the data will be read but this may include some invalid versions and this is how the soft state may be recreated. New data is written at the head of the log i.e. overwriting some of the null blocks . This new data comprises metadata and any data blocks. There may be no data blocks e.g. for an invalidate operation or one or more data blocks e.g. for a write operation . When data blocks are invalidated they are considered stale e.g. blocks and the position TM of the first non stale block is stored in memory. The log wraps circularly around the disk skipping the log header and a linear representation is also shown in .

The space used by invalidated stale data between TD and TM i.e. blocks is recovered when the header is rewritten to update the value of TD to equal TM. However some invalidated data blocks may lie in the range between TM and the head pointer . These invalidated blocks are recovered by compaction operations which may be triggered by for example changes in disk load or in remaining free log capacity. shows a flow diagram of an example compaction method. One compaction operation consists of reading the metadata block and any associated data blocks which start at TM block and rewriting them to the head block advancing the head appropriately. TM is then updated to point to the position of the next non stale block block which may allow TM to skip over not only the recently copied metadata and data blocks but also any subsequent bubble of invalidated data blocks. The compaction operation may be repeated. Compaction generally takes place only when the disk is idle however if most of the disk capacity is used e.g. then compaction may occur even if the disk is active.

A nuller is used to null blocks ahead of the head of the log so that there is a null block ahead of any newly written data. An alternative to using a nuller to null blocks ahead of the head of the log is to write a null block after each data block written to the head of the log and then to overwrite the null block with the next write. However if a write failure occurs this can result in errors in the log. If single writes are issued with the trailing null block being overwritten with the new metadata block then if the write fails part way through the log will be un terminated and undefined erroneous data will appear during recovery. One method to address this is to perform writes in two phases firstly writing the new data waiting for that to complete then committing it by overwriting the null with the new metadata block. However this will be very slow. Use of pre nulled blocks as shown in enables multiple writes to be issued in parallel which improves performance.

An alternative design that avoids pre nulling is to have the records be self verifying . In such an embodiment the record header includes an epoch number which is incremented every time the logger wraps around the beginning and a checksum over the records data and the metadata. During recovery if a record is read that has an earlier epoch than the previous one then the log is considered to be terminated at that point. This alternative and further examples of loggers are described in more detail below.

The relative positions of the four pointers described above are shown in . The linear representation also shows the block contents where h is log header m is metadata d is data 0 is null block e is end of disk x is arbitrary contents i.e. junk and italicized characters indicate valid but stale data metadata. In other words the invariant is H

Write invalidate is the main foreground operation. Each write invalidate request writes a metadata block s followed by data blocks if any in a single sequential write to the current head position H. If N blocks are written it is first checked that H N

Multiple write invalidate operations can be issued concurrently as long as they are written to contiguous non overlapping parts of the disk. However for failure recoverability the operations are acknowledged in the order in which they occur in the log. If the writes are issued to a logger in standby i.e. the volume is spun down the logger will force a spin up however this may be avoided where possible as the manager will probe the loggers first to check their status. In some examples a manager may not be able to off load writes to a logger in standby. Invalidations which may be issued to a logger in standby will be queued until the next spin up as they are not latency critical and will be acknowledged to the client once they are written to the log. If a log write would go off the end of the disk then it is wrapped around to the beginning i.e. block following the log header . In this case it is split it into two concurrent I Os.

Whenever the in memory data structures are updated the log record corresponding to the in memory tail is checked to see if it is now completely stale i.e. that there are no non stale data blocks between stale data blocks . If it is the in memory tail is advanced until this is no longer true i.e. it is advanced until the end of the contiguous stale data . If there are lots of stale blocks in the tail but no section of contiguous stale data which would enable the in memory tail to be advanced the valid blocks from the tail may be re written at the head of the log and the original versions in the tail marked as stale. This process may be referred to as compaction .

If the on disk tail pointer lags the in memory version the log header may be updated depending on whether the disk is idle or perhaps the available log space is too low. If the disk is idle and there are less than the target number of null blocks e.g. 1 GB and the format pointer FM has not caught up with the on disk tail TD then up to one batch e.g. between 1 to 16 MB of blocks may be nulled at a time. When the head catches up with the nuller any write requests are rejected. An alternative to rejecting requests would be to append a null block to the writes in this case but this would prevent concurrent writes since they would overlap unless a different mode e.g. specifically for serializing appends was used. Another technique would be to increase the nuller priority inversely proportional to the distance between H and FM as H approaches FM the nuller priority increases.

Reclaiming and reading is done without any log writing the latest version of any requested block and its location are available from the in memory structures so a read is scheduled. For reclaims since the client is happy with any block the logger attempts to pick the one which is closest to the tail to maximize the chances of tail advance. For both read and reclaim reads of contiguous data blocks are coalesced to minimize the number of I Os. Reclaims are rejected if spun down or busy reads are serviced spinning up the disk if required.

Background operations such as reclaiming nulling compaction queued invalidates and log header updates occur only when the disk is idle and the number of I Os issued concurrently each time the disk goes idle may be limited e.g. to 16 . The background operations may be prioritized such that queued invalidations are issued first then queued reclaims then a nulling request if possible then compaction if possible otherwise a log header update if possible.

Log recovery e.g. where the soft state has been lost consists of reading the tail pointer from the header reading each log record header and updating the in memory structures i.e. the soft state advancing to the next header wrapping around if necessary until a null block is reached which is the head. If an entry whose checksum does not match is reached then the process stops there nulls the first block and calls it the head. This is likely to be a log record that was not completely written before a crash and since it was not acknowledged because the write was not completed as shown in it can be discarded. However if there were writes to subsequent log locations these are also discarded by this process and therefore as described above writes are acknowledged in log order to ensure that only those writes which have not been acknowledged will be discarded. The checksums help guarantee that log appends are atomic along with the fact that data is only written when it is guaranteed that the new head will be a null block after a successful write which makes sure that the log is correctly terminated.

The in memory structures i.e. the soft state may comprise the following in addition to maintaining H TM TD and FM mentioned before 

The list is kept sorted by the log position of the client s tail. Thus the tail of the list points to the client whose linked list head will give the TM. Whenever a client s tail changes its position in the list is recomputed which is O clients but this is expected to be infrequent and to have a small number of clients.

When data is appended to the log a new log record is created which is appended to the head end of the appropriate client s list then each LBN list in the hash table is updated to point to the new record. If the LBN already points to some log record that record is de referenced if it goes to zero then it is deleted if it was the head of the list then the client is deleted if TM changed then it is updated.

Invalidation records are appended to the disk log but are not maintained in memory. The only in memory update is to the hash table or range map if an invalidate request with version v is received then ownership of the LBN range is taken away from any in memory records having a version which is less than or equal to v. The client ensures that for any LBN every write has a strictly greater version than any previously issued invalidate for that LBN. Otherwise it would not be possible to garbage collect the most recent invalidation record for any LBN.

As described above the integrity of the log is maintained at all times across failure and recovery of the logger component. This means that after recovery the log only reflects writes that have been issued by some client and that it reflects all writes that have been acknowledged to any client. These properties are guaranteed if it is possible to correctly find on recovery 

The first property may be guaranteed by keeping a pointer to a valid tail TD in the log header block. If the tail advances then the logger lazily updates this pointer in the log header block. Since the log is circular the head can eventually catch up with TD however the head cannot go past TD i.e. the record referenced by TD or any subsequent record is not overwritten.

The second property requires that the first invalid log header termination marker is identified when scanning the log during recovery. This can be done in a number of different ways and two examples that have been described above 

In another example a nuller as in the second bullet above may be used as the default and the method may fall back on appending a termination marker as in the first bullet above when the situation arises that a write would otherwise not be possible because there are insufficient blocks already nulled.

Another example which is described briefly above is to use an unforgeable epoch ID. In this scheme each valid log record header contains its own epoch ID and a copy of the previous log record s epoch ID. The epoch ID may be any bit string e.g. 1024 bits in length that cannot be forged. It could be generated for example using a strong random number generator or a secure digital signature based on a secret key. A fresh epoch ID may be generated for each log write but it is sufficient to generate a fresh epoch ID each time the circular log wraps around the beginning of the disk. On recovery every valid log header other than the very first one scanned i.e. the tail is checked to see if it correctly reflects the previous log record s epoch ID. A block that does not satisfy this property is not a valid log header and the log is considered terminated at that point.

All these schemes for providing log integrity prevent accidental corruption of the log as well as protecting against malicious spoofing attacks. An example of a spoofing attack is a manager writing data that resembles a log header from another manager followed by a log wraparound a logger failure and a logger recovery. If the recovery process does not correctly identify the valid portion of the log it may incorrectly interpret a data block as a log header if this data block was written by the malicious manager then this could result in corruption of some other manager s data. Thus the logger ensures that a data block is not misinterpreted as a correct log header. The first two schemes or the combination thereof achieve this by terminating the log with a special termination marker blocks beyond the termination marker are not read during recovery. The final scheme described above achieves this by preventing an adversary from guessing at spoof time the correct epoch ID that will be required at recovery time.

The manager controls the off loading of blocks deciding when to off load blocks and when to reclaim them. It is also responsible for ensuring consistency and performing failure recovery. To achieve this each manager maintains persistently the identities of a set of loggers with which it interacts referred to as the logger view as shown in . The number of loggers in the logger view may vary between managers or may be fixed. Having a large number of loggers provides managers with more options particularly where a number of them may be in standby however if data is spread among a large number of loggers the managers requires more loggers to be active to enable data to be reclaimed. The manager also maintains two in memory data structures . The data structure which is stored persistently may also be referred to as the hard state and the in memory data structures may be referred to collectively as the soft state.

The redirect cache stores for each block off loaded the block s LBN the identity of the logger storing the current data for the block and the corresponding version number . Version numbers are unique monotonically increasing values e.g. 64 bit quantities which ensure that the manager can identify the last written version of any block during failure recovery. The redirect cache may also comprise a one bit local flag not shown in which identifies that the latest version is already on the local disk i.e. in the home volume but that the off loaded copy has not yet been invalidated by the logger. The garbage cache stores the location of old versions of blocks. In the background the manager sends invalidation requests for these versions when these are committed by the logger they are removed from the garbage cache. Before invalidating the latest off loaded version of any block the manager checks first that a the data from this latest version has been written to the home volume e.g. that the local flag is set and b all older versions on other loggers than the one holding the latest version have been invalidated and acknowledged to be so.

The operation of the manager can be described with reference to which show flow diagrams of example methods of operation. The manager intercepts all read and write requests sent to the home volume block . For a read request Yes in block the manager first checks the redirect cache for existing logged versions block . If none is found No in block the read is serviced locally from the home volume block causing it to transition from standby to active if necessary Yes in block and block . Otherwise Yes in block the request is dispatched to the logger identified as having the latest version of the block block . Multiblock reads are split as required to fetch data from the home volume and or one or more loggers.

For a write request No in block the manager off loads the write to a logger blocks if the home volume is in standby if Yes in block . It also may offload the write if there are currently logged versions of any of the blocks Yes in block so that the new version is persistently recorded as the latest version. Alternatively where there is a currently logged version of any of the blocks Yes in block the manager could invalidate the currently logged version write the new version to the home volume and only acknowledge the write request when both of these operations have completed i.e. been acknowledged not shown in . Writes that are not off loaded No in both blocks and are sent directly to the home volume block .

To off load a write the manager first chooses a logger. In one embodiment the manager probes the loggers in its logger view block this may for example be done using subnet broadcast for efficiency. Each logger replies with a set of metrics such as the power state of the logger s volume its queue length the amount of available space etc block . The manager ranks the loggers using these metrics and selects one to off load the write to block . The manager sends the write request to the selected logger block and when the write is committed and acknowledged by the logger block the manager updates its redirect cache with the latest version and moves any older versions to the garbage cache block . Once the write has been confirmed either by the home volume or the logger the write can be acknowledged to the client which sent the intercepted request block .

In some examples the manager may select more than one logger in block and send the write request to each selected logger in block . The write may be acknowledged in block once a defined number of loggers or a defined proportion of loggers have confirmed the write. This is referred to as k way logging .

The probing of loggers in the logger view in block enables the manager to perform load balancing between loggers. However in some examples load balancing i.e. blocks may not be performed. In such an example the manager may select a logger based on other parameters or may select the same logger each time etc. In another example the manager may not probe loggers on interception of a write request as shown in but may instead dynamically track the status of the loggers in its logger view. In such an example blocks and are omitted and the manager selects a logger in block using the metrics which it is dynamically tracking.

When the home volume is idle the manager reclaims off loaded blocks from loggers in the background block and writes them to the home volume block . After the reclaimed blocks are written to disk the manager sends invalidation requests to the appropriate loggers. To ensure correct failure recovery the latest version of a block the LBN of which is in the redirect cache is invalidated only after all older versions the LBNs of which are in the garbage cache have been invalidated. Therefore invalidation requests are sent first to the logger with the oldest version block and once acknowledged by the logger block the redirect garbage cache can be updated block . The process blocks is repeated until the latest version of the block which may be stored in the redirect cache with a flag identifying that it has been written successfully to the home volume has been invalidated. The background reclaim and invalidation ensure that all blocks will eventually be restored to the home volume and that logger space will eventually be freed.

The manager also sends invalidation requests to loggers in relation to blocks which have been added to the garbage cache as a result of a more recent version being written to a logger as in block . These invalidation requests are sent as a background operation as shown in blocks and again as described above the oldest versions are invalidated before more recent versions.

The manager also controls state transitions to and from standby for the home volume as shown in . The manager monitors the elapsed time since the last read and the last write block if both of these have passed a certain threshold block where the read and write thresholds may be different it spins the volume down block and off loads all subsequent writes as shown in . In an example the thresholds may be set at 60 seconds of no reads and 10 seconds of no writes. In another example the threshold may only be dependent on a period of no reads with no criteria being specified based on writes.

The volume spins up again block when there is a read on a non off loaded block block as in block of or when the number of offloaded blocks meets or exceeds a limit block to avoid off loading very large amounts of data . Before putting the volume into standby in block the manager first ensures that there is at least one logger in its logger view that is using a set of disks different from its own and that is not currently in standby block . This ensures that any future writes to the home volume can be off loaded by the manager without waiting for disks to spin up. If there are no such loggers No in block then the manager does not spin down but periodically probes its logger set for any change in their status block .

This design is optimized for the common case during periods of intense activity the home volumes will be in the active state and all I Os will be local except for a small number of requests on blocks that are currently off loaded. During periods of low write dominated load the home volume is likely be in standby and writes will be off loaded to a logger.

If the manager cannot find any available loggers e.g. in response to the probe in block it spins up the home volume in the background and retries the request until a logger is found or the home volume is spun up. If a volume needs to be taken off line e.g. for maintenance then the manager spins it up as well as all volumes that it depends on or that depend on it. It then forces blocks to be reclaimed until the volume has all its own blocks and none of any other s i.e. its state is restored as if no off loading had occurred. As part of this process the manager may communicate with all managers to ask them to stop sending new offloaded data to its logger and to ask them to reclaim anything they have offloaded in the past. In parallel the manager will be reclaiming its own blocks until they are all back on the home volume.

Enterprise storage is expected to provide consistency and durability despite transient failures such as reboots as well as single disk permanent failures. At the volume level the failure resilience with off loading is the same as that without. However off loading can create failure dependencies between managers and loggers. With off loading at the rack or data center level a manager on machine A could off load blocks to a logger on machine B if machine B suffers a failure then the off loaded blocks would become unavailable on machine A until machine B was brought on line again. This can be addressed by off loading each block to multiple independent loggers using k way logging as described above . With k way logging a manager can tolerate up to k 1 failures in its logger view for retrieving its data.

Write off loading provides both consistency and durability across failures. Durability is achieved by acknowledging writes only when both data and metadata have been reliably persisted as described above i.e. write back caching is not used. Consistency is achieved by using versioned metadata to mark the latest version of a block. When a read is performed for a range of blocks it is quite possible that the required blocks are distributed over multiple loggers as well as the home volume as shown in . The manager uses the version information to ensure that the applications using the volume see a consistent view of the stored data. A checksum may also be added to the metadata to ensure that partial writes are correctly detected on failure recovery e.g. as described above .

If one or more machines reboot due to say a power failure all the loggers recover concurrently by scanning their persistent logs to reconstruct their soft state. Each manager can be brought on line when all the loggers in its logger view are on line. A manager recovers its soft state the redirect cache and garbage cache by requesting information about all blocks stored for it from each logger in its logger view. To optimize the common case of a clean shutdown reboot of a server the manager may write the soft state to a small metadata partition during shutdown this allows managers to restart after a clean shutdown without any network communication. In an example implementation a logger may write its soft state to a small metadata partition in a similar manner.

In order for off loaded blocks to have the same failure resilience as non off loaded blocks a manager s logger view may be restricted to loggers which have the same or higher failure resilience as the home volume. If the storage uses standard solutions such as RAID or RAID for all volumes then this property will be ensured and off loading will provide the same resilience to single disk failures as standard RAID solutions.

When a logger experiences a single disk failure it may push all off loaded blocks to other loggers or the appropriate manager which typically takes seconds to minutes. This reduces the risk of losing off loaded blocks due to multiple disk failures the risk may be further reduced by using k way logging as described above .

The above description refers to the off loading of blocks of data as identified by their LBN e.g. as shown in the data structures in . In other examples the data may be off loaded at a different level of granularity which may be smaller than or different to the block level. In an example a manager may off load byte ranges e.g. arbitrary byte ranges from objects e.g. from files in a file system . In such an example each off loaded byte of an object could have a different version stamp and be off loaded to a different location.

In order to be able to map from byte positions or in general numbered positions within an object to some metadata a data structure may be used. An example of such a data structure is a hash table with one entry per position however for large objects with thousands or millions of byte positions this requires huge amounts of memory. In practice large numbers of adjacent byte positions will share the same metadata since they are written together and therefore a range map may be used instead of a hash table. A range map offers the same functionality as a hash table based map but is more memory efficient.

Write off loading as described above may be used to provide an energy saving in a storage system by enabling volumes to be spun down when they are idle. Write off loading has a number of other applications such as to reduce disk I O over provisioning e.g. in a data center. Write off loading may be used simultaneously in one or more such applications. Use of write off loading to reduce over provisioning and or to use available disk resources when the system is heavily loaded is described in more detail below.

When multiple services applications run in a data center the resources are usually over provisioned for the peak load or load percentile e.g. of each separate service. This requires enough servers to handle the sum of those peaks. However in most cases the load peaks are not correlated and therefore the data center may be over provisioned for the peak load of the combination of the services. If instead the data center is provisioned for the maximum peak load or for the maximum peak total load the provisioning costs are reduced. This however may result in transient situations where particular disks or volumes are overloaded and this can be accommodated using write off loading.

During a transient overload period write requests to an overloaded disk are sent instead to one or more under loaded disks. Hence each workload opportunistically steals resources originally dedicated to other workloads when these latter ones do not need them. In addition or instead dedicated storage may be provided for write off loading which may be local to or remote from the rest of the storage. Instead of provisioning resources for each workload separately a pool of resources is effectively made available to all workloads and whichever workload needs the resources uses them. After the overload period the reclaim mechanism lazily moves the off loaded data from the opportunistic disks to the original disks. To implement write off loading for this application a monitoring mechanism is used to track the load on disks.

In addition to or instead of using these techniques in a transient overload period the methods described herein may be used for load balancing between storage devices e.g. where a first storage device is more loaded than one or more alternative storage devices.

The off loaded data may be replicated for availability and or for improved read performance e.g. using k way logging as described above . Selection criteria may be used to determine which disks receive off loaded writes. For example currently idle disks will perform the write quickly giving short write response times and disks that will likely be idle when the off loaded data is read will yield short read response times. The number of data copies k may be tuned to meet the target availability and or performance.

Write off loading provides a flexible approach. Write off loading may be transparent to applications with applications using the existing file system APIs application programming interfaces and a thin shim layer or file system provider translating them to the write off loading infrastructure. Alternatively applications may use an API to directly off load their writes.

The methods and examples described below are similar to those described above in relation to power saving applications. It will be appreciated that aspects of any of the methods described herein may be combined with aspects of any of the other methods described herein. Furthermore the methods may be used for any suitable application including but not limited to those described herein.

For example unmodified applications may use the Win32 or POSIX APIs to talk to a manager. The manager converts Win32 API to ObjectLogger API the API used for off loading write requests . Modified applications may link to the manager as a library and use the ObjectLogger API directly and such modified applications may use multiple managers but only one manager for each I O. The multiple managers could for example occur if the application itself was distributed over many nodes in this case each node would have a manager. In such a case the application maintains consistency of the data across the different managers e.g. making sure that different managers don t hold different data for the same object . Each instance of the distributed application however would talk to a single manager.

In the following example each application talks to only one manager. The manager decides on a per I O basis which set of storage servers to send a request to and this decision may be based on load information for the servers which is stored. In addition the manager incorporates a metadata service that keeps track of byte ranges for each object. The loggers which may also be referred to as Objectlogger servers utilize a log structured layout to quickly absorb writes. In some examples the absorbed writes are written back to their original locations but in other examples data is only written on the log i.e. reclaiming is not performed. This may be used for applications where the data is only temporary e.g. temporary files which are only of ephemeral use for example during a compile .

At a high level the API between the application and the manager supports reads and writes of versioned byte ranges within objects where the objects have IDs in a flat namespace. So 

API calls may be provided to retrieve the metadata state e.g. what objects and what byte ranges are you currently holding for this client what is the highest version number I am currently storing etc. These may be used for failure recovery and or so that the client does not have to keep its own copy of all this state.

The operation of the logger may be as shown in and described above although the method may operate a different granularity and may use different client object IDs. The logger or disk logger appends all writes to the head of a circular log e.g. as shown in and described above . Each log record contains a header and the data to be written the header may comprise some or all of the client ID object ID version a checksum and a valid bit. The checksum makes sure that in case of a failure it is possible to identify any partially written records. The valid bit enables detection of the end of the log after failure. Log records may be padded out to the nearest sector where the underlying disk device only supports sector aligned writes. Compaction may also occur as described above with reference to .

Disk loggers periodically broadcast metrics about their current load levels or alternatively managers may poll loggers for status data as shown in . The metrics received are cached by the managers. When the manager wants to choose a logger or more than one for writing or reading it uses this load information to make a selection of loggers. The broadcasts may be relatively infrequent to avoid overloading the network. In order to get timelier load information where it is needed these load metrics may also be piggybacked on all the RPC acknowledgements sent by the logger so that a manager that is actively issuing RPCs is kept up to date. Examples of metrics which may be used include long term loads on the system short term loads on the system disk queue length either instantaneous or smoothed aggregated over all I Os or broken out by reads vs. writes perhaps counted in I O operations or in terms of bytes being transferred diversity of machine configurations availability of servers client performance requirements and application behavior correlated peaks .

The manager may support replication for fault tolerance and this is also referred to as k way logging i.e. each write goes to at least k loggers out of its logger view of size m. The parameters k and m may be configurable e.g. when the manager is instantiated. The replication may use a protocol as shown in . For each write the load cache is examined block to pick the k least loaded loggers . The write is sent to all of the selected k loggers block and acknowledgements are received from the loggers. When acknowledgements have been received from at least m loggers block where m k the write is acknowledged to the client . In the background messages can be sent to any loggers having old versions to delete and discard them. In some examples m k or m is a fraction of k e.g. m 0.8 k .

In some examples in addition to k way logging or instead additional copies may be written to help with load balancing on reads. These additional copies may be written in main memory using main memory based objectloggers e.g. like a distributed cache and may be referred to as best effort replication. In some examples a larger number of copies than k may be initially written by the manager expecting m to complete more quickly and the extra copies eventually downgraded using the mark volatile flag to get back to the target replication factor k while still providing improved read performance if logger space is available. Different blocks of data or ranges of bytes may be replicated in different ways depending on the importance of the particular piece of data. For example the value of k may be different for different byte ranges and or only best effort replication in main memory may be used for some ranges.

The manager reclaims off loaded data from loggers in the background block and writes the data to the original location block . After the reclaimed data is written to disk the manager sends invalidation requests to the appropriate loggers . To ensure correct failure recovery the latest version of a particular byte range is invalidated only after all older versions which are in the garbage cache have been invalidated. Therefore invalidation requests are sent first to the logger with the oldest version block and once acknowledged by the logger block the redirect garbage cache can be updated block . The process blocks is repeated until the latest version of a particular byte range which may be stored in the redirect cache with a flag identifying that it has been written successfully to the original location has been invalidated. The background reclaim and invalidation ensure that all data will eventually be restored to the original location and that logger space will eventually be freed. In some applications however reclaim may not occur and the data may remain in the off loaded location.

Manager recovery involves contacting all loggers in the logger view or a quorum of m k 1 in the case of replication block and getting their in memory metadata block and then combining it block .

Both the loggers and the manager keep in memory metadata and the structure may be very similar for both. This metadata identifies where the latest valid version of every byte of every object lives and may be structured as a series of nested maps 

An example implementation of a programming model is described in more detail below. The programming model may be event driven such that even where there are lots of things going on concurrently there is only a single thread. The thread is managed by an event scheduler which fires events in time order. An event is simply a callback function pointer plus a context pointer plus a time. When the callback is invoked it can in turn schedule more events. The idea is that within an event callback function any state can be changed without worrying about race conditions however blocking calls as well as any other calls that take a long amount of time may not be made. The I O may be implemented using asynchronous calls. For example when a read or write is called on the logger it needs to do I O but it cannot block the read or write call itself so the completion has to be signaled through a callback.

In this example implementation all the code running on a given machine runs inside a single ber process with a single event scheduler and a number of components such as managers loggers etc. Each component has an API and an implementation. So for example the manager and logger components both export the objectlogger API. The raw disk and partition components both export the disk API. These components may be stacked by matching the interfaces e.g. the logger needs to talk to something with a disk API to store its data this can be a raw disk or a partition etc. The code base may be extended by adding a new component API and one or more components that implement it or by adding a new component that implements an existing API or by adding functionality to an existing component without changing its external interface.

The stacked model means that components can be hooked together every time the ber process is instantiated. This may be done through a configuration file the first thing that the ber process does is parse the configuration file and instantiate components accordingly.

In an example model the client applications link into the ber process and access loggers etc. through function calls. So for legacy applications that make POSIX or win32 calls a shim layer may be used that translates these into objectlogger calls. Also for threaded applications the shim layer switches between the threaded world and the event world. As a result there may be more than one shim layer such as POSIX win32 and win32 threaded objectlogger events.

A write may be considered consistent when any subsequent read will reflect it i.e. it is now in the global state but not necessarily across failures . At some later point it is also durable. With win32 files can be opened for writing in write back mode which means the write is consistent when the call returns but only durable when the next FlushFileBuffers call returns. In write through mode it is also durable when the write like call returns write like calls are WriteFile WriteFileEx and WriteFileGather . Win32 also supports asynchronous I O in this case the call returns when the completion callback is posted at this point the written data is consistent and or durable. On any write like call the shim layer can send it only to an objectlogger if any only to an NTFS NT file system if any or to both. On a read like call it correctly reads from the place s which have the latest consistent data read like calls are ReadFile ReadFileEx and ReadFileScatter .

As an example a shim layer for PostMark is described. PostMark uses the POSIX calls which is translated by the POSIX win32 layer into buffered write back writes. It is left to the application to call fsync when it needs durability. It is assumed that PostMark has an underlying NTFS file system and this is augmented with a logger for performance but the data will reside long term in the NTFS. The simplest design is send writes to both NTFS in buffered write back mode and to a logger. The write is completed when the NTFS write is completed i.e. the NTFS has the correct consistent state but not necessarily durable so reads go to NTFS. When fsync is called however the operation can be completed as soon as all outstanding logger writes for that file are completed.

Using such a shim layer writes will be fast because they are buffered by NTFS i.e. they do not wait for a disk I O. Fsync will be fast or faster than just using NTFS because the logger has good write performance. Fsync will be correct because the durable state can be recovered by combining the NTFS state with the logger state. Reads will be fast because they will mostly hit in the NTFS buffer cache and will be correct because the NTFS buffer cache will have the correct consistent state. Either static linking or Detours for binaries may be used to introduce the shim layers into a legacy application.

There are many different applications and scenarios where write off loading may be useful and a number of different examples are described below. For example database workloads may benefit from this approach. A database server usually has three main I O activities writes to the log aka WAL reads to fetch data pages from disk and writes to flush dirty data pages to disk. Access to the data pages might contain a substantial amount of random I O. When the database system becomes I O bound write off loading executes the writes fast by utilizing any available disks that are idle in the data centre. The mechanism does not hardcode the number of disks to be used opportunistically or the ID of such disks. The number of disks depends on the load presented to the system.

Another example is scientific applications. These usually have phases with extensive I O activity related to checkpointing their data to persistent storage after a period of CPU computation. For such applications write off loading employs many idle disk resources to execute this I O intensive phase quickly.

Email server applications like Microsoft Exchange and Hotmail could benefit from write off loading. During peak load for example in the morning Microsoft exchange could make use of idle servers that belong to other services to temporarily store incoming email.

Gaming applications such as Xbox Live could benefit from write off loading by employing other services idle servers during periods of high load. For example a data centre may host Xbox Live and other services like email or web hosting applications. The Xbox service might transparently and opportunistically utilize the other services resources to handle its peak loads.

I O is an issue with video servers. In some video applications a human editor downloads video clips and creates new video clips. Then the editor clicks save and waits for a while until the video clip is written to disk. Write offloading may help here since there is a large amount of data being written.

Computing based device comprises one or more processors which may be microprocessors controllers or any other suitable type of processors for processing executable instructions to control the operation of the device in order to perform write off loading. As described above a device may act as a manager and or a logger. Platform software comprising an operating system or any other suitable platform software may be provided at the computing based device to enable application software to be executed on the device. The application software may comprise a manager and or a logger which may also be referred to as a logger module .

The computer executable instructions may be provided using any computer readable media such as memory . The memory is of any suitable type such as random access memory RAM a disk storage device of any type such as a magnetic or optical storage device a hard disk drive or a CD DVD or other disc drive. Flash memory EPROM or EEPROM may also be used.

The computing based device further comprises a communication interface which enables requests to be received from other devices e.g. applications or other clients and or managers and communications to be made with other loggers. The computing based device further comprises or is connected to e.g. via a network a storage element . The storage element may for example comprise multiple storage devices .

The computing based device may further comprise one or more inputs for example which are of any suitable type for receiving media content Internet Protocol IP input etc and one or more outputs such as an audio and or video output to a display system integral with or in communication with the computing based device.

Although the present examples are described and illustrated herein as being implemented in a data center the system described is provided as an example and not a limitation. As those skilled in the art will appreciate the present examples are suitable for application in a variety of different types of storage systems including for business and non business applications e.g. in the home .

The above description refers to write off loading occurring when a storage device is powered down or overloaded. These are just two examples of situations where the storage device is unavailable and there may be other reasons that the storage device is unavailable which may trigger write off loading.

The term computer is used herein to refer to any device with processing capability such that it can execute instructions. Those skilled in the art will realize that such processing capabilities are incorporated into many different devices and therefore the term computer includes PCs servers mobile telephones personal digital assistants and many other devices.

The methods described herein may be performed by software in machine readable form on a tangible storage medium. The software can be suitable for execution on a parallel processor or a serial processor such that the method steps may be carried out in any suitable order or simultaneously.

This acknowledges that software can be a valuable separately tradable commodity. It is intended to encompass software which runs on or controls dumb or standard hardware to carry out the desired functions. It is also intended to encompass software which describes or defines the configuration of hardware such as HDL hardware description language software as is used for designing silicon chips or for configuring universal programmable chips to carry out desired functions.

Those skilled in the art will realize that storage devices utilized to store program instructions can be distributed across a network. For example a remote computer may store an example of the process described as software. A local or terminal computer may access the remote computer and download a part or all of the software to run the program. Alternatively the local computer may download pieces of the software as needed or execute some software instructions at the local terminal and some at the remote computer or computer network . Those skilled in the art will also realize that by utilizing conventional techniques known to those skilled in the art that all or a portion of the software instructions may be carried out by a dedicated circuit such as a DSP programmable logic array or the like.

Any range or device value given herein may be extended or altered without losing the effect sought as will be apparent to the skilled person.

It will be understood that the benefits and advantages described above may relate to one embodiment or may relate to several embodiments. The embodiments are not limited to those that solve any or all of the stated problems or those that have any or all of the stated benefits and advantages. It will further be understood that reference to an item refers to one or more of those items.

The steps of the methods described herein may be carried out in any suitable order or simultaneously where appropriate. Additionally individual blocks may be deleted from any of the methods without departing from the spirit and scope of the subject matter described herein. Aspects of any of the examples described above may be combined with aspects of any of the other examples described to form further examples without losing the effect sought.

The term comprising is used herein to mean including the method blocks or elements identified but that such blocks or elements do not comprise an exclusive list and a method or apparatus may contain additional blocks or elements.

It will be understood that the above description of a preferred embodiment is given by way of example only and that various modifications may be made by those skilled in the art. The above specification examples and data provide a complete description of the structure and use of exemplary embodiments of the invention. Although various embodiments of the invention have been described above with a certain degree of particularity or with reference to one or more individual embodiments those skilled in the art could make numerous alterations to the disclosed embodiments without departing from the spirit or scope of this invention.

