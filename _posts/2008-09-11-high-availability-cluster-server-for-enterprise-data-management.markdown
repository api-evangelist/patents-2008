---

title: High availability cluster server for enterprise data management
abstract: A high availability, scalable cluster server system for enterprise data management. The server is a cluster of two or more nodes. Each node runs one or more virtual servers. A virtual server consists of network resources and resources for enterprise data management. Failover is based on moving virtual servers from a failed node to a healthy node. The invention provides for network failover within the same node. Enterprise data management consists of data discovery, data categorization and applying enterprise policies on categorized data. One of the problems of data management is the diversity of policies and in some cases their conflicting requirements. According to one aspect of the invention, enterprise policies are broking into policy entities. Policy entities represent the simplest policy unit. Some of the policy entities are shared between the diversified enterprise policies. Identifying a data management policy with the highest priority and applying the policy entity that belongs to it resolve conflict in policy requirements.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07890626&OS=07890626&RS=07890626
owner: 
number: 07890626
owner_city: 
owner_country: 
publication_date: 20080911
---
The invention relates to a high availability and scalable cluster server for enterprise data management.

High availability cluster server is a server that continues to function even after a failure of system hardware or software. The usual way of providing high availability is to duplicate system components. If some component becomes unavailable another can be used instead. Scalable cluster server is a server that is able to increase performance and workload by adding more hardware or software resources.

A cluster is a group of servers and other resources that act like a single system and enable high availability and load balancing. The servers are referred to as nodes. Nodes typically consist of one or more instruction processors generally referred to as CPUs disks memory power supplies motherboards expansion slots and interface boards. In a master slave design one node of the system cluster is called the primary or master server and the others are called the secondary or slave servers. The primary and secondary nodes have similar hardware run the same operating system have the same patches installed support the same binary executables and have identical or very similar configuration. The primary and secondary nodes are connected to the same networks through which they communicate with each other and with devices connected to the network. Both kinds of nodes run compatible versions of software. Some high availability systems support virtual network interfaces where more than one IP Internet Protocol address is assigned to the same physical port. Services are associated with the virtual network interface and computing resources needed to perform the services. The virtual IP address does not connect a client with a particular physical server it connects the client with a particular service running on a particular physical server.

In some cases disks are directly attached to a node. This is referred to as Direct Attached Storage DAS . In other cases Storage Area Network SAN which is a high speed special purpose network or sub network interconnects different storage devices with the nodes.

Enterprise data management is the development and execution of policies practices and procedures that properly manage enterprise data. Some aspects of data management are security and risk management legal discovery Storage Resource Management SRM information lifecycle management ILM and content based archiving. In addition some companies have their own internal management policies. Another aspect of data management is data auditing. Data auditing allows enterprises to validate compliance with federal regulations and insures that data management objectives are being met. One of the challenges for data management products is to provide solutions to different aspects of data management in one platform. This is due to the various and sometimes conflicting requirements of different aspects of enterprise data management.

Security and risk management is concerned with discovery of sensitive data like Social Security number SSN credit card number banking information tax information and anything that can be used to facilitate identity theft. It is also concerned with enforcement of corporate policies for protection of confidential data protection of data that contains customer phrases and numeric patterns and compliance with federal regulations. Some of the federal regulations that are related to security and risk management are FRCP Federal Rules of Civil Procedure NPI Non Public Information regulation PII Personally Identifiable Information regulation FERPA Family Educational Rights and Privacy Act GLBA Gramm Leach Bliley Act HIPAA Health Insurance Portability and Accountability Act SOX Sarbanes Oxley Act and the U.S. Securities and Exchange Commission s SEC s Regulation.

Legal discovery refers to any process in which data is sought located secured and searched with the intent of using it as evidence in a civil or criminal legal case. Legal discovery when applied to electronic data is called e Discovery. E Discovery involves electronic evidence protection legal hold and chain of custody. A legal hold sometimes referred to as litigation hold is a process which an organization uses to preserve all forms of relevant information when litigation is reasonably anticipated. Chain of custody logs and documents how the data was gathered analyzed and preserved. Federal regulations details what how and when electronic data must be produced including production as part of the pre trial process.

SRM is the process of optimizing the efficiency and speed with which the available storage space is utilized. Among many things it involves removing duplicate contraband and undesirable files data retention and deletion moving or removing files based on metadata and content and automated file migration through integrated storage tiering. Tiered storage is the assignment of different categories of data to different types of storage media in order to reduce total storage cost

ILM is a sustainable storage strategy that balances the cost of storing and managing information with its business value. It provides a practical methodology for aligning storage costs with business priorities. ILM has similar objectives to SRM and is considered an extension of SRM.

Content based archiving identifies files to be archived based on business value or regulatory requirements. It enables policy driven and automated file migration to archives based on file metadata and content enables intelligent file retrieval and it locks and isolates files in a permanent archive when that is required by federal regulations.

There are inter dependencies between some components of enterprise data management. In some cases the components share the same requirements. In other cases they have conflicting requirements. For instance SRM policy may decide that a category of data should be deleted as it has not been accessed for a long time. At the same time legal discovery may decide to impose litigation hold on the same data because of its relevance to litigation.

One of the challenges to data management is the exponential growth of the enterprise data. Now many companies have more than 1 petabyte of stored data. Another challenge is diversity of devices where data exists. Data exists in file servers email servers portals web sites databases archives and in other applications. Another problem domain is the proliferation of data into the fringes of the enterprise network namely laptops and remote users.

Data management is based on data classification sometimes referred to as categorization. Categorization of data is based on metadata or full text search. Categorization rules specify how data is classified into different groups. For instance documents categorization could be based on who owns them their size and their content. Metadata consist of information that characterizes data. Sometimes it is referred to as data about data . Data categorization methods based on metadata group data according to information extracted from its metadata. A few examples of such information are the time a document was last accessed its owner its type and its size. There are many methods for accessing and extracting information from metadata. Some methods utilize file system utilities. File system utilities can only extract file system attributes. Document parsers sometimes called filters are used for extracting metadata from documents such as Microsoft Word Microsoft PowerPoint and PDF files. The three top commercial parsers being used now are Stellent KeyView and iFitler. Some software developers write their own parsers or use open source parsers such as Apache POI. Classification based on full text utilizes search technology. Full text search is used to identify documents that contain specific terms phrases or combination of both. The result of the search is used to categorize data. One of the widely used open source search engines is Lucene.

In addition to categorization data management involves formulation of policies to be applied to classified data. For example policies could be encrypting sensitive data auditing data retaining data archiving data deleting data modifying data access and modifying read and write permissions. Different policies could be grouped to form a top level enterprise wide policy or a departmental policy.

Part of data management is creation of data management reports. Reports could cover storage utilization data integrity duplicated data and results of executing compliance and internal policies. In some implementations classification rules policies results of data analysis and report definition files are stored in a database. Report definition files contain instructions that describe report layout for the reports generated from the database.

Enterprise data is stored in different devices dispersed across the network. To perform data analysis one can manually enter the location of the data which is daunting when many devices are connected to the network. Alternatively one can use methods or a combination of methods for automated data discovery. Some methods utilize Internet Protocol IP port scanners like nmap and Advanced IP Scanner. IP scanners determine services devices available in the network and the type of data source. Then a crawler is used to retrieve data. The type of crawler used depends on data source accessible through a network port. If the data source is a network file server file system crawlers are used to recursively scan the directory structure of the file system and retrieve files. If the data source is a database then database crawlers that utilize JDBC or LDBC are used. JDBC stands for Java Database Connectivity. LDBC stands for Liberty Database Connectivity. If the data source is an email server crawlers that use Messaging Application Programming Interface MAPI or connectors are used. MAPI is a Microsoft interface for components that connect to Microsoft exchange. An example of an email connector is Novell s Connector for Microsoft Exchange. Some enterprise data is stored in corporate web portals corporate intranets . Web crawlers are used to automatically traverse a corporate intranet by retrieving a document and recursively retrieving all documents that are referenced. Web crawlers are also called spiders or web robots. Crawlers for archives depend on the type of the archive. For instance crawlers for Network Data Management Protocol NDMP compatible archives utilize NDMP based crawlers. NDMP is an open standard protocol for enterprise wide backup of heterogeneous network attached storage. Some software vendors provide interface modules to help in writing connectors to their archives.

In general in one aspect the invention provides high availability and scalable cluster server systems for enterprise data management. Each system is a cluster of two or more autonomous servers called nodes or physical servers and computer program products and methods for operating such systems. One of the nodes in the cluster server is the master and the rest are the slaves. Each node runs one or more virtual servers. The nodes are connected to the same networks through which they communicate with each other and with devices where enterprise data is stored. The invention provides systems programs and methods for discovery of data stored in devices connected to the network and management of that data by creating categorization rules and management policies. Discovery uses network scanners and crawlers to identify the data source and retrieve data. Data discovered includes but not limited to network file systems email servers databases intranets and data archives.

Categorization is based on metadata extraction and analysis and full text search. Metadata analysis methods include but not limited to file system utilities and document parsers. Each node stores extracted metadata and result of metadata analysis in a metadata repository accessible to other nodes. Search engines are used to perform full text search. Each node stores search index in a repository accessible to other nodes. Categorization rules are applied to enterprise data to categorize it into different categories. Management policies are applied to each category. The invention provides methods to develop company s internal policies and policies to ensure regularity compliance.

Results of data management and report definitions are stored in a database in the master node. Data stored in the database also includes but not limited to categorization rules policies and report definition files.

Methods in the cluster create policy entities that represent the simplest policy unit. Each unit defines an action or a set of actions to be performed on data. Policy entities are combined to form policy groups. Policy entities could exclusively be owned by a policy group or they could be shared between different groups. A policy group could represent an internal policy or a regulatory compliance policy. Policy entities shared between two or more policy groups may have different or conflicting actions. The invention provides methods for resolving the conflicts. When there is a conflict higher priority is given to a shared policy entity that belongs to a regulatory compliance group. When two policy entities that are part of two different policy groups have different periods of enforcement the entity with the longer period has a higher priority.

Each node runs one or more virtual servers. A virtual server consists of network resources and resources for data management. Each virtual server owns data discovery and data management tasks and one or more virtual IP addresses. Virtual servers are managed as separate entities and they share physical resources on a physical server.

When one of the nodes fails its virtual servers are transparently transferred to one or more other nodes. This is achieved by providing a seamless connectivity between the nodes and the data sources. The connectivity between the data sources and the nodes is based on virtual IP technology. Nodes communicate with each other through a heartbeat network to determine the health of each other. The heartbeat can operate over an IP network infrastructure to determine the availability of nodes. If one of the nodes or one of its components fails so that a virtual server running in that node goes down failover occurs.

In a failover the virtual sever of the failed node is migrated to another node. Under certain failure conditions the seamless connectivity and redundant hardware and software components allow access to the data sources to be maintained without invocation of the failover process. Load balancing can be provided by distributing virtual servers from a failed node to multiple different nodes.

Node failover is disruptive and after failover the number of healthy nodes within a cluster decreases which may impact performance of the cluster. For this reason node failover is made the last resort by providing network failover and application recovery within a node. Node failover takes place only after all network ports in a node had failed or when an application had failed and attempts to restart it are not successful.

In general in another aspect the invention provides systems programs and methods where the loading of nodes is monitored so as to identify nodes that are less loaded than others. This information is used to perform load balancing. After failover virtual servers are migrated to nodes that are less loaded in preference to nodes that are more heavily loaded. Because nodes can support multiple virtual servers load balancing can be performed in this way during normal operation as well even in the absence of a failure.

Within the same node load balancing across the network ports could be achieved by redistributing virtual interfaces across different healthy network ports. Software monitors the load on different ports belonging to a node. If a port is handling much more network traffic than other ports some of its virtual interfaces are moved to ports which are less loaded.

In general in another aspect the invention provides systems programs and methods where to minimize occurrence of failover each node has multiple network ports. If one of the ports fails services are moved to one of the surviving ports. This allows multiple network port failures to occur without invocation of node failover so that node failover occurs only when there is no surviving port.

In general in another aspect to minimize occurrence of failover the invention provides methods to restart an application if it failed. For this to happen an application has to register with a software watchdog. The software watchdog monitors a registered application at regular intervals. If the application died the watchdog restarts it.

Implementations of the invention can realize one or more of the following advantages. Failover used only as a last resort and consequently the disruption caused by failover to the accessibility of services is limited. Total system performance is improved through load balancing.

In general in another aspect the invention provides systems programs and methods that allow replicating metadata and search index repositories belonging to one node into another node or nodes. This provides redundant repositories. In case the node that owns the repository failed its virtual servers are migrated to where the repositories are replicated. The master makes sure that the workload of the nodes that contain the replicated repositories is not high to ensure that they can handled extra work when virtual servers are migrated to them during failover. This alleviates the problem of re creating the metadata and the search index of the failed node after failover.

In general in another aspect the invention provides systems programs and methods that allow binding of applications to specific CPUs referred to as processors in multiprocessor nodes. Binding an application to a processor ensures that the application is run on the same processor which increases performance. The first step is to find the number processors in a node. The number of applications running on different processors depends on the number of processors. Processors that do not participate in binding are available for other applications.

In general in another aspect the invention provides systems programs and methods for expanding the storage in nodes by adding more drives. This enables the system to allocate more storage for the search index and the metadata repositories and the local database where the results of data management are stored. As a consequence the cluster can manage larger amount of enterprise data.

The details of one or more implementations of the invention are set forth in the accompanying drawings and the description below. Other features and advantages of the invention will become apparent from the description the drawings and the claims.

The server has a cluster of nodes Node A labeled Node B labeled . . . Node J labeled . Each node has direct attached storage DAS . Each node has one or more virtual servers. Node A has N virtual servers labeled VS VS . . . VSN. Node B has M virtual servers labeled VS VS . . . VSM. Node J has P virtual servers labeled VSJ VSJ VSJP. Virtual server VS has a virtual IP address V virtual server VS has a virtual IP address V . . . virtual server VSN has a virtual IP address VN. Virtual server VS has a virtual IP address V virtual server VS has a virtual IP address V . . . virtual server VSM has a virtual IP address VM. Virtual server VSJ has a virtual IP address VJ virtual server VSJ has a virtual IP address VJ . . . virtual server VSJP has a virtual IP address VJP. Each node has K links to network . Node is attached to the network by links . . . K. Node is attached to the network by links . . . K. Node is attached to the network by links . . . K. Node is the master node and contains database . Each node contains its own metadata repository and its own search index repository. Node contains metadata repository and search index repository . Node contains metadata repository and search index repository . Node contains metadata repository and search index repository . Network connects the nodes to data sources. The data sources are network file systems databases email servers Intranets and data archives .

Each node runs one or more virtual servers. A virtual server consists of network resources and resources for data management. Virtual servers own virtual IP addresses and methods for data discovery and data management. They are managed as separate entities and they share physical resources on a physical server. Since all physical servers have similar physical resources a virtual server can manage data on any physical server provided that their connectivity to data resources is sustained. Using virtual servers facilitates migrating data discovery and data management tasks during failover. The virtual IP address that is part of a virtual server does not connect a physical server to a data source. It connects services running on a particular physical server to a data source. Another advantage of virtual servers is that more than one virtual server running in a node provides parallel processing which increases performance.

Each node can have multiple network ports also called physical IP ports. Node has physical ports P P . . . PK. Node has physical ports P. P . . . PK. Node has physical ports P P PK. Link connects P to network link connects P to network and link K connects PK to network . Link connects P to network link connects P to the network and link K connects PK to network . Link connects P to network link connects P to the network and link K connects PK to network . If a physical port fails the node will recover as long as there are healthy physical ports on the node. Failure of the last port on a node causes failover to a healthy node. As an example assume that in node virtual address V is attached to the physical network port P. When P fails V is moved to P. If P fails V is moved to the next port. This is repeated until V resides on the last port VK. If PK fails node failover takes place and V moves to a physical port on another node.

A node in the cluster can act as either a master or a slave. There is only one master the rest of the nodes are slaves. The master contains a database . All nodes can access the database . The master coordinates the activities of the slaves and assigns data management jobs to them. Data management jobs include but not limited to data discovery crawling data categorization and executing management policies. The slaves report the resources they control to the master. The slave servers are only aware of their own resources workload and state. Slaves measure their CPU usage memory usage disk usage the load on their network and load on each of their virtual servers and provide this information to the master. The master maintains state information for the entire cluster. Information about workload of nodes is used during load balancing. Load balancing is attained by moving virtual servers to the less loaded nodes.

Nodes access source data and perform data categorization based on metadata and full text search. Extracted metadata is stored in metadata repositories and . Search index is stored in repositories and . Nodes apply policies to each category of data and the results are stored in database .

In the cluster a heartbeat protocol that operates over the network connection between nodes determines the availability of each server. A node knows about the failure of another node when it stops receiving heartbeat messages. Heartbeat over the network connection is based on the master probing the slaves using pings and or RPC Remote Procedure Call calls. Pings can be implemented on either private or public networks.

If the master does not receive a response from a slave within a specified time then the slave cannot be reached or there may be other problems with the slave. The specified time is adjustable and usually it is set to 3 seconds. If the master stops sending pings or RPC the slaves assume that the master could not be reached or that there may be other problems with the master. If the master failed the priority at which one of the slaves becomes a master depends on when it joined the cluster. The cluster keeps a record of the order in which the slaves joined the cluster. The first slave that joined the cluster is the one that will become the second master if the first master failed. If the second master failed the second slave that joined the cluster will become the third master. The process is repeated whenever a master failed.

Data discovery is an automated process to discover all devices in a network or could be restricted to specific devices like network file servers mail servers intranets database servers or archives. Discovery could also be based on a host name host IP address range of IP addresses or a subnet. A subnet is a portion of a network that shares a common address component by providing the IP address with the same prefix. After data is discovered a user can select whether to categorize all discovered data or a sub set of the data. The system allows the user to enter the location of the data source manually instead of using auto discovery.

In another embodiment of the invention instead of DAS storage SAN storage is used. SAN allows sharing of storage and simplifies storage management. SAN also tends to enable more effective disaster recovery processes as it spans a distant location containing a secondary storage array. Using SAN nodes could be located in different buildings separated by a few miles. If disaster in a building caused nodes in that building to fail virtual servers are migrated from the failed nodes to healthy nodes in other buildings.

In another embodiment of the invention some nodes within the same cluster use DAS storage and some use SAN storage.

According to another implementation of the invention a distributed database is used to replace the database connected to the master. In this implementation more than two nodes in a cluster have homogeneous databases. Databases in different nodes form a single distributed database in which an application in any node can access or modify the databases in different nodes in the cluster.

Accordingly the reader will see that the present invention provides a high availability and scalable system that manages different aspects of enterprise data in one platform. It automatically scans and finds data in all devices connected to a network including laptops and devices at remote sites. It manages different types of data stored in file systems databases emails servers intranets and archives. The system adopts a unified approach based on policy entities to build diversified enterprise and regulatory compliance policies.

While the above description contain several specifics these should not be construed as limitations on the scope of the invention but rather as examples of the some of the preferred embodiments thereof. Many other variations are possible. For example other embodiments of the system could have external storage for metadata and search index repositories instead of using storage attached to the nodes. The distributed database for storing results of data management could be based on heterogeneous databases instead of homogeneous databases.

The invention has been described in terms of particular embodiments. Other embodiments are within the scope of the following claims. For example steps of the invention can be performed to a different order and still achieve desirable results.

