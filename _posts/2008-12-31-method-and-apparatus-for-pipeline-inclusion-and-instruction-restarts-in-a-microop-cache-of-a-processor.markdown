---

title: Method and apparatus for pipeline inclusion and instruction restarts in a micro-op cache of a processor
abstract: Methods and apparatus for instruction restarts and inclusion in processor micro-op caches are disclosed. Embodiments of micro-op caches have way storage fields to record the instruction-cache ways storing corresponding macroinstructions. Instruction-cache in-use indications associated with the instruction-cache lines storing the instructions are updated upon micro-op cache hits. In-use indications can be located using the recorded instruction-cache ways in micro-op cache lines. Victim-cache deallocation micro-ops are enqueued in a micro-op queue after micro-op cache miss synchronizations, responsive to evictions from the instruction-cache into a victim-cache. Inclusion logic also locates and evicts micro-op cache lines corresponding to the recorded instruction-cache ways, responsive to evictions from the instruction-cache.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08127085&OS=08127085&RS=08127085
owner: Intel Corporation
number: 08127085
owner_city: Santa Clara
owner_country: US
publication_date: 20081231
---
This application is a continuation in part of U.S. application Ser. No. 12 326 885 filed Dec. 2 2008 entitled METHOD AND APPARATUS FOR PIPELINE INCLUSION AND INSTRUCTION RESTARTS IN A MICRO OP CACHE OF A PROCESSOR the content of which is hereby incorporated by reference.

This disclosure relates generally to the field of microprocessors. In particular the disclosure relates to novel techniques for handling operations in a micro op cache for a processor.

In some modern processors instructions have variable lengths and form a complex instruction set capable of complex tasks that may involve multiple simpler tasks thus the term complex instruction set computers CISC . Micro operations also known as a micro ops or uops are simpler internal instructions that can be produced by decoding the more complex instructions also referred to as macroinstructions.

Execution pipelines are often used. Instructions are provided to the front end of the pipeline by various arrays buffers and caches and micro ops are prepared and queued for execution. Such front end arrays that contain instruction lines may also includes self modifying code SMC bits to detect which instruction lines may have been overwritten by self modifying or cross modifying code.

For high performance processors that use these variable length instructions the decoding process can be costly in terms of circuitry power consumption and time. Some processors try to alleviate one or more of these costs through saving or caching the decoded micro ops to reuse them if execution of their corresponding macroinstructions is repeated.

One technique is called a micro op cache or microcode cache where micro ops are stored in cache lines or ways and tags associated with instruction pointers are used to lookup the micro ops directly rather than decoding the corresponding macro instruction each time. Some such micro op caches are discussed for example in U.S. Pat. No. 6 950 903. Micro op caches may be less costly and more power efficient than fetching and decoding macro instructions.

It will be appreciated that for correct functionality considerations such as processor inclusion any instruction line for which micro ops have been delivered into the execution pipeline may later need to be re delivered in an unmodified state. Therefore deallocation or eviction of the line in particular from an instruction cache cannot take place until all instructions from that line are no longer being processed in the execution pipeline.

One technique to protect such instruction lines from being evicted is to employ a victim cache to hold evicted lines until it can be determined that no instructions from that line are being processed in the execution pipeline. One way to make such a determination is to insert a special micro op into the pipeline when an entry is allocated into the victim cache. As long as new instruction fetches from the victim cache are not permitted then when that micro op retires in sequential order any instructions from the evicted line that were in front of the special micro op will have been retired as well and the corresponding entry can be deallocated from the victim cache.

Since the steps involved in decoding the variable length macroinstructions may be avoided micro op caches can potentially increase processor performance but such consideration as processor inclusion self modifying or cross modifying code instruction restarts and synchronization between sequences of decoded macroinstructions and cached micro ops can be complicated and may degrade those performance increases. To date the range of effective techniques for employing saved or cached micro ops to improve processing of instructions and reduce costs in terms of circuit complexity and power consumption while also handling the complicated issues of inclusion and instruction restarts in a processor have not been fully explored.

Methods and apparatus for instruction restarts and inclusion in processor micro op caches are disclosed herein. In some embodiments micro op cache lines have way storage fields to record the instruction cache ways that store corresponding macroinstructions. Instruction cache in use indications associated with instruction cache lines containing the instructions are updated upon micro op cache hits to indicate that the associated instructions are in use. In use indications can be located for updates in the instruction cache using the recorded instruction cache ways in micro op cache lines. Victim cache deallocation micro ops are enqueued in a micro op queue after micro op cache miss synchronizations which are forced responsive to line evictions from the instruction cache into the victim cache. Inclusion logic also locates and evicts the micro op cache lines corresponding to the recorded instruction cache ways responsive to evictions from the instruction cache. Such mechanisms can aid in handling self cross modifying code SMC XMC as well as front end restarts.

In some alternative embodiments a synchronization flag may be set in the last micro op retrieved from the micro op cache data array responsive to a subsequent micro op cache miss to indicate where micro ops from the macroinstruction translation engine are to be merged with micro ops retrieved from the micro op cache data array to supply the micro op queue. Similarly a synchronization flag may also be set in the last micro op from the macroinstruction translation engine in response to a subsequent micro op cache hit.

Some embodiments for inclusion of TLB translation look aside buffer entries have micro op cache inclusion fields which are set responsive to accessing the TLB entry. Inclusion logic may then flush the micro op cache or portions of the micro op cache and clear corresponding inclusion fields responsive to a replacement or invalidation of a TLB entry whenever its associated inclusion field had been set.

Thus such techniques may be useful to improve processing of instructions and reduce costs at least in terms of power consumption while also handling the complicated issues of inclusion and instruction restarts in a processor. As further described below implementation details of some techniques may also help to reduce costs in terms of circuit complexity.

These and other embodiments of the present invention may be realized in accordance with the following teachings and it should be evident that various modifications and changes may be made in the following teachings without departing from the broader spirit and scope of the invention. The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense and the invention measured only in terms of the claims and their equivalents.

Micro ops illustrate a set of micro ops generated by decoding macroinstructions found within the window of instruction bytes . An example of micro ops includes micro op corresponding to instruction and so with respect to the window of instruction bytes having a byte position offset of 3 micro op corresponding to instruction and so having a byte position offset of 7 two micro ops corresponding to instruction and so having a byte position offset of 9 micro op corresponding to instruction and so having a byte position offset of 15 micro op corresponding to instruction and so having a byte position offset of 19 micro op corresponding to instruction and so having a byte position offset of 22 three micro ops corresponding to instruction and so having a byte position offset of 25 and micro op corresponding to instruction and so having a byte position offset of 30.

In one embodiment of micro op cache structure micro op cache lines are organized by sets and ways. For example micro op cache line has way micro op cache line has way and micro op cache line has way N. For one embodiment a set of lines or ways may include multiple lines or ways all lines or ways in the set having the same tag. For an alternative embodiment the set of micro op cache lines may correspond to a multi bit field in a virtual and or physical memory address of a location storing the corresponding macroinstruction e.g. instruction of instruction bytes . Therefore it will be appreciated that multiple sets e.g. having the same tag values could occupy a larger fixed sized set e.g. having the same multi bit field in an memory address of ways in the micro op cache and that these design decisions may be made by those skilled in the art without departing from the principles of the present invention.

Multiple ways may be picked to store sequential micro ops for a set corresponding to a window of instruction bytes in such a manner as to facilitate a simple sorting algorithm for sequentially sorting ways within a set to maintain the sequential order of micro ops . Alternatively since the ways in a set correspond to a single window of instruction bytes the ways of a set may be sorted according to their smallest offsets.

It will be appreciated that since no instruction in the window of instruction bytes can be smaller than one byte permitting a set to have up to four ways each way containing up to eight micro ops would be sufficient to hold most combinations of decoded macroinstructions. In some embodiments practical observations may lead to reductions or to increases in the number of ways per set and or the number of micro ops per way. The number of ways per set may be reduced to three or to two in some embodiments for example to make sorting ways simpler and faster. The number of micro ops per way may be reduced to six or to four in some embodiments for example to conserve unused micro op storage within ways and or to more closely match the number of micro ops supported by a micro op queue. Results of such decisions based on practical observations may or may not have a significant impact on micro op cache hit or miss rates.

In some embodiments of micro op cache structure tag matching for instruction pointers may be decoupled from accessing micro op cache lines in a data array through a queue to store micro op cache line access tuples. A micro op cache line access tuple for example for cache line may include a way a valid bit a tag and offsets . To access micro ops corresponding to the macroinstruction for example a tag match for the instruction pointer produces a set of micro op cache line access tuples including the line access tuples for cache lines and since they have matching tags and . The line access tuple in the set having an offset of corresponding to the byte position of the instruction pointer for macroinstruction with respect to the window of instruction bytes may be used to access the three micro ops starting in the second storage position of way in the corresponding set of cache lines in a micro op data array. In some embodiments of micro op cache structure tags may be linear i.e. virtual to avoid the need for a translation lookaside buffer TLB in the micro op cache.

Thus the micro op cache structure uses sets of micro op cache line access tuples for example the set corresponding to tags and having offsets and respectively to coordinate instructions and with corresponding micro ops and in a micro op cache.

In some embodiments of micro op cache structure instruction cache ways and indicating which ways store instructions corresponding to micro ops in their respective micro op cache lines are recorded for micro op cache lines and . In order to facilitate inclusion self modifying or cross modifying code and instruction restarts. It will be appreciated that in response to a micro op cache hit e.g. at micro op cache lines and or instruction cache in use indications associated with instructions and or may be updated to indicate that their corresponding micro ops and or have been queued in an execution pipeline. Locating such instruction cache in use indications associated with instructions and or may be facilitated at least in part through use of the recorded instruction cache ways and for micro op cache lines and .

In some embodiments of micro op cache structure where it may be possible for an instruction e.g. instruction to extend beyond the boundary of instruction bytes and or beyond the boundary of an instruction cache line such conditions may be recorded for micro op cache lines in cross line indication fields X. For example cross line indication may be set for micro op cache lines to indicate that instruction extended beyond the boundary of instruction bytes and or beyond the boundary of an instruction cache line. In such a case instruction cache way indicating which way in the next instruction cache set stores the portion of instruction that extended beyond the boundary may be recorded for micro op cache line . Thus an instruction cache in use indication associated with instruction may be updated to indicate that its corresponding micro op has been queued in an execution pipeline.

On the other hand since instruction corresponding to micro op does not extend beyond the boundary of instruction bytes and or the boundary of an instruction cache line cross line indication may be left unset for micro op cache lines .

When there is a micro op cache miss macroinstruction translation engine MITE may be used for fetching and decoding macroinstructions to supply the micro op queue . Instruction pointers from tag match unit and or BPU may be stored in miss queue and supplied by Next IP Mux to MITE for fetching macroinstructions following a micro op cache miss. In some embodiments of apparatus portions of MITE may be disabled to conserve power when there are no IP entries in the miss queue and or Next IP Mux . Likewise micro op cache data array may be disabled to conserve power when there are no entries in match queue .

Of course since the additional number of steps for fetching and decoding macroinstructions in the MITE to supply the micro op queue take longer than simply accessing micro op cache data array with cache line access tuples from match queue some period of latency could be expected as a consequence of a micro op cache miss. But it will be appreciated that when the number of micro op cache line access tuples stored in match queue at the time of a micro op cache miss is sufficiently large to oversupply line access tuples to micro op cache data array then the latency for switching to MITE may be covered by a continued supply of micro ops from the cache data array to micro op queue . For example in some embodiments of micro op cache up to three micro op cache line access tuples may be stored into match queue per cycle wherein up to two micro op cache line access tuples may be used to concurrently access cache lines in micro op cache data array . Thus match queue would tend to fill up and to oversupply line access tuples to micro op cache data array thereby helping to cover the latency of a micro op cache miss.

In some embodiments of apparatus portions of MITE may be disabled to conserve power when there are no IP entries in the miss queue and or Next IP Mux . Likewise micro op cache data array may be disabled to conserve power when there are no entries in match queue . It will be appreciated that since micro op cache hits and cache misses are mutually exclusive either the macroinstruction translation engine or the micro op cache data array may be disabled for a substantial portion of time to conserve power. Thus such techniques may be useful to improve processing of instructions and reduce costs at least in terms of power consumption in a processor. Of course in some embodiments of apparatus there will be overlap for example to cover the latency of MITE or because MITE may not access as large of a window of instruction bytes as tag match but since MITE may in general consume more power than micro op cache data array a reasonable micro op cache hit rate would provide that MITE could be disabled to conserve power for a significant portion of time when there are no entries in miss queue and or Next IP Mux .

In some alternative embodiments of apparatus a synchronization flag may be set in the last micro op retrieved from the micro op cache data array resulting from one or more micro op cache hits in response to the first subsequent micro op cache miss to indicate to Mux where micro ops from MITE are to be merged with micro ops retrieved from the micro op cache data array to be supplied to micro op queue . Similarly a synchronization flag may also be set in the last micro op from MITE resulting from one or more micro op cache misses in response to the first subsequent micro op cache hit. Therefore the task of merging and synchronizing micro op flows from micro op cache and from MITE in Mux may be substantially simplified and it will be appreciated that details of such techniques may help to reduce costs in terms of circuit complexity.

MITE includes TLB instruction cache fetch and decode for fetching and decoding macroinstructions to supply the nucro op queue . Some embodiments of micro op cache include micro op cache lines having way storage fields to record an instruction cache way storing the corresponding macroinstructions. Embodiments of MITE also include victim cache and instruction cache in use indications associated with the corresponding macroinstructions the in use indications to be updated responsive to micro op cache hits at corresponding micro op cache lines and locatable through use of the instruction cache way recorded in the way storage fields for the first micro op cache lines of micro op cache .

In some embodiments of MITE micro op cache inclusion fields associated with TLB entries are set responsive to accessing the TLB entries. Inclusion logic is coupled with micro op cache inclusion fields and TLB . Inclusion logic includes flush logic to flush a micro op cache portion and clear inclusion fields for the micro op cache portion responsive to replacement or invalidation of TLB entries whenever the micro op cache portion s inclusion fields associated with the replaced or invalidated TLB entries were set. In some embodiments it may be cost effective to flush the entire micro op cache.

Inclusion logic is also coupled with micro op cache in use indications and instruction cache fetch to locate and evict micro op cache lines corresponding to recorded instruction cache ways in their way storage fields responsive to the eviction of the instruction cache ways from the instruction cache. Inclusion logic includes set match logic to identify a set of micro op cache lines including micro op cache lines corresponding to the evicted instruction cache ways and way match logic to match the evicted instruction cache ways with recorded instruction cache ways in the way storage fields of micro op cache lines in the set identified by match logic . Way match logic is operatively coupled with flush logic to identify which micro op cache lines to evict or flush from among the set. It will be appreciated that the identified set may include more than one micro op cache set.

It will be also appreciated that such techniques may be useful to improve processing of instructions and reduce costs in terms of power consumption and circuit complexity in a processor while also handling the complicated issues of inclusion and instruction restarts in the front end pipeline of a processor.

In processing block a tag match for an instruction pointer is performed. If a micro op cache miss is detected in processing block then processing proceeds to processing block . Otherwise processing continues in processing block where it is determined if the micro op cache hit is the first hit following one or more misses in which case in processing block where a synchronization flag is set in the last micro op from the macroinstruction translation engine preceding the micro op cache hit. If the micro op cache hit is not the first hit following one or more misses the processing skips directly to processing block where a set of micro op cache line access tuples having matching tags is retrieved. Then in processing block the set of micro op cache line access tuples is stored or enqueued in a match queue. In processing block micro op cache line access tuples from the match queue are used to concurrently access multiple cache lines in a micro op cache data array to retrieve micro ops which may then be supplied to a micro op queue. Processing then repeats in processing block .

Now if a micro op cache miss is detected in processing block then processing would proceed to processing block where it is determined if the micro op cache miss is the first miss following one or more hits in which case in processing block a synchronization flag is set in the last micro op retrieved from the micro op cache data array preceding the miss. If the micro op cache miss is not the first miss following one or more hits the processing skips directly to processing block where multiple instruction pointers are stored or enqueued in a miss queue responsive to the micro op cache miss. Now starting in processing block an instruction cache is accessed using instruction pointers from the miss queue to fetch or retrieve instructions. In processing block instructions are decoded to produce micro ops to supply a micro op queue. The micro ops are also provided in processing block to fill one or more ways in a set in the micro op cache. In processing block if processing instruction pointers enqueued in processing block in the miss queue is not finished processing repeats starting in processing block . Otherwise processing returns to processing block .

It will be appreciated that while certain processing blocks of process and other processes herein disclosed may be performed by different individual units and or in different pipelines of apparatus many of the processing blocks of process and other processes herein disclosed may in practice be performed concurrently.

For some embodiments a set of lines or ways in a micro op cache may include all the lines or ways in the set that have the same tag values. Alternatively a set of micro op cache lines may correspond to a multi bit field in a virtual and or physical memory address of a location storing the corresponding macroinstruction. For example in a virtually indexed micro op cache i.e. using virtual addresses where an instruction cache is physically indexed i.e. using physical addresses multi bit fields from the matching least significant portion of address bits of both addresses may be used to identify fixed sized sets of ways in both caches. For some embodiments the instruction cache waycorresponds to a way allocated according to a least recently used process in the instruction cache to hold an instruction cache line including the instruction. For some alternative embodiments the instruction cache waymay also correspond to a multi bit field from the matching least significant portion of virtual and or physical memory address bits.

In processing block instruction cache waystoring the instruction corresponding to the one or more micro ops is recorded for the micro op cache line at setin way. In processing block a determination is made whether the instant instruction is a cross line instruction in the instruction cache. If so in processing block another instruction portion is stored in the instruction cache line at setin some way in processing block the next instruction cache wayis also recorded for the micro op cache line at setin way and in processing block a cross line indication is set for the micro op cache line at setin wayto indicate that the instant instruction crosses an instruction cache line boundary. Otherwise processing skips to processing block .

In processing block processing waits for a micro op cache hit for the micro op cache line at setin way. Whenever such a micro op cache hit occurs processing proceeds in processing block where an in use indication associated with an instruction cache line at setin waycontaining the corresponding macroinstruction is updated. Next if it is determined in processing block that a cross line indication is set for the micro op cache line at setin waythen in processing block an in use indication is updated for the instruction cache line at setin way. Thus upon a micro op cache hit the in use indication s associated with the corresponding macroinstruction may be located in the instruction cache by using the instruction cache way s recorded for the micro op cache line that hits. Processing then proceeds to process .

In processing block a set of micro op cache lines including micro op cache line s corresponding with the evicted line of instructions is identified. It will be appreciated that the identified set in processing block may include more than one micro op cache set. For example if an instruction cache line can hold twice as many instructions as the corresponding micro ops which would fit into one micro op cache set then the set as identified in processing block may include two micro op cache sets. Furthermore as described above with regard to it may be possible for an instruction e.g. instruction to extend beyond the boundary of an instruction cache line. Therefore micro ops in a third micro op cache set may also correspond to an instruction partially stored in a particular instruction cache line. Accordingly a next instruction cache way e.g. way may be recorded for a micro op cache line in the micro op cache set n 1 and the same instruction cache way may be recorded for micro op cache lines in the micro op cache sets n and n 1.

In processing block the instruction cache way s recorded for the micro op cache lines are matched with instruction cache waystoring the evicted instructions to identify a micro op cache line at setin wayto evict or flush from among the set of micro op cache lines identified in processing block . In some embodiments multiple micro op cache ways in a particular micro op cache set or in multiple micro op cache sets may all record the matching instruction cache way. In some alternative embodiments waymay represent a multi bit e.g. 2 bit field in a virtual and or physical memory address of a location storing the corresponding macroinstruction may be matched to multiple micro op cache ways in a particular micro op cache set or in multiple micro op cache sets to identify a micro op cache line at setin wayto evict or flush. In processing block the micro op cache line at setin wayis evicted or flushed from the micro op cache responsive to the eviction from the instruction cache. Now in some embodiments another subsequent eviction from the instruction cache may require more quickly evicting or flushing micro op cache lines from the micro op cache in which case the entire set identified in processing block may be immediately flushed.

In some embodiments it may be possible for a micro op to be supplied to the micro op queue prior to the flush of its micro op cache line from the micro op cache in processing block but following an insertion of a victim cache deallocation micro op. For this reason in processing block a micro op synchronization is performed e.g. a micro op cache miss synchronization as in processing blocks and may be forced . Then in processing block a victim cache deallocation micro op is enqueued following the micro op cache synchronization to indicate upon sequential retirement of victim cache deallocation micro op that none of the instructions of the evicted instruction cache line are still in use. Thus process ensures correct handling of pipeline inclusion and instruction restarts in a micro op cache. Processing then proceeds to process .

In some processing systems coherency between the TLB translation look aside buffer in cache and page tables in memory must be maintained. For example if the operating system changes an attribute of a page it may need to invalidate the page from the TLB. When such an invalidation occurs all micro op cache lines associated with the invalidated page may need to be flushed. Since such events are rather rare one option may be to simply flush the entire micro op cache.

In the case of front end restarts a restarted instruction my generate a fatal error if it encounters a TLB miss. Therefore when a particular TLB entry is replaced due to a TLB miss any micro op cache lines containing micro ops that were generated from instructions corresponding to the replaced TLB entry may need to be flushed.

In processing block the corresponding micro op cache portion is flushed to maintain inclusion of the replaced or invalidated TLB entry. Then in processing block inclusion indications for that micro op cache portion are cleared. It will be appreciated that if TLB entry replacements or invalidations are rare events in some embodiments it may be cost effective to simply flush the entire micro op cache when such rare events occur. Next in processing block a TLB micro op is enqueued to indicate upon sequential retirement of the TLB micro op that none of the micro ops associated with the replaced or invalidated TLB entry are still in use. In some embodiments of process different types of TLB micro ops may be used for different TLB replacements e.g. in TLB misses or invalidations e.g. in TLB page invalidations . In processing block processing waits for the TLB micro op to be retired and upon retirement of the TLB micro op processing proceeds to processing block where the front end pipeline state is cleared. It will be appreciated that clearing the front end state may be performed in a number of ways including but not limited to flushing the front end state upon retirement of the TLB micro op or stopping the front end and or clearing the MITE . Then in processing block it is determined whether the replacement or invalidation of the TLB entry resulted from a TLB miss in which case the instruction from the TLB miss is refetched in processing block after clearing the front end pipeline state. In the case where invalidation of the TLB entry did not result from a TLB miss e.g. in a TLB page invalidation the cleared i.e. flushed or stopped front end may be restarted or continue processing at the next linear instruction pointer following the TLB page invalidation. In any case processing then returns to processing block .

Some embodiments may take the state of the inclusion indicator into account when determining which TLB entry to replace i.e. preferring to replace an entry whose indicator is not set. In some implementations more than 1 inclusion indicator per TLB entry may be provided so that smaller parts of the micro op cache would be flushed. For example with 4 bits per TLB entry the micro op cache is partitioned by set into 4 parts. In this way if a TLB entry is replaced only the sets in the micro op cache for which the corresponding bits are flushed.

While certain processing blocks of process and other processes herein disclosed may be performed by different individual units and or in different pipelines of apparatus many of the processing blocks of process and other processes herein disclosed may in practice be performed concurrently. It will be appreciated from the above description that processes and as described herein may provide effective techniques for employing cached micro ops to improve processing performance of instructions and reduce costs in terms of circuit complexity and power consumption while also handling the complicated issues of inclusion and instruction restarts in the front end pipeline of a processor.

As described above embodiments may implement mechanisms and protocols for dealing with self cross modifying code SMC XMC and front end FE restarts. Generally SMC XMC may be described as a store instruction being executed on a given core that may write to a memory location that contains instructions. In a pipelined machine an instruction supplied into the pipeline from the modified location that follows the store might be read from an instruction cache. As this instruction is stale i.e. an old instruction it should be flushed so that the modified instruction bytes can then be re read from memory. Another complexity involves a front end FE restart in which an instruction that gets to retirement needs to be re fetched and re executed i.e. the original instruction bytes . Since the memory location containing the instruction could have been changed externally the original instruction bytes of any instruction that may be re started are maintained in the local core until the instruction retires.

When a store operation hits in an instruction cache line a snoop field of that line is marked with a snoop hit i.e. a snoop indicator is set responsive to a snoop request. When the store operation that received this snoop hit retires a retirement unit e.g. a reorder buffer ROB nukes the pipeline and invalidates all instruction cache entries for which the snoop indicator is set. This saves the need to tag match the instruction cache again on the store address to see which instruction cache entry needs to be invalidated.

By maintaining inclusion of the pipeline in the instruction cache any instruction present in the pipeline is kept in the instruction cache at least until it retires. This enables stores to snoop the instruction cache and to detect SMC. It also enables re fetch of the original instruction bytes in case of a front end restart.

Note that an instruction cache line that has a snoop hit is only invalidated when it is purged so an instruction cache entry whose snoop indicator is set can still hit in the instruction cache. Embodiments may prevent micro ops originating from such an instruction cache line from being written into the micro op cache e.g. via inclusion logic of since when the actual purge occurs the micro op cache is not snooped and this would break the micro op cache instruction cache inclusion.

Further while accessing the micro op cache embodiments may cause the corresponding instruction cache in use bits to be updated. Otherwise micro ops from the micro op cache could be supplied into the pipeline while the instruction cache entry that maps the micro ops without its in use bit set could be victimized without being written to the IVC breaking inclusion. A cross 64 byte line instruction is mapped to a single 32B chunk in the micro op cache but affects the in use bit of two instruction cache lines. The originating instruction cache set of a given micro op cache line can be calculated from the micro op cache set and possibly a few bits from the micro op cache tag. As described each micro op cache line may store the instruction cache originating way. When a micro op cache line hits the instruction cache originating way information is used for accessing and setting the in use bit of the originating instruction cache line. When supplying the micro ops of a cross 64B line instruction from the micro op cache the in use bit of the next sequential instruction cache line needs also to be set. To this end micro op cache lines also hold the way of the next sequential originating instruction cache line.

In some implementations an IVC deallocation op can be sent while streaming micro ops from the micro op cache which can affect inclusion. Consider the following scenario the micro op cache misses on instruction A and following that the instruction cache misses on instruction A as well. The micro op cache proceeds to address B and hits B hit pointers are written into the micro op cache match queue. The instruction cache replaces the line containing B e.g. according to a least recently used LRU policy with the line containing A after it is obtained from memory B moves to the IVC and an IVC deallocation micro op is inserted into the pipeline after before instruction A. B is evicted from the micro op cache to maintain inclusion . The IVC deallocation micro op retires. Instruction A gets to the instruction cache goes through the MITE and then moves on to the backend. The merge mux flips to the micro op cache and B moves out of the micro op cache match queue into the backend. Now if a snoop to instruction B occurs the snoop misses even though the instruction is still in the machine. Or if for some reason instruction B needs to FE restart the original instruction bytes cannot be retrieved from the instruction cache.

To handle this situation embodiments may force a micro op cache miss and delay insertion of the IVC deallocation micro op until just after synchronization in the BPUQ. This ensures that this micro op passes the merge mux after all the current content in both of the micro op cache and the MITE pipeline. Notice that since the IVC deallocation is for performance it need not be issued in order.

Referring now to shown is a flow diagram of a method in accordance with one embodiment of the present invention. As shown in method may be used to maintain inclusivity between a micro op cache and an instruction cache particularly for corner conditions such as where a line of a micro op cache is evicted while one or more instructions of that line are still within the pipeline. As shown in method may begin by determining whether a first instruction i.e. instruction A is present in a micro op cache or instruction cache diamond . If so the instruction may be provided from that line and preferably from the micro op cache if it is present therein block . In this case method may conclude. Otherwise as seen in a miss occurs and miss processing may be performed to fetch the instruction from a memory hierarchy block . While this miss processing occurs assume that a next instruction in program order i.e. instruction B is searched for and hits in the micro op cache block . Thus a match queue entry for this instruction may be accessed.

Assume for purposes of a corner case that the line containing instruction B is selected for replacement in the instruction cache for example if it is the LRU line block . Accordingly this instruction may be placed into an instruction victim cache block . Then a current instruction e.g. instruction C may be forced to miss in the micro op cache. This causes injection of a synchronization into the micro op cache match queue and places C in the miss queue. Note that typically at this time a deallocate micro op may be generated for instruction B and sent such that when this micro op is retired instruction B may be deallocated from the victim cache. However because here the instruction is still present within the pipeline this deallocation micro op is delayed until after C.

Thus as shown in at a later time instruction A may be obtained from the memory hierarchy and inserted into the instruction cache. Furthermore the instruction may then proceed along to the processor pipeline e.g. the back end of the machine block . Now because instruction A has been sent for execution instruction B following it in program order may also be provided to the back end. In this case instruction B may now be provided to the pipeline via the micro op cache block . Then because C was forced to miss it passes the merge mux only after the synchronization from the micro op cache passes the merge mux and flips it to the MITE direction. In turn since the synchronization was put in the match queue after B if the synchronization has passed the merge mux so has instruction B. Thereafter the IVC deallocation micro op may be issued for this instruction by passing it through the merge mux block . That is because both instructions A and B will pass through the pipeline ahead of this deallocate micro op correct operation is guaranteed as when the deallocation micro op retires B is no longer in the pipeline. In this example the complete order in which these instructions pass through the merge mux is A B synchronization C and the deallocation micro op. Note further that this deallocation micro op need not be issued in order.

Further by using embodiments of the present invention eviction of cache lines may occur to maintain inclusivity between micro op cache and instruction cache. For example when a given instruction cache line is evicted e.g. for capacity reasons due to a snoop hit or otherwise one or more corresponding lines in the micro op cache may also be evicted to maintain inclusivity. Furthermore to address inclusivity issues when a snoop indicator is set for a line in the instruction cache at least one corresponding line in the micro op cache may be invalidated. Furthermore when one or more micro ops are sent from a line of the micro op cache to the pipeline an in use indicator for the corresponding cache line or lines in the instruction cache may be set to identify that these lines include instructions that are present in the pipeline.

While the present invention has been described with respect to a limited number of embodiments those skilled in the art will appreciate numerous modifications and variations therefrom. It is intended that the appended claims cover all such modifications and variations as fall within the true spirit and scope of this present invention.

