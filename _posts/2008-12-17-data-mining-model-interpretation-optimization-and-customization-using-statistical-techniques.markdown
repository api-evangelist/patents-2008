---

title: Data mining model interpretation, optimization, and customization using statistical techniques
abstract: A system, method, and program product for interpreting, optimizing, and customizing data mining models through the use of statistical techniques that utilize diagnostic measures and statistical significance testing. A data processing system is disclosed that includes a data mining system for mining data from a data warehouse in accordance with a data model, wherein the data model defines how data groups can be partitioned; and a data group analysis system that calculates a set of diagnostic measures and performs statistical significance tests for a defined data group.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08095498&OS=08095498&RS=08095498
owner: International Business Machines Corporation
number: 08095498
owner_city: Armonk
owner_country: US
publication_date: 20081217
---
This disclosure relates to a system and method for interpreting optimizing and customizing data mining models through the use of statistical techniques that utilize diagnostic measures and statistical significance testing.

Classical statistical methods and data mining have often been viewed as two competing methodologies for drawing conclusions from data. Classical statistics relies on stochastic models and hypothesis testing whereas data mining makes no assumptions and is data driven. Statistical methods offer established diagnostics in well defined contexts. Data mining is particularly well suited for exploratory data analysis and model creation using massive high dimensional datasets that may not be compiled using rigorous statistical experimental design such as data residing in an information warehouse. Data mining is heuristic and algorithmically driven designed to extract useful patterns automatically. However because patterns are found automatically data mining may find patterns that appear interesting but do not represent significantly different behaviors or outcomes.

Three problems exist in this context. First making a high cost or high value decision may require a rigorous interpretation of data mining results. In addition the data mining model may need to be customized and or optimized further for a specific application. Second the trend toward embedding data mining in business applications requires some mechanism to help a business analyst interpret the results correctly. Third as data mining becomes more operational the growing need for automating the data mining process requires that the embedded interpretation be more robust and reliable.

Currently no solution exists that addresses all three of these issues. Commercially available data mining workbenches and other data mining solutions may include certain statistical functionality for ad hoc operations such as variable selection data exploration data preparation etc. But these implementations of statistical functionality do not constitute a specific methodology to combine statistical techniques with data mining for addressing the issues described above. Accordingly a need exists for more robust analysis of data mining results.

The present invention relates to a system method and program product for evaluating data mining operations using diagnostic measures and statistical significance tests. In one embodiment there is a data processing system comprising a data mining system for mining data from a data warehouse in accordance with a data model wherein the data model defines how data groups can be partitioned and a data group analysis system that calculates a set of diagnostic measures and performs statistical significance tests for a defined data group.

In a second embodiment there is a computer readable medium computer having a computer program product stored thereon for analyzing data mining results comprising program code for mining data from a data warehouse in accordance with a data model wherein the data model defines how data groups can be partitioned and program code that calculates a set of diagnostic measures and performs statistical significance tests for a defined data group.

In a third embodiment there is a method for analyzing data mining results comprising mining data from a data warehouse in accordance with a data model wherein the data model defines how data groups can be partitioned and calculating a set of diagnostic measures and performing statistical significance tests for a defined data group.

In a fourth embodiment there is a method of processing data models comprising mining data from a data warehouse in accordance with a plurality of data models wherein each data model defines how data groups can be partitioned calculating one or more diagnostic measures for each of the data models and selecting a data model based on the statistical significance of the calculated diagnostic measure or measures.

Advantages of using this invention include improving the quality robustness and reliability of data mining results making interpretation of data mining results easier for analysts with or without a high degree of statistical expertise optimizing and customizing the data mining model in an application specific manner and improving and facilitating the automated interpretation of data mining results in an operational process.

The illustrative aspects of the present invention are designed to solve the problems herein described and other problems not discussed.

The drawings are merely schematic representations not intended to portray specific parameters of the invention. The drawings are intended to depict only typical embodiments of the invention and therefore should not be considered as limiting the scope of the invention. In the drawings like numbering represents like elements.

Referring to a computer system having a data processing system in accordance with an illustrative embodiment of the invention is shown. Data processing system may comprise any data processing application that analyses and processes data from a data warehouse . Data warehouse may contain any type of data collected or utilized for any purpose and may be distributed and or reside remotely. Data processing system may for instance be implemented as a program product stored in memory which when executed by processor causes computer system to behave in a specific manner.

Central to the data processing system is a data mining system that mines information from the data warehouse based on a data model in response to one or more inputs e.g. user inputs made via graphical user interface GUI system inputs from another computing device or application etc. Data mining may for instance include data cleansing transformations group discovery hypothesis generation predictive modeling and scoring interpretation etc. Applications of data mining are numerous and include for example gaining business or scientific insights providing key metrics of business performance forecasting etc.

Data processing system may also include one or more business processes that utilize the results of a data mining operation to drive automated business operations. For instance based on the results of a data mining operation a network may be dynamically configured to most optimally route network traffic a catalogue mailing list may be created health insurance cost saving measures may be proposed etc. Although shown integrated into data processing system it is understood that business processes may be implemented as separate systems that receive output from data processing system .

As noted above it is often difficult to gauge the efficacy of results from a data mining system without expert analysis to evaluate the results. In order to address this issue data processing system includes a data group analysis system for statically analyzing data groups defined in the data model using a statistical significance processing system . More particularly data group analysis system evaluates data at the group level. It can e.g. evaluate a data group against the entire set of data or against one or more other data groups.

For instance data group analysis system may examine the number of people living in a particular zip code earning more that a predetermined amount under the age of 30 who purchased a new car in the last year versus the same group over 30 who purchased a new car in the last year. In addition to simply providing raw data for these data groups statistical significance processing system utilizes one or more statistical processes to measure the statistical significance of the data group. For instance data group analysis system could utilize data mining system to report raw data indicating that 435 people under the age of the 30 did not purchase a new car and 13 did compared with 736 people over the age of the 30 who did not purchase a new car and 55 who did. Statistical significance processing system statistically analyzes the raw data to provide a measure regarding how significant the data is in these groups. By understanding how statistically significant the data is as defined in the model by such a group partition better business decisions can be made e.g. should an advertising campaign be launched to primarily target to those over the age of 30. Are there better demographic group partitions that should be considered etc.

Statistical significance processing system can thus be utilized to measure the efficacy of one or more different group partitions within a data model . Illustrative statistical tests and diagnostic measures that may be utilized are described in more detail below but may include for instance chi square test two sample t test Mann Whitney U test Kolmogorov Smirnov KS test two sample KS test etc. and diagnostic measures such as relative risk or odds ratio to measure the size of an effect and associated confidence intervals. The particular type of measure s and tests to be utilized will generally be determined based on the data type and how the data group is being defined and analyzed.

Once the measures for one or more data groups are obtained the results can be utilized in any fashion e.g. provided as output for further analysis utilized to select or identify the best indicators of business performance ranked fed into an automated business process that can then decide how to use the data mining results etc.

In addition a model customization system may utilize the results to identify which partitions within a model are the most statistically significant. Groups that are not statistically significant may for instance be pruned from the model for future operations. More significant groups may be prioritized higher in the model . In general model may be customized according to a set of business rules in an application specific manner.

In addition to analyzing the model at the group level a model analysis system may be provided to analyze the statistical significance of the model itself using statistical significance processing system . In this manner different data models may be compared to determine which model yields the most overall statistically significant results. Model optimization system may optimize the model. Illustrative measures include e.g. sensitivity specificity odds ratio accuracy positive predicted value negative predicted value etc. Further details are provided below.

One of the major tasks in data mining is clustering or partitioning of the data into data groups i.e. subsets or clusters with members of each group sharing similar characteristics. Interpreting the results of the clustering task may be difficult in terms of assessing how different a given cluster is from all other entities e.g. records or from another cluster. A classification model also may construct groups although classification algorithms operate differently and for different purposes than do clustering algorithms. Data groups can also be constructed using other algorithms e.g. regression models e.g. rank and divide records into groups by deciles quartiles etc. or query e.g. construct groups meeting a set of specific criteria or attributes .

However the data groups are constructed one of the challenges in interpreting the data mining results is to determine whether the partitioning is statistically meaningful or relevant to the problem being solved. This first example illustrates how the present solution is used to rigorously interpret the results for the cases of categorical and numeric variables in the context of clustering and classification.

As part of their customer retention program a credit card company may be interested in finding out the profile of customers who are more likely to attrite voluntarily close their accounts . Attrition is represented by a categorical variable with possible values of Y yes or N no . As part of their exploratory study the company uses clustering algorithms to segment customers according to a number of customer credit characteristics such as credit score delinquency late fees paid whether the customer exceeded credit limit etc. They discover nine clusters. Once they have identified the clusters the next step is to determine whether customers in a specific cluster are more or less likely to attrite than customers who do not belong to the cluster.

In Table 1 the customer attrition counts are provided for the cluster of interest and all others . We can see that the cluster attrition rates for these two groups are about 120 616 19 and 569 2615 22 respectively. In the example shown in Table 2 a chi square test 

Table 3 shows chi square critical values at different significance levels in which it can be seen that of value of 1.542 is not significant even at a 5 significance level. This lack of significance indicates that the cluster may not represent an important segment.

In addition a relative risk which is simply a ratio of attrition rates in the two groups a a b c c d is calculated as 120 616 569 2615 to yield a result of 0.89. Diagnostic measures such as relative risk need to be reported along with a confidence interval to indicate the uncertainty associated with the calculation. The 95 confidence interval can be computed using the expressions below 

The 95 Confidence Interval CI can be computed using the above expressions as 0.72 1.15 . We now have all the statistical information we need to make a decision regarding this cluster. For this cluster there is a modest 10 decrease in attrition risk 10 . The 95 CI lower and upper limits are relatively tight indicating that we may not need additional information i.e. more records to gain confidence. The modest relative risk close to 1 with a small chi square value indicates that customer attrition is not significantly different from that observed in the rest of the population. Evaluations can be performed for all nine clusters to determine whether any of them are significantly different from the rest of the population or from any other cluster e.g. pairwise comparison in terms of customer attrition.

Based on insights gained from their customer segmentation e.g. clustering analysis the credit card company has built a data mining model to help them better identify customers at high risk of attrition. In this example a decision tree classification model shown in provides a partitioning of the dataset in a nested fashion. Given a partitioning at a decision node questions about the significance of differences in two groups at that node can be addressed in a manner similar to that described for clustering.

In this example the credit card company wants to determine whether the tenure of customers how long their accounts have been open has a significant effect on attrition. In two groups at a member tenure node have a current balance 

The conditional relative risk can be computed using the expression provided above. Unlike in the clustering example here the relative risk is conditional since the definition of the groups depends on how the data were partitioned in building the decision tree . In this case relative risk is computed as RR 45 101 130 456 1.56. The lower and upper bounds of the 95 confidence interval are computed using the expressions given above for CI and are 1.005 2.43 . Thus we have a 56 increase in risk for members with tenure 21.5. The chi square value indicates that the difference in risk is significant. The lower bound of the 95 Cl is close to 1 however which suggests that we need more records to ascertain the increase in risk with greater degree of confidence.

The two examples discussed so far are for categorical variables. But a decision table such as Table 2 or Table 5 can be constructed for a numeric variable such as age height income etc. For example in a retail promotion scenario suppose that we have identified two interesting customer segments through clustering and would like to know whether the observed difference in the annual spending is significantly different for the two groups. We can perform a two sample t test to determine if the difference in spending between the two segments is statistically significant. If the distribution in the two populations cannot be assumed to be normal then a Mann Whitney U test a distribution free non parametric test can be performed.

If one would like to determine whether the observed distribution of a numeric variable e.g. income follows a reference distribution or whether two distributions are similar the Kolmogorov Smirnov KS test or two sample KS test respectively can be performed. Thus appropriate statistical significance tests are associated with data group discovery or definition by data mining algorithms in various contexts.

Predictive modeling is another major task in data mining. In this second example the test results for a decision tree model of credit card attrition are presented as a confusion matrix in table 5. The confusion matrix shows how well the model performed in predicting the actual attrition status of the customers in the test dataset.

Performing a chi square test results in a computed chi square value of 75 the results are highly significant even at a 99.9 confidence level. In addition to statistical significance of the results diagnostic measures such as sensitivity specificity accuracy odds ratio positive and negative predictive values can be computed. These diagnostic measures are of great practical value and in fields such as medical diagnostics reporting them along with the test results is mandatory.

The computed diagnostic measures shown in table 7 give insight and a rigorous basis to evaluate the model. While sensitivity of 0.69 may be good for addressing credit card attrition the same may not be acceptable for a HIV test. Similarly the specificity of 0.77 may not be adequate where a decision to perform high risk invasive surgery or provide treatment with grave side effects is involved. Also a low positive predictive value would suggest that model is prone to generate lots of false positives and therefore may not be acceptable if testing positive leads to costly decisions. Diagnostic measures can help make context specific decisions in a rigorous manner.

Frequently in the exploratory analysis stage several classification models are considered and the best one is chosen. Model analysis system provides a rigorous way to compare the models . McNemar s test closely related to the chi square test can be used to determine whether the observed difference in the misclassification rates of the two models is statistically significant.

Evaluation of a single classifier model and the comparison of models discussed so far have been for a single decision threshold. For a classifier model that provides a probability score for each record to belong to a class or group a threshold choice can be made across the range of scores each resulting in a different set of diagnostic measures. For example a plot of sensitivity vs. 1 specificity across the entire range of decision threshold is called the Receiver Operating Characteristics ROC curve. The ROC curve can be used to provide a global assessment of the model or compare two models. The area under the ROC curve AUC can be used as a diagnostic to assess the global quality of the classifier model. For example a value of 0.5 for the AUC implies that the classifier is no better than a random classifier a value of 1.0 makes it a perfect classifier and any value in between is evaluated for statistical significance. Using the ROC curve an optimal operating point can be determined and diagnostic measures can be generated. In the exploratory stage when several competing classifier models are considered ROC curves can be generated for the models. A determination can then be made as to whether the models are statistically significantly different from each other. The ROC approach is an indispensable tool in the context of automatic data mining.

As noted above the described embodiments can be implemented in a variety of ways. For instance they could be incorporated as additional functionality in existing software. This software could be a data mining workbench or a database centric tool suite providing expert analysts with more rigorous capabilities for data mining model interpretation.

Further the features could be implemented in an end user application or tool designed to address a particular business problem. For example an application for retail promotions could perform data mining clustering to discover customer segments. A promotion may have the objective of targeting a small number of customer groups with varying attributes using different marketing channels to appeal selectively to the groups. If the data mining yields a large number of clusters then applying statistical significance testing to pairs of clusters as described in Example 1 provides a rigorous basis for combining clusters to generate the necessary number of target groups with as much homogeneity within each group as possible.

Another implementation could be customizing a risk model based on a decision tree to identify subgroups with a certain risk profile using the diagnostic measures and statistical significance testing. Once identified a solution tailored specifically for that subgroup can be developed. For example for an auto insurance risk model this subgroup could be a group of owners who have expensive cars but seldom drive them. A specific insurance plan with a lower more competitive premium could be developed to target this group. As another example for a cancer risk model this subgroup could be coal miners who work seasonally in the coal mining industry. Essentially the current invention allows model customization to target specific risk groups when the overall risk model is high dimensional i.e. the model includes a large number of explanatory variables.

In a further example the features could be implemented through embedding a series of analysis steps applied to data mining results in an automated business process. For example an automated process could use database resident data mining capabilities to create or refresh a data mining model as new data flow into a database. Statistical significance tests could then be automatically applied to the mining results to perform automatic interpretation of the results and take an automatic action based on that interpretation.

One trend in business process optimization is the development of ready to go application specific information appliances in which software and hardware components are optimized to work together and provide complete and scalable vertical solutions. An example of such an appliance would be an analytical server that integrates business intelligence solutions optionally for a vertical application such as healthcare fraud analytics with a balanced warehouse solution. One or more of the described embodiments could integrated into such a solution to provide a sound statistical basis for making business decisions.

Below pseudocodes or high level natural language descriptions of illustrative algorithms for testing for differences between groups identified by data mining algorithms i.e. data analysis system and for choosing the best predictive model from a set of competing models model analysis system are shown. These pseudocodes describe how heuristics can be implemented for typical use cases discussed previously.

Referring again to it is understood that computer system may be implemented as any type of computing infrastructure. Computer system generally includes a processor input output I O memory and bus . The processor may comprise a single processing unit or be distributed across one or more processing units in one or more locations e.g. on a client and server. Memory may comprise any known type of data storage including magnetic media optical media random access memory RAM read only memory ROM a data cache a data object etc. Moreover memory may reside at a single physical location comprising one or more types of data storage or be distributed across a plurality of physical systems in various forms. Data warehouse may likewise reside at a single physical location comprising one or more types of data storage or be distributed across a plurality of physical systems in various forms.

I O may comprise any system for exchanging information to from an external resource. External devices resources may comprise any known type of external device including a monitor display speakers storage another computer system a hand held device keyboard mouse voice recognition system speech output system printer facsimile pager etc. Bus provides a communication link between each of the components in the computer system and likewise may comprise any known type of transmission link including electrical optical wireless etc. Although not shown additional components such as cache memory communication systems system software etc. may be incorporated into computer system .

Access to computer system may be provided over a network such as the Internet a local area network LAN a wide area network WAN a virtual private network VPN etc. Communication could occur via a direct hardwired connection e.g. serial port or via an addressable connection that may utilize any combination of wireline and or wireless transmission methods. Moreover conventional network connectivity such as Token Ring Ethernet WiFi or other conventional communications standards could be used. Still yet connectivity could be provided by conventional TCP IP sockets based protocol. In this instance an Internet service provider could be used to establish interconnectivity. Further as indicated above communication could occur in a client server or server server environment.

It should be appreciated that the teachings of the present invention could be offered as a business method on a subscription or fee basis. For example a computer system comprising a data processing system could be created maintained and or deployed by a service provider that offers the functions described herein for customers. That is a service provider could offer to deploy or provide the ability to statistically analyze data mining results as described above.

It is understood that in addition to being implemented as a system and method the features may be provided as a program product stored on a computer readable medium which when executed enables computer system to provide a data processing system . To this extent the computer readable medium may include program code which implements the processes and systems described herein. It is understood that the term computer readable medium comprises one or more of any type of physical embodiment of the program code. In particular the computer readable medium can comprise program code embodied on one or more portable storage articles of manufacture e.g. a compact disc a magnetic disk a tape etc. on one or more data storage portions of a computing device such as memory and or a storage system and or as a data signal traveling over a network e.g. during a wired wireless electronic distribution of the program product .

As used herein it is understood that the terms program code and computer program code are synonymous and mean any expression in any language code or notation of a set of instructions that cause a computing device having an information processing capability to perform a particular function either directly or after any combination of the following a conversion to another language code or notation b reproduction in a different material form and or c decompression. To this extent program code can be embodied as one or more types of program products such as an application software program component software a library of functions an operating system a basic I O system driver for a particular computing and or I O device and the like. Further it is understood that terms such as component and system are synonymous as used herein and represent any combination of hardware and or software capable of performing some function s .

The block diagrams in the figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that the functions noted in the blocks may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams can be implemented by special purpose hardware based systems which perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

Although specific embodiments have been illustrated and described herein those of ordinary skill in the art appreciate that any arrangement which is calculated to achieve the same purpose may be substituted for the specific embodiments shown and that the invention has other applications in other environments. This application is intended to cover any adaptations or variations of the present invention. The following claims are in no way intended to limit the scope of the invention to the specific embodiments described herein.

