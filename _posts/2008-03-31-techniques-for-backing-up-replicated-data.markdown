---

title: Techniques for backing up replicated data
abstract: Techniques for backing up replicated data are disclosed. In one particular exemplary embodiment, the techniques may be realized as a method for backing up replicated data comprising identifying replicated data on a primary node and a secondary node, and determining whether a backup is capable of being performed on the secondary node. In the event a backup is capable of being performed on the secondary node, the method may create a backup copy of the identified replicated data on the secondary node, and in the event a backup is not capable of being performed on the secondary node, the method may create a backup copy of the identified replicated data on the primary node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09355117&OS=09355117&RS=09355117
owner: Veritas US IP Holdings LLC
number: 09355117
owner_city: Mountain View
owner_country: US
publication_date: 20080331
---
The present disclosure relates generally to data backup and more particularly to techniques for backing up replicated data.

High availability computer clusters or other computer cluster configurations may use data storage shared among one or more nodes. Such configurations may imply that backups are taken from an active node which may maintain control over shared storage. Such configurations may also provide redundancy among nodes but the shared storage may create a single point of failure. To avoid this single point of failure some computer cluster configurations may utilize unshared storage which may be replicated between nodes. Other systems may utilize replicated data for redundancy availability and for other purposes. Such data replication may eliminate shared storage as a single point of failure but may introduce a level of complexity. Backups may be run on an active and or a primary node as they are in shared storage configurations. However it may be desirable to run a backup on a passive and or a secondary node which may be possible due to unshared replicated storage.

Coordination of a backup between multiple nodes utilizing replicated data may present significant challenges. Storage on a node may not be replicated data it may be local data utilized by the node. An application administrator a network administrator or another user may determine whether storage such as a particular volume used for application data should be replicated. Specifying that storage should be replicated may result in the storage being replicated from the primary and or active node to one or more secondary and or passive nodes. Performing a full backup of storage on a device or a file system may result in backing up storage which is not replicated together with replicated storage. Performing a backup on a secondary and or passive node may result in incomplete or inconsistent backups if a node or a component of a replication process is not replicating data properly. Performing a backup on a secondary and or passive node may fail if the secondary and or passive node is not available to perform a backup. Performing a backup on a primary and or active node and a secondary and or passive node may create multiple backups and a user may be required to choose between the backups. Performing multiple backups also may fail to alleviate any burden off of a primary and or active node and may require additional space and resources.

In view of the foregoing it may be understood that there are significant problems and shortcomings associated with current technologies utilized for backing up replicated data.

Techniques for backing up replicated data are disclosed. In one particular exemplary embodiment the techniques may be realized as a method for backing up replicated data comprising identifying replicated data on a primary node and a secondary node and determining whether a backup is capable of being performed on the secondary node. In the event a backup is capable of being performed on the secondary node the method may create a backup copy of the identified replicated data on the secondary node and in the event a backup is not capable of being performed on the secondary node the method may create a backup copy of the identified replicated data on the primary node.

In accordance with other aspects of this particular exemplary embodiment the techniques may be realized as an article of manufacture for backing up replicated data the article of manufacture comprising at least one processor readable carrier and instructions carried on the at least one carrier. The instructions may be configured to be readable from the at least one carrier by at least one processor and thereby cause the at least one processor to operate so as to identify replicated data on a primary node and a secondary node determine whether a backup is capable of being performed on the secondary node. In the event a backup is capable of being performed on the secondary node the processor may create a backup copy of the identified replicated data on the secondary node and in the event a backup is not capable of being performed on the secondary node the processor may create a backup copy of the identified replicated data on the primary node.

In accordance with further aspects of this particular exemplary embodiment the techniques may be realized as a system for backing up unshared storage in a clustered environment comprising one or more processors communicatively coupled to a server. The server may be configured to identify replicated data on a primary node and a secondary node and determine whether a backup is capable of being performed on the secondary node. In the event a backup is capable of being performed on the secondary node the server may create a backup copy of the identified replicated data on the secondary node and in the event a backup is not capable of being performed on the secondary node the server may create a backup copy of the identified replicated data on the primary node.

The present disclosure will now be described in more detail with reference to exemplary embodiments thereof as shown in the accompanying drawings. While the present disclosure is described below with reference to exemplary embodiments it should be understood that the present disclosure is not limited thereto. Those of ordinary skill in the art having access to the teachings herein will recognize additional implementations modifications and embodiments as well as other fields of use which are within the scope of the present disclosure as described herein and with respect to which the present disclosure may be of significant utility.

Referring to there is shown a system for backing up replicated data in accordance with an embodiment of the present disclosure. is a simplified view of system which may include additional elements that are not depicted. Network elements and may be communicatively coupled to network via appliances and . Network elements and may contain agents and . Appliance may enable access to storage and storage . Storage may contain volumes and . Storage may contain volume . Backup job may enable backup of data from storage to storage . Appliance may enable access to storage and storage . Storage may contain volumes and . Storage may contain volume . Backup job may enable backup of data from storage to storage . Storage may be shared storage accessible via appliances and .

Network may be a local area network LAN a wide area network WAN the Internet a cellular network a satellite network or another network that permits communication between network elements and appliances and and other devices communicatively coupled to network .

Network elements and may be application servers backup servers network storage devices or other devices communicatively coupled to network . Network elements and may utilize storage and for the storage of application data backup data or other data. Network elements and may be nodes replicating data which may utilize storage and as storage. In some embodiments network elements and may use replication but may not be clustered nodes. In one or more embodiments network elements and may be nodes which may be part of a clustered environment.

Appliances and may be continuous data protection and replication CDP R devices which may provide continuous data protection and replication CDP R services to network elements and . CDP R services may be provided through the use of a network switch or may be provided through a continuous data protection and replication CDP R appliance. In one or more embodiments appliances and may represent a network switch such as a fibre channel switch providing CDP R services to network elements and . Appliances and may be communicatively coupled to storage and .

Storage and may be local remote or a combination thereof to network elements and . Storage and may utilize a redundant array of inexpensive disks RAID a redundant array of inexpensive nodes RAIN tape disk or other computer accessible storage. In one or more embodiments storage and may be a storage area network SAN an internet small computer systems interface iSCSI SAN a Fibre Channel SAN a common Internet file system CIFS network attached storage NAS or a network file system NFS .

Storage and may contain volumes and respectively. Storage and may contain replicated data. In one or more embodiments storage and may contain shared and or unshared volumes. Storage may represent shared storage accessible via appliances and . In some embodiments backups may be made to shared storage. Backups to shared storage may enable a restore process to ensure that a backup may be located in a single location regardless of whether a backup job is run on an active and or primary node or a passive and or secondary node.

Volumes and may include data written by one or more applications hosted by network elements and . Volumes and may contain one or more user created data files such as for example a document a list an image file an email a posting a web page xml data a sound file and a video file. Volumes and may contain data stored in one or more formats or data structures. Data structures may be determined by an underlying platform or system supporting an application. Volumes and may be replicated data.

Backup job may be a backup job running on network element which may be capable of backing up one or more volumes files partitions blocks or other units of data from storage to storage . Backup job may be a backup job running on network element which may be capable of backing up one or more volumes files partitions blocks or other units of data from storage to storage . Backup jobs and may run independently of each other and may be capable of running on primary nodes active nodes secondary nodes and or passive nodes. Backup jobs and may communicate with processes or resources such as agents and to determine node status data status and other factors relevant to backup jobs. Backup jobs and may be started by a single backup request on one node that may start a backup job on one or more active and or primary nodes and a backup job on one or more passive and or secondary nodes containing replicated data. Backup jobs and may be scheduled to run at the same time.

Agents and may perform job control of one or more backup jobs running on a node. Agents and may be one or more processes running on a node that may facilitate replication and or clustering of one or more nodes. In one or more embodiments agents and may be cluster agents and may coordinate a data protection application s activities between nodes. Agents and may be able to access one or more resources on a node. Agents and may be able to communicate with agents running on other nodes directly via a shared cluster resource an Application Programming Interface API a Remote Procedure Call RPC an interface tables a web service Extensible Markup Language XML based interfaces Simple Object Access Protocol SOAP based interfaces common request broker architecture CORBA based interfaces and or other interfaces for sending or receiving information.

Backup jobs and may utilize agents and to determine one or more environment conditions. For example network element may be an active node in a clustered environment. Backup job may utilize agent to determine if one or more portions of data such as volumes and are replicated and or clustered data. Agent may query one or more application instances and determine that one or more applications are clustered applications utilizing unshared data. An application instance may indicate to Agent that it is a clustered application whose data is replicated among nodes in a cluster. For example a Microsoft Exchange Server may be running on network element and volume may contain unshared clustered Microsoft Exchange data. Volume may replicated by appliance across network to appliance and may be stored as volume on storage . Network element may be a passive node associated with storage . Volume may be a second volume associated with a second clustered application. Volume may replicated by appliance across network to appliance and may be stored as volume on storage . Agent may inform backup job that volumes and correspond to application instances which are unshared storage containing clustered data. Agent and agent may provide other information to one or more backup jobs. For example agent may inform backup job that an application utilizing volume is replicating properly. Agent may be able to determine a replication status by querying an application instance by monitoring one or more replication processes by verifying a replication cache by verifying a replication log by utilizing an API and or by querying a replication appliance. Agent may further inform backup job that an application utilizing volume is not replicating properly. Backup job may determine not to backup volume since it is being replicated properly and a backup may be taken on a passive node. Backup job may backup volume to volume on storage based on the information received from agent indicating that an application utilizing volume is not replicating properly. In one or more embodiments a user may specify a preference for where a backup job runs. A user may submit a backup job specifying that the backup job should run on the active node only the passive node only the primary node only the secondary node only the primary node only if the secondary node is not available or the active node only if the passive node is not available. In other embodiments a user may specify other conditions such as backup on a passive and or secondary node if an active and or primary node condition or threshold such as CPU utilization is met or exceeded. Backup job may verify that a setting on a backup job or other conditions permit backup on an active and or primary node. Volume may be a volume containing data which has not properly and or recently replicated from volume

Agent may provide backup job with information regarding replicated application data associated with one or more application instances on network element . Agent may determine that an application instance running on network element and utilizing volume is replicating properly. Backup job may receive this information and may determine that a backup may be completed on network element . Backup job may backup volume from storage to volume on storage . Performing a backup of volume on network element may enable off host backup of the application data in volume by enabling a passive and or secondary node to perform a backup on replicated data. Enabling a passive and or secondary node to backup replicated data may reduce a processing or computational load on an active and or primary node. Enabling a passive and or secondary node to backup replicated data may reduce input output I O traffic on an active and or primary node. Running multiple backup jobs may enable a backup job on a passive and or secondary node to begin backing up data without waiting for a command from an active and or primary node. Running multiple backup jobs may enable a backup job on an active and or primary node to terminate once it has determined that replication is occurring properly and may thus reduce a load on an active and or primary node. Backup on a passive and or secondary node may also reduce data loss which may otherwise occur due to a delay in a backup job processed on an active and or primary node. An active and or primary node controller may not respond as quickly to a connection loss or failover condition as a passive and or secondary node controller. The delay in response time when utilizing an active and or primary node controller to perform a backup may result in lost backup data.

Backup job may determine not to backup volume based at least in part on information received from agent indicating that an application utilizing volume is not replicating properly. Backup job may verify one or more backup job settings prior to determining not to backup volume . A user submitting a job may specify that a backup job may be performed on an active node only a primary node only a passive node only a secondary node only an active node if a passive node may not perform a backup or a primary node if a secondary node may not perform a backup. In some embodiments if a backup job determines that a backup may not be performed on an active and or primary node a backup may be run on data existing on the passive and or secondary node. In one or more embodiments if a backup may not be performed on a passive node and or secondary node the backup may fail. The backup job may provide notifications error messages or other information related to the failure. Backup job may verify additional conditions of a passive and or secondary node prior to performing a backup such as but not limited to node availability available memory available storage associated with a node a current node status or other conditions related to a node or associated storage.

A backup job may create backup data and metadata. Metadata may be information relating to the structure or other details of the backup data which may be stored with the backup data. Metadata may enable navigation of backup data for a recovery process. Metadata may enable granular recovery of backed up application data.

Referring to there is shown a method for backing up storage containing replicated data in accordance with an embodiment of the present disclosure. At block the method for backing up storage containing replicated data in accordance with an exemplary embodiment may begin.

At block a node a backup job is running on may be queried to determine one or more criteria related to data to be backed up. Multiple backup jobs may be running on similar schedules on multiple nodes. For example a backup job running on a secondary node may query the secondary node while a backup job which is running on a primary node from which the secondary node s data is replicated may query the primary node.

At block it may be determined whether any replicated data exists on the node. A backup job may query an agent or other process running on a node or other network element associated with the backup job. The agent may query one or more application instances to determine if an application is utilizing replicated data. If an application or other process is utilizing storage containing replicated data the method may continue at block . If storage containing replicated data is not being utilized the method may continue at block .

At block it may be determined whether data is being properly replicated to a secondary node. If data is not being properly replicated to a secondary node the method may continue at block . If data is being properly replicated to a secondary node the method may continue at block .

At block it may be determined whether a secondary node is available to perform a backup and preferred by the backup job settings. If the secondary node is available to perform a backup and preferred by the settings in the backup job the method may continue at block . If the secondary node is not available to perform a backup or is not preferred the method may continue at block .

At block it may be determined whether a backup job permits a backup to be performed on a primary node. If a backup job permits backup on an primary node or if a backup job requires a backup to be performed on a primary node the method may continue at block . If a backup job does not permit backup on a primary node the method continue at block .

At block storage containing replicated data that may not be capable of being backed up on a secondary node may be backed up on the primary node.

At this point it should be noted that backing up replicated data in accordance with the present disclosure as described above typically involves the processing of input data and the generation of output data to some extent. This input data processing and output data generation may be implemented in hardware or software. For example specific electronic components may be employed in a server or similar or related circuitry for implementing the functions associated with backup job control in accordance with the present disclosure as described above. Alternatively one or more processors operating in accordance with stored instructions may implement the functions associated with backup job control in accordance with the present disclosure as described above. If such is the case it is within the scope of the present disclosure that such instructions may be stored on one or more processor readable carriers e.g. a magnetic disk or other storage medium or transmitted to one or more processors via one or more signals embodied in one or more carrier waves.

The present disclosure is not to be limited in scope by the specific embodiments described herein. Indeed other various embodiments of and modifications to the present disclosure in addition to those described herein will be apparent to those of ordinary skill in the art from the foregoing description and accompanying drawings. Thus such other embodiments and modifications are intended to fall within the scope of the present disclosure. Further although the present disclosure has been described herein in the context of a particular implementation in a particular environment for a particular purpose those of ordinary skill in the art will recognize that its usefulness is not limited thereto and that the present disclosure may be beneficially implemented in any number of environments for any number of purposes. Accordingly the claims set forth below should be construed in view of the full breadth and spirit of the present disclosure as described herein.

