---

title: Reconfigurable coprocessor architecture template for nested loops and programming tool
abstract: The exemplary embodiment is for an architecture integrated in a generic System on Chip (SoC) and consisting of reconfigurable coprocessors for executing nested program loops performed in a functional unit array in parallel. The data arrays are accessed from one or more system inputs and from an embedded memory array in parallel. The processed data arrays are sent back to the memory array or to system outputs and enable the acceleration of nested loops. The coprocessors are connected either synchronously or using asynchronous first in first out memories (FIFOs), forming a globally asynchronous locally synchronous system and each coprocessor can be programmed by tagging and rewriting the nested loops in the original program and produces a coprocessor configuration per each nested loop group, which is replaced in the original code with coprocessor input/output operations and control.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08276120&OS=08276120&RS=08276120
owner: Coreworks, S.A.
number: 08276120
owner_city: Lisboa
owner_country: PT
publication_date: 20081008
---
This application claims the priority benefit under 35 U.S.C. 119 of U.S. Provisional Patent Application No. 60 983 798 filed on Oct. 30 2007 which is hereby incorporated in its entirety by reference.

This invention relates in general to computer architecture and in particular to acceleration of nested loops in algorithms.

Computing devices are becoming ubiquitous and many electronic devices can now be found amongst the objects carried by people in their everyday life mobile phones personal digital assistants portable audio players.

These objects have been enabled by embedded processors which follow the same computing paradigm known as von Neumann s architecture. As embedded devices become more complex they require faster and faster clock frequencies and consume more and more power. This is because conventional processors execute instructions sequentially and fetch data also sequentially. For battery powered devices the von Neumann computing paradigm cannot be sustained and alternatives must be found.

Recently there has been great interest in more parallel architectures to face the demanding computational needs of multimedia and communications algorithms. Application specific integrated circuits ASICs have been used to increase the number of operations done in parallel in critical parts of the algorithms thus avoiding increasing the clock frequency and therefore keeping the energy consumption within practical limits. However ASICs have long development times and once fabricated they cannot be changed. This is incompatible with fast changing market dynamics and the short lifespan of modern electronics.

Programmable solutions are in effect more desirable and this is how the technology of Reconfigurable Computing came into existence. A reconfigurable computer is a machine whose architecture can be changed at post silicon time by changing the contents of configuration memories. The essential element of a reconfigurable computer is a programmable multiplexer . The programmable multiplexer has inputs A and B an output C and a configuration bit S. If S is set to 0 a path is created from A to C if S is set to 1 a path is created from B to C. Having enough programmable multiplexers enables functional units and memory elements to be interconnected at will creating different hardware architectures on the fly for better executing different algorithms. The present invention is a template for deriving a class of reconfigurable architectures.

Existing reconfigurable architectures can be divided in two main kinds 1 fine grained arrays and 2 coarse grain arrays.

Fine grain arrays have gained widespread popularity in the form of Field Programmable Gate Arrays FPGAs . An FPGA is a large array of small programmable functional units for performing logic functions on narrow bit slices interconnected by a large network of programmable switches. The functional units are essentially programmable Look Up Tables LUTs and the network of switches consists of the programmable multiplexers described above. Commercial FPGA devices are available through companies like Xilinx Altera Actel Lattice etc. Although FPGAs enable creating circuits on demand by electrical programming the rich array of LUTs and routing switches represent a huge area and power penalty the same circuits implemented in dedicated hardware would be much smaller and less energy hungry. Therefore the use of FPGAs in battery operated devices has been the exception rather than the rule.

FPGAs have been combined with standard processors and specific blocks such as multipliers and embedded RAMs in order to mitigate the huge circuit areas required and improve performance. In this way only the more specific and critical parts of the algorithms are run on the reconfigurable fabric whereas other less critical parts are run on the embedded processors. Examples of such hybrid architectures have been proposed by some researchers 4 16 11 and introduced in the market by FPGA vendors. However these circuits are still wasteful in terms of silicon area and slow in terms of clock frequencies and configuration times.

Coarse grain arrays overcome the mentioned limitations of fine grain arrays at the cost of reduced flexibility and generality. Coarse grain arrays have been the object of recent research with quite a few architectures being proposed by researchers 3 6 12 5 7 8 10 9 13 2 14 17 and startup companies 18 19 . These arrays have functional units of higher granularity and less complex interconnection networks to better target DSP applications such as multimedia and communications. The functional units normally perform arithmetic and logic operations on words of a few bytes rather than on slices of a few bits. The result is a less general but much more compact and faster reconfigurable system requiring small amounts of configuration data which can be agilely and partially swapped at run time.

Another important aspect is how reconfigurable units are coupled with embedded microprocessors. Initially reconfiguration began at the processor functional unit level and was triggered by special instructions 15 12 1 . Later reconfigurable units became coprocessors tightly coupled with processors and still requiring special instructions in order to work 4 16 3 6 12 . More recently coprocessors attached to system busses and requiring no extensions of the host processor instruction set have become a major research topic 2 7 17 . Our work fits into the latest category.

The work in 2 presents a self timed asynchronous data driven implementation which given the difficulties of the timing scheme adopted needed a full custom silicon implementation somewhat impractical to use in a standard cell based technology. The architecture features two address generation processors which run microcode instructions to create the needed sequence of memory addresses.

The architecture in 7 uses undifferentiated 8 bit functional units including LUT based multipliers which are difficult to scale to 16 bit or 32 bit data words used in most multimedia and communications applications. The hierarchical interconnection scheme is structured enough to facilitate compilation. However this work represents a single architecture design rather than an architecture template adaptable and scalable for various applications.

The work closest to ours is the one described in 17 an architecture template consisting of an array of coarse grain functional units interconnected to a set of embedded memories and address generation modules. The address generation modules are implemented with cascaded counters which feed a series of arithmetic and logic units ALUs and multipliers for the generation of complex address sequences. A set of delay lines synchronize the control of functional units and memory operations.

In our approach the address generation blocks are implemented with programmable accumulators reducing the complexity of the hardware compared to using ALUs and multipliers. Instead of multiplying delay lines for synchronization we use a single delay line and multiple counters with programmable wrap around times to generate groups of enable signals with different delays. In this way the generation of some addresses can be delayed relatively to others enabling the execution of loop bodies expressed by unbalanced pipeline graphs. The enable signals accompany the data signals through each functional unit so they arrive with the needed delay at the next functional unit.

Our approach explicitly structures the interconnection networks partial crossbars to facilitate the operation of our programming tool. In fact the architecture template and the programming tool have been co designed to avoid creating hardware structures whose programming is difficult or intractable to automate.

We also consider data sources and data sinks which are not necessarily the data inputs and outputs of embedded processor. The origin and destination of the data may be any piece of hardware in the system not necessarily synchronous to the system clock. For that purpose we provide an interface simpler than processor busses and we use asynchronous FIFOs to connect the core to other cores running at a different clock speed.

The architectures derived from the proposed template are integrated in a generic System on Chip SoC and consist of reconfigurable coprocessors for executing nested program loops whose bodies are expressions of operations performed in a functional unit array. The functional units must be able to perform the atomic operations indicated in the expressions. The data arrays are accessed in parallel from one or more system inputs and from an embedded memory array. The processed data arrays are sent back to the memory array or to system outputs.

The architectures enable the acceleration of nested loops compared to execution on a standard processor where only one operation or datum access can be performed at a time. The invention can be used in a number of applications especially those which involve digital signal processing such as multimedia and communications. The architectures are used preferably in conjunction with von Neumann processors which are better at implementing control flow. The architectures feature an addresses generation block able to create complex sequences of addresses. The configuration register file store the information for programming the data path and the address generation block. The configuration register file is addressable so the system can be partially and runtime reconfigurable.

The architectures loop for the programmed number of iterations or until some conditions pre programmed in the functional units are flagged. Initialization is done using a control register polling is possible using a status register and requests are served upon assertion of a request signal.

The architectures can be scaled easily in the number of data stream inputs outputs embedded memories functional units and configuration registers.

We envision computational systems entailing several general purpose processors and several coprocessors derived from the proposed architectural template. The processors and coprocessors are connected either synchronously or using asynchronous first in first out memories FIFOs forming a globally asynchronous locally synchronous system.

Each coprocessor is programmed by tagging and rewriting the nested loops in the original processor code. The programming tool produces a coprocessor configuration per each nested loop group which is automatically replaced in the original code with coprocessor input output operations and control.

The available hardware in the coprocessor is modeled as a hardware graph. The expressions in loop bodies are modeled as data flow graphs. The source nodes of the data flow graph are memory outputs or system inputs and the sink nodes are memory inputs or system outputs. The expressions in the array addresses are modeled as address flow graphs. The address flow graphs continue the data flow graphs from memory ports through address generation nodes finally to timing nodes forming a complete system graph.

The timing information for the generation of addresses can be extracted from the system graph. The timing information is sent along with the data in the form of enable signals and used to synchronize the functional units.

The system graph is traversed from sinks to sources in order to map the data flow and address flow graphs to the hardware graph where the resources used are marked. Whenever there are multiple programmable multiplexer selections which result in different hardware resources being allocated to implement the intended flow a decision is taken on what to try next. If after a decision the mapping becomes impossible the latest hardware mapping is undone. If there are alternative decisions another decision is tried. If there are no more alternative decisions the previous decision is undone. This process continues until a mapping solution is found or the impossibility of mapping is proven. This procedure is exhaustive and complete.

The coprocessors derived from the proposed architecture template are capable of manipulating n bit data words. We distinguish between

The architectures are capable of executing one or more consecutive nested loop groups according to the following meta language definitions 

The top level view of the proposed architecture template is shown in . It basically consists of an array of functional units FUs and an array of embedded memories EMs .

The data processed by the FUs are sourced from the EMs system inputs or outputs of other FUs by the Read Crossbar. Each FU produces a single data output but it may also produce some flags as a secondary output. These flags are routed by the Read Crossbar to other FUs where they are used as control inputs. The data processed by the FUs is written back to the EMs or sent out to system outputs using routes defined by the Write Crossbar.

The addresses of the memories come from the Address Generator. Since the memory bandwidth is crucial for the performance of this system in all embedded memories are shown as dual port memories. Single port memories could also be used.

A configuration register file holds a set of registers containing data that define the configuration of programmable FUs read and write crossbars and address generation blocks. It also stores some constants used in the computation of addresses and data. The configuration register file is accessed through a configuration interface which is also used for accessing control and status registers. The configuration register file is addressable so the system can be partially and runtime reconfigurable.

The address generation block can be seen in . Its architecture resembles the top level view of the architecture itself that is it is like a smaller reconfigurable processor inside the reconfigurable processor. Instead of the FU array there is an Accumulator Array AA and the Read and Write crossbars appear as the Input Crossbar and Output Crossbar respectively.

The AA contains a collection of special accumulators which are enabled by signals coming from the Timing Unit. Some accumulators produce addresses which are routed to the memory ports by the Output Crossbar. The Output Crossbar also routes addresses stored in certain memories to be used as addresses of other memories. This provides for extreme flexibility in the generation of addresses at the expense of memory space. Other accumulators produce intermediate values which are fed back into the AA itself. The Input Crossbar provides for the feeding back of the intermediate values and routes constants from configuration register to the accumulators in order to generate complex address sequences.

The addresses are functions of the nested loop indices. The Timing Unit generates groups of signals for enabling the AA. The enable signals are routed to the AA by the Enable Crossbar. The Enable Cross bar also routes enable signals to the system inputs to time and synchronize the admission of external data. The enables of the accumulators accompany the generated address to its memory port in case of a read port the enable signals accompany the data read from the memory through the FUs. In the FUs the enable signals are delayed by the same amount as the data so that enables and data remain synchronous.

The control block shown in is responsible for responding to user commands to initialize start and poll the coprocessor. It also stalls the coprocessor in case some condition is detected in the functional units or in case the system inputs are data starving or the outputs are overflowed.

The loop indices advance by unit increments as given above in the nested loop syntax. Each nested loop group uses a group of indices implemented in the Timing Unit using cascaded counters where the end count value is programmed.

The Timing Unit is shown in . The programmable counters are interconnected in a matrix where the first row implements enable signals for the loop indices i j and k. The subsequent rows produce delayed versions of the loop indices. The last row always contains the most delayed version of the loop indices. The first counter represents index i and is incremented at every clock cycle. When a counter reaches the end value it wraps around and pulses the output signal to advance the next outer counter by one. When the final counter reaches the end the outmost loop finishes and the nested loop group is done. A priority encoder identifies the outmost loop end sel from the non null end count values programmed in the cascaded counters. This information will indicate which counter column terminates the processing and will be used in the control block.

The circuit to delay the basic index i enable by programmable values is shown in . Each flip flop D delays the i en t signal by one cycle. Programmable multiplexers select the delay wanted. If there is a break condition see below the state of the delay unit is frozen by disabling the flip flops and masking the output enables. If there are P memory ports in the system at most P different delayed versions of the enables are needed. In practice a lower number of delayed versions may be implemented.

Since real addresses do not advance necessarily by increments of one the accumulator units shown in and are responsible for generating more complex address sequences of the form given by address expression as above.

The Basic Accumulator Unit BAU shown in initializes to the value specified by the START input after the RESTART EN signal is pulsed and accumulates the values specified by the INCR input. The accumulator current and next outputs are given by signals CNT and CNT NXT respectively.

The Complex Accumulator unit shown in adds the following functionality to the BAU the accumulations are done in modulo specified by the configuration input MODULO and added to the value specified by the OFFSET input signal.

Each accumulator selects its EN and RESTART EN signals from the Enable Crossbar which is driven by the enable signal groups produced in the Timing Unit. As shown in first the enable group delayed version is selected and then the enable signals for EN and RESTART EN are selected from within the selected group. A similar scheme is used to select the enable signals used to acknowledge the admission of external data in the system. In this way the input of external data can be timed and synchronized.

The control configuration interface has a Request in input signal to indicate it is being selected and to validate the Address input vector which is used to select internal registers. The Write not Read signal chooses the intended action. The data is written to ctr data in ports and read from the ctr data out port. The Request out signal flags events such as the end of processing or that some condition has been detected and the coprocessor has been halted.

The data in interface has a Req in input signal vector. Each element Req in i indicates that the data in interface i is being selected and validates the Data in i vector containing the input data. The Ack in i signal is used to tell the core driving interface I that the request Req in i to read Data in i has been accepted and executed. The Ack in i signal comes from the Address Generator block where it is selected by the Enable Crossbar.

The data out interface has a Req out output signal vector. Each element Req out i indicates that the data out interface i is being selected and validates the Data out i vector containing the output data. Upon accepting the data sent out by this interface an acknowledge signal Ack out i must be asserted from the outside or otherwise the coprocessor will stall to prevent data loss. From an external point of view Ack out i should always be asserted unless it was impossible to accept the data from the last request.

A basic control unit is shown in . The coprocessor is enabled whenever the control bit En and the I O enable bit are asserted and remains enables until either the END t or the break signals remains unasserted. Whenever these signals pulse a logic 1 is caught in a flip flop which disables the coprocessor. An enabled co processor has the innermost loop index active by asserting signal i en t which in turn enables the outer loop indices and all the delayed versions of the enable groups.

If control output requests are enabled Req en 1 then the Request out signal is asserted when either the break signal or the most delayed END t DP pulses.

The END t and END t DP signals are the wrap around signals of the outmost loop a multiplexer uses signal end sel explained in to choose the index enable from signals i en j en or k en both the delay free and the delayed by DP cycles versions.

The Busy signal of the status register is generated as shown in . The coprocessor is busy if it is either enabled with i en t active or has not finished the processing i.e. END t DP or break have not been asserted.

The break signal is used to disable the generation of loop index enables in the Delay Unit . It is basically a registered inverted and one cycle delayed version of the break signal.

The selection of break conditions from functional units is illustrated in . For scalability reasons each functional i unit can only produce a single break condition signal cond i. Internally functional unit i may be programmed to fire the break condition for various reasons. However from an external perspective there is a single break signal per functional unit. A configuration bit cond i en tells whether break condition cond i is enabled.

The selection of I O dependent system enables is shown in . If the loop body expressions involve a system input i then the co processor can only be enabled if there is data available at that input which is signaled by the Req in i signal. Similarly if the results of the loop body expressions are being sent to system output j then the co processor can only be enabled if the data sent out is actually being read by another system which is signaled by the Ack out j signal. When this signal is asserted it means that the data sent in the last cycle has been read. When asserted configuration bits no in i and no out j indicate that system input i and system output j are not present in the loop body expressions and therefore cannot disable the system.

The coprocessor programming flow is illustrated in . The user starts by writing the nested loop sequence code according to the syntax given above. The coprocessor programming tool inputs the nested loop sequence code and a description of the hardware architecture and outputs the coprocessor configuration sequences in multiple formats text file software include file and configuration memory images in Hardware Description Language HDL . The text file is human readable and is used to give feedback to the user. The software include file contains the configuration memory images of the sequence it can be included in some program which will configure and run the coprocessor. The HDL configuration images are used in FPGA emulation for fast system verification or HDL simulation for detailed system verification. Additionally a software model of the architecture is compiled from the hardware description files which provides a compromise between the speed of FPGA emulation and the detail of HDL simulation. The results output data produced by the software HDL and FPGA models are analyzed by the user and used to guide the refinement of the input nested loop code.

The hardware architecture is described in a file which references the functional units used. The description of the functional units is placed in the functional unit store. The syntax of the hardware description file should be equivalent to the one given below 

The first step is there to analyze the expressions in the nested loop bodies and to create a complete system graph SG consisting of sub graphs for each nested loop group. This is done by function parseNLSC nested loop sequence code . Each nested loop group gives rise to a configuration memory image. The sub graph for each nested loop group has two parts the data flow graph and the address flow graph.

The edges in the DFG and AFG are directed from source nodes to intermediate nodes from intermediate nodes to other intermediate nodes and from intermediate nodes to sink nodes.

The DFG and the AFG can be concatenated in a single configuration graph CG by merging the memory sink nodes of the AFG with the memory source nodes of the DFG. The complete system graph SG can be constructed by concatenating successive CGs. One CG is concatenated with the next CG by merging the memory sink nodes of the current CG with the memory source nodes of the next CG. This allows leaving data in the embedded memories that will be used in the next co processor configuration. This mechanism can be called a conscious or intentional caching mechanism which should perform better than conventional caches which exploit stochastic locality.

The DFG for the nested loop group in this example is shown in . As can be seen the DFG follows the expression in the body of the nested loop group. Each node of the graph represents either an FU node or a memory node. Read write or FU operations are pipelined and the latency for each operation is indicated in . The longest path in the graph from memory reads a v or b w to memory write d u takes 9 cycles. The path from memory read c x to memory write d u takes 4 cycles. This means that memory read c x should be delayed 9 4 5 cycles relative to memory reads a v and b w and that memory write d u should be delayed 9 1 8 cycles relative to memory reads a v and b w . With the presented architecture template there are no delays in computing the addresses. Thus differences in latency come only from FUs with different number of pipeline stages. However extending this methodology to the case where the computation of addresses is affected by latency is straightforward.

The AFG for computing the addresses u v w and x is shown in . The computation of addresses advances with the enable signals i en and j en generated by the Timing Unit. The addresses that need to be delayed D cycles use a delayed enable group with signals i en t D and j en t D . Note that address u v and x need only one BAU to be computed whereas address w is more complex and needs a CAU fed by a BAU.

Concatenating the DFG and the AFG by merging homonym memory nodes u t 8 v t w t and x t 5 yields the CG for the nested loop group in this examples. Had there been a sequence of nested loop groups the respective CGs would be concatenated in a similar way to yield the complete SG.

Having created SG the nodes in this graph are ordered in a list in a breadth first fashion from the system output nodes towards the system input nodes. For the example given the order of nodes could be d u c x x t 5 a v b w v t and w t . This is what the function createNodelist SG in the main algorithm flow does.

Next function parseHW architecture description creates a graph that describes the hardware by means of function. The hardware graph follows the architectural description given above. Some hardware nodes map to SG nodes I O memory functional unit address accumulators configuration constants timing unit nodes other hardware nodes have no correspondence to nodes in SG but are useful for routing signals memory port crossbar multiplexer functional unit port. The selections of paths all the way up from system outputs and memory inputs up to system inputs and memory outputs passing through several levels of functional units constitute the data for each configuration. Unfolding the hardware graph as many times as the number of configurations gives us the complete hardware graph HG onto which SG is mapped.

The next step is to map the nodes in SG to nodes in HG. The recursive map procedure is outlined below.

Following the pseudo code above the first thing to do when mapping an SG node to a HG node is to get the immediate descendant nodes of that node in the graph. These nodes have already been mapped to a HG node since the algorithm proceeds from graph sinks to graph sources. Working from the descendant HG nodes one computes the list of common immediate ascendant HG nodes which have correspondence in SG and are reachable by unused multiplexers. This is a list of possible HG nodes that can be mapped to the node in question. The nodes in this list are searched to find suitable candidates. First the candidate HG node must be of the same type as the node adder multiplier memory etc. Second the node must not be in use. If either of these tests fail the procedure returns unsuccessfully. Having performed these checks the HG node is routed to its descendants that is the input multiplexers of the descendants are set to receive data from the node. If this is the last node to be mapped the procedure returns success. Otherwise the map procedure is recursively applied to the next node in SG. If successful the procedure returns success. Otherwise it means that the mapping of the next nodes can not be accomplished with the current mapping of the current or previous nodes. In this case the routes to the descendants are undone and the next candidate HG node is tried. After all candidate nodes are tried unsuccessfully the procedure returns unsuccessfully.

An example of application of a reconfigurable coprocessor instance created with the present invention is presented next. The example is a MPEG 1 layer III MP3 decoder algorithm. The algorithm has been run on 1 a conventional processor and on 2 the same conventional processor accelerated by a coprocessor instance. The processor is a 32 bit Harvard architecture with 0.81 DMIPS of performance. The coprocessor instance has been generated with the following parameterization 2 nested loops 32 bit data path 2 adders accumulators 2 multipliers shifters 3 dual port memory blocks totalling 4 kbytes of ROM and 8 kbytes of RAM.

The experimental results from running a set of MP3 benchmarks are shown in Table 1. From the initial profiling of the algorithm on the conventional processor we find that two procedures are taking 95 of the time the Polyphase Synthesis and the Inverse Modified Discrete Cosine Transform IMDCT . Thus if we accelerate these procedures in the coprocessor the potential for acceleration is 20. The Polyphase Synthesis procedure has been accelerated 18.7 times on average. The IMDCT procedure has been accelerated 43.9 times on average. This resulted in an overall algorithm acceleration of 11.9 times. The profiling of the complete system formed by the processor and coprocessor reveals a more balanced processing load distribution among the main procedures. In the complete system the Polyphase Synthesis and IMDCT procedures account for only 34 of the processing load in stark contrast to the 95 processing load before acceleration.

In this example the silicon area doubled as the result of adding the coprocessor instance. Since the performance has been multiplied by twelve this means that the processor coprocessor system dissipates roughly 6 times less power while still keeping the same level of performance.

It must be clear that the present reconfigurable coprocessor architecture template merely states the principles of the invention. Variations and modifications to the cited architecture can be made without moving away from the scope and principles of the invention. All these modifications and variations must be enclosed in the scope of the present invention and protected by the following claims.

