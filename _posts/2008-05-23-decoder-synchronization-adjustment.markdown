---

title: Decoder synchronization adjustment
abstract: A device is disclosed that makes packetized and encoded speech data audible to a listener, as is a method for operating the device. The device includes a unit for generating a synchronization request for reducing an amount of synchronization delay, and further includes a speech decoder that is responsive to the synchronization delay adjustment request for executing a time-warping operation for one of lengthening or shortening a duration of a speech frame. In one embodiment the speech decoder comprises a code excited linear prediction (CELP) speech decoder, and the CELP decoder time-warping operation is applied to a reconstructed excitation signal u(k) to derive a time-warped reconstructed signal u(k). The time-warped reconstructed signal u(k) is input to a Linear Predictor (LP) synthesis filter to derive a CELP decoder time-warped output signal y^(k) In another embodiment the speech decoder comprises a parametric speech decoder, and where an adaptation of the frame length N in the parametric speech decoder results in the use of a modified frame length N.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08243761&OS=08243761&RS=08243761
owner: Nokia Corporation
number: 08243761
owner_city: Espoo
owner_country: FI
publication_date: 20080523
---
This patent application is a continuing application of U.S. application Ser. No. 10 364 588 filed Feb. 11 2003 now U.S. Pat. No. 7 394 833 the disclosure of which is incorporated by reference in its entirety.

These teachings relate generally to methods and systems that process packetized audio data and more particularly these teachings pertain to systems and methods that receive and playback Internet Protocol IP speech or voice data packets such as wireless communications terminals and personal computer PC based terminals.

It is well known in packet based terminals and devices such as wireless communications terminals e.g. mobile and cellular telephones or personal communicators PC based terminals as well as IP telephony gateways that an audio device requests data to be converted into audio at regular fixed intervals. These intervals are not however synchronized to the reception of the data packets that contain the audio data. A given packet can contain one or more frames of data where the length or duration of an audio signal contained within the frame is generally in the range of 20 ms to 30 ms referred to herein generally as the frame length although a temporal measure is intended not a spatial measure. After reception the audio data frame is typically stored into a jitter buffer to await its calculated playout time. The playout time is the time during which the frame of audio data is to be converted to an audio signal such as by a digital to analog converter DAC then amplified and reproduced for a listener through a speaker or some other type of audio transducer. In the case of gateways and transcoders the audio is typically sent to a sample based circuit switched network. In that the audio device requests the frame data at random intervals relative to the receipt of the audio packets the data can be stored for a variable amount of time in the jitter buffer. The average storage time in the jitter buffer can be shown to be one half of the duration of the frame in addition to the desired jitter buffer duration. For example it can be demonstrated that if a packet resides in the jitter buffer first for a desired 10 ms after which it is playable the frame however will be fetched at some time during the next 20 ms resulting in the undesired average of 10 ms of additional storage time in the jitter buffer.

A problem arises because of the fact that in modern voice terminals and similar devices such as IP telephony gateways the audio device is synchronized to some local frequency source. The frequency source may be for example an oscillator or a telephone network clock signal. However in packet based terminals the packets containing the voice data arrive at a rate that is independent of and asynchronous to the frequency source that drives the audio device. The difference between the rate of IP packet arrival and the rate at which the audio device requests frames of voice data can create an undesirable and variable end to end delay also referred to as synchronization delay which can be as great as a packet length in duration. Voice over IP VoIP applications can be especially detrimentally affected by synchronization delay induced problems.

Furthermore due to slight differences in clock rates this difference between the rate of IP packet arrival and the rate at which the audio device requests frames of voice data can vary over time thus constituting a continuous re synchronization problem. Typically transmitter and receiver clocks running at different frequencies repeatedly introduce an underflow or overflow situation in the jitter buffer of a VoIP receiver. Because even short gaps or discontinuities in the audio playback cannot be tolerated the receiver needs to somehow react to this condition. In practice the receiver needs to perform re synchronization either by artificially generating a short segment of extra signal in the case of underflow or by discarding some of the received signal in the case of overflow. However the synchronization process should be performed with great care in order to avoid generating audible discontinuities in the reconstructed speech signal.

In EP 0 921 666 A2 Ward et al. are said to reduce degradation in packetized voice communications that are received by a non synchronized entity from a packet network by adjusting a depth of storage of a jitter buffer in the receiver. Units of voice sample data are stored in the jitter buffer as they are received. From time to time the rate of extraction of the stored units from the jitter buffer is accelerated by extracting two units but delivering only one or is retarded by not extracting a unit while delivering a substitute unit in its place. This technique is said to control the depth of storage in response to packet reception events such that the delay is minimized while providing a sufficient amount of delay to smooth the variances between packet reception events.

In WO 01 11832 A1 Nakabayashi describes the use of a receive buffer that stores packets received from a network interface and a reproduction controller that refers to the state of the receive buffer to carry out a sound reproduction operation. A decoder receives the stored data and the decoded data is provided to a DAC that is clocked by a reproduce clock. The process is said to prevent to the underflow and overflow of the receive buffer due to clock differences between the transmitter and the receiver and to prevent packet jitter that results in sound dropouts.

In U.S. Pat. No. 6 181 712 B1 Rosengren describes transmitting packets from an input stream to an output stream. When multiplexing transport streams packet jitter may be introduced to the extent that decoder buffers can underflow or overflow. To avoid this a time window is associated with a data packet and position information is provided in the packet concerning the position of the packet within the window.

The foregoing prior art techniques do not provide an adequate solution to the synchronization delay problem in VoIP and other applications.

Commonly assigned U.S. patent application Ser. No. 09 946 066 filed Sep. 4 2001 entitled Method and Apparatus for Reducing Synchronization Delay in Packet Based Voice Terminals by Jari Selin describes a system and method wherein synchronization is performed at the start of a talk spurt and not continuously.

Commonly assigned U.S. patent application Ser. No. 10 189 068 filed Jul. 2 2002 entitled Method and Apparatus for Reducing Synchronization Delay in Packet Based Voice Terminals by Resynchronizing During Talk Spurts by Ari Lakaniemi Jari Selin and Pasi Ojala which is a continuation in part of the foregoing application describes a method that operates when a frame containing audio data is sent to a decoder by measuring the synchronization delay determining by how much the synchronization delay should be adjusted and adjusting the synchronization delay in a content aware manner by adding or removing one or more audio samples in a selected current frame or in a selected subsequent frame so as not to significantly degrade the quality of the played back audio data. When the synchronization delay is adjusted by more than one audio sample the adjustment can be made by all of the determined audio samples in one adjustment or the adjustment can be made by less than all of the determined audio samples by using a plurality of adjustments. The adjusting operation selects if possible an unvoiced frame and discriminates against a transient frame. The determining operation can include measuring an average amount of time that a frame resides in the jitter buffer and adjusting the synchronization delay so that the average duration approaches a desired jitter buffer residency duration.

While the two foregoing approaches provide a significant advance over the prior art it would be desirable to even further enhance the ability of a packet based terminal to overcome the problems related to synchronization delay in order to even further improve the quality of audio delivered to a user.

The foregoing and other problems are overcome and other advantages are realized in accordance with the presently preferred embodiments of this invention.

This invention provides a system and method for performing synchronization and re synchronization through the use of a modified speech decoder. The invention is described in the context of examples drawn from two widely used but non limiting speech coding models including code excited linear prediction CELP and parametric coding. In the modified CELP speech decoder a time warping method is exploited to enable a best possible speech quality to be realized as well during the synchronization process. In the parametric coding embodiment an adaptation of the speech decoder frame length is used for synchronization purposes.

An aspect of this invention is to provide a new method for speech coding. According to prior art solutions the synchronization operation was required to wait for either the next speech pause or the next speech frame that is classified to be suitable for synchronization. While the latter case may provide a relatively fast reaction time for a synchronization request the resulting speech quality is dependent on the frame classification algorithm deciding when the synchronization operation is allowed to occur. Such an algorithm may represent a trade off between providing a fast reaction to a synchronization request and the resulting speech quality. For example a typical voiced speech segment can have a duration that exceeds 100 msec and can extend over several consecutive speech frames. However shortening or lengthening a voiced frame having a clear periodic structure runs the risk of disturbing the periodicity which can result in an audible distortion of the processed signal.

The teachings of this invention however enable the synchronization delay to be adapted both with a fast reaction time to a synchronization need or request and with improved speech quality. The synchronization can be performed during any frame as the time warping technique enables the processing of periodic voiced frames without degrading the speech quality. Furthermore the use of the time warping technique provides a flexible approach to selecting a most suitable adaptation step length without inducing distortion into the processed speech. The implementation cost is also low.

A device is disclosed that makes packetized and encoded speech data audible to a listener as is a method for operating the device. The device includes a unit for generating a synchronization request for reducing an amount of synchronization delay and further includes a speech decoder that is responsive to the synchronization delay adjustment request for executing a time warping operation for one of lengthening or shortening a duration of a speech frame. In one embodiment the speech decoder comprises a code excited linear prediction CELP speech decoder and the CELP decoder time warping operation is applied to a reconstructed excitation signal u k to derive a time warped reconstructed signal u k . The time warped reconstructed signal u k is input to a Linear Predictor LP synthesis filter to derive a CELP decoder time warped output signal y k In another embodiment the speech decoder comprises a parametric speech decoder where an adaptation of the frame length N in the parametric speech decoder results in the use of a modified frame length N.

The synchronization delay may be adjusted incrementally over a plurality of frames or the synchronization delay can be adjusted during one frame.

The device could be a cellular telephone or a personal communicator where the packetized encoded speech data is received through a wireless communications channel. The packetized encoded speech data may be used for a voice over IP VoIP application.

The device includes a jitter buffer for storing speech data and a jitter buffer controller and the unit that generates the synchronization delay adjustment request comprises the jitter buffer controller. The jitter buffer controller may determine an average amount of time that a frame resides in the jitter buffer and can then adjust the synchronization delay so that the average duration approaches a desired jitter buffer residency duration.

The unit that generates the synchronization request may where possible cause the synchronization delay to be adjusted during subjectively less important speech segments such as during an unvoiced frame.

It is noted at the outset that these teachings are applicable to any system or apparatus that processes and plays back packetized or framed audio data such as mobile and cellular telephones personal communicators PC based terminals IP telephony gateways and IP to circuit switched media transcoders. VoIP systems and devices can especially benefit from the use of this invention. These various types of devices and systems may all be referred to herein simply as a terminal. In general the teachings in accordance with this invention are applicable to any device that links packet based or packet switched voice to circuit switched or otherwise continuous voice. Hence the method of this invention is also applicable to these various devices including gateways and transcoders linking IP and PCM links.

It should further be noted that the description of this invention is made in the context of a media subsystem that employs the real time transport protocol RTP which currently is widely used for VoIP applications. However the teachings of this invention are not limited for use with an RTP based system. In general this invention may be practiced with any suitable system that provides functionality for at least accommodating a sequence numbering of packets a time stamping of packets and some type of packet payload identification. Thus while the ensuing description of the presently preferred embodiments of this invention will be made using RTP those skilled in the art should appreciate that the use of RTP is exemplary and is not to be read or construed in a limiting sense upon the practice of this invention.

By way of introduction is a simplified block diagram of a media subsystem A of an audio reproduction system and also depicts the media flow during audio reception. While described primarily in the context of a wireless communication device such as a cellular telephone or a personal communicator it should be appreciated that the system can also be by example and not by way of limitation a PC based terminal or an IP telephony gateway. Packets such as Internet Protocol IP packets arrive at a network interface from a physical layer in this case some type of packet switched media or network such as a dial up telephone line an Ethernet connection or a wireless air interface e.g. the packets are modulated onto an RF carrier or onto an optical carrier . The network interface may also be referred to as the IP stack of the system . The time of arrival of individual packets at the network interface is not synchronized with the operation of an audio device which may be by way of example only a DAC of a mobile telephone or the PCM connection of an IP telephony gateway. The audio device may implement one of a number of different interfaces such as but not limited to a Linux audio API application programming interface a Windows audio API or a PCM line card.

In the illustrated non limiting embodiment when a packet arrives at the network interface the media subsystem A receives a callback and the packet is received by a real time transport protocol RTP module . Reference with regard to RTP may be had to H. Schulzrinne S. Casner R. Frederick and V. Jacobson RTP a transport protocol for real time applications IETF Network Working Group RFC 1889 Standards Track January 1996 75 p. and to H. Schulzrinne RTP Profile for Audio and Video Conferences with Minimal Control IETF Network Working Group RFC 1890 Standards Track January 1996 18 p. Playout times for the frames contained within the packet are calculated. In a next step the RTP module inserts or queues the frames into a jitter buffer . When the audio device begins to run out of data to play it wakes up or interrupts the media subsystem A and makes a request to the RTP module for more data to play. In response the RTP module checks to determine if there is a sufficient amount of speech data in its internal audio buffer to fill the audio device buffer. If there is enough data it is written immediately to the audio device jump to step . The dequeued frame is then sent to an audio codec such as but not limited to a PCM codec a G.711 codes Pulse code modulation PCM of voice frequencies ITU T Recommendation G.711 1972 18 p. a G.723.1 codec Dual rate speech coder for multimedia communications transmitting at 5.3 and 6.3 kbit s ITU T Recommendation G.723.1 March 1996 27 p. or an AMR codec Mandatory speech codec speech processing functions. AMR speech codec transcoding functions 3GPP Technical specification Group Services and System Aspects 3G TS 26.090 V3.0.1 September 1999 61 p. which decodes the audio data and returns a decoded audio data frame to the RTP module at step . The various codecs that are useable by the apparatus and method of these teachings may or may not have a voice activity detection VAD function.

As will be described below in the preferred embodiments of this invention the codec includes at least one of a AbS type speech decoder such as a CELP speech decoder or a sinusoidal model parametric type of speech decoder that are modified to implement on demand a time warping operation for adjusting the length of a speech frame and hence adjusting the synchronization delay.

If there was no data to decode in the jitter buffer comfort noise or error concealment data may be generated depending on codec type and internal state. At step the RTP module writes the audio data to the audio device and the media subsystem A then returns to the sleep state to wait for the next wakeup.

In general the audio device requests data representing playable sound at regular intervals such as every 20 ms i.e. the codec frame rate . However the reception of the IP packets from the packet network is not synchronized to the play wakeups generated by the audio device . More particularly the audio device typically will process audio in blocks of data that are the same length as the frame length of the codec such as 20 ms. This results in an average synchronization delay of about one half of the frame length. For example if the average frame length is 20 ms then the average synchronization delay is about 10 ms.

Before discussing this invention in greater detail it will be useful to first review the two currently most important speech coder types i.e. one based on waveform coding and one based on parametric coding. The concepts of CELP coding time warping and parametric coding are then described.

Speech coding algorithms can be categorized in different ways depending on the criterion used. The most common classification of speech coding systems divides them into the two main categories of waveform coders and parametric coders. The waveform coders to which the CELP coder belongs attempt to directly preserve the waveform being coded without considering in any significant detail the characteristics of the speech signal. Thus in waveform coders the reconstructed signal converges toward the original signal with decreasing quantization error.

This perfect reconstruction property is not necessarily true for parametric coders which instead use a priori information concerning the speech signal obtained from different speech models and attempt to preserve the perceptually most important characteristics of speech rather than attempting to faithfully code the actual speech waveform. In parametric coders the reconstruction error does not converge to zero with decreasing quantization error. Parametric coders may also be referred to as source coders or vocoders.

It should be noted that an active speech signal can be classified into different sub categories. A typical classification would be a division into voiced periodic and unvoiced noise like frames. However usually the transition between unvoiced and voiced speech introduces a third classification category of a transient frame. Furthermore one may consider as a fourth classification category a so called onset frame which means the frame contains the start of an active speech period following a non active or low energy speech period. As such it is within the scope of this invention to selectively perform the speech decoder based time warping operation for adjusting the synchronization delay based at least in part on the type of frame or on the speech frame content with adjustments being made during for example unvoiced frames where possible. In the presently preferred embodiments it is generally not preferred to perform synchronization during transient or onset frames although these types of frames may include unvoiced or low energy speech and may thus be suitable for synchronization in some cases.

Referring to a CELP coder is basically an Analysis by Synthesis AbS hybrid coder that utilizes vector quantization. In the CELP coder a cascade of a time variant pitch predictor and LP filter is used. An all pole LP filter has the form shown in Equation 1 in where a. . . aare the coefficients is used to model the short time spectral envelope of the speech signal. A pitch predictor has the form shown in Equation 2 of utilizes the pitch periodicity of speech to model the fine structure of the spectrum. Typically the gain b is bounded to the interval approximately of 0 1.2 and the pitch period or similarly pitch lag to the interval approximately of 20 140 samples assuming by way of example a typical sampling frequency of 8 kHz . The pitch predictor may also be referred to as long term predictor LTP filter.

In the LTP filter is represented by the feedback loop containing the delay zand the gain. The LTP memory may also be viewed as a codebook consisting of overlapping code vectors. The codebook is typically referred to as the LTP codebook or as the adaptive codebook.

An excitation signal u n is produced by an excitation generator which typically is a codebook of different candidate vectors representing the noise like component in speech. The excitation signal is multiplied by a gain g to form an input signal to the LTP LP filter cascade. The codebook is often referred to as a stochastic or fixed codebook. The output of the filter cascade is a synthesized speech signal s n . In the encoder an error signal e n is computed by subtracting the synthesized speech signal s n from the original speech signal s n and an error minimizing procedure represented by error minimization block is employed to select the best excitation signal provided by the excitation generator . Typically a perceptual weighting filter is applied to the error signal prior to the error minimization procedure. The purpose of the weighting filter is to shape the spectrum of the error signal so that it is less audible. This can be achieved by emphasizing the error in the spectral valleys of the input speech and by de emphasizing the error in the regions of formant peaks. A typical weighting filter is derived from the LP filter coefficients in the manner shown in Equation 3 of where and are parameters that control the weighting as a function of frequency. The values of and are usually between 0 and 1 and they may be made fixed or adaptive.

A schematic diagram of the encoding procedure is shown in . This diagram is derived from an IS 641 coder TIA EIA IS 641 TDMA Cellular PCS Radio Interface Enhanced Full Rate Speech Codec Telecommunications Industry Association Electronic Industry Association Interim Standard 641 Global Engineering Documents 1996 which is an example of a high quality CELP coder. The blocks are numbered in their order of execution but all blocks are not necessarily executed in every subframe. For example in the IS 641 coder the frame is divided into four subframes. LP filter parameters are determined once per frame the open loop lag is determined twice per frame and the closed loop lag the LTP gain the fixed codebook excitation signal and its gain are all determined four times per frame.

Turning now to the AbS procedure for finding the fixed codebook excitation with the corresponding gain value this is accomplished by minimizing the criterion shown in Equation 4 of in matrix notation where xis a target vector containing the x n samples over the search horizon x is the corresponding synthesized signal and uthe excitation vector corresponding the fixed codebook excitation sequence. H is the weighted impulse response matrix of the LP filter and g is the gain. The same method is also applied for the adaptive codebook search by replacing g by b uby u and xby x respectively. The optimal gain can be found by setting the partial derivative of the cost function with respect to the gain equal to zero as shown in Equation 5 of . By substituting Equation 5 into Equation 4 the expression shown in Equation 6 of is obtained. The optimal excitation is typically searched by maximizing the latter term of Equation 6 .

The encoded parameters of the described CELP structure include LP filter coefficients pitch and pitch gain and the fixed codebook index together with its gain. The CELP decoder receives the parameters from the channel as shown in and a decoder excitation generator determines the fixed excitation signal by the received index and gain. The fixed excitation signal is filtered through the LTP LP filter cascade to produce the synthesized speech signal. The CELP decoder may also include a postfilter not shown to enhance the perceptual speech quality.

In the context of this invention time warping refers to a modification of a signal segment in such a way that its length is either shortened or lengthened in time. shows an example of time warping of an original signal middle trace to a resultant compressed time warped signal top trace or to a resultant expanded time warped signal bottom trace . In general a number of speech processing applications such as for example shortening the duration of original speech messages in a telephone answering machine rely on time warping of the original signal. For high quality time warping a number of algorithms have been proposed. Many of these algorithms use an overlap add principle either in the speech or the LP residual domain. Reference in this regard may be had to for example E. Moulines and W. Verhelst Time domain and frequency domain techniques for prosodic modification of speech Speech Coding and Synthesis Editors W. B. Kleijn and K. K. Paliwal pp. 519 556 Elsevier Science B.V. 1995.

In the TIA Enhanced Variable Rate Coder EVRC portions of the LP residual signal are either omitted or repeated in order to obtain the desired time warp. Reference in this regard can be had to TIA EIA IS 127 Enhanced variable rate codec speech service option 3 for wideband spread spectrum digital systems Telecommunications Industry Association Draft Document February 1996. The time warped speech signal is then obtained by filtering the modified residual signal through an LP synthesis filter. During voiced speech the omitting or repeating of speech samples is performed during low energy portions of the signal in order to avoid a perceptual quality degradation in the modified speech.

Turning now to sinusoidal coding it is well known that most parametric coders are based on a sinusoidal coding model where the speech signal or alternatively the LP residual signal is represented by a sum of sine waves of arbitrary amplitudes frequencies and phases. Reference in this regard can be had to R. J. McAulay and T. F. Quatieri Sinusoidal coding Speech Coding and Synthesis Editors W. B. Kleijn and K.K. Paliwal pp. 121 174 Elsevier Science B.V. 1995. Referring also to Equation 7 in the basics of sinusoidal coding are shown where for the mth sinusoidal component and t represent the amplitude and frequency respectively and represents a fixed phase offset. To obtain a frame wise representation the parameters are assumed to be constant over the analysis. Thus the discrete signal s n in a given frame is approximated by the expression shown in Equation 8 in where Aand represent the amplitude and phase respectively of each sine wave component associated with the frequency track L is the number of sine wave components and N is the frame length.

In order to achieve a smoothly evolving synthesized speech signal during sinusoidal coding a proper interpolation of the parameters is required to avoid discontinuities at the frame boundaries between successive frames. For amplitudes linear interpolation is widely used while the evolving phase may be interpolated using a cubic polynomial between the parameter pairs in succeeding frames. The interpolated frequency can be computed as a derivative of the phase function. Thus the resulting sinusoidal model can be defined as shown in Equation 9 in where A and represent the interpolated amplitude and phase contours respectively.

Having thus provided a background description of both CELP and sinusoidal parametric speech coding techniques in accordance with a CELP speech coding embodiment of this invention it is now shown that a time warping technique executed by the CELP speech decoder can be used to perform the desired synchronization in a packet switched terminal that receives speech or audio data from a channel. The approach of this invention enables synchronization to be achieved during periods of active speech regardless of the nature of the current speech signal and thus does not assume that synchronization will be performed only during a certain type or types of speech.

For the purposes of this invention it can be assumed that the synchronization process is controlled by an external device e.g. as in a jitter buffer controller A that requests via a SYNCH delay adjustment request line B the speech decoder portion of the codec to perform the synchronization or re synchronization operation either by shortening or lengthening the temporal duration of a frame. Once a synchronization request is made when decoding the next speech frame the decoding algorithm of the speech decoder of the codec is modified in such a way that the time warped and synthesized signal is aligned with the synchronization information. .

As one example the jitter buffer controller A may determine an average amount of time that a frame resides in the jitter buffer and the adjustment to the synchronization delay is made so that the average duration approaches a desired jitter buffer residency duration.

However during the time warping process a direct modification to the decoded LP residual signal u k in would result in a significant quality degradation due to the feedback loops in the LP and LTP filters and . This is especially true for the long term predictor LTP filter during voiced speech. One reason for the quality degradation is the parameter optimization process in the speech encoder which minimizes in block the squared error between the original signal and the signal generated by the LTP and fixed codebooks. After the time warping process in only the speech decoder there would exist a misalignment between the encoded parameters and the reconstructed excitation signal.

To avoid the quality degradation during all segments of speech the speech decoder shown in is presently preferred for use. Note that in this case the speech decoder is used in place of the speech decoder that forms a part of the codec in . Similar to the conventional CELP decoder shown in the improved CELP decoder receives the parameters from the channel and a decoder excitation generator determines the fixed excitation signal by the received index and gain. The fixed excitation signal is filtered through the LTP LP filter cascade to produce the synthesized speech signal.

In the presently preferred CELP speech decoder the reconstructed signals u k and y k are used to update the LTP and LP filter memories. The time warping procedure is applied to u k when requested by the SYNCH delay adjustment request signal B see in a time warping block to derive u k which is modified according to the synchronization information. The time warped signal u k is then fed through a second LP synthesis filter to derive the decoder time warped output signal y k which may be input to an postfilter not shown . In the preferred embodiment the filters y k and y k have their own separate memories.

The total amount and direction of the required synchronization to be performed by time warping block is determined from the jitter buffer and is indicated by the signal B see . This information can be expressed as for example a number of samples to be added or removed where a negative value for indicating removal of samples and a positive value denotes an addition of samples. In other embodiments the synchronization request information could be indicated by as an example a duration in milliseconds. The actual synchronization step used by the decoder is typically case dependent. In any event the decoder selects a most suitable step length. During voiced speech this step length is preferably but a few samples. As an example in an EVRC speech codec the maximum shift of a pitch pulse is 0.375 ms i.e. three samples assuming an 8 kHz sample rate whereas during unvoiced speech a somewhat longer step length can be tolerated. In general the selection of the appropriate time warping algorithm executed by time warping block depends on for example the coder design constraints including complexity and a maximum allowable delay. A most straightforward time warping algorithm simply removes speech samples from or adds speech samples to low energy portions of speech according to the synchronization information. However this approach does not generally permit large shifts in speech within a small time frame without experiencing some degradation in the time warped speech. Perceptually a more efficient method is based on for example an overlap add algorithm which on the other hand requires relaxed delay constraints. Overall the selection and operation of a most appropriate time warping algorithm is system coder specific as can be appreciated by those skilled in the art when guided by the foregoing teachings.

It can be noted that if synchronization of several milliseconds is requested it is preferable to perform the time warping process in a series of smaller steps to minimize the possibility of introducing speech quality degradation. It is also within the scope of the teachings of this invention to employ at least in part the frame classification approach presented in the above referenced commonly assigned U.S. patent application Ser. No. 10 189 068 filed Jul. 2 2002 entitled Method and Apparatus for Reducing Synchronization Delay in Packet Based Voice Terminals by Resynchronizing During Talk Spurts by Ari Lakaniemi Jari Selin and Pasi Ojala to allow by example the use of longer synchronization steps during subjectively less important segments of speech and to use shorter synchronization steps during the subjectively more important voiced speech and transient speech.

In a second embodiment of this invention i.e. one directed towards use with parametric speech coding the synchronization process for all speech segments is simpler because there is no feedback loop in the coding model. This property enables a more straightforward modification of the speech signal and the reconstruction process in the decoder only. In this invention an adaptation of the frame length N in the speech decoder according to the received synchronization information is used. The resulting speech decoding model is then defined as shown in Equation 10 in where Nis the modified frame length.

In general the amount of the modification i.e. the difference between the Nand N is defined by the same method as in the case of CELP coding as discussed above. Equation 10 in and the following paragraph define the technical modification to the parametric speech decoder. This approach results in the situation depicted in where the upper diagram represents the synthesized speech signal with frame length N and the bottom diagram depicts a situation with the modified frame length N N N .

As described in the previous sections of this invention the sinusoidal model and its modified version can be applied either to the speech signal itself or to the LP residual signal. The interpolation process used in parametric coders in general guarantees smooth signal behavior and thus a high speech quality regardless of moderate frame length modifications. The modified frame length defines also the interpolation interval. The modification of the interpolation interval can thus be interpreted as one technique for accomplishing the time warping of the speech signal to achieve re synchronization.

The methods and apparatus described above can be used either in a VoIP terminal where the speech is decoded and played back for the human user or in a gateway type of device where speech arriving from a packet switched network is required to decoded and transcoded to some other coding format for further transmission e.g. to the Public Switched Telephone Network .

It should be noted that although a discussion has been made of the use of this invention for re synchronization purposes the teachings of this invention can also be directly applied for speech modification e.g. in conjunction with adaptive jitter buffer control algorithms.

A most efficient and hence presently preferred implementation is to integrate the above described synchronization functionality as part of the speech decoder. The advantage of this approach is the direct access that can be had to the speech decoder parameters which can then be directly used as part of the time warping technique.

As was noted above and as non limiting examples the apparatus and method described above can be embodied within a wireless communications device such as a mobile telephone a cellular radiotelephone or a personal communicator and the packet can be a speech packet received from a radio channel. The apparatus and method described above may also be embodied within a personal computer PC based telephony system an Internet Protocol IP telephony gateway or an IP to circuit switched media transcoder. VoIP enabled devices and terminals can especially benefit from the teachings of this invention. However the teachings of this invention are not to be construed as being limited to use in only the foregoing exemplary applications devices and terminals.

Furthermore while disclosed in the context of transmitting storing and reproducing voice or speech data it can be appreciated that these teachings are not limited to only voice or speech data and that other types of audio data such as music may also benefit from the use of the methods and apparatus disclosed herein.

