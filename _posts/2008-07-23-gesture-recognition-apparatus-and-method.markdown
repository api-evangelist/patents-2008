---

title: Gesture recognition apparatus and method
abstract: A method of identifying a human gesture using a machine includes providing a time sequence of data related to the human gesture; transforming the time sequence of data into waveforms; extracting features from the waveforms; and identifying the human gesture based on the extracted features.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08005257&OS=08005257&RS=08005257
owner: The United States of America as represented by the Secretary of the Navy
number: 08005257
owner_city: Washington
owner_country: US
publication_date: 20080723
---
This application is a continuation in part of U.S. nonprovisional patent application Ser. No. 11 586 750 filed on Oct. 5 2006 now U.S. Pat. No. 7 606 411 which application is incorporated by reference herein and the benefit of 35 U.S.C. 120 is claimed.

The inventions described herein may be manufactured and used by or for the Government of the United States of America for governmental purposes without the payment or any royalties thereon or therefore.

The invention relates in general to machine vision and in particular to devices for recognizing human gestures.

Several approaches have been made to perform recognition of human motion with emphasis on real time computation. The most frequently used methodologies for recognition of body motion and dynamic gestures are based on the analysis of temporal trajectories of motion parameters hidden Markov models and state space models and static activity templates. Other conventional techniques have attempted to represent motion by collecting optical flow over the image or region of interest throughout the sequence although this is computationally expensive and often not robust. Another conventional technique combines several successive layers of image frames of a moving person in a single template. This single template represents temporal history of a motion allowing a match of actual imagery to a memorized template to produce recognition of a motion gesture.

The above conventional techniques have been conducted in controlled laboratory environments with fixed lighting and constant distance to the subject. Actual field conditions will present external stimuli resulting in difficulties in recognition. Thus it is apparent that different approaches are required for real life applications outside of a laboratory such as the flight deck of an aircraft carrier.

Flight deck operations are a dance of chaos with steam constant motion and crowding. These operations are conducted during day or night rain or snow when visibility is extremely poor. Moreover fleet operations are continually subject to reduced manning and resistance to change. It is desired that in these kinds of conditions Unmanned Combat Air Vehicles UCAV shall be launched from the flight decks of aircraft carriers.

To launch a UCAV from a flight deck the UCAV must be controlled during taxiing before takeoff and after landing. Simply hooking a tow tractor to the aircraft has been considered but was deemed too slow especially since the aircraft are required to recover every 45 seconds and need to taxi out of the landing area for the next aircraft. Alternatively providing the aircraft director controller with a joystick to control the aircraft would tax his her workload with an additional process and would negatively impact training and operations.

Further if the UCAV is to be controlled on deck using a joystick the UCAV would necessarily be controlled via radio RF link. However an RF link from a control device to the UCAV is undesirable because of the EMI electromagnetic interference intensive environment on the flight deck. Another alternative is a tethered connection using a control device physically tethered to the UCAV. However such a tethered connection may be potentially unsafe for the personnel on the deck during high activity periods.

Like manned aircraft the UCAVs taxi before launch after recoveries or during re spotting. In the case of manned aircraft flight deck controllers signal directions to pilots for taxiing the aircraft around the deck or airfield. It would be desirable if these signals were used to develop an automatic taxiing system for unmanned aircraft as well. If such a system were developed it would enable a seamless transition of UCAVs into naval aviation.

It is an object of the invention to provide a method and apparatus for the recognition of human gestures by a machine.

It is an object of the invention to provide a method and apparatus for the recognition of human gestures by a machine wherein the method requires much less computation than known methods.

One aspect of the invention is a method of identifying a human gesture comprising providing a time sequence of data related to the human gesture transforming the time sequence of data into waveforms extracting features from the waveforms and identifying the human gesture based on the extracted features.

In one embodiment the providing step includes providing a time sequence of pixel images of the human gesture using at least one video camera. In another embodiment the providing step includes providing the data using accelerometers attached to the human.

The extracting step may include extracting static features from the waveforms and may further include extracting dynamic features from the waveforms. In one embodiment the extracting step includes extracting hand position phase and frequency from a right hand waveform and a left hand waveform.

The identifying step may include identifying the human gesture by comparing one or more of the hand position phase and frequency from the right hand waveform and the hand position phase and frequency from the left hand waveform to at least one rule that describes the human gesture.

The method may further comprise classifying hand positions into vertical and horizontal ranges. The vertical and horizontal ranges are preferably defined in terms of a characteristic length of a human performing the gesture. A preferred characteristic length is the distance from a cranial point to a torso point.

Another aspect of the invention is a computer readable medium containing a computer program for performing a method of identifying a human gesture the method comprising transforming a time sequence of data related to the human gesture into waveforms extracting features from the waveforms and identifying the human gesture based on the extracted features.

A further aspect of the invention is an apparatus for identifying a human gesture comprising means for providing a time sequence of data related to the human gesture means for transforming the time sequence of data into waveforms means for extracting features from the waveforms and means for identifying the human gesture based on the extracted features.

The means for providing may comprise for example accelerometers or at least one video camera. The means for transforming the means for extracting and the means for identifying may comprise at least one computer.

The invention will be better understood and further objects features and advantages thereof will become more apparent from the following description of the preferred embodiments taken in conjunction with the accompanying drawings.

The invention will be described with reference to signals or gestures used by aircraft carrier personnel to direct pilots onboard an aircraft carrier. show some of the gestures used by controllers on aircraft carriers. These gestures may be used to direct a UCAV in accordance with the methods and apparatus of the invention. Although the aircraft carrier control gestures are used to describe the invention the principles of the invention may be used for recognition of virtually any type of human gesture involving movement of the hands and arms.

The method of the invention may be divided into two main parts data acquisition and gesture recognition. After gesture recognition occurs the UCAV or other device responds to the gesture that it has recognized. The UCAV or other device is programmed to respond to the gesture by manipulation of wheels motors gears etc. The programming of the movement of the UCAV or other device is well known and is not discussed further.

The purpose of data acquisition is to capture a time sequence of data related to the gesture being performed. The data include hand positions and velocities in two or three dimensional space. Two exemplary devices for acquiring a time sequence of hand positions and velocities are accelerometers and video camera data acquisition systems.

The physical reference dimension is a characteristic length measurement of a human that can be used to calculate the position of the hands with respect to the rest of the body. Examples of characteristic length measurements shown in and B are stand up height H leg inseam L arm length A or distance D from a cranial point C to a torso point T. Given any one of these characteristic lengths one can use standard anthropomorphic formulas to calculate the other characteristic lengths.

The initial position of the operator s hands is needed to provide the initial conditions or baseline position from which subsequent positions may be measured. After the initial conditions are determined the operator may begin gesturing. During the gesturing the accelerations of the operator s hands are measured. The accelerations may be integrated to obtain the velocity and the velocity may be integrated to obtain the position or displacement. The hand velocities are used to extract the frequency of cyclic hand motions in all three axes. The hand positions with respect to a cranial point C or some other reference point are used to extract vertical and horizontal positions of the hands as well as the phase of the hand motion.

Another method of data acquisition is using video cameras. is a schematic top view of a UCAV . In the nose for example of the UCAV is a video camera connected to a computer . In this case the hands of the operator are tracked by the video camera . To track the operator s hands one may use for example visible color active infrared or retro reflective markers placed on the hands of the operator . In addition a characteristic length dimension of the operator is marked by one of the types of markers mentioned above. A preferred characteristic length dimension is the distance D between the cranium C and the torso T. It is important to know this dimension to determine the vertical and horizontal position of the hands with respect to the body. The markers may be placed directly on the cranium C and torso T as shown in .

Although humans vary in size the proportions between different characteristic lengths of the body are approximately the same. As discussed above if one characteristic length of a human is known one can use standard anthropomorphic formulas to calculate the other characteristic lengths. The cranial torso left hand and right hand markers enable one to extract the horizontal and vertical positions x and y axis of the four points on each frame. These positions may be normalized to the head cranial marker so that the head becomes the reference or origin of a Cartesian coordinate system. It is also possible to use two cameras in a stereo vision configuration to provide position information for a third dimension z .

Reliable data acquisition typically includes an error filtering method. Known error filtering methods that may be used in the invention include for example a data delay method a statistical method and a least squares estimate method. The end result of the data acquisition portion of the invention is for example a left hand and a right hand waveform. shows examples of left hand and right hand waveforms for four gestures MOVE AHEAD TURN LEFT TURN RIGHT and STOP. The horizontal axes of the waveforms represent time and the vertical axes represent position or displacement.

Hand signals typically comprise static poses dynamic gestures or a combination of both. The dynamic gestures typically are cyclic hand motions in three dimensional space. The detection of motion requires some threshold because even when a human is not gesturing there may be some movement. The faster the sampling rate the less frames that are needed to detect motion. By way of example if the sampling rate is 10 frames per second one may use 15 20 frames about 1.5 2.0 seconds as a measurement window. In the case of camera data acquisition hand movement of 6 pixels per second or greater may be considered as MOTION. Otherwise the hand is judged to have NO MOTION.

Another method for detecting motion is based on the amount of movement of the hands. Specifically the change in hand position between two consecutive frames is measured. The dimension D is used as a reference dimension. If a hand moves a distance greater than one fifth of D between two consecutive time frames then it is judged that the hand has MOTION. If a hand moves a distance equal to or less than one fifth of D between two consecutive time frames then it is judged that the hand has NO MOTION.

Once motion has occurred and is detected the time sequence of hand positions and velocities are used to extract signal features. To obtain the signal features the sequential hand positions and velocities are accumulated in a data buffer. The length of the data buffer varies depending on the data acquisition rate. Typically the data buffer should contain approximately 1.5 to 2.0 seconds worth of motion data. The minimum required buffer length is two consecutive data points that is two frames. The reliability of the signal feature extraction is proportional to the length of the time buffer. However a longer time buffer may cause difficulties with transitions between gestures. Therefore the length of the time buffer may be optimized to ensure reliable signal feature extraction and recognition of transitions between gestures.

In the context of the gestures shown in the positions of the hands have meaning only in relation to their positions with respect to the rest of the body. For any given operator there are variations in hand motion and position each time the same gesture is performed. Also for any group of operators there are variations in motion and position for the same gesture. Therefore hand position is determined by classifying all possible hand positions into a few critical ranges. Because humans vary in size the ranges must be in terms of a characteristic dimension of the human. One could use a variety of characteristic dimensions but a preferred reference dimension is the distance D between the cranial point C and the torso point T . The ranges for the positions of the hands are expressed in terms of D.

The ranges for the x axis position of the hands are shown in . Using the y axis as a vertical centerline if a hand position is less than 0.3 D from the y axis the position is INSIDE. If a hand position is from 0.3 D to 0.7 D from the y axis the position is MIDDLE. If a hand position is greater than 0.7 D from the y axis the position is OUTSIDE.

It is also necessary to determine whether or not an arm is extended. For example one can raise his arm vertically so that his arm is extended but still be in the INSIDE position. To determine arm extension a pivot point P is defined on the positive y axis a distance of 0.375 D from the origin point C. If the distance between P and the hand or hand is greater than 0.75 D then the hand or is EXTENDED.

When data is acquired using a video camera it is possible that the camera may be vertically higher than the operator such that the camera is viewing the operator from an angle. In that case the actual y axis position of the hands may be different than the image captured by the camera . To correct this distortion a fuzzy logic method may be used to adjust the height values for the hands. It should be noted that using the fuzzy logic method produces accurate height values for the hands even when there is no distortion due to the vertical camera location.

The theory and methodology of fuzzy logic systems are described in Fuzzy Logic Systems for Engineering A Tutorial by Jerry M. Mandel IEEE Proceedings Vol. 83 No. 2 pp. 345 377 March 1995 which is incorporated by reference herein. There is no single rigid method for designing a fuzzy logic system. One embodiment of a fuzzy logic system for the present invention is a hand height adjuster shown in . The fuzzy height detector utilizes a three step process 1 The crisp value of hand height is fuzzified by computing membership values of the hand height 2 The membership values are related to their corresponding singleton function and 3 The fuzzy variables are defuzzified to produce crisp hand positions. In this embodiment the fuzzification is performed using triangular membership functions that are fixed ahead of time and the defuzzification is performed using singleton functions that allow one to compute a centroid position to thereby obtain a crisp output.

First the crisp vertical hand height value is fuzzified. The crisp value is the value determined by the camera image. shows a graphical fuzzification method. The horizontal baseline represents the y axis as shown in . Fifteen anchors points A A are chosen. Anchor A corresponds to the cranial point C having a y axis value of 0 anchor A corresponds to pivot point P and anchor A corresponds to the torso point T. The scale of the baseline in is arbitrary and is not to scale in . The distance D from point C A to point T A is assigned a value of 1 unit.

The values or distances for each anchor point A A are shown in . Note that values above the cranial point C are negative as in . The construction of the four triangles is based on knowledge of the system characteristics as described in the Mandel article. The distance on the V axis from each anchor point A A A and A to the apex of the respective triangles is 1 but not necessarily scaled like the baseline . For any crisp y axis position on the baseline there is at least one fuzzy value that is determined by the intersection of a vertical line from the crisp position on the baseline with the sides of one or more of the triangles. For example crisp point H has a fuzzy value of V. Crisp point Q has fuzzy values of Vand V. The four triangles define eight lines . Mathematically each line is of the form V my b where V is the fuzzy value m is the slope of the line and b is the V axis intercept.

The fuzzy values are then defuzzified using a defuzzifier. shows a defuzzification method. The y axis ranges VERY HIGH HIGH MEDIUM LOW and VERY LOW are indicated on as in . For each of the eight lines of a singleton function C C is created in . The locations of the singleton functions C C on the y axis are chosen to minimize or eliminate the distortion caused when the imaging camera is vertically above the operator. The locations of the singleton functions C C on the y axis that is 0.9 0.02 0.05 0.35 0.45 0.55 1.06 and 1.11 are actually coefficients of D the distance from the cranial point C to the torso point T. For a given y axis hand position each singleton function C C has a fuzzy value set V V that is determined as discussed above with regard to .

The output of the fuzzy logic system for a crisp input is CiVi Vi where Ci is the y axis position of each singleton function C C and Vi is the corresponding fuzzy value s for that singleton function. For example referring to the crisp y axis value H has only one fuzzy value V. The vertical line from H intersects line at V. Because only line is intersected only the singleton function C has a nonzero value. Calculating CiVi Vi yields 0.9 V Vequals 0.9 or VERY HIGH. Another example is the crisp point Q . The vertical line from Q intersects lines and at Vand V. Only the singleton functions C and C corresponding to lines and have nonzero values. Calculating Ci Vi Vi yields 0.35 V 0.45 V V V . The actual numerical result depends on the values of Vand V.

Another feature used to characterize gestures is the phase of motion of one hand compared to the phase of motion of the other hand. Phase of motion is relative to each of the x y and axes. For example relative to the x axis cyclic motion of the right and left hands is IN PHASE when the absolute values of the maximum positions furthest horizontal distances from the origin point C on the right and left hands is the same or approximately the same and the absolute values of the minimum positions nearest horizontal distances from the origin point C of left and right hands is the same or approximately the same. Otherwise the cyclic motion of the hands with respect to the x axis is OUT OF PHASE. Put another way the hand motion is IN PHASE with respect to the x axis if both hands are moving toward the zero on the x axis simultaneously or both hands are moving away from zero on the x axis simultaneously. Similarly the cyclic hand motion may be IN PHASE or OUT OF PHASE with respect to the y and z axes.

For cyclic motions another identifying feature is the frequency of the motion. In general the frequency relates to the speed at which the operator desires the UCAV or other device to respond. An exemplary threshold for fast motion is greater than 1 Hertz and for slow motion is less than 0.5 Hertz.

Referring to the human is facing forward so that the right hand is and the left hand is . In the titles of the gestures are from the point of view of a pilot in an aircraft or in the invention the camera in the UCAV . For example in the gesture or command is Turn to Left. Left is the left hand side of a pilot sitting facing forward in an aircraft. However the descriptions of the gestures in the boxes in the right side of each FIG. are from the point of view of the operator who is facing the aircraft and actually performing the gesture.

Below is a listing of the gestures shown in . For each gesture or command a corresponding rule is given that is used to uniquely identify that command or gesture. The rules are composed of features of combinations of features. The features include one or more of left and or right hand position on the x axis INSIDE MIDDLE OUTSIDE left and or right hand position on the y axis VERY LOW LOW MIDDLE HIGH VERY HIGH left hand and or right hand EXTENDED phase of cyclic hand motion with respect to one or more of the x y and z axis IN PHASE or OUT OF PHASE and frequency of cyclic hand motion FAST or SLOW . The gesture names listed below correspond to . However for the identifying features included in each rule for each gesture right and left correspond to the point of view of the camera pilot . Thus the descriptions in the boxes on the right side of and the identifying features listed below use opposite conventions for right and left. 

Each command or gesture below may be uniquely identified using only x axis and y axis information two dimensions . However where z axis features are stated these features may be used with a three dimensional data acquisition system to further confirm accuracy of the gesture.

Of course many other types of gestures are susceptible to recognition using the invention. The invention has been tested and proven in a laboratory environment using an off the shelf video camera and a computer mounted on a wheeled robot. Visible color markers worn by the operator were used for data acquisition. The computer successfully recognized various gestures and the wheeled robot responded accordingly.

While the invention has been described with reference to certain preferred embodiments numerous changes alterations and modifications to the described embodiments are possible without departing from the spirit and scope of the invention as defined in the appended claims and equivalents thereof.

