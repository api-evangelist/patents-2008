---

title: Systems and methods for reservoir sampling of streaming data and stream joins
abstract: Algorithms and concepts for maintaining uniform random samples of streaming data and stream joins. These algorithms and concepts are used in systems and methods, such as wireless sensor networks and methods for implementing such networks, that generate and handle such streaming data and/or stream joins. The algorithms and concepts directed to streaming data allow one or more sample reservoirs to change size during sampling. When multiple reservoirs are maintained, some of the algorithms and concepts periodically reallocate memory among the multiple reservoirs to effectively utilize limited memory. The algorithms and concepts directed to stream joins allow reservoir sampling to proceed as a function of the probability of a join sampling operation. In memory limited situations wherein memory contains the sample reservoir and a join buffer, some of the stream join algorithms and concepts progressively increase the size of the sampling reservoir and reallocate memory from the join buffer to the reservoir.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08392381&OS=08392381&RS=08392381
owner: The University of Vermont and State Agricultural College
number: 08392381
owner_city: Burlington
owner_country: US
publication_date: 20080508
---
This application claims the benefit of priority of U.S. Provisional Patent Application Ser. No. 60 916 669 filed May 8 2007 and titled Adaptive Progressive Reservoir Sampling Of Streaming Data And Stream Joins which is incorporated by reference herein in its entirety.

Subject matter of this disclosure was made with government support under National Science Foundation Grant No. IIS 0415023. The government may have certain rights in this subject matter.

The present invention generally relates to the field of statistical sampling. In particular the present invention is directed to systems and methods for reservoir sampling of streaming data and stream joins.

Uniform random sampling has been known for its usefulness and efficiency for generating consistent and unbiased estimates of an underlying population. In this sampling scheme every possible sample of a given size has the same chance to be selected. Uniform random sampling has been heavily used in a wide range of application domains such as statistical analysis computational geometry graph optimization knowledge discovery approximate query processing and data stream processing.

When data subject to sampling come in the form of a data stream e.g. stock price analysis and sensor networks monitoring sampling encounters two major challenges. First the size of the stream is usually unknown a priori and therefore it is not possible to predetermine the sampling fraction i.e. sampling probability by the time sampling starts. Second in most cases the data arriving in a stream cannot be stored and therefore have to be processed sequentially in a single pass. A technique commonly used in this scenario is reservoir sampling which selects a uniform random sample of a given size from an input stream of an unknown size. Reservoir sampling has been used in many database applications including clustering data warehousing spatial data management and approximate query processing.

Conventional reservoir sampling selects a uniform random sample of a fixed size without replacement from an input stream of an unknown size see Algorithm 1 below . Initially the algorithm places all tuples in the reservoir until the reservoir of the size of r tuples becomes full. After that each ktuple is sampled with the probability r k. A sampled tuple replaces a randomly selected tuple in the reservoir. It is easy to verify that the reservoir always holds a uniform random sample of the k tuples seen so far. Conventional reservoir sampling assumes a fixed size reservoir i.e. the size of a sample is fixed .

In addition to its usefulness in sampling in the context of data streams uniform random sampling has been extensively used in the database community for evaluating queries approximately. This approximate query evaluation may be necessary due to limited system resources like memory space or computation power. Two types of queries have been mainly considered 1 aggregation queries and 2 join queries. As between the two types it is far more challenging for join queries because uniform random sampling of join inputs does not guarantee a uniform random sample of the join output.

In the context of data stream processing others have addressed that challenge with a focus on streaming out without retaining a uniform random sample of the result of a sliding window join query with limited memory. There are however many data stream applications for which such a continuous streaming out is not practical. One example is applications that need a block of tuples instead of a stream of tuples to perform some statistical analysis like median variance etc. For these applications there should be a way of retaining a uniform random sample of the join output stream.

Another example comes from the applications that collect results of join queries from wireless sensor networks using a mobile sink. Data collection applications have been extensively addressed in research literature. In these applications a mobile sink traverses the network and collects data from sensors. Thus each sensor needs to retain a uniform random sample of the join output instead of streaming out the sample tuples toward the sink.

A natural solution to keep a uniform random sample of the join output stream is to use reservoir sampling. However keeping a reservoir sample over stream joins is not trivial since streaming applications can be limited in memory size.

In one implementation the present disclosure is directed to a method of maintaining a uniform random sample by a machine. The method includes establishing in a machine memory a sampling reservoir having a size receiving a data stream containing sequentially arriving tuples sampling the data stream so as to store ones of the sequentially arriving tuples in the sampling reservoir so as to create stored tuples while sampling adjusting the size of the sampling reservoir in a controlled manner and after adjusting the size continuing sampling the data stream and storing ones of the sequentially arriving tuples in the sampling reservoir so as to maintain a sample of the data stream with a certain uniformity confidence.

In a particular example of the immediately foregoing implementation the machine memory has a limited size and the method further includes establishing in the machine memory a plurality of sampling reservoirs each having a size receiving a plurality of data streams each containing a plurality of sequentially arriving tuples the plurality of data streams corresponding respectively to the plurality of sampling reservoirs checking whether the size of any one or more of the plurality of sampling reservoirs should be changed and for each of the plurality of sampling for which the size should be changed adjusting the size of that one of the plurality of sampling reservoirs as a function of the limited size of the machine memory.

In another implementation the present disclosure is directed to a method of performing join sampling by a machine. The method includes establishing in a machine memory a sampling reservoir having a sampling reservoir size and a join buffer having a join buffer size simultaneously receiving a plurality of data streams join sampling the plurality of data stream so as to create a plurality of join sample tuples storing the plurality of join sample tuples in the join buffer reservoir sampling the plurality of join sample tuples so as to create a plurality of reservoir sample tuples and storing the plurality of reservoir sample tuples in the sampling reservoir.

In a further implementation the present disclosure is directed to a computer readable medium containing computer executable instructions for performing a method of maintaining a uniform random sample. The computer executable instructions include a first set of computer executable instructions for receiving a data stream containing sequentially arriving tuples a second set of computer executable instructions for sampling the data stream so as to store ones of the sequentially arriving tuples in a sampling reservoir so as to create stored tuples a third set of computer executable instructions for adjusting the size of the sampling reservoir in a controlled manner while sampling and a fourth set of computer executable instructions for continuing sampling the data stream after the adjusting of the size and storing ones of the sequentially arriving tuples in the sampling reservoir so as to maintain a sample of the data stream with a certain uniformity confidence.

In a particular example of the immediately foregoing implementation the machine memory has a limited size and the computer executable instructions further include computer executable instructions for establishing in the machine memory a plurality of sampling reservoirs each having a size computer executable instructions for receiving a plurality of data streams each containing a plurality of sequentially arriving tuples the plurality of data streams corresponding respectively to the plurality of sampling reservoirs computer executable instructions for checking whether the size of any one or more of the plurality of sampling reservoirs should be changed and computer executable instructions that for each of the plurality of sampling reservoirs for which the size should be changed adjusts the size of that one of the plurality of sampling reservoirs as a function of the limited size of the machine memory.

In yet another implementation the present disclosure is directed to a computer readable medium containing computer executable instructions for performing a method of maintaining a uniform random sample. The computer executable instructions include a first set of computer executable instructions for establishing in a machine memory a sampling reservoir having a sampling reservoir size and a join buffer having a join buffer size a second set of computer executable instructions for simultaneously receiving a plurality of data streams a third set of computer executable instructions for join sampling the plurality of data stream so as to create a plurality of join sample tuples a fourth set of computer executable instructions for storing the plurality of join sample tuples in the join buffer a fifth set of computer executable instructions for reservoir sampling the plurality of join sample tuples so as to create a plurality of reservoir sample tuples and a sixth set of computer executable instructions for storing the plurality of reservoir sample tuples in the sampling reservoir.

In still another implementation the present disclosure is directed to a system that includes at least one processor for processing computer executable instructions and memory functionally connected to the at least one processor the memory containing computer executable instructions for performing a method of maintaining a uniform random sample. The computer executable instructions include a first set of computer executable instructions for receiving a data stream containing sequentially arriving tuples a second set of computer executable instructions for sampling the data stream so as to store ones of the sequentially arriving tuples in a sampling reservoir so as to create stored tuples a third set of computer executable instructions for adjusting the size of the sampling reservoir in a controlled manner while sampling and a fourth set of computer executable instructions for continuing sampling the data stream after the adjusting of the size and storing ones of the sequentially arriving tuples in the sampling reservoir so as to maintain a sample of the data stream with a certain uniformity confidence.

In a particular example of the immediately foregoing implementation the memory includes a portion having a limited size and further contains computer executable instructions for establishing in the portion of the memory a plurality of sampling reservoirs each having a size computer executable instructions for receiving a plurality of data streams each containing a plurality of sequentially arriving tuples the plurality of data streams corresponding respectively to the plurality of sampling reservoirs computer executable instructions for checking whether the size of any one or more of the plurality of sampling reservoirs should be changed and computer executable instructions that for each of the plurality of sampling reservoirs for which the size should be changed adjusts the size of that one of the plurality of sampling reservoirs as a function of the limited size of the portion of the memory.

In yet still another implementation the present disclosure is directed to a system that includes at least one processor for processing computer executable instructions and memory functionally connected to the at least one processor the memory containing computer executable instructions for performing a method of maintaining a uniform random sample. The computer executable instructions include a first set of computer executable instructions for establishing in a machine memory a sampling reservoir having a sampling reservoir size and a join buffer having a join buffer size a second set of computer executable instructions for simultaneously receiving a plurality of data streams a third set of computer executable instructions for join sampling the plurality of data stream so as to create a plurality of join sample tuples a fourth set of computer executable instructions for storing the plurality of join sample tuples in the join buffer a fifth set of computer executable instructions for reservoir sampling the plurality of join sample tuples so as to create a plurality of reservoir sample tuples and a sixth set of computer executable instructions for storing the plurality of reservoir sample tuples in the sampling reservoir.

The present invention relates to the development by the present inventors of novel algorithms for reservoir sampling of data streams and stream joins as well as to systems and methods that implement these algorithms. The novel algorithms include adaptive reservoir sampling for single and multiple reservoirs in which the size of the reservoir s at issue is are dynamically increased and or decreased in size during the sampling. The novel algorithms also include fixed and progressive increasing in size reservoir join sampling. These algorithms as well as examples of implementations of these algorithms in systems and methods are described below in detail in the following sections.

Prior to proceeding however it is useful at this point to introduce the notion of uniformity confidence UC since a number of the specific algorithms presented utilize it in decision making that addresses tradeoffs inherent in increasing the size of a sampling reservoir. Uniformity confidence is the probability that a sampling algorithm generates a uniform random sample. A sample is a uniform random sample if it is produced using a sampling scheme in which all statistically possible samples of the same size are equally likely to be selected. In this case we say the uniformity confidence in the sampling algorithm equals 100 . In contrast if some statistically possible samples cannot be selected using a certain sampling algorithm then we say the uniformity confidence in the sampling algorithm is below 100 . Thus we define uniformity confidence as follows 

For reservoir sampling the uniformity confidence in a reservoir sampling algorithm which produces a sample S of size r denoted as S is defined as the probability that S is a uniform random sample of all the tuples seen so far. That is if k tuples have been seen so far then the uniformity confidence is 100 if and only if every statistically possible S has an equal probability to be selected from the k tuples. As seen below if the reservoir size is decreased from r to r 0 then there is a way to maintain the sample in the reduced reservoir such that every statistically possible S has an equal probability of being in the reduced reservoir whereas if the reservoir size is increased from r to r 0 then some statistically possible S s cannot be selected.

There are situations where it would be desirable to change the size of one or more sampling reservoirs dynamically as sampling proceeds. A first example scenario is illustrated in which shows a wireless sensor network that includes a plurality of wireless sensors distributed in a plurality of spatial clusters here three clusters A C. For the collection of readings from sensors each cluster A C has an associated proxy A C that includes a memory that stores sensor readings from the sensors of that cluster and acts as a data cache. A mobile sink navigates network to periodically collect data from proxies A C. Memory of each proxy A C however is limited and therefore may store only samples of the readings. Each proxy A C may very well keep multiple reservoir samples one for each sensor in the corresponding cluster A C. A software application for example a monitoring and analysis application aboard a computer such as a general purpose computer e.g. laptop desktop server etc. may demand that the size of a reservoir be in proportion to the number of readings generated so far by the corresponding sensor. If for example the sampling rates of sensors change over time the reservoir sizes should be adjusted dynamically as the sampling rates of the sensors change.

As those skilled in the art will readily appreciate in the foregoing example each sensor may include one or more of any type of transducer or other measurement device suitable for sensing the desired state s at the location of that sensor. Examples of such device types include but are not limited to temperature transducers motion transducers e.g. accelerometers displacement transducers flow transducers speed transducers pressure transducers moisture transducers chemical sensing transducers photo transducers voltage sensors electrical current sensors electrical power sensors and radiation transducers among many others. Wireless sensors proxies mobile sinks and computers suitable for use respectively as wireless sensors proxies A C mobile sink and computer are well known in the art and therefore need not be described in any detail herein for those skilled in the art to implement concepts of the present invention to their fullest scope. The same holds true for actual software applications that correspond to software application .

As another example of a situation in which it could be beneficial to dynamically change the size of a sampling reservoir is approximate query processing. For instance others have adapted reservoir sampling to sample the result of a join operation. Random sampling over joins requires information about the statistics of base relations. However such information may not be available and therefore it may not be possible to pre estimate the size of an intermediate join result and accordingly pre allocate an appropriate reservoir size. Even if available such statistics are often inaccurate at run time and the size of an intermediate join result may be much larger than estimated. If while the sampling is in progress the reservoir size becomes too small to represent the intermediate join result adequately then the reservoir size should be increased. Furthermore if the total available memory for the query processor is limited increasing the size of a reservoir would force the release of some memory elsewhere possibly decreasing the size of another reservoir.

In a further example periodic queries a variation of continuous queries are appropriate for many real time streaming applications such as security surveillance and health monitoring. In the periodic query model once a query is registered to the system query instances are instantiated periodically by the system. Upon instantiation a query instance takes a snapshot of tuples that arrived since the last instantiation of the query. Consider a common situation in which due to the nature of data streams and their potentially high arrival rates a technique like random sampling is used to reduce the stream rate. As the query is instantiated periodically the system may keep a reservoir sample of stream data arriving between the execution times of two consecutive query instances. If at some point the reservoir size has become too small to represent the stream adequately the system should provide a way to increase the reservoir size for better representing the stream data at the execution time of the next query instance. Moreover it is not uncommon that multiple queries run simultaneously in the system. In this case each query may have its own reservoir sample maintained. Besides at any point in time one or more queries can be registered to or expired from the system. In order to adapt to this dynamic change of the query set the system should be able to adaptively reallocate the memory among all reservoirs of the current query set.

Although in a variety of situations it is desirable to adjust the reservoir size while the sampling is in progress this adjusting does not come for free. As described below such an adjustment may have a negative impact on the statistical quality of the sample in terms of the probability of the sample being uniform. Motivated by this observation the development of algorithms for adaptive size reservoir sampling over data streams considered the following two main factors 1 reservoir size or equivalently sample size and 2 sample uniformity. An appropriate sample size depends on data characteristics such as the size mean and variance of the population. Sample uniformity brings an unbiased representation of the population and is especially desirable if it is not clear in advance how the sample will be used.

Development of adaptive size reservoir sampling shows that on one hand if the reservoir size decreases the sample in the reduced reservoir can be maintained with a 100 uniformity confidence. On the other hand if the reservoir size increases it is not possible to maintain the sample in the enlarged reservoir with a 100 uniformity confidence and in this case there is a tradeoff between the size of the enlarged reservoir and the uniformity confidence.

The following section presents a novel algorithm called adaptive reservoir sampling for maintaining a reservoir sample for a single reservoir after the reservoir size is adjusted. If the size decreases the algorithm maintains a sample in the reduced reservoir with a 100 uniformity confidence by randomly evicting tuples from the original reservoir. If the size increases the algorithm finds the minimum number of incoming tuples that should be considered in the input stream to refill the enlarged reservoir such that the resulting uniformity confidence exceeds a given threshold. Then the algorithm decides probabilistically on the number of tuples to retain in the enlarged reservoir and randomly evicts the remaining number of tuples. Eventually the algorithm fills the available room in the enlarged reservoir using the incoming tuples.

Following the presentation of a single reservoir algorithm subsequent sections extend the single reservoir algorithm to an adaptive multi reservoir sampling and describe experiments using real sensor networks data sets. The experimental results demonstrate the adaptivity of the adaptive multi reservoir sampling algorithm through two sets of experiments. The first set of experiments shows the sizes of multiple reservoirs changing adaptively to the change in the sampling rate of sensors and the second set of experiments shows the effects of these changes on the samples uniformity.

As mentioned above using this algorithm to dynamically decrease the size of the reservoir during sampling allows the uniformity confidence to remain 100 while using the algorithm to dynamically increase the size of the reservoir during sampling results in a uniformity confidence less than 100 though the decrease in confidence can be controlled by controlling the size increase and vice versa. Using Equation 1 above the effect of decreasing and increasing the size of the reservoir on the uniformity confidence can be demonstrated as follows.

Decreasing the Size of the Reservoir. Suppose the size of a reservoir is decreased from r to r 0 immediately after the ktuple arrives see . Then the sample in the reduced reservoir can be maintained by randomly evicting tuples from the original reservoir. With this random eviction in place there are

Increasing the Size of the Reservoir. Suppose the size of a reservoir is increased from r to r 0 immediately after the ktuple arrives see . Then the reservoir has room for 6 additional tuples. Clearly there is no way to fill this room from sampling the k tuples as they have already passed by. Only incoming tuples can be used to fill the room. The number of incoming tuples used to fill the enlarged reservoir is denoted as m and is called the uniformity confidence recovery tuple count. 

For the sake of better uniformity some of the r existing tuples are allowed to be evicted probabilistically and replaced by some of the incoming m tuples. In Algorithm 2 the number of tuples evicted or equivalently the number of tuples retained are randomly picked. Clearly the number of tuples that are retained x can be no more than r. Besides x should not be less than r m if m

Examining this formula shows that the uniformity confidence increases monotonously and saturates as m increases. shows this pattern for one setting of k r and . Note that the uniformity confidence never reaches 100 as exemplified by which magnifies the uniformity confidence curve of for m 9000.

Based on the foregoing the exemplary adaptive reservoir sampling algorithm works as shown in Algorithm 2 above. As long as the size of the reservoir does not change it uses conventional reservoir sampling to sample the input stream Line 3 . If the reservoir size decreases by the algorithm evicts tuples from the reservoir randomly Line 6 . After that the algorithm continues sampling using the conventional reservoir sampling Line 3 . On the other hand if the reservoir size increases by the algorithm computes the minimum value of m using Equation 3 that causes the uniformity confidence to exceed a given threshold Line 9 . Then the algorithm flips a biased coin to decide on the number of tuples x to retain among the r tuples already in the reservoir Line 10 . The probability of choosing the value x where max 0 r m x r is defined as 

After that the algorithm randomly evicts r x tuples from the reservoir Line 11 and refills the remaining reservoir space with r x tuples from the arriving m tuples using conventional reservoir sampling Line 12 . Eventually the algorithm continues sampling the input stream using the conventional reservoir sampling Line 3 as if the sample in the enlarged reservoir were a uniform random sample of the k m tuples.

In this section we extend the adaptive reservoir sampling algorithm for a practical application of multi reservoir sampling in which samples are collected in memory limited situations such as wireless sensor networks using a mobile sink. See and accompanying description as an example of such a network. Applications of data collection over wireless sensor networks using a mobile sink have recently received significant research attention. These applications take advantage of the mobility to improve the process of data gathering. Again in such applications at least one mobile sink roves the network and collects data from sensors in its proximity thereby reducing the in network communications and increasing the lifetime of the network. While a wireless sensor network is presented as an example those skilled in the art will readily appreciate that adaptive multi reservoir sampling of the present disclosure can be readily implemented in any of a variety of applications such as health monitoring for patient care senior citizen safety etc. surveillance for border security building security etc. power grid monitoring for grid protection load distribution etc. environmental monitoring for traffic control habitat monitoring etc. structural diagnostics for bridge or building monitoring and repair etc. and target tracking for military operation etc. among others. As those skilled in the art will appreciate each of these examples involves the collection of sensor data from multiple sensors that may be provided in a wired or wireless sensor network or hybrid containing both wired and wireless sensors. Again is illustrative of a wireless sensor network that could be used for any of these applications. Those skilled in the art will readily understand the modifications needed to implement the present invention in a wired or hybrid type sensor network.

In this example it is assumed that the processing power of each proxy such as each proxy A C of is sufficient to carry out the required computations. For this scenario an adaptive multi reservoir sampling algorithm is based on the following key ideas. First an objective of the algorithm is to adaptively adjust the memory allocation in each proxy so that the size of each reservoir is allocated in proportion to the number of readings i.e. tuples generated so far by the corresponding sensor. More specifically this objective is to allocate the memory of size M to the reservoirs R R . . . Rv of v input streams S S . . . Sv so that at the current time point t the size r t of each reservoir Ris proportional to the total number of tuples k t seen so far from S. The rationale behind this objective is explained below. Second the algorithm adjusts the memory allocation only if the relative change in the size of at least one reservoir is above a given memory adjustment threshold and the resulting uniformity confidence for all reservoirs exceeds a given uniformity confidence threshold.

Three criteria can be used to determine a statistically appropriate sample size for a given population. These criteria are the confidence interval the confidence level and the degree of variability in the population. Confidence interval is the range in which the true value of the population is estimated to be. Confidence level is the probability value associated with a confidence interval. Degree of variability in the population is the degree in which the attributes being measured are distributed throughout the population. A more heterogeneous population requires a larger sample to achieve a given confidence interval. Based on these criteria the following simplified formula for calculating a statistically appropriate sample size is provided assuming 95 confidence level and 50 degree of variability note that 50 indicates the maximum variability in a population 

At the current time point t this computed reservoir size r t may be different from the reservoir size r t adjusted at time point t t

To control the frequency of memory allocation adjustment we consider the adjustment only if the relative change in the computed size Equation 8 exceeds a given threshold denoted as for some R that is the adjustment is considered if Equation 11 holds for some i 1 2 . . . v.

Based on the foregoing problem formulation Algorithm 3 works as follows. As long as there are no tuples arriving from any stream the algorithm stays idle Lines 2 4 . Upon the arrival of a new tuple from any stream it computes r t for those streams from which tuples arrived Line 5 and computes r t and t for all streams Lines 6 9 . Then it checks if the relative change in the size of any reservoir is larger than the memory adjustment threshold using Equation 11 Line 10 . If so it computes m t for all of the enlarged reservoirs Lines 12 and 13 . Then it checks if the uniformity confidence computed using Equation 10 exceeds the given threshold for every enlarged reservoir Lines 14 15 . If so for each of the adjusted reservoirs it applies the corresponding steps of the adaptive reservoir sampling algorithm see Algorithm 2 and updates r t to the current computed reservoir size r t Lines 16 25 .

The purpose of this evaluation is to empirically examine the adaptivity of the exemplary multi reservoir sampling algorithm Algorithm 3 above with regard to reservoir size and sample uniformity. Two sets of experiments were conducted. The objective of the first set of experiments was to observe how the reservoirs sizes change as data arrive. The objective of the second set of experiments was to observe the uniformity of the reservoir samples as the reservoirs sizes change.

Data Setup. We use a real data set collected from sensors deployed in a research lab over a period of about five weeks. Sensors mounted with weather boards collected timestamped topology information along with humidity temperature light and voltage values once every 31 seconds. Collection of data was done using the TinyDB in network query processing system built on a TinyOS platform. TinyDB and TinyOS are research components that evolved out of laboratories of Intel Research Berkeley and University of California Berkeley. 

The resulting data file includes a log of about 2.3 million readings collected from these sensors. The schema of records was date yyyy mm dd time hh mm ss.xxx epoch int moteid int temperature real humidity real light real voltage real . In this schema epoch was a monotonically increasing sequence number unique for each mote. Moteid ranged from 1 to 58. Data from three motes of ID 5 ID 28 and ID 57 had incomplete readings and thus were discarded. This left readings from 55 motes used in the experiments. reports the total number of readings from each mote. Temperature is in degrees Celsius. Humidity is temperature corrected relative humidity ranging from 0 to 100 . Light is in Lux. Voltage is expressed in volts ranging from 2.0 to 3.0 volts.

Algorithm setup. In Algorithm 3 the uniformity confidence threshold was set to 0.90. It is believed that this value is adequately large to constrain the frequency of adjusting the memory allocation. To check the effect of the total available memory size on the frequency of change in reservoir sizes we ranged the value of M from 1000 tuples to 5000 tuples and range the memory adjustment threshold from 0.1 to 0.5. Readings acquired for the whole first day of the experiment were used in the experiments. Data collection was done every 1 hour and accordingly report results on the change in reservoir size and sample uniformity every hour.

Change in reservoir size. shows the changes in the sizes of the 55 reservoirs. For better visibility shows the changes for 5 selected reservoirs. In the beginning i.e. by the end of the 2nd hour the total available memory is enough to store all reading from all motes and therefore the reservoir sizes increase linearly. Then the reservoir sizes started fluctuating. The fluctuations were smooth and small in the first stage from the 2nd to the 4th hour larger in the second stage from the 4th to the 21st hour and eventually diminished in the last stage after the 21st hour . This pattern of changes is attributed to the characteristics of data sets used in the experiments. In the first stage there was no tangible difference between the numbers of readings acquired by different motes. Therefore reservoir sizes stayed almost constant. In the second stage the differences started increasing and therefore the changes in reservoir sizes became more frequent and more tangible. The saturations in reservoir sizes in the last stage indicate that the number of readings acquired by each mote were large enough and therefore did not cause a change in the computed reservoir size see Equation 6 .

With a larger value of the memory adjustment threshold 0.5 shows a similar pattern except that the changes in reservoir sizes happened less frequently and saturated earlier. The reason for these observations can be easily seen from Equations 6 and 11. Results obtained for varying other parameters M and show similar patterns and are omitted due to space constraint.

Sample Uniformity. We used 2 statistics as a metric of the sample uniformity. Higher 2 indicates lower uniformity and vice versa. For each value v in a domain D 2 statistics measures the relative difference between the observed number of tuples o v and the expected number of tuples e v that contain the value v. That is 

As mentioned above this disclosure also addresses the problem of reservoir sampling over memory limited stream joins. Novel concepts directed to this problem and two algorithms for performing reservoir sampling on the join result are presented below. These algorithms are referred to herein as the reservoir join sampling RJS algorithm and the progressive reservoir join sampling PRJS algorithm. In the RJS algorithm the reservoir size is fixed. As a result the sample in the reservoir is always a uniform random sample of the join result. Therefore RJS fits those applications that may use the sample in the reservoir at any time e.g. continuous queries . This algorithm however may not accommodate a memory limited situation in which the available memory may be too small even for storing tuples in the join buffer. In such a situation it may be infeasible to allocate the already limited memory to a reservoir with an adequately large size.

The PRJS algorithm on the other hand is designed to alleviate this problem by increasing the reservoir size during the sampling process. For this the conventional reservoir sampling technique of RJS is replaced with what is referred to herein as progressive reservoir sampling. As will be seen below progressive reservoir sampling is the case of adaptive reservoir sampling see Algorithm 2 above in which the sampling reservoir size is increased during sampling. A key idea of PRJS is to exploit the property of reservoir sampling that the sampling probability keeps decreasing for each subsequent tuple. Based on this property the memory required by the join buffer keeps decreasing during the join sampling. Therefore PRJS releases the join buffer memory not needed anymore and allocates it to the reservoir.

Evidently a larger reservoir sample represents the original join result more closely. It however comes at a cost in terms of the uniformity of the sample. Once the reservoir size is increased the sample s uniformity is damaged. Besides even after the enlarged reservoir is filled again with new tuples the sample s uniformity is still not guaranteed i.e. the sample s uniformity confidence stays below 100 . See Equation 3 and accompanying description. There is thus a tradeoff that a larger increase of reservoir size leads to lower uniformity confidence after the reservoir is filled again. Therefore PRJS is suitable for those applications that can be tolerant in terms of the uniformity of the sample. Specifically it fits those applications that use the sample at a predetermined time such as applications of data collection over wireless sensor networks such as wireless sensor network of . Given such a tradeoff PRJS is designed so that it determines how much the reservoir can be increased given a sample use time and a uniformity confidence threshold.

The present inventors have performed extensive experiments to evaluate the RJS and PRJS algorithms with respect to the two competing factors size and uniformity of sample . The inventors have also compared the two algorithms in terms of the aggregation error resulting from applying AVG on the join result. The experimental results confirm understanding of the tradeoffs. The RJS and PRJS algorithms as well as a description of the experiments are presented and described below.

Prior to describing the algorithms and experiments conventional reservoir sampling and progressive reservoir sampling are briefly reviewed and uniform join sampling will be discussed as these are used in formulating the RJS and PRJS algorithms presented below. The conventional reservoir sampling algorithm is presented in the Background section above as Algorithm 1. Initially the algorithm places all tuples in the reservoir until the reservoir of size r tuples becomes full. After that each ktuple is sampled with the probability r k. A sampled tuple replaces a randomly selected tuple in the reservoir. This way the reservoir always holds a uniform random sample of all the tuples seen from the beginning.

As described above relative to Equation 3 and in connection with adaptive reservoir sampling when the size of the sample reservoir is increased i.e. the reservoir size is progressively increased the uniformity confidence UC Equation 3 will be less than 100 increases monotonously and saturates as the uniformity confidence recovery tuple count m increases. As mentioned above progressive reservoir sampling is one case of adaptive reservoir sampling Algorithm 2 above wherein the size of the reservoir is only increased. A progressive reservoir sampling algorithm is as follows 

Based on the above discussion the progressive reservoir sampling works as shown in Algorithm 4. As long as the size of the reservoir does not increase it uses the conventional reservoir sampling to sample the input stream Line 3 . Once the reservoir size increases by the algorithm computes the minimum value of m using Equation 3 that causes the UC to exceed a given threshold Line 5 . Then the algorithm flips a biased coin to decide on the number of tuples x to retain among the r tuples already in the reservoir Line 6 . The probability of choosing the value x is defined in Equation 4 above.

After that the algorithm randomly evicts r x tuples from the reservoir Line 7 and refills the remaining reservoir space with r x tuples from the arriving m tuples using the conventional reservoir sampling Line 8 . Eventually the algorithm continues sampling the input stream using the conventional reservoir sampling Line 3 as if the sample in the enlarged reservoir were a uniform random sample of the k m tuples.

Every join result tuple may be classified as either an S probe join tuple or an S probe join tuple. When a new tuple sarrives on Sand is joined with a tuple s W sis said to produce an S probe join tuple. An S probe join tuple is defined symmetrically. A tuple s Smay first produce S probe join tuples when it arrives. Then before it expires from W it may produce S probe join tuples with tuples newly arriving on S. n s is a function which returns the number of S probe join tuples produced by a tuple s Sbefore it expires from W. n s is defined symmetrically.

Tuples arrive in a data stream in a monotonically increasing order of the timestamp. In other words there is no out of order arrival. The available memory M is limited and insufficient for the join buffer to hold all tuples of the current sliding windows. It is assumed the initial reservoir size r is given. Under this join sampling processing model the present inventors have observed that as time passes memory requirement on the join buffer can be lowered and memory from the join buffer can be transferred to the reservoir. This makes the results of progressive reservoir sampling applicable to this processing model.

As will be seen in the following sections each of the new RJS and PRJS algorithms may be considered to have two phases 1 a join sampling phase and 2 a reservoir sampling phase. The sampling probabilities used in the first phase are denoted as pand the sampling probability used in the second phase are denoted as p. In the specific RJS and PRJS algorithms presented below the join sampling phase utilizes a particular uniform join sampling algorithm known as the UNIFORM algorithm. The UNIFORM algorithm Algorithm 5 appears immediately below.

The UNIFORM algorithm streams out a uniform random sample of the result of a sliding window join query in a memory limited stream environment. Algorithm 5 outlines the steps of the algorithm for one way join from Sto S. Join in the opposite from Sto S is symmetric. The basic assumption of the algorithm is that n s i 1 2 i.e. the number of S probe join tuples produced by a tuple s Sbefore it expires from W see Table 2 below is known. The algorithm works with two prediction models that provide n s 1 a frequency based model and 2 an age based model. The frequency based model assumes that given a domain D of the join attribute A for each value v D a fixed fraction f v of the tuples arriving on Sand a fixed fraction f v of the tuples arriving on Shave value v of the attribute A. The age based model assumes that for a tuple s Sthe S probe join tuples produced by ssatisfies the conditions that 1 the number of S probe join tuples produced by sis a constant independent of sand 2 out of the n s S probe join tuples of s a certain number of tuples is produced when sis between the age g 1 and g. These definitions are symmetric for a tuple s S. The choice of a predictive model is not important to the novelty of concepts disclosed herein thus without loss of generality the frequency based model is used in the rest of this disclosure.

For the frequency based model n s W f S A the join sampling probability pis computed by first obtaining the expected memory usage i.e. the expected number of tuples retained in the join buffer in terms of pand then equate this to the amount of memory available for performing the join and solving it for p. The expected memory usage of Wthus obtained as 

Given p the algorithm proceeds as shown in Algorithm 5. When a tuple sarrives on S the UNIFORM algorithm looks for every s Wsuch that s A s A Line 1 . Then it outputs s sif this sis the tuple sis waiting for the output of the next sample tuple Line 4 and then decides on the next sfor s Line 5 . Moreover once sarrives on S the UNIFORM algorithm flips a coin with bias pto decide the next S probe join tuple of s Lines 8 9 . To do that the UNIFORM algorithm picks X at random from the geometric distribution with parameter p G p . If all remaining S probe join tuples of sare rejected in the coin flips sis discarded Lines 10 12 .

Algorithm 6 applies reservoir sampling on the output of the UNIFORM algorithm. Thus it uses a fixed size reservoir and always holds a uniform random sample in the reservoir. Algorithm 6 outlines the steps of RJS. Given a fixed reservoir of size r the first r join sample tuples produced by the UNIFORM algorithm are directly placed in the reservoir Lines 3 4 . After that each join sample tuple is re sampled using reservoir sampling with a probability pso that a p p r k 1 that is p r k 1 p Line 6 .

k is an index of the original join output tuples that would be generated from the join. Since join sampling selects only a portion of them the value of k should be estimated. This estimation may be done as follows. When a tuple sproduces an S probe join tuple 1 ptuples would be generated on average from the exact join since the algorithm samples a join result tuple with probability p. Therefore k k 1 p Line 8 . This estimation process is symmetric for S probe join tuples.

Also based on the foregoing immediately following is a specific example of a PRJS algorithm Algorithm 7 .

A key idea behind PRJS and Algorithm 7 is to utilize the property of reservoir sampling that the sampling probability keeps decreasing for each subsequent tuple see Algorithm 1 Background section above . This property allows the algorithm to release memory from the join buffer and transfer it to the reservoir. However as mentioned above the benefit of increasing a reservoir size comes at a cost on the uniformity of the sample. PRJS needs to know the values of m uniformity confidence recovery tuple count and uniformity confidence threshold . Given the time left until the sample use or collection time denoted as T the number of tuples denoted as l that would be generated during T if there were no join sampling is computed as follows 

As mentioned several times above there is a tradeoff between the presented RJS and PRJS algorithms i.e. Algorithms 6 and 7 respectively. Thus in one experiment the aim was to compare these two algorithms in terms of the two traded factors 1 the achieved reservoir sample size and 2 the achieved recovered uniformity of the sample. In addition another set of experiments was performed to put the evaluations in the database context. Specifically an aggregation AVG was performed on the reservoir sample and comparison made on the aggregation errors between the two algorithms.

Algorithm setup Both window sizes Wand W were set to 500 time units and the two stream rates and were set to 1 tuple per time unit and 5 tuples per time unit respectively. Memory allocated to join buffer was 50 of the memory required for an exact result. The initial size of reservoir was 100 i.e. r 100 tuples which represented 6 of the total available memory. Both the uniformity confidence threshold and the refill confidence threshold were set to 0.90. It is believed this value is sufficiently large to constrain the increase of reservoir size in PRJS. Unless stated otherwise the results reported were obtained as an average of the results of 50 runs.

Data streams setup Stream data sets each containing tuples amounting to 10000 time units were generated. Values of join attribute in the input stream tuples were generated assuming the frequency based model as indicated above. The values were drawn from a normal distribution with mean 1000 and variance 1000. Values of aggregate attribute were drawn from a normal distribution with mean 1000 and variance 10000.

An objective of this experiment was to observe how the size of a sample in the reservoir changes size over time. shows the average sample size over time at the interval of 10 time units for both PRJS and RJS. For PRJS the sample size increased linearly until the enlarged reservoir was filled and then the increase saturated. The same happened for RJS but sample size did not ever exceed the initial reservoir size.

The purpose of this set of experiments was to test the uniformity of the sample in the reservoir. The chi squared 2 statistic was used as a metric of the sample uniformity. Higher 2 indicates lower uniformity and vice versa. The 2 statistic measures for each value v in a domain D the relative difference between the observed number of tuples o v and the expected number of tuples e v that contain the value. That is 

Since PRJS evicts some tuples from the reservoir in order to refill the reservoir with the incoming tuples the uniformity can be damaged more if there is some sort of dependence in the arrival of join attribute values on the input streams. Therefore an experiment was conducted to test the effect of the ordering of tuples in the input streams by the join attribute. For this partially sorted streams were generated. This was done by configuring the values in the domain of the attribute into a tree structure. In the tree the value in a parent node had a precedence in appearing in the input stream over the values in the children nodes. Between siblings there was no precedence conditions. The number of children of each node was fixed and was parameterized as fanout. As the value of fanout decreased the stream became more sorted. That is when fanout 1 the stream was totally sorted. The value of fanout was set to 2 3 and 4 as shown in . shows that for PRJS there was more damage on the uniformity when the degree of the input stream ordering was higher. On the other hand RJS is not sensitive for any kind of ordering in the input stream. This is evident for RJS and thus the graph is omitted.

In this set of experiments RJS and PRJS were compared in terms of the accuracy of aggregation AVG query results. The average absolute error AE at the interval of 500 time units for each algorithm is reported. Absolute error is defined as follows 

The results shown in demonstrate that right after the reservoir size increased PRJS gave a larger aggregation error but after that as the sample size increased the aggregation errors decreased. The curve of PRJS crosses over the curve of RJS even before reaching the sample use time marked as a circle on the PRJS curve . This happens because the benefit of the enlarged reservoir size dominates over the damage in the uniformity. As the uniformity recovers more the aggregation error decreases more.

It is noted that the algorithms aspects and embodiments described herein for example any one or more of Algorithms 2 7 above may be conveniently implemented using one or more machines e.g. general purpose computing devices devices incorporating application specific integrated circuits devices incorporating systems on chip etc. programmed according to the teachings of the present specification as will be apparent to those of ordinary skill in the computer arts. Appropriate software coding can readily be prepared by skilled programmers based on the teachings of the present disclosure as will be apparent to those of ordinary skill in the software art.

Such software may be a computer program product that employs one or more machine readable media and or one or more machine readable signals. A machine readable medium may be any medium that is capable of storing and or encoding a sequence of instructions for execution by a machine e.g. a general purpose computing device and that causes the machine to perform any one of the methodologies and or embodiments described herein. Examples of a machine readable medium in the form of a non volatile machine readable medium include but are not limited to a magnetic disk e.g. a conventional floppy disk a hard drive disk an optical disk e.g. a compact disk CD such as a readable writeable and or re writable CD a digital video disk DVD such as a readable writeable and or rewritable DVD a magneto optical disk a read only memory ROM device a random access memory RAM device a magnetic card an optical card a solid state memory device e.g. a flash memory an EPROM an EEPROM and any combination thereof. A machine readable medium as used herein is intended to include a single medium as well as a collection of physically separate media such as for example a collection of compact disks or one or more hard disk drives in combination with a computer memory. As those skilled in the art will readily appreciate the term non volatile as used above and in the amended claims excludes encoded signals that propagate via electromagnetic energy pressure energy or other form of energy.

Examples of a computing device include but are not limited to a computer workstation a terminal computer a server computer a handheld device e.g. tablet computer a personal digital assistant PDA a mobile telephone etc. a web appliance a network router a network switch a network bridge a computerized device such as a wireless sensor or dedicated proxy device any machine capable of executing a sequence of instructions that specify an action to be taken by that machine and any combination thereof.

Memory may include various components including but not limited to a random access read write memory component e.g a static RAM SRAM a dynamic RAM DRAM etc. a read only component and any combination thereof. In one example a basic input output system BIOS including basic routines that help to transfer information between elements within computer system such as during start up may be stored in memory . Memory may also include e.g. stored on one or more machine readable media instructions e.g. software embodying any one or more of the aspects and or methodologies of the present disclosure. In another example memory may further include any number of instruction sets including but not limited to an operating system one or more application programs other program modules program data and any combination thereof.

Computer system may also include one or more storage devices . Examples of storage devices suitable for use as any one of the storage devices include but are not limited to a hard disk drive device that reads from and or writes to a hard disk a magnetic disk drive device that reads from and or writes to a removable magnetic disk an optical disk drive device that reads from and or writes to an optical media e.g. a CD a DVD etc. a solid state memory device and any combination thereof. Each storage device may be connected to bus by an appropriate interface not shown . Example interfaces include but are not limited to Small Computer Systems Interface SCSI advanced technology attachment ATA serial ATA universal serial bus USB IEEE 13144 FIREWIRE and any combination thereof. In one example storage device may be removably interfaced with computer system e.g. via an external port connector not shown . Particularly storage device and an associated machine readable medium may provide nonvolatile and or volatile storage of machine readable instructions data structures program modules and or other data and or data storage for computer system . In one example software may reside completely or partially within machine readable medium . In another example software may reside completely or partially within processor .

In some embodiments such as a general purpose computer computer system may also include one or more input devices . In one example a user of computer system may enter commands and or other information into the computer system via one or more of the input devices . Examples of input devices that can be used as any one of input devices include but are not limited to an alpha numeric input device e.g. a keyboard a pointing device a joystick an audio input device e.g. a microphone a voice response system etc. a cursor control device e.g. a mouse a touchpad an optical scanner a video capture device e.g. a still camera a video camera touchscreen a digitizer pad and any combination thereof. Each input device may be interfaced to bus via any of a variety of interfaces not shown including but not limited to a serial interface a parallel interface a game port a Universal Serial Bus USB interface a FIREWIRE interface a direct interface to the bus a wireless interface e.g. a Bluetooth connection and any combination thereof.

Commands and or other information may be input to computer system via storage device e.g. a removable disk drive a flash drive etc. and or one or more network interface devices . A network interface device such as network interface device may be utilized for connecting computer system to one or more of a variety of networks such as network and one or more remote devices connected thereto. Examples of a network interface device include but are not limited to a network interface card a modem a wireless transceiver e.g. a Bluetooth transceiver and any combination thereof. Examples of a network include but are not limited to a wide area network e.g. the Internet an enterprise network a local area network e.g. a network associated with an office a building a campus a group of wireless sensors or other group of data streaming devices or other relatively small geographic space a telephone network a direct connection between two computing devices and any combination thereof. A network such as network may employ a wired and or a wireless mode of communication. In general any network topology may be used. Information e.g. data software etc. may be communicated to and or from computer system via the one or more network interface devices .

In some embodiments such as a general purpose computer computer system may further include a video display adapter for communicating a displayable image to a display device such as display device . Examples of a display device include but are not limited to a liquid crystal display LCD a cathode ray tube CRT a plasma display and any combination thereof. In addition to a display device a computer system may include one or more other peripheral output devices including but not limited to an audio speaker a printer and any combination thereof. Such peripheral output devices may be connected to bus via a peripheral interface . Examples of a peripheral interface include but are not limited to a serial port a USB connection a FIREWIRE connection a parallel connection and any combination thereof.

A digitizer not shown and an accompanying pen stylus if needed may be included in order to digitally capture freehand input. A pen digitizer may be separately configured or coextensive with a display area of display device . Accordingly a digitizer may be integrated with display device or may exist as a separate device overlaying or otherwise appended to the display device.

Exemplary embodiments have been disclosed above and illustrated in the accompanying drawings. It will be understood by those skilled in the art that various changes omissions and additions may be made to that which is specifically disclosed herein without departing from the spirit and scope of the present invention.

