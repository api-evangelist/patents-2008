---

title: Data gather scatterâ€”redistribution machine
abstract: A method, system, and computer program product to transfer data between two application data structures by copying a data gather scatter program (DGSP) from an exporting process address space where a first data structure is located, to a location in shared memory visible to an importing process address space; assembling a parameter set identifying the data structure; starting a data gather scatter-redistribution machine (DGS-RM) in an importing process space where a second application data structure is located; passing the first parameter set, the DGSP copy, and a second parameter set identifying a second application data structure and a second DGSP to the DGS-RM; and creating master and worker stack machines. The master stack machine identifies a contiguous chunk of the first data structure. The worker stack machine identifies contiguous chunks of the second data structure representing the same number of bytes as the contiguous chunk of the first data structure and transferring to (from) one or more identified chunks of the second data structure from (to) the single chunk of the first application data structure.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07962451&OS=07962451&RS=07962451
owner: International Business Machines Corporation
number: 07962451
owner_city: Armonk
owner_country: US
publication_date: 20080528
---
This application is based upon and claims priority from prior U.S. patent application Ser. No. 10 963 091 filed on Oct. 12 2004 now U.S. Pat. No. 7 392 256 the entire disclosure of which is herein incorporated by reference in its entirety.

This application is a continuation in part of U.S. patent application Ser. No. 09 517 167 entitled Data Gather Scatter Machine filed on Mar. 2 2000 now U.S. Pat. No. 6 820 264 the entire disclosure of which is herein incorporated by reference.

The invention relates to multiprocessor computers and more particularly to directly transferring data between discontiguous data buffers belonging to different processes.

Message Passing Interface MPI defines a standard application programming interface API for using several processes at one time to solve a single large problem or a job on a multiprocessor and often multi node computer i.e. commonly one process per CPU across 1 or more multi CPU nodes . Each job can include multiple processes. A process can also commonly be referred to as a task. Each process or task can compute independently except when it needs to exchange data with another task. The program passes the data from one task to another as a message. Examples of multiprocessor computers are e.g. an IBM eServer Cluster 1600 available from IBM Corporation Armonk N.Y. and supercomputers available from Cray Silicon Graphics Hewlett Packard and the like.

The primary purpose of a Data Gather Scatter Program Data Gather Scatter Machine hereinafter referred to as DGSP DGSM or DGSM is to provide an efficient portable solution for handling MPI datatypes in MPI operations. DGSM is described in the co pending commonly assigned herewith U.S. patent application Ser. No. 09 517 167 entitled Data Gather Scatter Machine filed on Mar. 2 2000 the entire disclosure of which is herein incorporated by reference. It is important to note that any API that allows data of arbitrarily complex discontiguity to be described for the purpose of copying it could also use the DGSP DGSM techniques. All MPI message passing is performance sensitive so efficient treatment of MPI datatypes is crucial. Some MPI operations such as MPI 1 sided and MPI IO require that the representation of a datatype defined by the application at one task of a MPI job be interpreted at another task. Thus portability of the representation is also required.

The DGSP DGSM mechanism involves compiling a low level program DGSP for each MPI datatype the application constructs. This DGSP can be used in the task where it is created or delivered to another task. When delivered to another task that other task can run the DGSP. A DGSP is run on a stack machine DGSM implemented in software to interpret the layout of a datatype and handle the data transfer.

The DGSP is a compact portable and general representation of any MPI datatype consisting of five basic DGSM instructions COPY MCOPY GOSUB ITERATE and CONTROL. Since the underlying communication subsystem for which the original DGSM was developed provides a buffered contiguous data stream between tasks a basic DGSM assumes that at least one of the two buffers it performs on is contiguous. One of these buffers the one which can be discontiguous is normally an application defined data structure rather than a usual communication buffer. DGSM executes a gather from a discontiguous buffer an application data structure to a contiguous staging buffer or a scatter from a contiguous staging buffer to an application data structure.

In certain shared memory optimizations the data of a data structure belonging to one task defined by a datatype created at that task needs to be copied into a data structure at some other task on the same OSI Operating System Image . The datatype defining the data structure belonging to the second task has been created only at the second task. Each datatype is available only within the address space of the task where it was created. The datatype created by each task will have been compiled into a DGSP also available only within the address space of the task where it was created. A common solution for a data transfer between two discontiguous application data structures having non uniform data layouts is to stage the transfer through a contiguous intermediate buffer. When the data structures belong to different tasks the intermediate buffer must be in shared memory which is addressable by both tasks.

In this approach data is first gathered from the source application buffer into the intermediate buffer and then scattered from the intermediate buffer to the destination application buffer. The DGSP DGSM mechanism can be applied to both the gather and the scatter. Since the intermediate buffer size is always limited buffer reuse has to be supported in a fill drain cycle. The basic DGSM is designed to be activated repeatedly each time being told how many bytes to process. The state of the machine at the end of one activation is retained so the next activation can resume with respect to the application buffer where the previous activation left off. To allow reuse of a single staging buffer or of a pool of staging buffers each activation of the DGSM is provided with a new pointer to space in the intermediate buffer along with the number of bytes to be copied. At the data source side one DGSM gathers the requested number of bytes into the intermediate buffer. Then at the data destination side another DGSM scatters a specified number of bytes from the intermediate buffer to a discontiguous application data structure. One DGSM runs in each process and each DGSM depends on the DGSP and other information created within the address where that DGSM runs. A few variables in shared memory allow the gather DGSP to tell the scatter DGSP that the staging buffer has been filled so the scatter can begin. Later the scatter DGSP tells the gather DGSP that all data from the staging buffer has been scattered and the buffer may be overwritten with more data. The cycle can continue until all data has been transferred via the staging buffer. There is no distinction between a DGSP to be interpreted for source gather or for destination scatter DGSM runs. The DGSP machine instruction COPY differs for gather or scatter only in which direction it copies the specified number of bytes.

Since each byte is copied twice in going from the source to the destination this solution is inefficient in terms of time taken for data transfer and CPU demand especially when the size of the data to be transferred is large. More severely any memory system bandwidth bottleneck is double stressed by this two stage approach. Therefore a need exists to overcome the problems with the prior art as discussed above.

An embodiment of the present invention is directed to a method system and computer program product for transferring data between two application data structures. The method called a cross memory attach function includes copying a first data gather scatter program DGSP from an exporting process address space where a first application data structure is located to a location in shared memory that is visible to an importing process address space assembling a first parameter set in the shared memory location that identifies the first application data structure in the exporting process address space starting a data gather scatter redistribution machine DGS RM in an importing process space where a second application data structure is located passing the first parameter set and the copy of the first DGSP to the DGS RM both the first parameter set and copy having been copied to shared memory and a second parameter set identifying the second application data structure and a second DGSP both local to the importing process space and creating a master stack machine and a worker stack machine.

The method has a pull mode and a push mode. In the pull mode the master stack machine running under control of the DGSP previously copied from the exporting process space identifies a contiguous chunk of the first application data structure in the exporting process address space. Next the worker stack machine running under control of the DGSP in the importing process address space identifies as many contiguous chunks of the second application data structure in the importing process address space as needed to receive all bytes of the contiguous chunk of the first application data structure and then copies pulls the bytes of the identified chunk of the first application data structure from the first application data structure to the as many as needed identified contiguous chunks of the second application data structure.

In the push mode the master stack machine running under control of the DGSP previously copied from the exporting process space identifies a contiguous chunk of the first application data structure in the exporting process address space. Next the worker stack machine running under control of the DGSP in the importing process address space identifies as many contiguous chunks of the second application data structure in the importing process address space as needed to supply the bytes needed for the contiguous chunk of the first application data structure and then copies pushes the bytes for the identified chunk of the first application data structure to the first application data structure from the as many as needed identified contiguous chunks of the second application data structure memory.

The master stack machine identifies the contiguous chunk of the first application data structure step by attaching a memory segment where a current chunk of the first application data structure is located calculating a relocation constant by subtracting a segment address of the first application data structure in the exporting process space from a segment address at which the attached memory segment is mapped in the importing space and determining if the contiguous chunk of the first application data structure crosses a segment boundary.

If the contiguous chunk of the first application data structure does not cross a segment boundary then the method processes the currently attached chunk and proceeds to the next chunk of the first application data structure to be copied to or from.

If the contiguous chunk of the first data structure does cross a segment boundary then the method determines the length of a contiguous fragment of the current chuck of the first application data structure contained within the current memory segment processes the current fragment detaches the attached memory segment attaches a subsequent segment and calculates a new relocation constant.

The method works for both contiguous and discontiguous data structures. The application data structures may be located within the same address space or different address spaces.

The method works in conjunction with prior art methods to determine the most efficient manner of transferring data by caching the average length of contiguous data chunks represented by a single interpretation of a data gather scatter program DGSP with the DGSP as a granularity metadata and determines if the granularity metadata exceeds a predetermined granularity threshold. If the granularity metadata is less than or equal to the predetermined granularity threshold then the method allocates an intermediate buffer and runs two data gather scatter machines DSGM the first DGSM to copy data from a data source application data structure to the intermediate buffer and the second DGSM to copy data from the intermediate buffer to a data destination application data structure.

If the granularity metadata exceeds the predetermined granularity threshold then the method determines if the total amount of data to be transferred exceeds a predetermined data threshold. If the total amount of data to be transferred is less than the predetermined data threshold then the method again allocates an intermediate buffer and runs two data gather scatter machines DSGM the first DGSM to copy data from a data source application data structure to the intermediate buffer and the second DGSM to copy data from the intermediate buffer to a data destination application data structure. But if the total amount of data to be transferred is exceeds the predetermined data threshold then the cross memory attach function is performed.

An exemplary embodiment of the invention is discussed in detail below. While specific implementations are discussed it should be understood that this is done for illustration purposes only. A person skilled in the relevant art will recognize that other components and configurations may be used without parting from the spirit and scope of the invention.

Data Gather Scatter Redistributed Machine DGS RM is an extension to DGSM for support of efficient data transfer between two non uniform discontiguous application data structures. There is a potential for using this DGS RM in any situation where possibly discontiguous data residing in one address space needs to be copied to another address space where the destination layout is also potentially discontiguous. The described embodiment involves copying data from one address space to another however a simplified DGS RM could be applied to copy from one discontiguous data buffer to a second discontiguous data buffer in a single address space. MPI creates this need for discontiguous to discontiguous copying but other multi process jobs could also. The compilation of a DGSP discussed in the prior cross referenced U.S. patent application Ser. No. 09 517 167 is more closely tied to MPI than is the data transfer engine DGSM of the prior application or DGS RM.

In one embodiment the mechanisms of the present invention are incorporated and used in a distributed computing environment such as the one depicted in . An exemplary distributed computing environment includes for instance a plurality of frames coupled to one another via a plurality of LAN gates . Frames and LAN gates are described in detail below.

In one example distributed computing environment includes eight 8 frames each of which includes a plurality of Systematic Multi Processor SMP nodes . Since an SMP Symmetric Multi Processor node normally runs all Central Processing Units CPUs under a single OSI the terms OSI and SMP can be considered interchangeable in this discussion. In one instance each frame includes sixteen 16 processing nodes. Each processing node is for instance an RS 6000 or RISC System 6000 computer running AIX a UNIX based operating system. Each processing node within a frame is coupled to the other processing nodes of the frame via for example an internal LAN connection. Additionally each frame is coupled to the other frames via LAN gates .

As examples each LAN gate includes either an RS 6000 computer any computer network connection to the LAN or a network router. However these are only examples. It will be apparent to those skilled in the relevant art that there are other types of LAN gates and that other mechanisms can also be used to couple the frames to one another.

In addition to the above the distributed computing environment of is only one example. It is possible to have more or less than eight frames or more or less than sixteen nodes per frame. Further the processing nodes do not have to be RS 6000 computers running AIX. Some or all of the processing nodes can include different types of computers and or different operating systems. All of these variations are considered a part of the claimed invention.

In an embodiment of the present invention in shared memory optimizations it is often necessary for a datatype defined at one task or process to be interpreted at some other tasks on the same OSI Operating System Image . These shared memory optimizations exploit a dynamic cross memory attach mechanism to avoid half the memory copies. The dynamic cross memory attach mechanism enables one task of a parallel job to register a message source or destination memory range representing a message buffer i.e. containing an application data structure as a shared memory region which can be attached by other tasks running on the same OSI. The attached memory is mapped into the address space of the attaching task. Since these message buffers are normally working data structures in the application their location and layout is determined by the needs of the application rather than by a communications subsystem. Once an export and attach is set up data can be transferred directly from one task s message buffer to another task s. The MPI standard allows for any given data transfer in collective communication a different data layout at different tasks as long as the message type signatures are identical e.g. a message of 100 contiguous integers and one of 100 discontiguous integers have identical type signatures because each represents 100 integers . Data layout in the registered region could be different from the data layouts in the communication buffers of those tasks who attach the registered region and both could be discontiguous. The principles of the present invention extend the basic DGSM concepts to support data transfer between two non uniform discontiguous application data structures both within a single address space and via cross memory attach.

Referring initially to a Task L running in address space and a Task R running in address space each owns an application data structure respectively. A DGS RM runs in a single process space or task space so when there are two tasks involved one side of the data transfer is chosen to run the DGS RM. This means one data gather scatter program DGSP originates in an exporting task space i.e. the space registering the memory object remote from the DGS RM and the DGSP is to be first delivered to the task which is to run the DGS RM by copying it to a shared memory which is visible to the task which will run DGS RM . The compactness and portability of a DGSP makes this practical.

The task space in which DGS RM runs or the attaching task space is referenced as Task L for local and the task where the remote application data structure and DGSP reside is Task R for remote . When there is one source data structure and one destination data structure the decision as to which process is to run the DGS RM is arbitrary. In one example Task L is selected to run DGS RM and Task R sets up for that run by exporting the address range i.e. memory object containing its application data structure and copying its DGSP as well as information needed for attaching that exported range to a control data structure located in shared memory where DGS RM can access it.

As part of the action of exporting an address range a process returns a cross memory handle identifier which another process can use to attach that memory. This handle is among the information transferred in the shared memory control space . Referring now to when the DGS RM is to start in process space L it will input a parameter set handle R base R buffer R count R DGSP representing the remote application data structure delivered via the control data structure in the shared memory segment and a second locally available parameter set L buffer L count L DGSP representing the local data structure . The DGS RM routine will then create and run two stack machines one for each DGSP. Of the two machines one has a role as master machine and the other as the worker machine . In one embodiment of this cross memory implementation the master DGSM interprets R DGSP while the worker DGSM interprets L DGSP . The reverse convention in which the worker processes R DGSP is also workable but this choice to have R DGSP always processed by the master makes the management of segment boundaries to be discussed below easier to implement.

For convenience in the description of push and pull the operation is viewed from the vantage of the local process running the DGS RM rather than from the remote process exporting a memory object. However there are no limitations in defining push pull process from the vantage point of the remote process . Thus there are two modes of operation push and pull. This classification is based on whether the data is pulled from the remote process exported memory or pushed into it. In the pull mode the master machine is for the gather side data handling and worker machine is for the scatter side while the other way around is the push mode. By incorporating two DGS RM running modes the decision about where the DGS RM runs is independent of the direction in which the data is to flow. This allows more versatility in choosing the specific side of the transfer on which to run the DGS RM allowing for consideration of other factors in addition to the data movement direction. In one embodiment of the cross memory attach mechanism of the present invention data transfer is carried forward i.e. the DGS RM is run by the task which attaches the registered shared memory region. The DGS RM runs in the importing task memory space so that it is able to access the memory space of both the importing task and the exporting task. This attaching task can be either the source of the data or the destination. A single export can include a number of memory segments but the attach is performed one segment at a time so special treatment is needed for application data structures that cross a segment boundary in the registered shared memory region. In a present embodiment implementing this logic in the master machine is more convenient.

The worker stack machine within DGS RM is similar to a basic DGSM. Each activation of the worker machine by the master is provided an address of an accessible contiguous chunk of remote memory and the number of bytes to be copied into that segment in the push mode or to be copied out of that segment in the pull mode. Like the difference between the gather and scatter in running DGSM the difference between the DGS RM worker machine running in the two modes lies in the handling of the COPY instruction in which direction it copies the specified number of bytes.

Note that a chunk of memory is any contiguous portion of memory defined by the layout of the application data structure regardless of length or segment boundaries i.e. the size of one chunk may or may not be the same size as another chunk and may not necessarily be contained within a single memory segment . For purposes of this application the words chunk block and section may be used interchangeably even though there may be slightly different connotations in meaning. Further the word fragment is used to refer to any contiguous portion of a chunk contained entirely within one segment.

The master stack machine is a variant of a DGSM. In the basic DGSM a COPY instruction represents a contiguous chunk of an application data structure and enough bytes are to be copied from a staging or other contiguous buffer to fill it for scatter or to drain it for gather. In DGS RM a master machine COPY instruction still represents a contiguous chunk of an application data structure to be filled drained. But rather than transferring the contiguous chunk by a memory copy the COPY instruction will invoke the worker machine to process the data.

An operational flow diagram of the cross memory attach function is depicted in . First a DGSP is originated in the remote process space at step . Next this DGSP is delivered at step from the remote process space to shared memory . At step the address range of the remote application data structure and the above mentioned related information handle are also exported to a control data structure within the shared memory . Next the DGS RM is started in the local process space at step . The DGS RM accepts the remote parameter set from the control data structure as well as a local parameter set representing the local data structure at step . Next at step the DGS RM creates a master stack machine and a worker stack machine and then determines the operating mode pull or push at step . If the operating mode is push the DGS RM will perform the push function shown in . If the operating mode is pull the DGS RM will perform the pull function shown in .

In the pull mode which is described in detail in and illustrated in the master stack machine running under the control of the remote DGSP identifies a contiguous chunk of data in the exported application data structure and attaches the segment in which it starts at step . The specific steps for performing the attach function step will be discussed in more detail later. Note in solid arrows represent data flow and dashed arrows represent control from master machine to worker machine . Two consecutive invocations of the worker machine are shown for clarity. The state cache tracks the current iteration of both the master and worker machines . The master stack machine identifies the contiguous chunk of the remote application data structure to the worker machine at step . The worker stack machine running under the control of the local DGSP delivers these identified bytes of data to as many locations in the local application data structure as required. clearly shows benefits brought by the DGS RM approach data is copied directly from the source to the destination buffer without the need for an intermediate buffer hence the memory bandwidth requirement alleviated and CPU overhead is reduced.

The push mode operates similarly to the pull mode . In this case the master stack machine running under control of R DGSP identifies a contiguous chunk of available memory in the exported application data structure and attaches the segment in which it starts at step . Then at step the worker machine under L DGSP control identifies contiguous bytes of data in the local application data structure and pushes these bytes to the contiguous memory in the remote application data structure that the master machine has identified at step .

Recall that a DGSP is compiled before any data transfer and is always interpreted in relation to a data object address. That is a DGSP represents the pattern of data in an application data structure but any particular application is likely to use the same pattern for more than one object. For example as shown in in a 10 10 matrix of 4 byte integers with elements of a column stored adjacent in memory integers that represent a row will be one column size or 40 bytes apart. In this example an MPI Datatype which can be compiled to a DGSP describing a row as every 10th integer beginning with offset zero can be used to gather scatter any of the 10 rows by using the address of the first integer in the row as the row s address. The DGSP does not need to change. The user function call on the exporting side of a DGS RM supplies a parameter triple object address count datatype while the DGS RM itself runs in another address space.

For a basic DGSM running the example MPI Datatype DGSP the address of each integer in the row must be calculated in turn by adding each offset computed from interpreting the DGSP to the object address. If A is the address of the row the address sequence generated by the basic DGSM would be A 0 A 40 A 80 . . . A 360 . The machine memory architecture for which this implementation of DGS RM is developed is segmented. Each segment is 256 MB 2 28 bytes so the low 28 bits of an address represent an offset within a segment and the remaining higher order bits represent a segment identifier.

Within any process the concatenation of segment ID and segment offset produces a flat virtual address space. When an exported memory range of one address space containing an application data structure is to be attached into the import side address space only one memory segment at a time is attached and the address at which it is mapped in the attaching address space is unlikely to be the address at which it is mapped in the exporting side address space. The master machine of the DGS RM is processing a data structure the exporting address space where segment boundaries must be managed explicitly while the DGS RM runs in and process a data structure in the attaching address space where the address space can be treated as flat.

Referring to one embodiment of the logic associated with the attach sequence performed by master machine is discussed in detail. Initially a variable referred to as identified which is used in the attach processing is set to first. At DGS RM startup there is no segment attached so master machine attaches the exported segment where the identified e.g. first chunk of the application data structure begins STEP . As an example assume the address of this first chunk is very near the end of segment three of the exporting task s virtual memory so nine chunks are in segment three and the last chunk is in segment four. Assume the operating system maps the first attach to segment seven of the attaching side address space. To allow the DGS RM master machine to run as if it were processing within the flat virtual address space in which the DGSP and remote application data structure reside a relocation constant C is calculated at attach time by subtracting the segment address in the exporting address space R base from the address at which the segment maps in the attaching space a value returned by the function call to do the attach STEP . In this example C for the first attach is 0 40000000 and the master machine generates address sequence A C 0 A C 40 . . . A C 360 .

Thereafter the master machine calls the worker machine to copy push or pull the data based on as much of the section identified by the master that is in the current segment STEP .

A determination is then made as to whether the data crosses the segment boundary INQUIRY . If the exported data object is contained within a segment then a further inquiry is made as to whether there is another section to be processed INQUIRY . If not then processing is complete. However if there is another section of the application data structure master machine identifies the section STEP . Then a determination is made as to whether the new section begins in the currently attached segment INQUIRY . If so processing continues with STEP . Otherwise processing continues at STEP .

Returning to INQUIRY if the exported data object spreads across a segment boundary master machine detaches the current segment STEP attaches the segment in which the next chunk is found and calculates a new relocation constant STEP . For the above example if the operating system maps the newly attached segment segment four in the exporting address space again to segment seven in the attaching address space the new relocation constant C becomes 0 30000000 and the location of the last chunk is still A C 360 . The modification of C hides the complexity of segment mapping from the rest of the DGS RM master machine. In the rare case where a single chunk of a data object lies across one or more segment boundaries the DGS RM simply calls the worker machine to process that part of the chunk in the current attached segment replaces the current attach with the next segment containing the continuation of the chunk and has the worker process that continuation before going on to identify another chunk.

Subsequent to attaching the next segment and calculating a new relocation constant the worker machine copies as much of the uncompleted section as in the newly attached segment STEP and processing continues with INQUIRY .

The concepts for applying DGS RM to a non segmented addressing system are simplifications of the described technique and are equally claimed as invention.

A simplification of DGS RM that omits attach detach and uses zero as a relocation constant can be used to copy data from one application data structure to another when both data structures are in the same address space. The MPI standard allows a process to send data to itself with one data object as source and another as destination and this simplified DGS RM can be used to provide send to self when at least one object is discontiguous.

There are cases where the DGSM approach based on a contiguous intermediate buffer is more efficient than the cross memory attach function and being able to recognize those cases is useful. Note that running DGS RM at least for cross memory attach incurs extra setup and tear down costs compared to a data transfer running two DGSMs doing fill drain on a pre allocated intermediate buffer. An activation of the worker machine to copy may be more expensive for a small chunk than the additional copy it saves. If the amount of data in the transfer is small or the master machine side data is fine grained it is possible that the extra overhead exceeds the benefit of avoiding one copy. The granularity concern is due to overhead per contiguous chunk on small chunks vs. savings per byte in avoiding double copy. The message size threshold is due to overhead in setting up and taking down the cross memory attach vs. saving per byte in avoiding double copy. In those cases the DGSM approach should be used.

To recognize data patterns for which the DGSM staging buffer approach should be used DGSP compilation is enhanced to gather information to support a heuristic in the DGS RM approach as illustrated in . While compiling a DGSP at step the compiler routine keeps track of the number of contiguous data chunks a single interpretation of that DGSP will represent. Together with the amount of data one interpretation of the DGSP transfers the average length of each contiguous data chunk is calculated at step . This granularity metadata is cached with the DGSP at step and checked during data transfer at step . The DGS RM compares this granularity metadata to a predetermined threshold at step . If the buffer is recognized to be fine grained average length of each contiguous data chunk on the master side is less than a predetermined threshold at step the DGS RM at step will allocate an intermediate buffer and run two DGSM machines to gather and scatter data as shown in the Prior Art method of . Otherwise the DGS RM compares the total amount of data to a second predetermined threshold at step . If the total amount of data is small at step again the DGS RM at step will allocate an intermediate buffer and run two DGSM machines to gather and scatter data. If the granularity is not too small and there is a sufficient total amount of data the DGS RM enters the cross memory attach sequence at step See and creates the master and worker machines . The thresholds are tunable based on platform independent factors such as message size and platform dependent characters such as memory copy bandwidth cross memory setup teardown cost and the cost of each activation of the worker machine the cost of the subroutine call .

One of the major benefits brought by the cross memory attach mechanism is the minimized number of memory copies. To an application time and system resource devoted to communication is just overhead and reducing that overhead is valuable. Depending on the solution the new cross memory attach mechanism solution could carry greater overhead than the DGSM solution therefore the heuristic approach in choosing between the two methods offers great benefit.

In an example embodiment of the present invention the platform can be any computer on which multiple processes each running a process with its own address space but under a single OSI which supports mapping of address ranges between distinct address spaces can be run to exploit multiple processors which may not have access to a single common address space. It will be apparent to those skilled in the art that implementation is not limited to such an exemplary platform.

While various embodiments of the present invention have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of the present invention should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

