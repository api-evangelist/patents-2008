---

title: Zero downtime mechanism for software upgrade of a distributed computer system
abstract: In a distributed computer system with cluster architecture, a number of service requests are redirected from a first instance of the cluster to a second instance of the cluster for execution. A software patch is applied to one or more software components running on an application server node of the first instance of the cluster. A number of service requests are redirected from the second instance of the cluster to the first instance of the cluster for execution. The software patch is applied on the one or more software components running on an application server node of the second instance of the cluster. A number of new service requests are directed to the upgraded second instance of the cluster for processing.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09229707&OS=09229707&RS=09229707
owner: SAP SE
number: 09229707
owner_city: Walldorf
owner_country: DE
publication_date: 20081218
---
The field of the invention relates generally to electronic data processing and distributed computer systems. More specifically a system and a method are described for applying software upgrades to a distributed computer system with no downtime.

Distributed computer systems typically include a number of separate hardware and software nodes connected by a network. Each separate node runs software to process various operations as required by the users of the system. Usually a number of nodes of a distributed computer system execute user requests in parallel. Such architecture gives several advantages of the distributed computer systems over the standalone computer systems. One of the advantages is continuity of operations or resilience. If one of the nodes fails the user requests are handled by the rest of the nodes of the distributed computer system. Another advantage is scalability. The number of nodes of a distributed computer system could be easily increased or decreased as required by the operative load of the system in different periods.

The resilience and scalability of distributed computer systems makes them very popular for providing various enterprise services. Distributed computer systems are also applied for running mission critical business applications. In recent years enterprise services and all online computer services in general have become an area of high competition. Accordingly the requirements for the operability of the computer systems are very strong especially with respect to continuity of operations.

Distributed computer systems as any other computer system exit operational mode in the periods for installing or applying software upgrades. During its lifecycle a distributed computer system is regularly upgraded for multiple reasons e.g. found bugs inefficient processing statutory changes etc. The downtime that is caused by the installation of software harms user satisfaction especially for mission critical enterprise applications. On the other hand prolonging the periods between software upgrades could raise issues with the functionality of a mission critical system.

The increasing complexity of the computer systems require shorter periods between upgrades. On the other hand the competition and the growing user demands require minimizing downtime periods. However there is still no robust and universal solution that allows installation of software upgrades on distributed systems with no downtime.

A system and a method for installing software upgrades in a distributed computer system with no downtime are described. In a distributed computer system with cluster architecture a number of service requests are redirected from a first instance of the cluster to a second instance of the cluster for execution. An execution in progress of a service request is also redirected from the first instance of the cluster to the second instance of the cluster. The execution in progress proceeds from a state that is saved on a public store by the first instance of the cluster. A software patch is applied to one or more software components running on application server nodes of the first instance of the cluster.

A number of service requests are redirected from the second instance of the cluster to the first instance of the cluster for execution. An execution in progress of a service request is also redirected from the second instance of the cluster to the upgraded first instance of the cluster. The execution in progress proceeds from a state that is saved on the public store by the second instance of the cluster. The software patch is applied on the one or more software components running on an application server node of the second instance of the cluster. A number of new service requests are directed to the upgraded second instance of the cluster for processing.

Distributed computer system architecture typically includes a number of separate server nodes running a similar set of software components to execute various user service requests. In this document the term software component means software code executed on an application server. In this document the term service request means a way by which a user accesses an application running on a distributed computer system. With service requests the user induces the application to perform an action e.g. to provide an enterprise service to process input data to send back output date etc. In this document a service request may represent a set of service requests from a single user.

The separate server nodes usually communicate with each other by means of a computer network. An example for such distributed computer system architecture is computer cluster. There are different types of computer cluster layouts. Generally computer cluster includes a number of application servers with similar or identical configuration working in parallel in several cluster instances. The incoming user service requests are distributed between the application servers in accordance with predefined rules e.g. load balancing user sessions etc.

In order to build a computer cluster appropriate hardware and software are required. From software perspective the hardware components in a cluster have to run operating system that enable clustering. Examples of such operating systems are Windows Server family developed by Microsoft Corp. and the open software Linux server OS. On a higher level the system platforms that are used for developing and running the necessary computer applications have to support clustering as well.

The system platforms utilize the capabilities of the operating systems to build cluster execution environment for the computer applications. An example for such a system platform is Java Enterprise Edition Java EE system platform. Java EE is the name for a bundle of related programs for developing and running application programs written in the Java programming language. It was initially presented by Sun Microsystems Inc. The platform is not specific to any one processor or operating system. Java EE specification had become a publically recognized standard under which various vendors provide proprietary solutions for building application servers and clusters respectively.

System further includes a plurality of cluster instances . In only one of cluster instances is illustrated in detail. All cluster instances represent a single hardware and each of them has similar structure. There are at least two cluster instances in a cluster. Dispatcher forwards the user requests to an appropriate cluster instance . Cluster instances store common data in public store . For example the binary data of a running enterprise software application is stored in public store . The binary data provides program code and related information necessary for execution of the application. In this document public store means a store that is shared among all application servers of cluster instances .

Central services instance is an instance or a module in system that handles the communication and synchronization between cluster instances . Central services instance is responsible for lock administration of shared resources message exchange and load balancing within the cluster. With the help of central services instance a software application can lock and release shared resources and objects. Central services instance may maintain a table to store lock requests and lock statuses.

Each cluster instance runs at least one application server AS illustrated with node . In another embodiment of the invention cluster instance is AS and nodes are different server processes e.g. Java virtual machines JVMs . Each cluster instance further includes control framework . One of the main purposes of control framework is to start stop and monitor the performance of the modules of cluster instance .

Communicator is another module of the cluster instance which handles the incoming service requests. Communicator forwards the requests to available nodes for processing. Central services instance may provide information to communicator about the availability of application server nodes . In an embodiment of the invention communicator reads service requests from a stack or a queue. Then communicator decides which node is available to handle the request and sends the requests accordingly. Alternatively when one of nodes has sufficient resources to consume a request it may take the request process it and write a result back into communicator to be returned to the originator of the request through dispatcher .

Cluster instance further includes processor and display module . Processor executes all computer instructions of cluster instance as required by different modules. Display renders user interface for monitoring and administrating cluster instance by a user with sufficient privileges.

In one embodiment of the invention each node is a Java EE compliant application server AS . The AS includes components on three logical levels. On the lowest level are included low level sub systems that provide functions such as class loading cluster communication persistent configuration data management etc. On the next level are included components that provide various runtime functionality and application programming interfaces APIs . Such components are services libraries and interfaces. Third level encompasses the applications that are deployed on the AS.

Service manager and session manager are low level components from the lowest level of AS. Service manager is responsible for the lifecycle of AS components on the middle level. Service manager acts as a container of a number of interfaces libraries and services . Interfaces libraries and services ensure running environment for processing user service requests. Deploy controller and deploy service are special services responsible for software installations upgrades and de installations. The necessary software changes are communicated by patch tool to deploy controller through a deploy controller API. From there the changes to third level software components e.g. applications are applied by deploy service . The changes to second level components e.g. interfaces libraries and services are applied by service manager .

Session manager controls user sessions in node . Session manager also enables session failover safety mechanism. Sessions are used to keep the state of a user accessing an application within one or more service requests. An active user session is a session which is currently bound with a request that is processed by node while an inactive user session is currently not bound with a request. User sessions may be stored inside memory space of node . User session may also be stored in public store region which is not damaged if node stops unexpectedly e.g. for applying a software upgrade. Thus the user sessions stored in public store remain unaffected and they can be mapped to another node and even to another cluster instance. Thus if an execution of a request is in progress on a server node and the sever node fails or stops the execution of the request may proceed on another node of another cluster instance from the state stored in the corresponding user session on public store by session manager .

At block an original installation of the identified software components is ensured. In this document the term original installation means an installation of the version of the software components that is running before the upgrade. The original installation is required in a case the upgrade is unsuccessful and the software components have to be rolled back to their version before upgrade. At block an additional precaution measure is taken by making a backup of a cluster instance before the upgrade. The backup may be performed by a separate patch tool connected to the distributed system at block .

Once all precaution measures are taken at block the implementation of patch upgrade starts with redirecting the incoming service requests from one of the instances to another cluster instance for execution. The distributed system or the cluster could have an unlimited number of instances. For the purposes of this document it is sufficient to illustrate the invention with a distributed computer system that has two instances first and second. It is obvious for one of ordinary skill in the art to see that the same approach could be applied for upgrading distributed systems with more than two active instances.

When the incoming requests are redirected on the first instance of the cluster may be executed one or more previously received service requests. For such executions in progress instead of failing the corresponding requests a failover mechanism is triggered to continue the executions on another instance. At block the executions in progress are redirected to the second instance of the cluster. As it was explained earlier in paragraph 0031 with respect to user sessions with the states of the executions is in progress are stored in a public store area. The module responsible for dispatching the requests between cluster instances assigns the requests which execution is in progress to another active instance of the cluster e.g. to the second instance. The second instance of the cluster reads the state of the execution from the public store and proceeds from there.

At block a patch tool applies the software patch to the application server of the first instance of the cluster. More specifically the patch tool may use the standard deployment mechanism of application servers running in the first instance. The upgrades or patches to the identified software components are applied to binary data of the components in the public store at block . In an embodiment of the invention the patch may be applied to a separate copy of the binary data of the software components in the public store e.g. a different database schema. Alternatively the changes could be applied directly the stored binary data of the components. The changes of the binary data stored in the public store do not interfere with the previous version of the binary data stored in the local file systems of the application server nodes of the cluster. At block the binary data of the in the local file systems of all application server nodes of the first instance are synchronized with the patched binary data in the public store.

At block the upgraded instance of the cluster is tested. In order to perform the test a number of test requests may be routed to the upgraded instance. The test requests may be sent from a specific user or from a specific location. The dispatch module of the cluster or the communicator module of the first instance depending on the cluster configuration could recognize such requests and submit them for execution by the upgraded instance. The result of the executed test requests is examined at block . In case of negative result the applied software upgrade or patch is rolled back at block . At block the upgrade process ends unsuccessfully. If the result is positive the process continues at block .

In one embodiment of the invention it is possible the upgraded first instance of the cluster to be set in productive mode before redirecting the service requests from the second instance of the cluster. In this case the two instances of the cluster may work in parallel running different versions of the upgraded software components. Such parallel running requires compatibility between the versions of the upgraded software. The compatibility requirements include backward and forward compatible data exchange channels e.g. protocols between the instances and the structure of the exchanged data e.g. messages using these channels. The original and the patched versions of the software components will run and exchange data between the different instances within the cluster. Compatibility requirements may also be effected by the structure of the messages that are exchanged between the instances of the cluster and a central resource or control module. Further the compatibility requirements may be effected by the structure of a shared database where software components store data. Such compatibility requirements could make an upgrade far more complex than necessary.

When the two instances are not allowed to work in parallel with different versions of the upgraded software components the compatibility requirements may be far less restrictive. In this case compatibility requirements would mainly concern the synchronization of the user sessions stored in the public store. The upgraded cluster instance should be able to read the state of the requests which execution is in progress and to continue their execution at block . Regardless whether the two instances operate in parallel with different versions of the upgraded software components the distributed system continues operations without downtime.

At block the binary data in the local file systems of all application server nodes of the second instance are synchronized with the patched binary data stored in the public store for the patched software components. At block a feedback from the synchronization is examined. In a case of negative result an error is reported to patch tool the reasons for failure are analyzed and the upgrade of the second instance continues at block . Alternatively the whole cluster upgrade could be rolled back at block and the process ends at block as illustrated with the dotted line in . The decision for a rollback may be automatic or may depend from a system administrator.

If the examination of the feedback at block shows positive result the service requests are executed correctly and the upgraded instance is reconnected to the cluster at block . With this the rolling patch is accomplished with zero downtime and the upgrade process ends at block .

Distributed system includes two cluster instances illustrated with modules and . In each cluster instance a communicator or Internet communication manager ICM module and a plurality of application servers also called Web application servers WebASs run. ICM module receives or gets the requests from web dispatcher and forwards them to an appropriate WebAS depending on overall load. Another criteria for routing a user request to a specific WebAS is the distribution of active user sessions. When a set of user requests are processed in stateful mode they have to be routed to the same WebAS.

In in distributed system in addition to the standard modules patch tool module is added. Patch tool is connected to cluster instance that will be upgraded first. The actions that patch tool performs during the software upgrade are initiated and managed by administrator . Alternatively the software upgrade may be organized in a manner that does not require initiating the tasks by administrator . As patch tool may be used Java Support Package Manager JSPM tool. When connected to cluster instance patch tool makes back up copy of the cluster configuration and data directed by administrator as illustrated in .

In one embodiment of the invention patch tool is used to identify the software components that have to be upgraded and that are applicable for the rolling patch upgrade approach. Patch tool may also be used for various user notifications regarding the patching process. Another important function that patch tool provides is checking the number of active instances of the distributed computer system. Rolling patch approach with zero downtime is possible only when there are at least two active cluster instances. Patch tool should have access to the original installations of the software components involved in the upgrade. When an upgrade of a cluster instance fails patch tool rolls back the patching.

The ICM of cluster instance should be able to redirect all productive user service requests from instance to instance which is the other active instance of distributed system . As illustrated on ICM redirects the service requests which execution was not finalized at instance before entering rollback upgrade mode. In other words ICM failovers the service requests that are in process. The execution of those service requests continues on cluster instance .

Cluster instance reads the state of the requests in process from the corresponding sessions that are persisted on public store . Therefore the running WebASs of cluster sessions have to enable and support data based session management without restarting. Also rolling upgrade with zero downtime requires configuration of cluster sessions that allows a new WebAS node to be started in already running instance without reading its configuration data from public store .

Patch tool accesses the deploy controller of one of application servers in instance through the corresponding deploy controller API. Through deploy controller patch tool sets the rolling deploy mode and initiates the software upgrades of the involved software components. When a system software component is upgraded e.g. a library a service or an interface the deploy controller delivers the patch to the service manager of the current WebAS. The service manager provides a running environment for all system components of the application server including the deploy controller. Server manager handles the upgrade of the system component in shared database and synchronizes the running component with the changed binary data.

When an application software component is upgraded the deploy controller delivers the patch to deploy service. The deploy service is the standard instrument for installing and activating application software components in a SAP NetWeaver Java application server or WebAS as illustrated in . Both deploy controller and deploy service are services that run in each WebAS in cluster instances and . The deploy service is responsible for the upgrade of the application component in shared database and for the synchronization of the running component with changed binary data.

Patch tool checks test results and approves or rejects the upgrade. If not approved the patch is rolled back. Patch tool could perform the check automatically or this could be managed by administrator . After the patch is approved the upgrade process is resumed through deploy controller. ICM continues to redirect service requests to the active cluster instance .

In one embodiment of the invention instance is reconnected before disconnecting instance . In such a case both instances work in parallel and execute different versions of the software components that are upgraded. In order to enable such parallel work of different versions of the software components additional compatibility requirements are addressed. For example data structures where the components persist their states or related application data have to be compatible or if data is exchanged it has to be reformatted accordingly.

Patch tool ensures synchronization of the software components running on WebASs of instance with the upgraded binary data in shared database . Patch tool performs this task through the deploy controllers the deploy services and the service managers of the application servers. The upgrade of instance may be checked for correctness by patch tool as described above in reference to . If the testing results are correct the patch is approved. Otherwise the upgrade is rolled back.

In the above description numerous specific details are set forth to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize however that the invention can be practiced without one or more of the specific details or with other methods components techniques etc. In other instances well known operations or structures are not shown or described in details to avoid obscuring aspects of the invention.

Reference throughout this specification to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment is included in at least embodiment of the invention. Thus the appearance of the phrases in one embodiment or in an embodiment in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics may be combined in any suitable manner in one or more embodiments.

