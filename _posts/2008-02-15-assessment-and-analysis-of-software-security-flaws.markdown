---

title: Assessment and analysis of software security flaws
abstract: Security assessment and vulnerability testing of software applications is performed based at least in part on application metadata in order to determine an appropriate assurance level and associated test plan that includes multiple types of analysis. Steps from each test are combined into a “custom” or “application-specific” workflow, and the results of each test may then be correlated with other results to identify potential vulnerabilities and/or faults.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08499353&OS=08499353&RS=08499353
owner: Veracode, Inc.
number: 08499353
owner_city: Burlington
owner_country: US
publication_date: 20080215
---
This application claims priority to and the benefits of U.S. provisional patent application Ser. No. 60 901 874 filed on Feb. 16 2007 the entire disclosure of which is incorporated herein by reference.

In recent years many companies and government agencies have been exposed to negative press and legal proceedings due to high profile security breaches in which sensitive data has been either inadvertently disclosed or stolen. While many of these incidents were the result of human error a significant percentage was traced back to poorly designed software architecture and or applications. Conventional techniques for testing software applications can identify many vulnerabilities but no one methodology is failsafe. Furthermore although many security analysis techniques require significant time and resources to administer not every application necessitates the same level or degree of analysis.

As a result companies face a difficult trade off between the desire to test software and limitations on available resources and time. Moreover many companies do not have the expertise to apply some of the more intricate and complex security assessment techniques and thus look to industry experts for such services. This creates yet another challenge in that often what is being tested is highly sensitive proprietary software. Companies are eager to have these applications tested using the most effective methods but are also reluctant to grant others access to key software assets. What is needed therefore is a security assessment platform that permits an outside team to design and execute custom software security assessments against varying types of applications and to perform an analysis that is responsive to evolving threats does not interfere with the execution of the application and does not threaten the proprietary nature of an application.

In general the present invention facilitates security assessment and vulnerability testing of software applications in a manner responsive to the technical characteristics and the business context in which the application operates collectively application metadata . The invention may for example determine an appropriate assurance level and test plan to attain it. In many instances a test plan may dictate performance of different types of analyses. In such cases the individual tasks of each test are combined into a custom or application specific workflow and the results of each test may be correlated with other results to identify a wide range of potential vulnerabilities and or faults that are detected by the different tests. As such a programmer reviewing the results can better understand how different potential vulnerabilities may relate to each other or in fact be caused by a common flaw.

Furthermore once an application is deployed the universe of threats that may impact the application continues to expand and therefore the platform preferably provides the infrastructure and methods for continuous periodic or event triggered application assessments even as the application operates in a secure production environment. Application users and or owners may also simultaneously view both the application infrastructure e.g. source code architectural components object code abstractions user case diagrams UML diagrams and or website maps as it exists in their operational environments and the results of the periodic security assessments which can remain stored within the analysis platform. For example in one implementation the analysis platform runs on a server accessible to the application user via the Internet. The server periodically uploads or otherwise accesses the application performs a security analysis and alerts the user to the results. Application owners and or users may access the results of this and previous assessments which are stored on or retrievable by the server.

Accumulating both application specific metadata and security analysis and assessment results for numerous applications from many companies facilitates benchmarking of applications against other applications at many levels within an organization. Use of various anonymizing and scrubbing techniques i.e. removing any information that could be deemed proprietary and or identify an application s user or owner permits the sharing of assessment data among otherwise unrelated entities. Benchmarking may take place on a global scale i.e. across all applications being monitored within particular subsets of applications e.g. those from a specific industry and or working with a specific technology or based on personnel e.g. for a particular developer team organization or company .

Therefore in one aspect the invention provides a method for assessing vulnerabilities of software applications. A plurality of software assessment testing engines are provided each engine being configured to perform vulnerability tests on a software application. Metadata related to the application such as technical characteristics e.g. source code binary code URLs user names passwords APIs application data etc. and business context information relating to the architecture and deployment software application are received and based thereon an assurance recommendation is provided. The method further includes defining a vulnerability test plan including multiple vulnerability tests based on the preferred assurance level.

In some embodiments the vulnerability tests are executed and their results correlated to facilitate their review within the context and or portion of the application in which flaws or vulnerabilities were found. In some cases the execution may be repeated as indications of new threats or updates to the application are received. The results may be stored in a database for subsequent review analysis and or reporting. In some implementations access to the test results may be limited to specific users based on authentication credentials or other identification techniques. Further information that may be used to identify the source or other confidential details of the software application may be removed or obfuscated e.g. encrypted to allow the test results to be shared among many users for benchmarking purposes. In implementations in which the test results and or the application code itself is transmitted over public networks e.g. the Internet digital rights management techniques may be used to ensure that only those with proper authority can view the test results.

In another aspect the invention provides a security assessment platform. In some embodiments the platform includes a communications server for receiving technical characteristics and business context information relating to a software application and testing engines for performing a plurality of vulnerability tests thereon. The platform may also include a testing workflow module for defining an assurance level for the application based on the technical characteristics and business context information defining a vulnerability test plan that includes multiple vulnerability tests based on the assurance level and correlating the results of vulnerability tests to identify related faults in the application.

The platform may in some instances include a database module for storing the results of the tests as well as application information. The platform may also include a benchmark and reporting module for removing proprietary information from the results of the vulnerability tests and providing statistical reporting of the results of the vulnerability tests in comparison to other software applications other developers and or other companies. In some cases an abstraction layer may be used to generalize how application information is presented to and or received from the testing engines.

Other aspects and advantages of the invention will become apparent from the following drawings detailed description and claims all of which illustrate the principles of the invention by way of example only.

The techniques and supporting systems described herein provide a comprehensive and customizable approach to detecting security flaws in software applications recommending remedial courses of action and reporting and benchmarking against for example industry wide statistics other developers and or other development teams from within or outside of an organization. Software applications may include but are not necessarily limited to any sort of instructions for a machine including for example without limitation a component a class a library an script an applet a logic table a data block or any combination or collection of one or more of any one or more of these. An appropriate level type and frequency of security analysis needed for a software application may depend on many factors including but not necessarily limited to the technical details of an application e.g. the language in which it is written and the platform on which is to be deployed as well as the business context in which the application operates. For example an application that is customer facing and facilitates high volume secure transactions such as banking or ecommerce will require rigorous testing to ensure that customer data is not jeopardized. Conversely applications such as document control systems or desktop applications that are implemented entirely within an organization and operated behind secure firewalls require less stringent testing. Therefore balancing the added costs for executing additional security assessments and testing with the risks of potential for losses is critical

The analysis engine receives application code and programs from users either via the entity operating the platform or directly from customers using the platform as a subscription service. The analysis engine interacts with various testing engines and code review modules as well with assessment and threat databases and includes benchmarking and reporting capabilities for comparing assessment results among applications developers teams and or organizations. In one embodiment for example the analysis engine interacts with a dynamic testing engine a static testing engine a pen testing engine and a module for performing manual code review .

More specifically the dynamic analysis engine interacts with the application as an external entity and executes the application in a manner that mirrors or emulates the runtime environment in which it operates. In some embodiments the dynamic analysis engine receives a description of the interfaces to the application sends test and or simulation data to the application via the interfaces and analyzes the received responses. The test data may be application specific e.g. provided with the application as a library data file or structured input or application agnostic such as data and or scripts known to exploit application vulnerabilities. Based on the responses the dynamic analysis engine determines whether any security defects exist in the application and the extent to which it may be vulnerable to certain threats. The defects may be reported in real time e.g. via the communications server and or stored in a database for subsequent analysis and reporting.

The static analysis engine receives a binary or bytecode version of the application as input. For example a high level semantic model of the application is created containing control flow and data flow graphs of the application and this model then analyzed for quality defects including security flaws by a set of analysis scans.

The pen testing engine performs penetration testing of the application . Penetration testing includes for example simulating and analyzing various web based interactions between a client and the server on which the application operates. This includes executing standard HTTP commands such as GET and POST analyzing FORM elements and scripting elements both client and server side and manipulating inputs to elicit known vulnerabilities.

The analysis engine may also receive input from manual review processes executed using a manual code review module . Manual review processes typically include a human operator visually reviewing source code to determine if proper coding form and standards have been followed and looking for extra functions often left in applications such as trap doors easter eggs and similar undocumented functionality.

The data scripts and functions used to operate the various testing engines and the analysis engine may be stored in a security threat database . The database may be operated as a stand alone server or as part of the same physical server on which the analysis engine operates. Portions of the threat database may in some cases be provided by entities other than the entity operating the platform on a subscription basis allowing the database to be kept up to date as threats and malware evolve over time. Likewise the results of each test and the overall analysis process may be stored in an assessment results database . In some embodiments the applications and analysis results are stored in an encrypted format using a unique key provided to the owner of the analyzed application such that only it can access and review the results of the analysis. In such cases decryption of the analysis is limited to authorized personnel and all traces of the analysis are deleted from memory other than the database following completion.

Examples of database applications that may provide the necessary features and services include the MySQL Database Server by Sun Microsystems the PostgreSQL Database Server by the PostgreSQL Global Development Group of Berkeley Calif. or the ORACLE Database Server offered by ORACLE Corp. of Redwood Shores Calif.

More specifically the assurance recommendation engine receives applications and application metadata and automatically determines various characteristics of the application. For example the recommendation engine may recognize the programming language used to write the application specific libraries used within the application the development environment used to build the application application programming interfaces APIs available to users the size of the application as well as other technical qualities. Moreover the entity responsible for submitting the application which may be the owner of the application a licensee or an end user may provide additional business context information such as the required availability e.g. 99.99 uptime expected throughputs or transaction volumes types of users who will operate the application whether the application will be exposed to the public the operating system in which the application executes other applications with which the application interacts and others. The recommendation engine considers these technical and business characteristics and application metadata and determines a recommended assurance level. As described in more detail below the assurance levels are used by the workflow constructor to define an assessment workflow based on various testing techniques such as dynamic application testing static binary testing automated and manual pen testing as well as manual code review.

Once a workflow has been established by the workflow constructor a workflow engine submits the application to the various testing engines. The results of these tests may include such items as error rates specific occurrences of errors compliance with industry standards as well as other data. The assessment correlation engine correlates the different test results received from the testing engines and organizes them by application module and type of error identifies duplicates and recognizes correlations among different errors.

The analysis engine also may include a grading and reporting module that includes a benchmark module an anonymizer and a flaw viewer . The benchmark module compares the testing and analysis results for one or more applications having similar application profiles and or metadata. This allows the application s owner to see how the application s architecture and security features measures up against other similar applications.

In some instances the benchmark engine calculates and compares test results at a more granular level. For example an organization may wish to determine which of its developers or development teams produces the best code the most secure applications or is most prone to development errors. By including information such as the code author development group and or other organizational information the platform may be used within a company to identify core strengths and or key weaknesses.

The anonymizer removes company specific information from the results and or aggregates the results such that they may be provided to subscribers or the public in general. In this manner the platform provides global view of software development and implementation trends related to security and vulnerability testing across a wide spectrum of industries and technologies.

As an example a bank may be developing a new customer service application that allows its clients to execute transactions via the Web. Based on the technology used to develop the application e.g. Active Server Pages java PHP the fact that the application is available to the general public and the information transmitted is highly sensitive account numbers PINs etc. the assurance recommendation engine may determine that this application be tested as fully as possible. Each testing engine will then process the application either remotely or as received at the platform and the results are correlated into a comprehensive assessment report. Once completed project managers at the bank may log into the platform using secure IDs and passwords biometric authentication PKI techniques or other such methods and using the flaw viewer review and comment on any vulnerabilities identified during testing. On some cases the project managers may also see how the application fared against similar applications submitted by other banks.

Regardless of the implementation the method of implementing and distributing the various components of the platform is arbitrary. For example in some implementations all components of the platform may be completely contained within an organization e.g. within a firewall accessible via a VPN or intranet and available as an on demand service as part of an overall development methodology. In other embodiments the platform may be implemented as a web based service available to numerous organizations that subscribe to the platform and are therefore able to subject their software applications to structured security assessment testing on an as needed basis. Furthermore various anonymizing or aggregation techniques can be used to remove or otherwise protect proprietary information and or data that would identify the application owner. Assessment results from numerous applications across industries technical platforms application sizes etc. can be extracted to provide cross entity benchmarking data to platform subscribers. In addition analysis of the assessment results and subsequent monitoring of the applications for undetected security flaws or unexpected operational reactions to certain threats for example allow the platform and specifically the workflow engine to be refined and improved. By operating the platform as a centralized yet secure resource for multiple entities assessment data can be used for historical and industry benchmarking as well as to upgrade the techniques used to determine assurance levels and built appropriate workflows.

In such cases the need to securely transmit application code both binary and source to and from the platform is crucial. One method for implementing the needed security measures is via digital rights management DRM . In general DRM refers to various access control technologies used by publishers and copyright holders to limit access to and or usage of digital media or devices. Just as DRM is used to protect conventional copyrighted material e.g. audio and video content it may also be employed to protect source and binary code of an application as well the analysis and testing results generated by the platform . More specifically a DRM packager may be used to encrypt some or all of the application information and produce a key to decrypt the information. A DRM engine executes the encryption and decryption functions that allow users to securely view application data via a remote device. Further operational and functional characteristics of DRM modules are set forth below.

Referring now to one embodiment of the assessment and recommendation techniques of the invention includes three phases a data collection phase an assurance level determination phase and a workflow build phase. More specifically the data collection phase includes collecting technical details STEP about the application such as the platform on which it will be built and or implemented the network topology over which it will operate the language or languages used to develop the application third party applications or modules the application will interact with or use the security environment in which the application will operate as well as other application characteristics. In addition the business context in which the application will operate is determined STEP and combined with the technical details to produce an application profile P. In one non limiting example some or all of the business factors identified in the Federal Information Processing Standard FIPS i.e. damage to reputation financial loss or business liability harm to business interests unauthorized release of sensitive information personal safety civil liability and potential criminal violations can be used as guidelines for measuring the security risks in light of the business context of the application. Each of the FIPS factors can be assigned a rating e.g. as n a minimal moderate or serious and in some embodiments certain factors are weighted more than others according to relative importance e.g. as defined by a user or industry standards . For example an application that processes healthcare data including personally identifiable information may accord a rating of serious to factors such as damage to reputation liability unauthorized release of sensitive information and criminal liability but n a for personal safety. In instances in which this analysis has previously been done and an assurance level already determined that assurance level can be imported STEP and in some circumstances updated if necessary.

If an assurance level was provided with the application as part of the data collection phase DECISION STEP the analysis workflow can be built. Otherwise the assurance recommendation engine reviews the application profile P and determines an appropriate assurance level STEP . One approach for determining an appropriate assessment level is to consider the ratings assigned to each of the business context factors and select an appropriate assurance level based on the highest rating. For example if any of damage to reputation financial loss harm to business interests release of sensitive information or civil or criminal violations are rated serious the highest assessment level is recommended. If however all factors are either minimal or n a except for e.g. the civil violations factor which is assigned a moderate rating a lower but still relatively high assurance level is specified. Table 1 below summarizes one possible mapping of business impact factors and their ratings to recommended assessment levels.

The recommended assurance level and in some cases options to modify the level can then be presented to the user STEP who selects the assurance level STEP for the particular application.

In the workflow build phase varying combinations of analysis techniques can be used to adapt a security review workflow to the particular technical and business criteria of an application with one key goal being the reduction of false negatives i.e. undetected security flaws. Different types of analysis e.g. automated manual static dynamic etc. have different false negative rates because they are either unable to detect particular security defects 100 false negative rate or they have varying levels of false negatives depending on the threat. As a result introducing additional security analysis processes into the workflow lowers the false negative rate. But multiple analysis techniques require the expenditure of more time and resources and so should be integrated into the workflow when they contribute meaningfully to the overall reliability of the analysis or to lower the false negative rate below a predetermined threshold.

In one implementation the workflow W is constructed STEP by selecting different analysis techniques from the following table. The higher the desired assurance level the more analysis techniques are recommended. The analysis techniques are arranged according to the time and resources estimated to perform the analysis thereby minimizing costs and only introducing more stringent analyses when the impact of a security event is greater. Once the workflow is determined and approved by the user the various analysis techniques are performed. Table 2 below illustrates how various analysis techniques may be used against applications with different assurance levels.

Combining multiple types of application analysis generally produces a broader application vulnerability profile. For example combining binary static analysis and dynamic analysis techniques provides increased accuracy and more informative analysis results because the outcome of a binary static analysis can be used as input into a secondary dynamic analysis. The dynamic analysis process itself produces two results a dynamic assessment and a static coverage map. The static coverage map contains each dynamic path used to reach a flaw detected during the static analysis.

The static results dynamic results and static coverage map are used to produce a report of static flaws not pathed lowest priority static flaws with a dynamic path high priority and dynamic flaws not related to the portions of the application that have been statically analyzed e.g. environment configuration . The data flow and control flow graphs generated by static analysis may also be used to compute a dynamic test case for each identified flaw. In such cases input data and an input vector may be generated that will recreate and retest each flaw dynamically to determine if the flaws have been addressed. More specifically and with reference to the following steps can be performed to combine results from both the static and dynamic testing 

In some embodiments continuous application assurance provides for automatic re analysis of an application. Re analysis is triggered by changes in the external application environment e.g. threat space business intelligence detected attacks and or the implementation of enhanced analysis capabilities e.g. a new scan has been added to an analysis workflow to detect new class of vulnerability . An intelligent re analysis decision can be made by taking into account factors such as application profile previous vulnerability assessment results and the type of change e.g. threat and or scan capability .

A decision to initiate a re analysis can be based for example on an application s technological profile metadata describing the application s functionality the deployment environment of the application new information about vulnerabilities that may affect the application and or increases in a likelihood of a threat. External data feeds and internal scan capabilities database are used to trigger rescans of the application. For example suppose a new vulnerability is discovered in how data is transmitted and processed using XML and Web Services that did not exist when the application was first scanned. All applications having metadata that includes both XML and Web Services are identified and the relevant analysis workflows are updated with the new scan information and re processed.

In one embodiment with reference to the initial steps for an application specific or customer driven rescan include 

In addition to single application rescans as described above a platform wide rescan may also be initiated in which multiple applications possibly owned and or operated by unrelated entities are rescanned. In addition application owners may subscribe to a periodic and or event driven rescan service that continuously determines if rescans are necessary and if so performs the appropriate analysis. More specifically and referring to one method for implementing a global rescan includes the following steps 

In some embodiments in which a static binary analysis is performed remotely e.g. within the security assessment platform separate from the operational environment in which the application is implemented or where its source code is stored the results of the binary analysis can be linked to the original application source. These results are typically stored and managed securely on within the platform but can be viewed by a remote user together with local application source code using a viewer application.

Referring to one method for providing simultaneous viewing of identified application flaws along with the application source code that caused the flaws can include the following steps 

In some embodiments the platform provides a common repository for application metadata as well as assessment results for numerous applications across a variety of technical and business implementations and or of known quality. By maintaining such a database the platform can provide cross application reporting that compares a particular application or family of applications to others in the same industry to applications that use the same technology and or based on other criteria rendering one class of application relevant to another. In some instances assessment results may be compared to those generated by a template application to determine the quality of the application as compared to an application of known quality. Such reporting referred to as peer benchmarking allows an organization to gauge the effectiveness of its own security initiatives relative to other companies in the same industry. Because the assessment platform provides consistent and repeatable security analysis techniques a common assessment vocabulary and a large sample size the information provided to users has a greater global relevance than individual application assessment data.

Referring to one method for providing peer benchmarking reporting to users of the platform includes the following steps 

Once the results database is populated with assessment results from a sufficient number of applications users can specify and view various reports. Some reports for example can indicate how statistically an application compares to its peers by indicating the percentage of all assessed applications or some subset thereof that resulted in fewer potential vulnerabilities. In one example with reference to the benchmark reporting process can include the following steps 

The vulnerability assessment process consumes and produces data that is considered highly confidential by most organizations. For example input into the analysis phase can include application source code application binaries and debug symbols and or environment data URLs usernames passwords site maps . Because of the sensitive nature of this data and because they indicate potentially exploitable security flaws in the associated application provision is desirably made to keep the analysis results confidential. In instances in which the platform is operated as a centralized offsite service the need to secure this sensitive information becomes even more crucial. In various embodiments the DRM packager and engine provide the following capabilities 

Using the DRM engine steps may be taken to protect the initial data provided as input to the assessment process as well as the analysis results. Once the submission data has been packaged into a secure container access is granted to the trusted analysis application for the duration of the analysis. Analysis results can then be packaged into a secure container for remote viewing. A trusted secure viewer application in conjunction with the DRM Client engine and access token ensures that the analysis results are viewed by authorized users and prevents unauthorized copying via printer cut paste print screen or file copy.

Referring to the following steps provide the secure receipt and analysis of application source files and assessment data to and within the platform 

Referring to once the analysis data is stored in the database it can then be packaged and transmitted using similar DRM techniques and the following steps 

The invention can be embodied in other specific forms without departing from the spirit or essential characteristics thereof. The foregoing embodiments are therefore to be considered in all respects illustrative rather than limiting on the invention described herein.

