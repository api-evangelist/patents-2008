---

title: Persistent reply cache integrated with file system
abstract: A system and method integrates a persistent reply cache with operation of a file system executing on a storage system. In an illustrative embodiment, the persistent reply cache is embodied as a file comprising a combination of contents of (i) one or more enhanced non-volatile log records of non-volatile electronic storage, (ii) “dirty” in-core buffer cache data structures, and (iii) on-disk data structures corresponding to those dirty buffer cache data structures. Integration is achieved through atomic operation of the reply cache with the file system to store information associated with a reply, e.g., to a client request, on electronic storage implemented as in-core buffer cache memory of the storage system, as well as on magnetic storage implemented as disks of the system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08161236&OS=08161236&RS=08161236
owner: NetApp, Inc.
number: 08161236
owner_city: Sunnyvale
owner_country: US
publication_date: 20080423
---
The present invention relates to storage systems and more specifically to a persistent reply cache used in a storage system.

A storage system is a computer that provides storage services relating to the organization of information on writeable persistent storage devices such as non volatile memories and or disks. The storage system typically includes a storage operating system that implements a file system to logically organize the information as a hierarchical structure of data containers such as files and directories on e.g. the disks. Each on disk file may be implemented as set of data structures e.g. disk blocks configured to store information such as the actual data for the file. A directory on the other hand may be realized as a specially formatted file in which information about other files and directories are stored.

The storage system may be further configured to operate according to a client server model of information delivery to thereby allow many clients to access files and directories stored on the system. In this model the client may comprise an application executing on a computer that connects to the storage system over a computer network such as a point to point link shared local area network wide area network or virtual private network implemented over a public network such as the Internet. Each client may request the services of the storage system by issuing file system protocol messages or requests such as the conventional Network File System NFS protocol requests to the system over the network identifying one or more files to be accessed. In response a file system executing on the storage system services the request and returns a reply to the client.

Many versions of the NFS protocol require reply caches for their operation. A reply cache may serve many purposes one of which is to prevent re execution replay of non idempotent operations by identifying duplicate requests. By caching reply information for such operations replies to duplicate requests may be rendered from cached information as opposed to re executing the operation with the file system. For example assume a client issues an NFS request to the storage system wherein the request contains a non idempotent operation such as a rename operation that renames e.g. file A to file B. Assume further that the file system receives and processes the request but the reply to the request is lost or the connection to the client is broken. A reply is thus not returned to the client and as a result the client resends the request. The file system then attempts to process the rename request again but since file A has already been renamed to file B the system returns a failure e.g. an error reply to the client even though the operation renaming file A to file B had been successfully completed . A reply cache attempts to prevent such failures by recording the fact that the particular request was successfully executed so that if it were to be reissued for any reason the same reply will be resent to the client instead of re executing the previously executed request which could result in an inappropriate error reply .

Typically the reply cache has been implemented in volatile memory of a storage system. This poses the issue that if retransmission of the request e.g. an NFS request occurs as a result of non responsiveness while the storage system reboots a failure similar to that described above can occur. For example suppose that the storage system processes the request to rename file A to file B and after processing the request the storage system powers down and performs a reboot operation. Since the rename operation had been effected in the file system image before the reboot operation re execution of the request results in a spurious error if the reply cache is volatile there is no way to prevent this error. In other words when the storage system powers down and reboots the contents of the reply cache are lost.

A solution to the above described problem is to implement the reply cache in persistent storage. However previous attempts to implement such a reply cache encountered a number of difficulties. For instance persistent storage of reply cache information on e.g. disk typically imposes too great a performance penalty for general use. That is the input output I O latency associated with accessing disk for reply cache information imposes a substantial performance penalty. On the other hand storage in non volatile memory such as random access memory is often expensive because of the storage space requirements. Here the amount of non volatile memory needed to accommodate a typical size for the reply cache is substantial and thus often prohibitively expensive.

The present invention overcomes the disadvantages of the prior art by providing a system and method for integrating a persistent reply cache with operation of a file system executing on a storage system. In an illustrative embodiment the persistent reply cache is embodied as a file comprising a combination of contents of i one or more enhanced non volatile log records of non volatile electronic storage ii dirty in core buffer cache data structures and iii on disk data structures corresponding to those dirty buffer cache data structures. Integration is achieved through atomic operation of the reply cache with the file system to store information associated with a reply e.g. to a client request on electronic storage implemented as in core buffer cache memory of the storage system as well as on magnetic storage implemented as disks of the system. To that end the invention takes advantage of a consistency model of the file system to ensure that contents of the dirty in core buffer cache data structures of the reply cache are consistent with corn responding on disk data structures of that cache. Advantageously the invention ensures that operation of the persistent reply cache is atomic with respect to file system operations in a simple effective and reliable way.

The present invention is directed to a system and method for integrating a persistent reply cache with the operation of a file system of a storage operating system executing on a storage system. As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a storage system implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows NT or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In an illustrative embodiment described herein the storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term WAFL is employed it should be taken broadly to refer to the file system component of any storage operating system that is otherwise adaptable to the teachings of this invention.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network a disk assembly directly attached to a client or host computer and illustratively a cluster of interconnected storage system nodes. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written generally in terms of a log structured file system the teachings of the present invention may be utilized with any suitable file system including a write anywhere file system.

The clients may be general purpose computers configured to interact with the node in accordance with a client server model of information delivery. That is each client may request the services of the node and the node may return the results of the services requested by the client by exchanging packets over the network . The client may issue packets including file based access protocols such as the Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of data containers such as files and directories.

The cluster access adapter comprises a plurality of ports adapted to couple the node to other nodes of the cluster . In the illustrative embodiment Ethernet is used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N modules and D modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the N D module for communicating with other N D modules in the cluster .

The non volatile memory comprises electronic storage illustratively embodied as a solid state non volatile random access memory NVRAM array having either a back up battery or other built in last state retention capabilities e.g. non volatile semiconductor memory that hold the last state of the memory in the event of any power loss to the array. As described herein a portion of the non volatile memory is organized as temporary yet persistent non volatile log storage NVLOG capable of maintaining information in the event of a failure to the storage system.

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code. A portion of the memory may be further organized as a buffer cache for holding data structures such a buffer cache blocks associated with the present invention. The processor and adapters may comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage service implemented by the node. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over the network which may comprise point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the computer network may be embodied as an Ethernet network or a Fibre Channel FC network. Each client may communicate with the node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the in formation is preferably stored on the disks of array . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Storage of information on each array is preferably implemented as one or more storage volumes that comprise a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each logical volume is generally although not necessarily associated with its own file system. The disks within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data. An illustrative example of a RAID implementation is a RAID 4 level implementation although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

To facilitate access to the disks the storage operating system implements a file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named data containers such as directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of blocks on the disks that may be exported as named logical unit numbers luns .

In addition the storage operating system includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on the disks of the node . To that end the storage server includes a file system module in cooperating relation with a RAID system module and a disk driver system module . The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the SCSI protocol.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the log structured file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store meta data describing the layout of its file system these meta data files include among others an inode file. A file handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

Operationally a request from the client is forwarded as a set of one or more packets over the computer network and onto the node where it is received at the network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the log structured file system . Here the file system generates operations to load retrieve the requested data from disk if it is not resident in core e.g. in buffer cache . If the information is not in the buffer cache the file system indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in the buffer cache for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by node in response to a request issued by client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the node. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

In an illustrative embodiment the storage server is embodied as D module of the storage operating system to service one or more volumes of array . In addition the multi protocol engine is embodied as N module to i perform protocol termination with respect to a client issuing incoming data access request packets over the network as well as ii redirect those data access requests to any storage server of the cluster . Moreover the N module and D module cooperate to provide a highly scalable distributed storage system architecture of the cluster . To that end each module includes a cluster fabric CF interface module adapted to implement intra cluster communication among the modules.

The protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N module function as protocol servers that translate file based and block based data access requests from clients into CF protocol messages used for communication with the D module . That is the N module servers convert the incoming data access requests into file system primitive operations commands that are embedded within CF messages by the CF interface module for transmission to the D modules of the cluster . Notably the CF interface modules cooperate to provide a single file system image across all D modules in the cluster . Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster.

Communication between the N module and D module is illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing is mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from Network Appliance Inc. The SpinFS protocol is described in the above referenced U.S. Patent Application Publication No. US 2002 0116593.

The CF interface module implements the CF protocol for communicating file system commands among the modules of cluster . Communication is illustratively effected by the D module exposing the CF API to which an N module or another D module issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface on N module encapsulates a CF message request as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster . In either case the CF decoder of CF interface on D module de encapsulates the CF protocol request and processes the file system command.

It is important to note how the persistent reply cache described herein may provide its basic function of avoiding spurious request re execution both when a reply to the client is lost i.e. a client protocol reply and when a reply to the N module is lost i.e. a CF protocol reply . For example the CF protocol might be configured to utilize CF identifiers for requests made by the N module to the D module. Although use of those identifiers as the basis of the persistent reply cache would address the loss of CF protocol replies an issue would still exist concerning the loss of client protocol replies. In an illustrative embodiment the CF protocol may be instead configured to pass client protocol request identifiers as part of the CF protocol request. These latter client request identifiers can be used as the basis of the persistent reply cache such that the resulting implementation provides protection against the loss of either the CF protocol replies or the client protocol replies.

As noted the present invention is directed to a system and method for integrating a persistent reply cache with the operation of a file system executing on a storage system. Implementation of the persistent reply cache described herein is generally dependent on the operation of the file system such as the log structured file system which operates in an integrated manner with the use of non volatile memory a portion of which is organized as the NVLOG . Many requests executed processed by the log structured file system are recorded in the NVLOG with each request being considered complete once the NVLOG record is marked complete. Execution of these requests generally requires some type of state change and as such the requests are considered non idempotent requests including e.g. rename requests.

As an example assume the file system executes a client request forwarded by a protocol server of the multi protocol engine to rename a file from A to B. Broadly stated the file system executes processes the request by e.g. retrieving appropriate blocks of a directory from disk loading the blocks into the buffer cache and changing modifying the blocks including an appropriate block entry of the directory to reflect renaming of the file to B. The file system then marks the modified buffer cache blocks including the directory entry block that now contains the name B for the file as dirty so that they may be written to disk. At this point the file system does not write the dirty blocks to disk but instead waits until execution of a consistency model event e.g. a consistency point CP of the system.

Meanwhile the file system creates a file system operation record of the request and stores the record in the NVLOG . Subsequently during the CP the contents of the record are not written flushed to disk but rather the processing results of those contents as represented in the dirty buffer cache blocks are flushed to disk. That is only the dirty buffer cache blocks and not the file system operation record are written to disk. However once the changes to be made to the file system are essentially reflected in the file system operation record and stored in the NVLOG processing of the request is considered complete and the file system notifies the protocol server of such completion. The protocol server thereafter generates a reply containing information indicating e.g. a successful completion of the request and returns the reply to the client .

According to the invention a file system operation record may be enhanced to include one or more fields that contain reply cache information. As used herein reply cache information denotes information including the contents of a reply indicating that a request was previously processed executed by the file system so that if the request were to be reissued e.g. by the client for any reason the same reply can be resent to the client instead of re executing that previously executed request which could result in an inappropriate error reply . In an illustrative embodiment these reply cache information fields may be appended to the file system operation record to form the enhanced NVLOG record or the reply cache information can be integrated within fields of the enhanced . Notably implementation of the persistent reply cache involves enhancements to the file system operation record that include the necessary information to reconstruct the associated reply cache changes.

Once recorded in the NVLOG a request is considered complete and a response reply can be generated. Considering a request complete represents a guarantee that any file system change together with any associated change to the persistent reply cache will be consistently stored on final persistent storage e.g. disk. Upon incorporation into a CP storage of the request s changes on disk is effectively performed. Until that time however the presence of the request information in the NVLOG is the means whereby that guarantee is ensured. If the storage system reboots initializes the on disk file system is at the state represented by the last CP. Any records present in the NVLOG after storage system initialization are re executed so that the file system on disk structures can be brought to the state represented by all requests that have been acknowledged and securing the guarantee represented by that acknowledgement.

In an illustrative embodiment the persistent reply cache is embodied as a file comprising a combination of contents of i one or more enhanced NVLOG records ii dirty in core buffer cache data structures and iii on disk data structures corresponding to those dirty buffer cache data structures. Changes made to the reply cache file as represented by the dirty buffer cache structures storing the results of the NVLOG record contents are written and committed to the corresponding on disk data structures during the CP. Once that committal takes place the associated enhanced NVLOG records can be discarded since the file system changes including those to the persistent reply cache have been committed to disk. Until that point however the NVLOG records must be maintained for use in the event of a system reboot or loss of power. Because there has been no effective commitment of changes represented by these NVLOG records to disk the only way that the changes represented by these records both with regard to normal file system state and persistent reply cache state can be realized e.g. in the event of such a power loss is to provide for the re execution of these NVLOG records on system restart.

As further noted integration of the persistent reply cache file with file system operation is illustratively achieved through atomic operation of the reply cache with the file system to store information associated with a reply e.g. to a client request on electronic storage implemented as in core buffer cache memory of the storage system as well as on magnetic storage implemented as disks of the system. To that end the invention takes advantage of the file system consistency model described herein to ensure that contents of the reply cache including dirty in core buffers on disk structures and information in enhanced NVLOG records are always consistent with the other contents of the file system also including dirty in core buffers on disk data structures and information in NVLOG records .

The form of the persistent reply cache on disk i.e. the on disk structure of the reply cache may be configured to reflect the protocols used as well as the type of load device and file system characteristics. In an illustrative embodiment the reply cache is organized as a generic bin and bucket structure although it will be understood to those skilled in the art that other on disk reply cache organizations can be used in accordance with the teachings of the invention described herein. In such a bin and bucket structure requests from the same source are given their own subsection of the reply cache or bin . A bin comprises one or more buckets wherein each bucket holds a single request recorded in the reply cache. Note that the definition of from the same source may vary depending upon the protocol. For example for the NFS v2 3 and 4 protocols requests with the same flow i.e. the same i source client and destination storage system IP address ii source and destination port and iii protocol number and version are considered from the same source. For the NFSv4.1 protocol currently being finalized requests within the same session are considered from the same source.

In an illustrative embodiment the file system updates the persistent reply cache file on disk via a CP as blocks are changed. The enhanced NVLOG record and in core dirty buffer cache blocks are consistently associated with this persistent reply cache file. As a file the persistent reply cache is defined by an inode or similar data structure that organizes the bins and buckets as a single tree structure layout of indirect and direct data blocks on disk although it will be apparent to those of skill in the art that other on disk layouts of the reply cache such as organizing each bin as a file or similar object may be advantageously used in accordance with the teachings of the invention. Thus any changes to a particular bin or bucket illustratively involves updating of 4 kB blocks which may be scattered over the disks in accordance with the write anywhere capability of the log structured file system.

Further to the illustrative embodiment each bin has a certain allocated size e.g. 24 kB within the persistent reply cache file. For example the first 24 kB of the file may be allocated for a first bin the next 24 kB may be allocated for a second bin etc. Moreover within each bin a bucket has a specific location. In the context of a file the on disk layout of the persistent reply cache illustratively assumes a simple two dimensional structure wherein the bins comprise a first dimension and within each bin the buckets comprise a second dimension although other on disk layout structures may also be employed.

The array includes a plurality of bucket data records buckets wherein each bucket contains a time value or timestamp indicating the time the bucket was created or last accessed a size value indicating the size of reply data for the bucket and a tag identifying the bucket. For NFSv2 v3 the tag is illustratively a transaction identifier XID of the request whereas for NFSv4 which allows multiple file system operations within a single NFS request the tag comprises the combination of the XID and position of any individual file system operations associated with an NFS COMPOUND request. In general whenever a single client protocol request results in multiple operations at the file system level as e.g. when processing that request results in issuance of more than one CF protocol request the structure of the tag must make provision for distinct values for each of the issued file system requests in order to perform the client protocol request.

It should be noted that the NFSv4 protocol organizes its requests differently than NFSv2 and v3. The NFS v2 and v3 protocols define a number of individual RPC procedures such as rename remove read and write each of which is embodied as a request. However the NFSv4 protocol defines only one request called COMPOUND which illustratively contains a series of such procedures. The COMPOUND request thus allows execution of multiple file system procedures operations in a single request. Therefore reply cache information associated with the COMPOUND request denotes the position of the actual procedure e.g. file system operation within that request. For example the COMPOUND request may comprise lookup remove and rename procedures with each of those procedures representing a separate operation to the file system. The COMPOUND request is illustratively parsed within the appropriate protocol server e.g. NFS server of the multi protocol engine and each of its constituent operations is individually sent to the file system for execution. Accordingly each operation has its own reply cache record.

In an illustrative embodiment an in core reply cache structure is provided that corresponds to the on disk reply cache structure. Accordingly the in core reply cache structure illustratively includes both bin and bucket data structures and respectively although other in core reply cache structures can be advantageously used with of course corresponding on disk structures . An example of such an in core reply cache structure is disclosed in U.S. patent application Ser. No. 12 148 930 titled FlowBased Reply Cache which is hereby incorporated by reference as though fully set forth herein. is a schematic block diagram of an in core bin data structure of the reply cache that may be advantageously used with the present invention. The in core bin data structure comprises a hash table containing a plurality of entries . The hash table is used to locate bin entries in the reply cache and is illustratively based on the low order bits of a conventional hash function directed to the request flow e.g. the contents of the bin label. Each hash table entry contains first and last pointers to the first and last bin entries of a chained data structure illustratively embodied as a circular list which entries share a value for the low order bits of the hash of the bin label.

Each bin entry includes a forward pointer that references points to the next bin entry or the head of the list on the last entry and a backward pointer that points to the previous bin or head of the list on the first entry . Each bin entry or bin also includes a disk location illustratively expressed as an offset within the associated file of the on disk reply cache structure corresponding to the bin a bin label a time value and an in core bucket data structure . The contents of the bin label are similar to bin label and to that end illustratively comprise a source address of the client issuing the requests stored in the bin a source port number of that client a destination address of the storage system a destination port number of the storage system an RPC protocol number for the requests and an RPC protocol version for the requests. The time value or timestamp indicates the time the bin was created or last accessed.

The hash table field of the bucket data structure is configured to organize a hash table used to locate buckets. The hash table is illustratively based on the low order bits of a hash of tag values for the buckets bucket tags and includes a plurality of hash table entries wherein each hash table entry is associated with a particular value of the low order bits of the hash of the bucket tag. For efficiency the buckets of array are also illustratively organized into a series of separate hash chains not shown referenced by the entries . As a result each hash table entry contains a first memory address and a last memory address that reference first and last buckets of a chain or set of buckets within the bin that share those low order bits. Thus each bucket of the array belongs to one of a series of hash chains and to the LRU list for the bin there is one LRU list per bin . In operation the hash chain may be used to find a bucket having a particular tag however if the bucket cannot be located then an existing bucket must be freed up or recycled to essentially provide a storage memory location for that bucket. The LRU pointers i.e. the last LRU pointer are used to locate buckets for recycling purposes.

Each bucket comprises a LRU list pointer that references a next entry on the LRU list i.e. the next least recently used entry or NULL if this is the least recently used entry a forward pointer that references a next bucket of its hash chain having the same low order hash bits or the head of the hash chain on the last entry and a backward pointer that references the previous bucket of the hash chain having the same low order hash bits or head of the hash chain on the first entry . Note that the forward and backward pointers facilitate removal of the bucket from the middle of a chain. In contrast there is only one LRU list pointer because when recycling an existing bucket from the LRU list the referenced bucket is always removed from an end of the list.

The bucket entry also comprises a request bucket tag e.g. an XID a reply size value indicating the size of the reply data for this bucket entry and a reply pointer that points to the location of the reply data in memory. Note that the reply data is referenced by the pointer because it is generally variable in length otherwise the reply data may be included in the bucket entry . Lastly the bucket entry comprises a time value or timestamp indicating the time that the entry was created or last accessed and an in process flag described further herein .

In an illustrative embodiment an update specified by the request that changes the state of the persistent reply cache typically involves changes to e.g. one or more portions of a data container e.g. a file represented in the reply cache. Such changes can be performed by program code similar to the code that updates data structures within the rest of the file system. Just as with more typical file system changes the changes to the reply cache state are performed by writing to buffers in memory e.g. buffer cache blocks . During a CP these state changes include reply cache changes together with other file system changes. Because of the nature of a CP as an atomic all or nothing update to the file system state reply cache changes are always included with the changes for the request that they represent.

The enhanced NVLOG record also includes an operation type indicating the type of operation request associated with the record. A bin identifier ID comprises an ID e.g. in the form of an index into an in core data structure such as a table having a set of pointers to the bins of the in core bin data structure . That is the file system maintains an in core bin mapping table used to locate an in core bin based on an ID in the form of an index . Note that there is also an index via a bin ID into an on disk data structure such as a file used to locate bins on disk. Similarly a bucket ID comprises an ID e.g. in the form of an index into in core and on disk data structures to buckets within a bin. A bucket tag identifies the bucket uniquely within the bin and to that end comprises information used to determine whether a given request matches a previously processed request. Illustratively the tag is an XID of the request for NFSv2 v3 or the combination of the XID and ordinal position of a file system operation associated with an NFS COMPOUND request for NFSv4 .

The ordinal position of the file system operation within the NFS COMPOUND request is recorded in order to reply back properly to the N module when the CF protocol is used or otherwise to provide information to allow partially executed requests to be properly resumed when the requests are reissued. For example assume a system power down occurs and the storage system reboots during execution of the COMPOUND request. In response the NFS server starts re processing e.g. re parsing the operations included in the request. To that end the NFS server may issue an operation e.g. a rename operation in ordinal position of the COMPOUND request to the file system for execution. The file system then examines the reply cache to determine whether it had previously executed that operation. In order to make this determination the file system searches for a record having inter alia a particular bucket tag e.g. XID and ordinal position for NFSv4 XID for NFSv2 3 . If the operation was executed then the file system returns the answer reply in the matching reply cache record. When requesting execution of subsequent operations in the COMPOUND request those operations will have a bucket tag with the same XID and higher ordinal positions. Depending on whether they had been previously executed the subsequent operations will either be found in the reply cache if previously executed or not thus indicating they need to be executed for the first time.

The NVLOG record further includes re execute information info including parameters of the request as well as any choices made in request processing needed to re execute the request in the event of a storage system reboot. For example for a rename request renaming file A to B the re execute info may include information such as i the filename B ii the Mode number of file B iii the rename operation etc. Note that the fundamental purpose of the NVLOG record is to re perform the operation reflected in the record because it had not been flushed to disk since the CP had not completed . Note also that it is not sufficient that the file system merely re execute the operation and flush the result to disk the file system must guarantee that the result of the re execution is exactly the same as the previous result. For example if name A was removed the first time name A is also removed the second time. Reply info contains either the reply to the request or information sufficient to generate the reply.

The following descriptions illustrate example operational procedures involving the persistent reply cache of the present invention. Broadly stated a client sends a request e.g. an NFS request to the storage system where the request traverses the multi protocol engine to a protocol server e.g. NFS server which performs its processing of the request. As part of that processing the NFS server prepares a message that instructs the file system as to the operation specified by the request. If the file system protocol does not support reply cache processing or if the request is an idempotent request then reply cache processing is not needed. Otherwise the NFS server tags the message e.g. with a flag of a message header to indicate to the file system that the request supports reply cache processing inserts the bin ID and bin label in the message and forwards the message to the file system.

In an illustrative embodiment the NFS server queries the file system as to the bin ID corresponding to a particular bin label or flow . Upon receiving the corresponding bin ID the NFS server locally stores that ID so that when it receives a request having that flow it can forward the bin ID associated with the request to the file system . If this is a first request received from this client source the file system creates the bin and an associated bin ID. For example the file system creates a bin and its entries using inter alia the flow information provided by the NFS server. The flow is hashed placed on an appropriate hash chain using the appropriate pointers and assigned a disk location.

In response to receiving the message with the bin ID and bin label the file system locates the correct bin and verifies that the bin label matches the label in the request. Note that the bin label is passed in the message because it is possible that the in core bins may be timed out of their memory locations. If the bin labels do not match an error is generated that instructs the file system to create a bin for the flow. However if the labels do match then the bin ID is a valid location pointer to a bin and its associated array of buckets in memory. The file system then hashes a bucket tag e.g. the XID of the request and uses the low order bits of that hash to select an appropriate hash table entry of hash table . Starting from the first address pointer in the hash table entry hash chain the file system searches the hash chain for a bucket with a matching bucket tag . If the matching tag is found the file system determines whether the in core bucket is marked in process .

Note that two situations may arise upon finding a matching bucket entry. One situation is that the request has been executed and as such is a duplicate so that the file system need only return a copy of the reply stored in the reply cache. The other situation is that the file system is currently executing the request i.e. processing of the request has not yet completed. Typically this latter situation arises where the storage system has taken a long time to respond due to interference or file system latency such that the client sends another copy of the request assuming that the original must have been dropped. For this in process situation it is undesirable for the file system to re execute the request yet there is no reply to return.

Thus if the bucket is marked in process e.g. by asserting the in process flag then the file system returns to the NFS server an indication of this in process situation or alternatively waits until the in process condition is cleared. In the former situation the server merely drops ignores the request and does not respond to the client until processing of the original request completes at which time a reply is generated. Once processing completes the file system clears the in progress flag for the request bucket . Thereafter or if the bucket is not marked in process the reply information in the bucket is located using the reply pointer and provided to the NFS server along with an indication that the request is a duplicate such that the server can return the reply to the client.

However if a matching tag is not found then the file system removes the least recently used bucket from the LRU list and removes that bucket from its old hash chain. To that end the file system hashes the old bucket tag of that entry selects the old hash chain and then removes the bucket from that chain. The file system then modifies the bucket to reflect the request sets changes the bucket tag and marks the flag of the entry as in process. Thereafter the bucket tag is hashed an appropriate hash chain is selected and the bucket is loaded onto that selected hash chain. The bucket is then initialized with information that can be derived from the request and that is useful in generating a reply such as parameters of the request. Further the in core memory address of the bucket is saved in e.g. a data structure of the file system that stores information relevant to processing of the request. The file system then completes processing of the request i.e. it completes initializing and setting up its in core bin and bucket data structures for this request and completes execution of the rename operation. The file system thereafter stores the reply information via reply pointer to the location of the information in memory in the bucket .

In Step the file system updates the in core image of the on disk bucket information to reflect the new information i.e. stored in dirty buffer cache blocks of the corresponding bucket. As noted the file system maintains buffer cache for storing old information in disk blocks that it subsequently processes modifies . The buffer cache illustratively comprises the on disk image of those blocks on the disk preferably so that they do not have to be read from disk. Blocks that have been modified with the new information and have not yet been written to disk are stored in dirty buffer cache blocks. In other words the in core manifestation of the persistent reply cache comprises both clean and dirty buffer cache blocks. In this context the term clean denotes dirty buffer cache blocks that have been written to disk during a CP but that have not been invalidated discarded from the buffer cache. A clean buffer cache block thus exactly matches its corresponding on disk block. If there were infinite memory resources in the storage system it would be preferable to maintain these clean blocks in the buffer cache blocks so that there would be no need to subsequently read them from disk. Since there are not infinite resources the buffer cache is also organized on an LRU basis and if some clean blocks have not been accessed for some time they are discarded.

Note that Step corresponds to those portions of the on disk structure as reflected by the dirty blocks of the buffer cache e.g. label flow tag time size reply data . Note also that the in core data structures as well as these buffer cache structures have different purposes i.e. the in core structures are organized to facilitate use in processing whereas the buffer cache structures are organized to facilitate writing to disk during a CP .

Upon completion of the CP the NVLOG record is complete the buffer cache structures have been modified and updated and the in core bin and bucket structures are consistent. Note that upon marking the NVLOG record complete the file system has atomically executed the rename request and updated the persistent reply cache to reflect the fact that it has executed the rename. In Step the reply is returned to the client via the NFS server and the procedure ends at Step .

In Step the file system retrieves the necessary contents i.e. the bin and bucket structures of the on disk persistent reply cache file and thereafter adds each on disk bin structure to the initially empty set of in core bin structures . To that end in Step the file system creates an in core bin corresponding to each appropriate on disk bin and in Step inserts the created in core bin onto the circular list based on a hash of the bin label. Note that the construction of the LRU list may be deferred to a later step. In Step for each bin the file system retrieves the array of buckets from disk.

The file system then adds each on disk bucket to the initially empty set of in core buckets for the associated in core bin . To that end in Step the file system creates an in core bucket corresponding to each appropriate on disk bucket and in Step references points the in core buckets to their associated in core bin . In Step the file system inserts each in core bucket onto an appropriate hash chain based on a hash of the bucket tag and in Step inserts the in core bucket onto a bucket LRU list at a proper location based on timestamp from the corresponding on disk bucket . In Step the file system adjusts the timestamp of the associated in core bucket to reflect the timestamp of the on disk bucket and in Step adjusts the timestamp of the associated in core bin if the timestamp of the bucket is later than the timestamp .

In Step the file system constructs the LRU list for the in core bins using their timestamps and in Step performs an NVLOG replay if necessary e.g. not a clean shutdown restart such as a panic situation or unexpected reboot . In an illustrative embodiment NVLOG replay comprises re execution of each enhanced NVLOG record in the NVLOG. As part of the re execution e.g. in the case of a rename operation the file system processes the operation and performs an equivalent update of both the in core and on disk structures that correspond to the persistent reply cache entry enhanced NVLOG record for that operation. Note that there are two differences between NVLOG replay and the manner in which requests are generally re executed by the file system 1 The file system does not search for a bucket with a matching tag. Instead it immediately selects a new bucket e.g. from the LRU list and stores the information for the request in that bucket. 2 The storage system does not return a reply to the client because it is only re executing the request. The procedure then ends at Step .

While there have been shown and described illustrative embodiments for integrating a persistent reply cache with operation of a file system executing on a storage system it is to be understood that various other adaptations and modifications may be made within the spirit and scope of the present invention. For example as noted the results i.e. dirty buffer cache blocks of all new enhanced NVLOG records in the persistent reply cache are written to disk at a consistency model event e.g. a CP of the file system. Depending on the workload it may be possible to reduce the number of disk blocks of the reply cache file written per CP by using conventional logging techniques. Instead of implementing all changes to the on disk reply cache immediately as part of the CP the necessary changes can be written to a sequential on disk log e.g. embodied as a sequential file as a representation of the changes e.g. as instructions appended as part of the CP. Periodically e.g. after a number of CPs those changes represented in the sequential on disk log are reflected in the reply cache file on disk but since many entries will have been overwritten many times in the interim this technique can result in substantial I O reduction.

The foregoing description has been directed to specific embodiments of this invention. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. For instance it is expressly contemplated that the components and or structures described herein can be implemented as software including a computer readable medium having program instructions executing on a computer hardware firmware or a combination thereof. Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the invention. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention.

