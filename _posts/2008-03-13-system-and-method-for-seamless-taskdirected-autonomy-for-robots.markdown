---

title: System and method for seamless task-directed autonomy for robots
abstract: Systems, methods, and user interfaces are used for controlling a robot. An environment map and a robot designator are presented to a user. The user may place, move, and modify task designators on the environment map. The task designators indicate a position in the environment map and indicate a task for the robot to achieve. A control intermediary links task designators with robot instructions issued to the robot. The control intermediary analyzes a relative position between the task designators and the robot. The control intermediary uses the analysis to determine a task-oriented autonomy level for the robot and communicates target achievement information to the robot. The target achievement information may include instructions for directly guiding the robot if the task-oriented autonomy level indicates low robot initiative and may include instructions for directing the robot to determine a robot plan for achieving the task if the task-oriented autonomy level indicates high robot initiative.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08271132&OS=08271132&RS=08271132
owner: Battelle Energy Alliance, LLC
number: 08271132
owner_city: Idaho Falls
owner_country: US
publication_date: 20080313
---
This invention was made with government support under Contract No. DE AC07 05 ID14517 awarded by the U.S. Department of Energy. The government has certain rights in the invention.

The present application is related to U.S. patent application Ser. No. 11 428 769 filed Jul. 5 2006 now U.S. Pat. No. 7 668 621 issued Feb. 23 2010 U.S. patent application Ser. No. 11 428 757 filed Jul. 5 2006 now U.S. Pat. No. 8 073 564 issued Dec. 6 2011 U.S. patent application Ser. No. 11 428 729 filed Jul. 5 2006 now U.S. Pat. No. 7 801 644 issued Sep. 21 2010 U.S. patent application Ser. No. 11 428 650 filed Jul. 5 2006 now U.S. Pat. No. 7 620 477 issued Nov. 17 2009 U.S. patent application Ser. No. 11 428 646 filed Jul. 5 2006 now U.S. Pat. No. 7 584 020 issued Sep. 1 2009 U.S. patent application Ser. No. 11 428 637 filed Jul. 5 2006 now U.S. Pat. No. 7 587 260 issued Sep. 8 2009 U.S. patent application Ser. No. 11 428 621 filed Jul. 5 2006 now U.S. Pat. No. 7 974 738 issued Jul. 5 2011 U.S. patent application Ser. No. 11 428 743 filed Jul. 5 2006 now U.S. Pat. No. 7 211 980 issued May 1 2007 the disclosures of each of which are incorporated by reference herein in their entireties. This application is also related to U.S. patent application Ser. No. 12 553 794 filed Sep. 3 2009 pending and U.S. patent application Ser. No. 13 049 788 filed Mar. 16 2012 pending.

The present invention relates generally to robotics and more specifically to software architectures for realizing levels of autonomy in robots.

Historically robot behaviors have been created for specific tasks and applications. These behaviors have generally been reinvented time and again for different robots and different applications. There has been no sustained attempt to provide a kernel of basic robot competence and decision making that can be used to bootstrap development across many different applications.

Some architectures have been proposed that provide a generic application programming interface API for querying various sensors and commanding various actuators however many of these architectures have been limited to raw inputs and outputs rather than provide the intelligence and behavior to a robot. As a result the behavior functionality created for one robot may not be easily ported to new robots. Other architectures have been proposed to allow limited behaviors to port across different robot platforms but these have generally been limited to specific low level control systems.

The problem with robots today is that they are not very bright. Current robot intelligence is really just a grab bag of programmed behaviors to keep mobile robots from doing stupid things like getting stuck in corners or running into obstacles. The promise of wireless robots is that they can be sent into remote situations that are too difficult or dangerous for humans. The reality is that today s robots generally lack the ability to make any decisions on their own and rely on continuous guidance by human operators watching live video from on board cameras.

Most commercial robots operate on a master slave principle. A human operator completely controls the movement of the robot from a remote location using robot based sensors such as video and Global Positioning System GPS . This setup often requires more than one operator per robot to navigate around obstacles and achieve a goal. As a result very skilled operators may be necessary to reliably direct the robot. Furthermore the intense concentration needed for controlling the robot can detract from achieving mission goals.

Although it has been recognized that there is a need for adjustable autonomy robot architectures currently define levels of autonomy that are only selectable by the user. As a result performance of the robot and its human controller is often dependent on the human controller s understanding of the robot s autonomy levels and when the various levels are appropriate.

Therefore there is a need for systems and methods that provide a more intuitive approach for the user to direct a robot s activities without having to understand the details of available robot behaviors and autonomy levels.

Embodiments of the present invention comprise methods and systems that provide a more intuitive approach for the user to direct a robot s activities without having to understand the available robot behaviors or autonomy levels. This intuitive approach provides a bridge between a user s intentions and a robot s behaviors by creating a new user interface and adjustments to robot initiative and behavior that does not need to be directly controlled by the user.

An embodiment of the present invention comprises a Graphical User Interface GUI for controlling a robot. The GUI includes an environment map window a robot designator one or more task designators and a control intermediary. The environment map window is configured for displaying a map of an environment proximate the robot. The robot designator is configured for showing a robot position and a robot pose in the environment map window. The task designators are configured for a user to position them in the environment map window and indicate a task for the robot to achieve. The control intermediary links user defined tasks input by the user with robot instructions issued by the GUI to the robot. The control intermediary analyzes a position of the task designators relative to a position of one or more robot components associated with the task designators. Responsive to the analysis the control intermediary determines task oriented robot instructions including defining autonomy level and intelligent behavior for the robot and communicates target achievement information to the robot. The target achievement information may include instructions for guiding the robot to achieve the task if the task oriented autonomy level comprises low robot initiative. The target achievement information may also include instructions for directing the robot to determine a robot plan for achieving the task if the task oriented autonomy level comprises high robot initiative.

Another embodiment of the present invention comprises a method for controlling a robot. The method includes providing a user interface for controlling the robot and positioning at least one task oriented target in a map on the user interface representing an environment of the robot. The method also includes determining a task oriented autonomy level of the robot responsive to a change in the at least one task oriented target. If the change in the task oriented target is smaller than a change threshold the method includes instructing the robot to achieve the task oriented target by intervention instructions from the user interface. If the change in the task oriented target is larger than the change threshold the method includes instructing the robot to achieve the task oriented target using robot initiative to determine a robot plan for achieving the task oriented target and implementing the robot plan.

Another embodiment of the present invention also comprises a method for controlling a robot. The method includes receiving instructions for achieving at least one task oriented target from a user interface. The instructions include at least one of intervention instructions robot initiative instructions and instructions for setting a task oriented autonomy level. If the instructions are robot initiative instructions the method includes developing a robot plan to achieve the task oriented target and performing the robot plan. If the instructions are intervention instructions then the method includes performing the intervention instructions for achieving the task oriented target and if present overriding the robot plan.

Another embodiment of the present invention comprises a robot platform. The robot platform includes at least one perceptor for perceiving environmental variables of interest at least one locomotor for providing mobility to the robot platform and a system controller. The system controller executes a task oriented autonomy system that receives instructions from a user interface using a communication interface. The instruction are for achieving at least one task oriented target and include at least one of intervention instructions robot initiative instructions and instructions for setting a task oriented autonomy level. If the instructions are robot initiative instructions the system controller develops a robot plan to achieve the at least one task oriented target and performs the robot plan. If the instructions are intervention instructions the system controller performs the intervention instructions from the user interface to achieve the task oriented target and if present overrides the robot plan.

Another embodiment of the present invention comprises a robot control system. The robot control system includes a user computer and a robot platform. The user computer includes a memory and one or more processors for executing a user interface. The user interface is configured for positioning at least one task oriented target in a map on the user interface representing an environment of the robot. The user interface also determines a task oriented autonomy level of the robot responsive to a change in the task oriented target. If the change in the task oriented target is smaller than a change threshold the user interface instructs the robot to achieve the task oriented target using intervention instructions from the user interface. If the change in the task oriented target is larger than the change threshold the user interface instructs the robot to achieve the task oriented target using robot initiative to determine a robot plan for achieving the task oriented target and implementing the robot plan. The robot platform includes a locomotor and a system controller. If robot initiative instructions are received from the user computer the system controller develops and performs the robot plan. If intervention instructions are received from the user computer the robot platform performs the intervention instructions to achieve the task oriented target and if appropriate overrides the robot plan.

In the following description circuits and functions may be shown in block diagram form in order not to obscure the present invention in unnecessary detail. Conversely specific circuit implementations shown and described are exemplary only and should not be construed as the only way to implement the present invention unless specified otherwise herein. Additionally block definitions and partitioning of logic between various blocks is exemplary of a specific implementation. It will be readily apparent to one of ordinary skill in the art that the present invention may be practiced by numerous other partitioning solutions. For the most part details concerning timing considerations and the like have been omitted where such details are not necessary to obtain a complete understanding of the present invention and are within the abilities of persons of ordinary skill in the relevant art.

In this description some drawings may illustrate signals as a single signal for clarity of presentation and description. It will be understood by a person of ordinary skill in the art that the signal may represent a bus of signals wherein the bus may have a variety of bit widths and the present invention may be implemented on any number of data signals including a single data signal.

Furthermore in this description of the invention reference is made to the accompanying drawings which form a part hereof and in which is shown by way of illustration specific embodiments in which the invention may be practiced. The embodiments are intended to describe aspects of the invention in sufficient detail to enable those skilled in the art to practice the invention. Other embodiments may be utilized and changes may be made without departing from the scope of the present invention. The following detailed description is not to be taken in a limiting sense and the scope of the present invention is defined only by the appended claims.

Headings are included herein to aid in locating certain sections of the following detailed description. These headings should not be considered to limit the scope of the concepts described under any specific heading. Furthermore concepts described in any specific heading are generally applicable in other sections throughout the entire specification.

The system controller may include a processor operably coupled to other system devices by internal buses . By way of example and not limitation the processor may be coupled to a memory through a memory bus . The system controller may also include an internal bus for coupling the processor to various other devices such as storage devices local input devices local output devices and local displays .

Local output devices may be devices such as speakers status lights and the like. Local input devices may be devices such as keyboards mice joysticks switches and the like.

Local displays may be as simple as light emitting diodes indicating status of functions of interest on the robot platform or may be as complex as a high resolution display terminal.

The communication channels may be adaptable to both wired and wireless communication as well as supporting various communication protocols. By way of example and not limitation the communication channels may be configured as a serial or parallel communication channel such as for example USB IEEE 1394 802.11a b g cellular telephone and other wired and wireless communication protocols.

The perceptors may include inertial sensors thermal sensors tactile sensors compasses range sensors sonar Global Positioning System GPS Ground Penetrating Radar GPR lasers for object detection and range sensing imaging devices and the like. Furthermore those of ordinary skill in the art will understand that many of these sensors may include a generator and a sensor to combine sensor inputs into meaningful actionable perceptions. For example sonar perceptors and GPR may generate sound waves or sub sonic waves and sense reflected waves. Similarly perceptors including lasers may include sensors configured for detecting reflected waves from the lasers for determining interruptions or phase shifts in the laser beam.

Imaging devices may be any suitable device for capturing images such as for example an infrared imager a video camera a still camera a digital camera a Complementary Metal Oxide Semiconductor CMOS imaging device a charge coupled device CCD imager and the like. In addition the imaging device may include optical devices for modifying the image to be captured such as for example lenses collimators filters and mirrors. For adjusting the direction at which the imaging device is oriented a robot platform may also include pan and tilt mechanisms coupled to the imaging device. Furthermore a robot platform may include a single imaging device or multiple imaging devices.

The manipulators may include vacuum devices magnetic pickup devices arm manipulators scoops grippers camera pan and tilt manipulators and the like.

The locomotors may include one or more wheels tracks legs rollers propellers and the like. For providing the locomotive power and steering capabilities the locomotors may be driven by motors actuators levers relays and the like. Furthermore perceptors may be configured in conjunction with the locomotors such as for example odometers and pedometers.

Software processes illustrated herein are intended to illustrate representative processes that may be performed by the robot platform or robot controller . Unless specified otherwise the order in which the processes are described is not intended to be construed as a limitation. Furthermore the processes may be implemented in any suitable hardware software firmware or combinations thereof. By way of example software processes may be stored on the storage device transferred to the memory for execution and executed by the processor .

When executed as firmware or software the instructions for performing the processes may be stored on a computer readable medium i.e. storage device . A computer readable medium includes but is not limited to magnetic and optical storage devices such as disk drives magnetic tape CDs compact disks DVDs digital versatile discs or digital video discs and semiconductor devices such as RAM DRAM ROM EPROM and Flash memory.

Conventionally robot architectures have been defined for individual robots and generally must be rewritten or modified to work with different sensor suites and robot platforms. This means that adapting the behavior functionality created for one robot platform to a different robot platform is problematic. Furthermore even architectures that propose a hardware abstraction layer to create a framework for accepting various hardware components still may not create a robot abstraction layer wherein the abstractions presented for high level behavioral programming are in terms of actionable components or generic robot attributes rather than the hardware present on the robot.

A notable aspect of the present invention is that it collates the sensor data issued from hardware or other robotic architectures into actionable information in the form of generic precepts. Embodiments of the present invention may include a generic robot architecture GRA which comprises an extensible low level framework which can be applied across a variety of different robot hardware platforms perceptor suites and low level proprietary control application programming interfaces APIs . By way of example some of these APIs may be Mobility Aria Aware Player etc.

At the lower level the GRA includes a hardware abstraction level which provides for portable object oriented access to low level hardware perception and control modules that may be present on a robot. The hardware abstraction level is reserved for hardware specific classes and includes for example implementations for the actual robot geometry and sensor placement on each robot type.

Above the hardware abstraction level the GRA includes a robot abstraction level which provides atomic elements i.e. building blocks of generic robot attributes and develops a membrane between the low level hardware abstractions and controls. This membrane is based on generic robot attributes or actionable components which include robot functions robot perceptions and robot status. Each generic robot attribute may utilize a variety of hardware abstractions and possibly other robot attributes to accomplish its individual function.

The robot abstraction level may include implementations that are generic to given proprietary low level APIs. Examples of functions in this class level include the interface calls for a variety of atomic level robot behaviors such as for example controlling motion and reading sonar data.

The GRA enables substantially seamless porting of behavioral intelligence to new hardware platforms and control APIs by defining generic robot attributes and actionable components to provide the membrane and translation between behavioral intelligence and the hardware. Once a definition for a robot in terms of platform geometries sensors and API calls has been specified behavior and intelligence may be ported in a substantially seamless manner for future development. In addition the object oriented structure enables straightforward extension of the architecture for defining new robot platforms as well as defining low level abstractions for new perceptors motivators communications channels and manipulators.

The GRA includes an interpreter such that existing and new robot behaviors port in a manner that is transparent to both the operator and the behavior developer. This interpreter may be used to translate commands and queries back and forth between the operator and robot with a common interface which can then be used to create perceptual abstractions and behaviors. When the common language supported by the GRA is used by robot developers it enables developed behaviors and functionality to be interchangeable across multiple robots. In addition to creating a framework for developing new robot capabilities the GRA interpreter may be used to translate existing robot capabilities into the common language so that the behavior can then be used on other robots. The GRA is portable across a variety of platforms and proprietary low level APIs. This is done by creating a standard method for commanding and querying robot functionality that exists on top of any particular robot manufacturer s control API. Moreover unlike systems where behavior stems from sensor data the GRA facilitates a consistent or predictable behavior output regardless of robot size or type by categorizing robot and sensor data into perceptual abstractions from which behaviors can be built.

The Generic Robot Architecture also includes a scripting structure for orchestrating the launch of the different servers and executables that may be used for running the GRA on a particular robot platform. Note that since these servers and executables e.g. laser server camera server and base platform application will differ from robot to robot the scripting structure includes the ability to easily specify and coordinate the launch of the files that may be needed for specific applications. In addition the scripting structure enables automatic launching of the system at boot time so that the robot is able to exhibit functionality without any operator involvement i.e. no need for a remote shell login .

The Generic Robot Architecture may access configuration files created for each defined robot type. For example the configuration files may specify what sensors actuators and API are being used on a particular robot. Use of the scripting structure together with the configuration enables easy reconfiguration of the behaviors and functionality of the robot without having to modify source code i.e. for example recompile the C C code .

The GRA keeps track of which capabilities are available e.g. sensors. actuators mapping systems communications on the specific embodiment and uses virtual and stub functions within the class hierarchy to ensure that commands and queries pertaining to capabilities that an individual robot does not have do not cause data access errors. For example in a case where a specific capability such as a manipulator does not exist the GRA returns special values indicating to the high level behavioral control code that the command cannot be completed or that the capability does not exist. This makes it much easier to port seamlessly between different robot types by allowing the behavior code to adapt automatically to different robot configurations.

The above discussion of GRA capabilities has focused on the robot oriented aspects of the GRA. However the robot oriented class structure is only one of many class structures included in the GRA. For example the GRA also includes multi tiered class structures for communication range sensing cameras and mapping. Each one of these class structures is set up to provide a level of functional modularity and allow different sensors and algorithms to be used interchangeably. By way of example and not limitation without changing the behavioral code built on the GRA at the robot behavior level it may be possible to swap various mapping and localization systems or cameras and yet achieve the same functionality simply by including the proper class modules at the hardware abstraction level and possibly at the robot abstraction level. Additional capabilities and features of each of the levels of the GRA are discussed below.

Action device abstractions may include for example vacuum devices magnetic pickup devices arm manipulators scoops grippers camera pan and tilt manipulators and the like.

The communication abstractions present substantially common communications interfaces to a variety of communication protocols and physical interfaces. The communication channels may be adaptable to both wired and wireless communication as well as supporting various communication protocols. By way of example and not limitation the communication abstractions may be configured to support serial and parallel communication channels such as for example USB IEEE 1394 802.11a b g cellular telephone and other wired and wireless communication protocols.

Locomotion abstractions may be based on robot motion not necessarily on specific hardware components. For example and not limitation motion control abstractions may include drive steering power speed force odometry and the like. Thus the motion abstractions can be tailored to individual third party drive controls at the hardware abstraction level and effectively abstracted away from other architectural components. In this manner support for motion control of a new robot platform may comprise simply supplying the APIs which control the actual motors actuators and the like into the locomotion abstraction framework.

The perception abstractions may include abstractions for a variety of perceptive hardware useful for robots such as for example inertial measurements imaging devices sonar measurements camera pan tilt abstractions GPS and iGPS abstractions thermal sensors infrared sensors tactile sensors laser control and perception abstractions GPR compass measurements EMI measurements and range abstractions.

While the hardware abstraction level focuses on a software model for a wide variety of hardware that may be useful on robots the robot abstraction level as illustrated in focuses on generic robot attributes. The generic robot attributes enable building blocks for defining robot behaviors at the robot behavior level and provide a membrane for separating the definition of robot behaviors from the low level hardware abstractions. Thus the each robot attributes may utilize one or more hardware abstractions to define its attribute. These robot attributes may be thought of as actionable abstractions. In other words a given actionable abstraction may fuse multiple hardware abstractions that provide similar information into a data set for a specific robot attribute. For example and not limitation the generic robot attribute of range may fuse range data from hardware abstractions of an IR sensor and a laser sensor to present a single coherent structure for the range attribute. In this way the GRA presents robot attributes as building blocks of interest for creating robot behaviors such that the robot behavior can use the attribute to develop a resulting behavior e.g. stop slow down turn right turn left etc. .

Furthermore a robot attribute may combine information from dissimilar hardware abstractions. By way of example and not limitation the position attributes may fuse information from a wide array of hardware abstractions such as perception modules like video compass GPS laser and sonar along with control modules like drive speed and odometry. Similarly a motion attribute may include information from position inertial range and obstruction abstractions.

This abstraction of robot attributes frees the developer from dealing with individual hardware elements. In addition each robot attribute can adapt to the amount and type of information it incorporates into the abstraction based on what hardware abstractions may be available on the robot platform.

The robot s attributes as illustrated in are defined at a relatively low level of atomic elements that include attributes of interest for a robot s perception status and control. Some of these robot attributes include robot health robot position robot motion robot bounding shape environmental occupancy grid and range . It will be readily apparent to those of ordinary skill in the art that the modules shown in are a representative rather than comprehensive example of robot attributes. Note that the term robot attributes is used somewhat loosely given that robot attributes may include physical attributes such as robot health and bounding shape as well as how the robot perceives its environment such as the environmental occupancy grid and range attributes .

The robot health abstractions may include for example general object models for determining the status and presence of various sensors and hardware modules determining the status and presence of various communication modules determining the status of on board computer components.

The robot bounding shape abstractions may include for example definitions of the physical size and boundaries of the robot and definitions of various thresholds for movement that define a safety zone or event horizon as is explained more fully below.

The robot motion abstractions may include abstractions for defining robot motion and orientation attributes such as for example obstructed motion velocity linear and angular accelerations forces and bump into obstacle and orientation attributes such as roll yaw and pitch.

The range attributes may include for example determination of range to obstacles from lasers sonar infrared and fused combinations thereof.

In more detail illustrates a representative embodiment of how a range abstraction may be organized. A variety of coordinate systems may be in use by the robot and an operator. By way of example a local coordinate system may be defined by an operator relative to a space of interest e.g. a building or a world coordinate system defined by sensors such as a GPS unit an iGPS unit a compass an altimeter and the like. A robot coordinate system may be defined in Cartesian coordinates relative to the robot s orientation such that for example the X axis is to the right the Y axis is straight ahead and the Z axis is up. Another robot coordinate system may be cylindrical coordinates with a range angle and height relative to the robot s current orientation.

The range measurements for the representative embodiment illustrated in are organized in a cylindrical coordinate system relative to the robot. The angles may be partitioned into regions covering the front left right and back of the robot and given names such as for example those used in .

While not shown those of ordinary skill in the art will recognize that with the exception of the Left Side and Right Side regions embodiments may include regions in the back which are a mirror image of those in the front wherein the Front portion of the name is replaced with Rear. 

Furthermore the range attributes define a range to the closest object within that range. However the abstraction of regions relative to the robot as used in the range abstraction may also be useful for many other robot attributes and robot behaviors that may require directional readings such as for example defining robot position robot motion camera positioning an occupancy grid map and the like.

In practice the range attributes may be combined to define a more specific direction. For example directly forward motion may be defined as a geometrically adjusted combination of Right In Front L Front R Front Front Left Side and Front Right side .

Returning to the robot abstractions may include robot position attributes . Mobile robots may operate effectively only if they or their operators know where they are. Conventional robots may rely on real time video and global positioning systems GPS as well as existing maps and floor plans to determine their location. However GPS may not be reliable indoors and video images may be obscured by smoke or dust or break up because of poor communications. Maps and floor plans may not be current and often are not readily available particularly in the chaotic aftermath of natural accidental or terrorist events. Consequently real world conditions on the ground often make conventional robots that rely on a priori maps ineffective.

Accurate positioning knowledge enables the creation of high resolution maps and accurate path following which may be needed for high level deliberative behavior such as systematically searching or patrolling an area.

Embodiments of the present invention may utilize various mapping or localization techniques including positioning systems such as indoor GPS outdoor GPS differential GPS theodolite systems wheel encoder information and the like. To make robots more autonomous embodiments of the present invention may fuse the mapping and localization information to build 3D maps on the fly that let robots understand their current position and an estimate of their surroundings. Using existing information map details may be enhanced as the robot moves through the environment. Ultimately a complete map containing rooms hallways doorways obstacles and targets may be available for use by the robot and its human operator. These maps also may be shared with other robots or human first responders.

With the on board mapping and positioning algorithm that accepts input from a variety of range sensors the robot may make substantially seamless transitions between indoor and outdoor operations without regard for GPS and video drop outs that occur during these transitions. Furthermore embodiments of the present invention provide enhanced fault tolerance because they do not require off board computing or reliance on potentially inaccurate or non existent a priori maps.

Embodiments of the present invention may use localization methods by sampling range readings from scanning lasers and ultrasonic sensors and by reasoning probabilistically about where the robot is within its internal model of the world. The robot localization problem may be divided into two sub tasks global position estimation and local position tracking. Global position estimation is the ability to determine the robot s position in an a priori or previously learned map given no information other than that the robot is somewhere in the region represented by the map. Once a robot s position has been found in the map local tracking is the problem of keeping track of the robot s position over time and movement.

The robot s state space may be enhanced by localizaton methods such as Monte Carlo techniques and Markovian probability grid approaches for position estimation as are well know by those of ordinary skill in the art. Many of these techniques provide efficient and substantially accurate mobile robot localization.

With a substantially accurate position for the robot determined local tracking can maintain the robot s position over time and movement using dead reckoning additional global positioning estimation or combinations thereof. Dead reckoning is a method of navigation by keeping track of how far you have gone in any particular direction. For example dead reckoning would determine that a robot has moved a distance of about five meters at an angle from the current pose of about 37 degrees if the robot moves four meters forward turns 90 degrees to the right and moves forward three meters. Dead reckoning can lead to navigation errors if the distance traveled in a given direction or the angle through which a robot turns is interpreted incorrectly. This can happen for example if one or more of the wheels on the robot spin in place when the robot encounters an obstacle.

Therefore dead reckoning accuracy may be bolstered by sensor information from the environment new global positioning estimates or combinations thereof. With some form of a map the robot can use range measurements to map features to enhance the accuracy of a pose estimate. Furthermore the accuracy of a pose estimate may be enhanced by new range measurements e.g. laser scans into a map that may be growing in size and accuracy. In Simultaneous Localization and Mapping SLAM information from the robot s encoders and laser sensors may be represented as a network of probabilistic constraints linking the successive positions poses of the robot. The encoders may relate one robot pose to the next via dead reckoning. To give further constraints between robot poses the laser scans may be matched with dead reckoning including constraints for when a robot returns to a previously visited area.

The robot abstractions may include environmental occupancy grid attributes . One form of map that may be useful from both the robot s perspective and an operator s perspective is an occupancy grid. An environmental occupancy grid formed by an environmental occupancy grid abstraction is illustrated in . In forming an occupancy grid a robot coordinate system may be defined in Cartesian coordinates relative to the robot s orientation such that for example the X axis is to the right the Y axis is straight ahead and the Z axis is up. Another robot coordinate system may be cylindrical coordinates with a range angle and height relative to the robot s current orientation. Furthermore occupancy grids may be translated to other coordinate systems for use by an operator.

An occupancy grid map may be developed by dividing the environment into a discrete grid of occupancy cells and assigning a probability to each grid indicating whether the grid is occupied by an object. Initially the occupancy grid may be set so that every occupancy cell is set to an initial probability. As the robot scans the environment range data developed from the scans may be used to update the occupancy grid. For example based on range data the robot may detect an object at a specific orientation and range away from the robot. This range data may be converted to a different coordinate system e.g. local or world Cartesian coordinates . As a result of this detection the robot may increase the probability that the particular occupancy cell is occupied and decrease the probability that occupancy cells between the robot and the detected object are occupied. As the robot moves through its environment new horizons may be exposed to the robots sensors which enable the occupancy grid to be expanded and enhanced. To enhance map building and localization even further multiple robots may explore an environment and cooperatively communicate their map information to each other or a robot controller to cooperatively build a map of the area.

An example occupancy grid map as it might be presented to an operator is illustrated in . The grid of occupancy cells can be seen as small squares on this occupancy grid map . A robot path is shown to illustrate how the robot may have moved through the environment in constructing the occupancy grid map . Of course those of ordinary skill in the art with recognize that depending on the application and expected environment the occupancy grid map may be defined in any suitable coordinate system and may vary in resolution i.e. size of each occupancy cell . In addition the occupancy grid map may include a dynamic resolution such that the resolution may start out quite coarse while the robot discovers the environment then evolve to a finer resolution as the robot becomes more familiar with its surroundings.

A robot platform may include a robot intelligence kernel may also be referred to herein as an intelligence kernel which coalesces hardware components for sensing motion manipulation and actions with software components for perception communication behavior and world modeling into a single cognitive behavior kernel that provides intrinsic intelligence for a wide variety of unmanned robot platforms. The intelligence kernel architecture may be configured to support multiple levels of robot autonomy that may be dynamically modified depending on operating conditions and operator wishes.

The robot intelligence kernel RIK may be used for developing a variety of intelligent robotic capabilities. By way of example and not limitation some of these capabilities including visual pursuit intruder detection and neutralization security applications urban reconnaissance search and rescue remote contamination survey and countermine operations.

Referring back to the software architecture diagram of the RIK comprises a multi level abstraction including a robot behavior level and a cognitive level . The RIK may also include the robot abstraction level and the hardware abstraction level discussed above.

Above the robot abstraction level the RIK includes the robot behavior level which define specific complex behaviors that a robot or a robot operator may want to accomplish. Each complex robot behavior may utilize a variety of robot attributes and in some cases a variety of hardware abstractions to perform the specific robot behavior.

Above the robot behavior level the RIK includes the cognitive level which provides cognitive conduct modules to blend and orchestrate the asynchronous events from the complex robot behaviors and generic robot behaviors into combinations of functions exhibiting cognitive behaviors wherein high level decision making may be performed by the robot the operator or combinations of the robot and the operator.

Some embodiments of the RIK may include at the lowest level the hardware abstraction level which provides for portable object oriented access to low level hardware perception and control modules that may be present on a robot. These hardware abstractions have been discussed above in the discussion of the GRA.

Some embodiments of the RIK may include above the hardware abstraction level the robot abstraction level including generic robot abstractions which provide atomic elements i.e. building blocks of generic robot attributes and develop a membrane between the low level hardware abstractions and control based on generic robot functions. Each generic robot abstraction may utilize a variety of hardware abstractions to accomplish its individual function. These generic robot abstractions have been discussed above in the discussion of the GRA.

While the robot abstraction level focuses on generic robot attributes higher levels of the RIK may focus on relatively complex robot behaviors at the robot behavior level or on robot intelligence and operator collaboration at the cognitive level .

The robot behavior level includes generic robot classes comprising functionality common to supporting behavior across most robot types. For example the robot behavior level includes utility functions e.g. calculate angle to goal and data structures that apply across substantially all robot types e.g. waypoint lists . At the same time the robot behavior level defines the abstractions to be free from implementation specifics such that the robot behaviors are substantially generic to all robots.

The robot behavior level as illustrated in may be loosely separated into reactive behaviors and deliberative behaviors . Of course it will be readily apparent to those of ordinary skill in the art that the modules shown in are a representative rather than comprehensive example of robot behaviors.

The reactive behaviors may be characterized as behaviors wherein the robot reacts to its perception of the environment based on robot attributes hardware abstractions or combinations thereof. Some of these reactive behaviors may include autonomous navigation obstacle avoidance guarded motion visual tracking laser tracking get unstuck behavior and reactive planning. As examples and not limitations details regarding some of these behaviors are discussed in the section below regarding application specific behaviors.

In contrast deliberative behaviors may be characterized as behaviors wherein the robot may need to make decisions on how to proceed based on the results of the reactive behaviors information from the robot attributes and hardware abstractions or combinations thereof. Some of these deliberative behaviors may include waypoint navigation with automatic speed adjustment global path planning and occupancy change detection. As examples and not limitations details regarding some of these behaviors are discussed in the section below regarding application specific behaviors.

The cognitive conduct level as illustrated in represents the highest level of abstraction wherein significant robot intelligence may be built in to cognitive conduct modules as well as significant operator robot collaboration to perform complex tasks requiring enhanced robot initiative . Cognitive conduct modules blend and orchestrate asynchronous firings from the reactive behaviors deliberative behaviors and robot attributes into intelligent robot conduct. Cognitive conduct modules may include conduct such as GoTo wherein the operator may simply give a coordinate for the robot to go to and the robot takes the initiative to plan a path and get to the specified location. This GoTo conduct may include a combination of robot behaviors robot attributes and hardware abstractions such as for example obstacle avoidance get unstuck reactive path planning deliberative path planning and waypoint navigation.

Another representative cognitive conduct module is human detection and pursuit wherein the robot may react to changes in the environment and pursue those changes. This human detection and pursuit conduct may also include pursuit of other objects such as for example another robot. The human detection and pursuit conduct may include a combination of robot behaviors see robot attributes and hardware abstractions such as for example occupancy change detection laser tracking visual tracking deliberative path planning reactive path planning and obstacle avoidance.

Other representative cognitive conduct modules include conducts such as exploration and reconnaissance conduct combined with map building leader follower conduct and search and identify conduct .

Of course it will be readily apparent to those of ordinary skill in the art that the cognitive conduct modules shown in are a representative rather than a comprehensive example of robot conduct that may be implemented using embodiments of the present invention.

A notable aspect of the RIK is that the cognitive conduct level and robot behaviors generally operate from a perception of speed of motion in relationship to objects and obstacles. In other words rather than being concerned with spatial horizons and the distance away from an object the cognitive conduct and robot behaviors are largely concerned with temporal horizons and how soon the robot may encounter an object. This enables defining the cognitive conduct and robot behaviors in a relativistic sense wherein for example the modules interpret motion as an event horizon wherein the robot may only be concerned with obstacles inside the event horizon. For example a robot behavior is not necessarily concerned with an object that is 10 meters away. Rather the robot behavior may be concerned that it may reach the object in two seconds. Thus the object may be within the event horizon when the object is 10 meters away and the robot is moving toward it at 5 meters second whereas if the object is 10 meters away and the robot is moving at 2 meters second the object may not be within the event horizon.

This relativistic perception enables an adaptation to processing power and current task load. If the robot is very busy for example processing video it may need to reduce its frequency of processing each task. In other words the amount of time to loop through all the cognitive conduct and robot behaviors may increase. However with the RIK the cognitive conduct and robot behaviors can adapt to this difference in frequency by modifying its behaviors. For example if the time through a loop reduces from 200 Hz to 100 Hz the behaviors and conducts will know about this change in loop frequency and may modify the way it makes a speed adjustment to avoid an object. For example the robot may need a larger change in its speed of motion to account for the fact that the next opportunity to adjust the speed is twice more distant in the future at 100 Hz than it would be at 200 Hz. This becomes more apparent in the discussion below regarding the guarded motion behavior.

To enable and control this temporal awareness the RIK includes a global timing loop in which cognitive conduct and robot behaviors may operate. Using this global timing loop each module can be made aware of information such as for example average time through a loop minimum and maximum time through a loop and expected delay for next timing tick.

With this temporal awareness the robot tends to modify its behavior by adjusting its motion and motion of its manipulators relative to its surroundings rather than adjusting its position relative to a distance to an object. Of course with the wide array of perceptors the robot is still very much aware of its pose and position relative to its environment and can modify its behavior based on this positional awareness. However with the RIK the temporal awareness is generally more influential on the cognitive conduct modules and robot behaviors than the positional awareness.

To enhance the operator robot tradeoff of control the intelligence kernel provides a dynamic autonomy structure which is a decomposition of autonomy levels allowing methods for shared control to permeate all levels of the multi level abstraction. Furthermore the intelligence kernel creates an object oriented software architecture which may require little or no source code changes when ported to other platforms and low level proprietary controllers.

The dynamic autonomy structure of the RIK provides a multi level harmonization between human intervention and robot initiative across robot behaviors. As capabilities and limitations change for both the human and the robot due to workload operator expertise communication dropout and other factors the RIK architecture enables shifts from one level of autonomy to another. Consequently the ability of the robot to protect itself make decisions and accomplish tasks without human assistance may enable increased operator efficiency.

At the lowest level referred to as teleoperation mode the robot may operate completely under remote control and take no initiative to perform operations on its own. At the second level referred to as safe mode robot movement is dependent on manual control from a remote operator. However in safe mode the robot may be equipped with a level of initiative that prevents the operator from causing the robot to collide with obstacles. At the third level referred to as shared mode the robot can relieve the operator from the burden of direct control. For example the robot may use reactive navigation to find a path based on the robot s perception of the environment. Shared mode provides for a balanced allocation of roles and responsibilities. The robot accepts varying levels of operator intervention and may support dialogue through the use of scripted suggestions e.g. Path blocked Continue left or right and other text messages that may appear within a graphical interface. At the fourth level referred to as collaborative tasking mode a high level of collaborative tasking may be developed between the operator and the robot using a series of high level tasks such as patrol search region or follow path. In collaborative tasking mode operator intervention occurs on the tasking level while the robot manages most decision making and navigation. At the highest level referred to as autonomous mode a robot may behave in a substantially autonomous manner needing nothing more than being enabled by an operator and perhaps given a very high level command such as for example survey the area or search for humans. 

The autonomy levels are structured in the intelligence kernel such that each new level of autonomy is built on and encompasses the subsequent level. For example a guarded motion mode processing explained more fully below may include the behavior and representational framework utilized by the teleoperation mode processing but also include additional levels of robot initiative based on the various robot attributes e.g. related to directional motion created in response to the teleoperation mode . Shared mode may include all of the functionality and direct control of safe mode but also allows robot initiative in response to the abstractions produced through the guarded motion mode processing e.g. fused range abstractions created in response to the direction motion abstractions . In addition the collaborative tasking mode may initiate robot responses to the abstractions created in shared mode processing such as recognition that a box canyon has been entered or that a communication link has been lost.

For a robotic system to gracefully accept a full spectrum of intervention possibilities interaction issues cannot be handled merely as augmentations to a control system. Therefore opportunities for operator intervention and robot initiative are incorporated as an integral part of the robot s intrinsic intelligence. Moreover for autonomous capabilities to evolve the RIK is configured such that a robot is able to recognize when help is needed from an operator other robot or combinations thereof and learn from these interactions.

As an example in one representative embodiment the robot includes a SONY CCD camera that can pan tilt and zoom to provide visual feedback to the operator in the teleoperation mode . The robot may also use this camera with increased robot initiative to characterize the environment and even conduct object tracking.

In this embodiment the RIK provides a graduated process for the robot to protect itself and the environment. To do so the RIK may fuse a variety of range sensor information. A laser range finder may be mounted on the front and sonar perceptors may be located around the mid section of the robot. The robot also may include highly sensitive bump strips around its perimeter that register whether anything has been touched. To protect the top of the robot especially the cameras and mission specific sensors placed on top of the robot infrared proximity sensors may be included to indicate when an object is less than a few inches from the robot. Additional infrared proximity sensors may be placed on the bottom of the robot and point ahead of the robot toward the ground in order to prevent the robot from traveling into open space e.g. traveling off of a landing down a stairway . Together these sensors provide a substantial field of protection around the robot and allow the operator to command the robot with increased confidence that the robot can take initiative to protect itself or its environment.

However avoiding obstacles may be insufficient. Many adverse environments may include forms of uneven terrain such as rubble. The robot should be able to recognize and respond to these obstacles. Inertial sensors may be used to provide acceleration data in three dimensions. This inertial information may be fused with information from the wheel encoders giving velocity and acceleration of the wheels and electrical current draw from the batteries to produce a measure of unexpected resistance that may be encountered by the robot. As part of the dynamic autonomy the operator may be able to choose to set a resistance limit that will automatically stop the robot once the specified threshold has been exceeded. The resistance limit may be useful not only for rough terrain but also in situations when the operator needs to override the safe motion capabilities based on the obstacle avoidance sensors to do things like push chairs and boxes out of the way and push doors open.

In addition the RIK enables operators to collaborate with mobile robots by defining an appropriate level of discourse including a shared vocabulary and a shared cognitive work space collaboratively constructed and updated on the fly through interaction with the real world. This cognitive work space could consist of terrain overlaid with semantic abstractions generated through autonomous recognition of environmental features with point and click operator validation and iconographic insertion of map entities. Real time semantic maps constructed collaboratively by humans ground robots and air vehicles could serve as the basis for a spectrum of mutual human robot interactions including tasking situation awareness human assisted perception and collaborative environmental understanding. Thus the RIK enables human robot communication within the context of a mission based on shared semantic maps between the robotic system and the operator.

With reference to additional details of the dynamic autonomy structure and corresponding operation modes can be discussed.

In teleoperation mode the operator has full continuous control of the robot at a low level. The robot takes little or no initiative except for example to stop after a specified time if it recognizes that communications have failed. Because the robot takes little or no initiative in this mode the dynamic autonomy implementation provides appropriate situation awareness to the operator using perceptual data fused from many different sensors. For example a tilt sensor may provide data on whether the robot is in danger of overturning. Inertial effects and abnormal torque on the wheels i.e. forces not associated with acceleration are fused to produce a measure of resistance as when for example the robot is climbing over or pushing against an obstacle. Even in teleoperation mode the operator may be able to choose to activate a resistance limit that permits the robot to respond to high resistance and bump sensors. Also a specialized interface may provide the operator with abstracted auditory graphical and textual representations of the environment and task.

Some representative behaviors and attributes that may be defined for teleoperation mode include joystick operation perceptor status power assessment and system status.

In safe mode the operator directs movements of the robot but the robot takes initiative to protect itself. In doing so this mode frees the operator to issue motion commands with less regard to protecting the robot greatly accelerating the speed and confidence with which the operator can accomplish remote tasks. The robot may assess its own status and surrounding environment to decide whether commands are safe. For example the robot possesses a substantial self awareness of its position and will attempt to stop its motion before a collision thereby placing minimal limits on the operator. In addition the robot may be configured to notify the operator of environmental features e.g. box canyon corner and hallway immediate obstacles tilt resistance etc. and also continuously assesses the validity of its diverse sensor readings and communication capabilities. In safe mode the robot may be configured to refuse to undertake a task if it does not have the ability i.e. sufficient power or perceptual resources to safely accomplish it.

Some representative behaviors and attributes that may be defined for safe mode include guarded motion resistance limits and bump sensing.

In shared mode the robot may take the initiative to choose its own path responds autonomously to the environment and work to accomplish local objectives. This initiative is primarily reactive rather than deliberative. In terms of navigation shared mode may be configured such that the robot responds only to its local e.g. a two second event horizon or a six meter radius sensed environment. Although the robot may handle the low level navigation and obstacle avoidance the operator may supply intermittent input often at the robot s request to guide the robot in general directions. For example a Get Unstuck behavior enables the robot to autonomously extricate itself from highly cluttered areas that may be difficult for a remote operator to handle.

Some representative behaviors and attributes that may be defined for shared mode include reactive planning get unstuck behavior and obstacle avoidance.

In collaborative tasking mode the robot may perform tasks such as for example global path planning to select its own route requiring no operator input except high level tasking such as follow that target or search this area perhaps specified by drawing a circle around a given area on the map created by the robot . For all these levels the intelligence resides on the robot itself such that off board processing is unnecessary. To permit deployment within shielded structures a customized communication protocol enables very low bandwidth communications to pass over a serial radio link only when needed. The system may use multiple and separate communications channels with the ability to reroute data when one or more connection is lost.

Some representative cognitive conduct and robot behaviors and robot attributes that may be defined for collaborative tasking mode include waypoint navigation global path planning go to behavior retro traverse behavior area search behavior and environment patrol.

In autonomous mode the robot may perform with minimal to no operator intervention . For behaviors in autonomous mode the operator may simply give a command for the robot to perform. Other than reporting status to the operator the robot may be free to plan paths prioritize tasks and carry out the command using deliberative behaviors defined by the robots initiative.

Some representative behaviors and attributes that may be defined for autonomous mode include pursuit behaviors perimeter surveillance urban reconnaissance human presence detection geological surveys radiation surveys virtual rail behavior countermine operations and seeking improvised explosive devices.

Conventionally robots have been designed as extensions of human mobility and senses. Most seek to keep the human in substantially complete control allowing the operator through input from video cameras and other on board sensors to guide the robot and view remote locations. In this conventional master slave relationship the operator provides the intelligence and the robot is a mere mobile platform to extend the operator s senses. The object is for the operator perched as it were on the robot s back to complete some desired tasks. As a result conventional robot architectures may be limited by the need to maintain continuous high bandwidth communications links with their operators to supply clear real time video images and receive instructions. Operators may find it difficult to visually navigate when conditions are smoky dusty poorly lit completely dark or full of obstacles and when communications are lost because of distance or obstructions.

The Robot Intelligence Kernel enables a modification to the way humans and robots interact from master slave to a collaborative relationship in which the robot can assume varying degrees of autonomy. As the robot initiative increases the operator can turn his or her attention to the crucial tasks at hand e.g. locating victims hazards dangerous materials following suspects measuring radiation and or contaminant levels without worrying about moment to moment navigation decisions or communications gaps.

The RIK places the intelligence required for high levels of autonomy within the robot. Unlike conventional designs off board processing is not necessary. Furthermore the RIK includes low bandwidth communication protocols and can adapt to changing connectivity and bandwidth capabilities. By reducing or eliminating the need for high bandwidth video feeds the robot s real world sensor information can be sent as compact data packets over low bandwidth 

Kalman filters are efficient recursive filters that can estimate the state of a dynamic system from a series of incomplete and noisy measurements. By way of example and not limitation many of the perceptors used in the RIK include an emitter sensor combination such as for example an acoustic emitter and a microphone array as a sensors. These perceptors may exhibit different measurement characteristics depending on the relative pose of the emitter and target and how they interact with the environment. In addition to one degree or another the sensors may include noise characteristics relative to the measured values. In robotic applications Kalman filters may be used in many applications for improving the information available from perceptors. As one example of many applications when tracking a target information about the location speed and acceleration of the target may include significant corruption due to noise at any given instant of time. However in dynamic systems that include movement a Kalman filter may exploit the dynamics of the target which govern its time progression to remove the effects of the noise and get a substantially accurate estimate of the target s dynamics. Thus a Kalman filter can use filtering to assist in estimating the targets location at the present time as well as prediction to estimate a targets location at a future time.

As a result of the Kalman filtering or after being processed by the Kalman filter information from the hardware abstractions and robot attributes may be combined to develop other robot attributes. As examples the robot attributes illustrated in include position movement obstruction occupancy and other abstractions .

With the robot attributes developed information from these robot attributes may be available for other modules within the RIK at the cognitive level the robot behavior level and the robot abstraction level .

In addition information from these robot attributes may be processed by the RIK and communicated to the robot controller or other robots as illustrated by the lower portion of . Processing information from the robot conduct behavior and attributes as well as information from hardware abstractions serves to reduce the required bandwidth and latency such that the proper information may be communicated quickly and concisely. Processing steps performed by the RIK may include a significance filter a timing module prioritization and bandwidth control .

The significance filter may be used as a temporal filter to compare a time varying data stream from a given RIK module. By comparing current data to previous data the current data may not need to sent at all or may be compressed using conventional data compression techniques such as for example run length encoding and Huffman encoding. Another example would be imaging data which may use data compression algorithms such as Joint Photographic Experts Group JPEG compression and Moving Picture Experts Group MPEG compression to significantly reduce the needed bandwidth to communicate the information.

The timing module may be used to monitor information from each RIK module to optimize the periodicity at which it may be needed. Some information may require periodic updates at a faster rate than others. In other words timing modulation may be used to customize the periodicity of transmissions of different types of information based on how important it may be to receive high frequency updates for that information. For example it may be more important to notify an operator or other robot of the robots position more often than it would be to update the occupancy grid map .

The prioritization operation may be used to determine which information to send ahead of other information based on how important it may be to minimize latency from when data is available to when it is received by an operator or another robot. For example it may be more important to reduce latency on control commands and control queries relative to map data. As another example in some cognitive conduct modules where there may be significant collaboration between the robot and an operator or in teleoperation mode where the operator is in control it may be important to minimize the latency of video information so that the operator does not perceive a significant time delay between what the robot is perceiving and when it is presented to the operator.

These examples illustrate that for prioritization as well as the significance filter the timing modulation and the bandwidth control communication may be task dependent and autonomy mode dependent. As a result information that may be a high priority in one autonomy mode may receive a lower priority in another autonomy mode.

The bandwidth control operation may be used to limit bandwidth based on the communication channel s bandwidth and how much of that bandwidth may be allocated to the robot. An example here might include progressive JPEG wherein a less detailed i.e. coarser version of an image may be transmitted if limited bandwidth is available. For video an example may be to transmit at a lower frame rate.

After the communication processing is complete the resultant information may be communicated to or from the robot controller or another robot. For example the information may be sent from the robot s communication device across the communication link to a communication device on a robot controller which includes a multi robot interface .

Some robot attributes such as the mapping and localization attribute may use information from a variety of hardware abstractions as well as other robot attributes . The mapping and localization attribute may use sonar and laser information from hardware abstractions together with position information and local position information to assist in defining maps of the environment and the position of the robot on those maps. Line is bold to indicate that the mapping and localization attribute may be used by any or all of the environment abstractions . For example the occupancy grid abstraction uses information from the mapping and localization attribute to build an occupancy grid as is explained among other places above with respect to . Additionally the robot s map position attribute may use the mapping and localization attribute and the occupancy grid attribute to determine the robot s current position within the occupancy grid.

Bold line as shown in indicates that any or all of the robot abstractions and environment abstractions may be used at higher levels of the RIK such as the communications layer explained above with respect to and the behavior modulation explained below with respect to .

As an example the event horizon attribute may utilize and fuse information from robot abstractions such as range and movement. Information from the event horizon attribute may be used by behaviors such as for example the guarded motion behavior and the obstacle avoidance behavior . Bold line illustrates that the guarded motion behavior and the obstacle avoidance behavior may be used by a variety of other robot behaviors and cognitive conduct such as for example follow pursuit conduct virtual rail conduct countermine conduct area search behavior and remote survey conduct.

The descriptions in this section illustrate representative embodiments of robot behaviors and cognitive conduct that may be included in embodiments of the present invention. Of course those of ordinary skill in the art will recognize these robot behaviors and cognitive conduct are illustrative embodiments and are not intended to be a complete list or complete description of the robot behaviors and cognitive conduct that may be implemented in embodiments of the present invention.

In general in the flow diagrams illustrated herein T indicates an angular velocity of either the robot or a manipulator and V indicates a linear velocity. Also generally T and V are indicated as a percentage of a predetermined maximum. Thus V 20 indicates 20 of the presently specified maximum velocity which may be modified depending on the situation of the robot or manipulator. Similarly T 20 indicates 20 of the presently specified maximum angular velocity of the robot or manipulator. It will be understood that the presently specified maximums may be modified over time depending on the situations encountered. In addition those of ordinary skill in the art will recognize that the values of linear and angular velocities used for the robot behaviors and cognitive conduct described herein are representative of a specific embodiment. While this specific embodiment may be useful in a wide variety of robot platform configurations other linear and angular velocities are contemplated within the scope of the present invention.

Furthermore those of ordinary skill in the art will recognize that the use of velocities rather than absolute directions is enabled largely by the temporal awareness of the robot behaviors and cognitive conduct in combination with the global timing loop. This gives the robot behaviors and cognitive conduct an opportunity to adjust velocities on each timing loop enabling smoother accelerations and decelerations. Furthermore the temporal awareness creates a behavior of constantly moving toward a target in a relative sense rather than attempting to move toward an absolute spatial point.

Autonomous navigation may be a significant component for many mobile autonomous robot applications. Using autonomous navigation a robot may effectively handle the task of traversing varied terrain while responding to positive and negative obstacles uneven terrain and other hazards. Embodiments of the present invention enable the basic intelligence necessary to allow a broad range of robotic vehicles to navigate effectively both indoors and outdoors.

Many proposed autonomous navigation systems simply provide GPS waypoint navigation. However GPS can be jammed and may be unavailable indoors or under forest canopy. A more autonomous navigation system includes the intrinsic intelligence to handle navigation even when external assistance including GPS and communications has been lost. Embodiments of the present invention include a portable domain general autonomous navigation system which blends the responsiveness of reactive sensor based control with the cognitive approach found through waypoint following and path planning. Through its use of the perceptual abstractions within the robot attributes of the GRA the autonomous navigation system can be used with a diverse range of available sensors e.g. range inertial attitude bump and available positioning systems e.g. GPS laser RF etc. .

The autonomous navigation capability may scale automatically to different operational speeds may be configured easily for different perceptor suites and may be easily parameterized to be portable across different robot geometries and locomotion devices. Two notable aspects of autonomous navigation are a guarded motion behavior wherein the robot may gracefully adjust its speed and direction near obstacles without needing to come to a full stop and an obstacle avoidance behavior wherein the robot may successfully navigate around known obstacles in its environment. Guarded motion and obstacle avoidance may work in synergy to create an autonomous navigation capability that adapts to the robots currently perceived environment. Moreover the behavior structure that governs autonomous navigation allows the entire assembly of behaviors to be used not only for obstacles but for other aspects of the environment which require careful maneuvering such as landmine detection.

The robot s obstacle avoidance and navigation behaviors are derived from a number of robot attributes that enable the robot to avoid collisions and find paths through dense obstacles. The reactive behaviors may be configured as nested decision trees comprising rules that fire based on combinations of these perceptual abstractions.

The first level of behaviors which may be referred to as action primitives provide the basic capabilities important to most robot activity. The behavior framework enables these primitives to be coupled and orchestrated to produce more complex navigational behaviors. In other words combining action primitives may involve switching from one behavior to another subsuming the outputs of another behavior or layering multiple behaviors. For example when encountering a dense field of obstacles that constrain motion in several directions the standard confluence of obstacle avoidance behaviors may give way to the high level navigational behavior Get Unstuck as is explained more fully below. This behavior involves rules which when activated in response to combinations of perceptual abstractions switch between several lower level behaviors including Turn till head is clear and Backout. 

The need for guarded motion has been well documented in the literature regarding unmanned ground vehicles. A goal of guarded motion is for the robot to be able to drive at high speeds either in response to the operator or software directed control through one of the other robot behaviors or cognitive conduct modules while maintaining a safe distance between the vehicle and obstacles in its path. The conventional approach usually involves calculating this safe distance as a product of the robot s speed. However this means that the deceleration and the distance from the obstacle at which the robot will actually stop may vary based on the low level controller responsiveness of the low level locomotor controls and the physical attributes of the robot itself e.g. wheels weight etc. . This variation in stopping speed and distance may contribute to confusion on the part of the operator who may perceive inconsistency in the behavior of the robot.

The guarded motion behavior according to embodiments of the present invention enables the robot to come to a stop at a substantially precise specified distance from an obstacle regardless of the robot s initial speed its physical characteristics and the responsiveness of the low level locomotor control schema. As a result the robot can take initiative to avoid collisions in a safe and consistent manner.

In general the guarded motion behavior uses range sensing e.g. from laser sonar infrared or combinations thereof of nearby obstacles to scale down its speed using an event horizon calculation. The event horizon determines the maximum speed the robot can safely travel and still come to a stop if needed at a specified distance from the obstacle. By scaling down the speed by many small increments perhaps hundreds of times per second it is possible to ensure that regardless of the commanded translational or rotational velocity guarded motion will stop the robot at substantially the same distance from an obstacle. As an example if the robot is being driven near an obstacle rather than directly toward it guarded motion will not stop the robot but may slow its speed according to the event horizon calculation. This improves the operator s ability to traverse cluttered areas and limits the potential for operators to be frustrated by robot initiative.

The guarded motion algorithm is generally described for one direction however in actuality it is executed for each direction. In addition it should be emphasized that the process shown in operates within the RIK framework of the global timing loop. Therefore the guarded motion behavior is re entered and executed again for each timing loop.

To begin decision block determines if guarded motion is enabled. If not control transitions to the end of the guarded motion behavior .

If guarded motion is enabled control transfers to decision block to test whether sensors indicate that the robot may have bumped into an obstacle. The robot may include tactile type sensors that detect contact with obstacles. If these sensors are present their hardware abstractions may be queried to determine if they sense any contact. If a bump is sensed it is too late to perform guarded motion. As a result operation block causes the robot to move in a direction opposite to the bump at a reduced speed that is 20 of a predefined maximum speed without turning and then exits. This motion is indicated in operation block as no turn i.e. T 0 and a speed in the opposite direction i.e. V 20 .

If no bump is detected control transfers to decision block where a resistance limit determination is performed. This resistance limit measures impedance to motion that may be incongruous with normal unimpeded motion. In this representative embodiment the resistance limit evaluates true if the wheel acceleration equals zero the force on the wheels is greater than zero the robot has an inertial acceleration that is less than 0.15 and the resulting impedance to motion is greater than a predefined resistance limit. If this resistance limit evaluation is true operation block halts motion in the impeded direction then exits. Of course those of ordinary skill in the art will recognize that this is a specific implementation for an embodiment with wheels and a specific inertial acceleration threshold. Other embodiments within the scope of the present invention may include different sensors and thresholds to determine if motion is being impeded in any given direction based on that embodiment s physical configuration and method of locomotion.

If motion is not being impeded control transfers to decision block to determine if any obstacles are within an event horizon. An event horizon is calculated as a predetermined temporal threshold plus a speed adjustment. In other words obstacles inside of the event horizon are obstacles that the robot may collide with at the present speed and direction. Once again this calculation is performed in all directions around the robot. As a result even if an obstacle is not directly in the robot s current path which may include translational and rotational movement it may be close enough to create a potential for a collision. As a result the event horizon calculation may be used to decide whether the robot s current rotational and translational velocity will allow the robot time to stop before encroaching the predetermined threshold distance. If there are no objects sensed within the event horizon there is no need to modify the robot s current motion and the algorithm exits.

If an obstacle is sensed within the event horizon operation block begins a safety glide as part of the overall timing loop to reduce the robot s speed. As the robot s speed is reduced the event horizon proportional to that the speed is reduced. If the reduction is sufficient the next time through the timing loop the obstacle may no longer be within the event horizon even though it may be closer to the robot. This combination of the event horizon and timing loop enables smooth deceleration because each loop iteration where the event horizon calculation exceeds the safety threshold the speed of the robot either translational rotational or both may be curtailed by a small percentage. This enables a smooth slow down and also enables the robot to proceed at the fastest speed that is safe. The new speed may be determined as a combination of the current speed and a loop speed adjustment. For example and not limitation New speed current speed 0.75 loop speed adjust . The loop speed adjust variable may be modified to compensate for how often the timing loop is executed and the desired maximum rate of deceleration. Of course those of ordinary skill in the art will recognize that this is a specific implementation. While this implementation may encompass a large array of robot configurations other embodiments within the scope of the present invention may include different scale factors for determining the new speed based on a robot s tasks locomotion methods physical attributes and the like.

Next decision block determines whether an obstacle is within a danger zone. This may include a spatial measurement wherein the range to the obstacle in a given direction is less than a predetermined threshold. If not there are likely no obstacles in the danger zone and the process exits.

If an obstacle is detected in the danger zone operation block stops motion in the current direction and sets a flag indicating a motion obstruction which may be used by other attributes behaviors or conduct.

As mentioned earlier the guarded motion behavior operates on a global timing loop. Consequently the guarded motion behavior will be re entered and the process repeated on the next time tick of the global timing loop.

In general the obstacle avoidance behavior uses range sensing e.g. from laser sonar infrared or combinations thereof of nearby obstacles to adapt its translational velocity and rotational velocity using the event horizon determinations explained earlier with respect to the guarded motion behavior. As stated earlier the obstacle avoidance behavior works with the guarded motion behavior as building blocks for full autonomous navigation. In addition it should be emphasized that the processes shown in operate within the RIK framework of the global timing loop. Therefore the obstacle avoidance behavior is re entered and executed again for each timing loop.

To begin the translational velocity portion of decision block determines if waypoint following is enabled. If so control transfers out of the obstacle avoidance behavior to a waypoint following behavior which is explained form fully below.

If waypoint following is not enabled control transfers to decision block to first test to see if the robot is blocked directly in front. If so control transfers to operation block to set the robot s translational speed to zero. Then control transfers out of the translational velocity behavior and into the rotational velocity behavior so the robot can attempt to turn around the object. This test at decision block checks for objects directly in front of the robot. To reiterate the obstacle avoidance behavior like most behaviors and conducts in the RIK is temporally based. In other words the robot is most aware of its velocity and whether objects are within an event horizon related to time until it may encounter an object. In the case of being blocked in front the robot may not be able to gracefully slow down through the guarded motion behavior. Perhaps because the object simply appeared in front of the robot without an opportunity to follow typical slow down procedures that may be used if an object is within an event horizon. For example the object may be another robot or a human that has quickly moved in front of the robot so that the guarded motion behavior has not had an opportunity to be effective.

If nothing is blocking the robot in front decision block tests to see if a detection behavior is in progress. A detection behavior may be a behavior where the robot is using a sensor in an attempt to find something. For example the countermine conduct is a detection behavior that is searching for landmines. In these types of detection behaviors obstacle avoidance may want to approach much closer to objects or may want to approach objects with a much slower speed to allow time for the detection function to operate. Thus if a detection behavior is active operation block sets a desired speed variable based on detection parameters that may be important. By way of example and not limitation in the case of the countermine conduct this desired speed may be set as Desired Speed Max passover rate Scan amplitude Scan Speed . In this countermine conduct example the Max passover rate may indicate a maximum desired speed for passing over the landmine. This speed may be reduced by other factors. For example the Scan amplitude Scan Speed term reduces the desired speed based on a factor of how fast the mine sensor sweeps an area. Thus the Scan amplitude term defines a term of the extent of the scan sweep and the Scan Speed defines the rate at which the scan happens. For example with a large Scan amplitude and a small Scan Speed the Desired Speed will be reduced significantly relative to the Max passover rate to generate a slow speed for performing the scan. While countermine conduct is used as an example of a detection behavior those of ordinary skill in the art will recognize that embodiments of the present invention may include a wide variety of detection behaviors such as for example radiation detection chemical detection and the like.

If a detection behavior is not in progress decision block tests to see if a velocity limit is set. In some embodiments of the invention it may be possible for the operator to set a velocity limit that the robot should not exceed even if the robot believes it may be able to safely go faster. For example if the operator is performing a detailed visual search the robot may be performing autonomous navigation while the operator is controlling a camera. The operator may wish to keep the robot going slow to have time to perform the visual search.

If a velocity limit is set operation block sets the desired speed variable relative to the velocity limit. The equation illustrated in operation block is a representative equation that may be used. The 0.1 term is a term used to ensure that the robot continues to make very slow progress which may be useful to many of the robot attributes behaviors and conduct. In this equation the Speed Factor term is a number from one to ten which may be set by other software modules for example the guarded motion behavior to indicate a relative speed at which the robot should proceed. Thus the desired speed is set as a fractional amount between zero and one in 0.1 increments of the Max Limit Speed.

If a velocity limit is not set operation block sets the desired speed variable relative to the maximum speed set for the robot i.e. Max Speed with an equation similar to that for operation block except Max Speed is used rather than Max Limit Speed.

After the desired speed variable is set by operation block or decision block tests to see if anything is within the event horizon. This test may be based on the robot s physical dimensions including protrusions from the robot such as an arm relative to the robot s current speed. As an example using an arm extension something inside the event horizon may be determined by the equation Min Front Range

Where the Min Front Range indicates a range to an obstacle in front 1.0 is a safety factor Arm Extension indicates the distance beyond the robot that the arm currently extends and Current Velocity indicates the robot s current translational velocity.

If there is something detected within the event horizon operation block sets the current speed based on the distance to the obstacle. Thus the example equation in block sets the speed based on the range to the object less a Forward Threshold set as a safety factor. With this speed guarded motion has an opportunity to be effective and the speed may be reduced further on the next iteration of the timing loop if the object is still within the event horizon. After setting the speed control transfers out of the translational velocity behavior and into the rotational velocity behavior.

If there is nothing detected within the event horizon operation block sets the speed to the desired speed variable that was set previously by operation block or . After setting the speed control transfers out of the translational velocity behavior and into the rotational velocity behavior .

To begin the rotation velocity portion of decision block determines if waypoint following is enabled. If so control transfers out of the obstacle avoidance behavior to a waypoint following behavior which is explained form fully below.

If waypoint following is not enabled control transfers to decision block to first test to see if the robot is blocked directly in front. If so control transfers to operation block to set the robot s translational speed to zero. Then control transfers out of the translational velocity behavior and into the rotational velocity behavior so the robot can attempt to turn around the object. This test at decision block checks for objects directly in front of the robot. To reiterate the obstacle avoidance behavior like most behaviors and conducts in the RIK is temporally based. In other words the robot is most aware of its velocity and whether objects are within an event horizon related to time until it may encounter an object. In the case of being blocked in front the robot may not be able to gracefully slow down through the guarded motion behavior. Perhaps because the object simply appeared in front of the robot without an opportunity to follow typical slow down procedures that may be used if an object is within an event horizon. For example the object may be another robot or a human that has quickly moved in front of the robot so that the guarded motion behavior has not had an opportunity to be effective.

If nothing is blocking the robot in front decision block tests to see if a detection behavior is in progress. A detection behavior may be a behavior where the robot is using a sensor in an attempt to find something. For example the countermine conduct is a detection behavior that is searching for landmines. In these types of detection behaviors obstacle avoidance may want to approach much closer to objects or may want to approach objects with a much slower speed to allow time for the detection function to operate. Thus if a detection behavior is active operation block sets a desired speed variable based on detection parameters that may be important. By way of example and not limitation in the case of the countermine conduct this desired speed may be set as Desired Speed Max passover rate Scan amplitude Scan Speed . In this countermine conduct example the Max passover rate may indicate a maximum desired speed for passing over the landmine. This speed may be reduced by other factors. For example the Scan amplitude Scan Speed term reduces the desired speed based on a factor of how fast the mine sensor sweeps an area. Thus the Scan amplitude term defines a term of the extent of the scan sweep and the Scan Speed defines the rate at which the scan happens. For example with a large Scan amplitude and a small Scan Speed the Desired Speed will be reduced significantly relative to the Max passover rate to generate a slow speed for performing the scan. While countermine conduct is used as an example of a detection behavior those of ordinary skill in the art will recognize that embodiments of the present invention may include a wide variety of detection behaviors such as for example radiation detection chemical detection ground penetrating radar and the like.

If a detection behavior is not in progress decision block tests to see if a velocity limit is set. In some embodiments of the invention it may be possible for the operator to set a velocity limit that the robot should not exceed even if the robot believes it may be able to safely go faster. For example if the operator is performing a detailed visual search the robot may be performing autonomous navigation while the operator is controlling a camera. The operator may wish to keep the robot going slow to have time to perform the visual search.

At decision block the process checks to see if the robot is blocked in front. If so the process performs a series of checks to see where other obstacles may be to determine a desired rotational velocity and direction. This obstacle checking process begins with decision block testing to see if the robot is blocked on the left side. If the robot is blocked on the left side and also in front operation block sets a new value for a turn velocity to the right. In the representative embodiment illustrated in a positive rotational velocity is defined as a turn to the left and a negative rotational velocity is defined as a turn to the right. Thus generally Turn left is a positive value indicating a rotational velocity to the left and Turn right is a negative value indicating a rotational velocity to the right. Thus operation block reduces the rotational velocity in the current direction by about one half plus a small offset used to ensure that the rotational velocity does not reach zero. After setting the new rotation velocity the process exits.

If the robot is not blocked on the left decision block tests to see if the robot in blocked on the right. If so operation block sets a new value for a turn velocity to the right similar to that velocity setting in operation block . In other words set the rotational velocity to the left to about one half plus a small offset used to ensure that the rotational velocity does not reach zero. After setting the new rotation velocity the process exits.

If the robot is blocked in the front but not on the left or right the process then decides which way to turn to get around the blockage by checking to see whether the nearest obstacle in a measurable range is to the right or left and adjusting the rotational velocity to be away from the obstacle. Operation block checks to see if the nearest obstacle is to the left. If so operation block sets the rotational velocity to the right i.e. away from the obstacle at a velocity of 30 of a maximum defined rotational velocity. If the nearest obstacle is not to the left operation block sets the rotational velocity to the left at a velocity of 30 of a maximum defined rotational velocity. After setting the new rotation velocity by either operation block or the process exits.

If the robot was not blocked in front based on decision block then decision block performs a threading the needle process. This starts with decision block determining a range to obstacles that may still be in front of the robot but not directly blocking the robot. To do this decision block test to see if Min Front Range is greater than two times a predefined threshold for the front direction and to see if Min Narrow Front is greater than two times the robot s length. If both these tests are true it may be relatively clear in front and the process decides to reduce the rotational velocity in the current direction to make the direction more straight ahead until the next global timing loop. Therefore decision block tests to see if the current rotational direction is left. If so decision block tests to see if the magnitude of the left rotational velocity is greater than twice a turn threshold. If so operation block reduces the rotational velocity in the left direction by one half and the process exits. If the current rotational direction is not left decision block tests to see if the magnitude of the right rotational velocity is greater than twice a turn threshold. If so operation block reduces the rotational velocity in the right direction by one half and the process exits.

If decision block or evaluates false decision block tests to see if anything is currently within the event horizon.

This test may be based on the robot s physical dimensions including protrusions from the robot such as an arm relative to the robot s current speed. In addition this test is likely the same as the event horizon described above for the translational velocity when discussing decision block in . In other words is the Minimum Front Range less that an Event Range. Wherein the Event Range 1.0 Arm Extension 1.75 Abs Current Velocity .

If there is nothing within the event horizon i.e. decision block evaluates false there is likely no need to change the current rotational velocity so the process exits. If there is something within the event horizon but not within the threading the needle process or blocking the robot in front the rotational velocity may be adjusted at a more gradual rate. Thus if operation block evaluates true decision block tests to see if the closest object is on the left side. If so operation block sets a new rotation velocity to the right. If the closest object is not on the left operation block sets a new rotation velocity to the left. The rotational velocity that is set in operation blocks and are similar except for direction. In this representative embodiment the rotational velocity may be set as a function of the Event Range from the event horizon test of decision block . Thus the rotational velocity may be set as Event Range Min Front Range 4.

As mentioned earlier the obstacle avoidance behavior operates on the global timing loop. Consequently both the translational velocity and rotational velocity may be adjusted again on the next time tick of the global timing loop allowing for relatively quick periodic adjustments to the velocities.

A get unstuck behavior as illustrated in includes significant robot initiative to extricate itself from the stuck position with little or no help from the operator. Sometimes when a robot is operating under its own initiative or even under operator control the robot may get stuck and have difficulty getting free from that position. Often times the operator may have limited understanding of the robots position relative to the robots understanding with its wide variety of perceptors. In general the get unstuck behavior may use range sensing e.g. from laser sonar infrared or combinations thereof to determine nearby obstacles and their position relative to the robot.

The get unstuck behavior begins at decision block by determining if the current path is blocked. This blocked situation may be defined as obstacle present in front on the front right side and on the front left side. If the path is blocked control transfers to operation block which is explained below. For an example and using the range definitions defined above under the description of the range attribute a blocked path may be defined by the Boolean equation 

Wherein robot forward thresh is a predetermined threshold parameter that may be robot specific to define a safety distance or maneuverability distance away from the robot.

If the path is not blocked decision block determines if forward motion and turning motion is obstructed. If motion is obstructed control transfers to operation block which is explained below. For an example this motion obstruction may be determined by the Boolean equation 

If motion is not obstructed decision block determines if the robot is in a box canyon. If the robot is not in a box canyon the get unstuck behavior exits because it appears the robot is not in a stuck situation. If the robot is in a box canyon control transfers to operation block . For an example this box canyon situation may be defined by the Boolean equation 

Wherein robot turn thresh is a predetermined threshold parameter which may be robot specific to define a maneuverability distance that enables the robot to turn around.

Once the determination has been made that the robot may be stuck operation block begins the process of attempting to get unstuck. Operation block performs a back out behavior. This back out behavior causes the robot to backup from its present position while following the contours of obstacles near the rear sides of the robot. In general the back out behavior uses range sensing e.g. from laser sonar infrared or combinations thereof of nearby obstacles near the rear sides to determine distance to the obstacles and provide assistance in following the contours of the obstacles. However the back out behavior may also include many robot attributes including perception position bounding shape and motion to enable the robot to turn and back up while continuously responding to nearby obstacles. Using this fusion of attributes the back out behavior doesn t merely back the robot up but rather allows the robot to closely follow the contours of whatever obstacles are around the robot.

For example movements the robot may attempt to equalize the distance between obstacles on both sides keep a substantially fixed distance from obstacles on the right side or keep a substantially fixed distance between obstacles on the left side. As the back out behavior progresses decision block determines if there is sufficient space on a side to perform a maneuver other than backing out. If there is not sufficient spaces control transfers back to operation block to continue the back out behavior. If there is sufficient space on a side control transfers to operation block . As an example the sufficient space on a side decision may be defined by the Boolean equation 

Once sufficient space has been perceived on the right or left operation block performs a turn until head is clear behavior. This behavior causes the robot to rotate in the sufficient space direction while avoiding obstacles on the front sides. As the turn until head is clear behavior progresses decision block determines if and when the head is actually clear. If the head is not clear control transfers back to the operation block to continue the turn until head is clear behavior. If the head is clear control transfers to operation block .

Once the head is clear decision block determines whether an acceptable egress route has been found. This egress route may be defined as an acceptable window of open space that exists for the robot to move forward. To avoid potential cyclical behavior the acceptable window may be adjusted such that the robot does not head back toward the blocked path or box canyon. If an acceptable egress route has not been found control transfers back to operation block to attempt the back out behavior again. If an acceptable egress route is found the unstuck behavior exits. As a specific example the window may be defined by the equation window 1.25 meters seconds in behavior 10.0 and the egress route may be defined as true if the window

As with the guarded motion behavior the get unstuck behavior operates on a global timing loop. Consequently the get unstuck behavior will be re entered and the process repeated on the next time tick.

The Real Time Occupancy Change Analyzer ROCA algorithm compares the state of the environment to its understanding of the world and reports to an operator or supporting robotic sensor the position of and the vector to any change in the environment. The ROCA robot behavior includes laser based tracking and positioning capability which enables the robot to precisely locate and track static and mobile features of the environment using a change detection algorithm that continuously compares current laser scans to an occupancy grid map. Depending on the laser s range the ROCA system may be used to detect changes up to 80 meters from the current position of the laser range finder. The occupancy grid may be given a priori by an operator built on the fly by the robot as it moves through its environment or built by a combination of robot and operator collaboration. Changes in the occupancy grid may be reported in near real time to support a number of tracking capabilities such as camera tracking or a robotic follow capability wherein one or more robots are sent to the map location of the most recent change. Yet another possible use for the ROCA behavior is for target acquisition.

A notable aspect of the ROCA behavior is that rather than only providing a vector to the detected change it provides the actual X Y position of the change. Furthermore the ROCA behavior can operate on the move meaning that unlike most human presence detection systems which must be stationary to work properly it can detect changes in the features of the environment around it apart from of its own motion. This position identification and on the move capability enable tracking systems to predict future movement of the target and effectively search for a target even if it becomes occluded.

In general once the robot has identified a change the change may be processed by several algorithms to filter the change data to remove noise and cluster the possible changes. Of the clustered changes identified the largest continuous cluster of detected changes i.e. hits may be defined as locations of a change e.g. possible intruder within either the global coordinate space as a vector from the current pose of the robot other useful coordinate systems or combinations thereof. This information then may be communicated to other robot attributes robot behaviors and cognitive conduct within the RIK as well as to other robots or an operator on a remote system.

As discussed earlier when discussing the range attribute a variety of coordinate systems may be in use by the robot and an operator. By way of example a local coordinate system may be defined by an operator relative to a space of interest e.g. a building or a world coordinate system defined by sensors such as a GPS unit an iGPS unit a compass an altimeter and the like. A robot coordinate system may be defined in Cartesian coordinates relative to the robots orientation such that for example the X axis is to the right the Y axis is straight ahead and the Z axis is up. Another robot coordinate system may be cylindrical coordinates with a range angle and height relative to the robot current orientation.

The software flow diagram shown in includes representative components of an algorithm for performing the ROCA robot behavior. As stated earlier ROCA process assumes that at least some form of occupancy grid has been established. However due to the global timing loop execution model details probabilities and new frontiers of the occupancy grid may be built in parallel with the ROCA process . The ROCA process begins at decision block by testing to determine if the robot includes lasers the laser data is valid an occupancy grid is available and the ROCA process is enabled. If not the ROCA process ends.

If decision block evaluates true process block performs a new laser scan which includes obtaining a raw laser scan calculating world coordinates for data included in the raw laser scan and converting the world coordinates to the current occupancy grid. The raw laser scan includes an array of data points from one or more lasers sweeps with range data to objects encountered by the laser scan at various points along the laser sweep. Using the present occupancy grid and present robot pose the array of range data may be converted to an occupancy grid referred to as laser return occupancy grid similar to the present occupancy grid map.

Next decision block tests to see if the current element of the array of range data shows an occupancy element that is the same as the occupancy element for the occupancy grid map. If so control passes to decision block at the bottom of the range data processing loop which is discussed later.

If there is a difference between the laser return occupancy cell and the corresponding cell for the occupancy grid map decision block tests the laser return occupancy cell to see if it is part of an existing change occurrence. In other words if this cell is adjacent to another cell that was flagged as containing a change it may be part of the same change. This may occur for example for an intruder that is large enough to be present in more than one occupancy grid. Of course this test may vary depending on for example the granularity of the occupancy grid accuracy of the laser scans and size of the objects of concern. If decision block evaluates true operation block clusters this presently evaluated change with other change occurrences that may be adjacent to this change. Then control will transfer to operation block .

If decision block evaluates false the presently evaluated change is likely due to a new change from a different object. As a result operation block increments a change occurrence counter to indicate that there may be an additional change in the occupancy grid.

Operation block records the current change occurrences and change clusters whether from and existing cluster or a new cluster then control transfers to decision block .

Decision block tests to see if the change occurrence counter is still below a predetermined threshold. If there are a large number of changes the changes may be due to inaccuracies in the robot s current pose estimate. For example if the pose estimate indicates that the robot has turned two degrees to the left but in reality the robot has turned five degrees to the left there may be a large number of differences between the laser return occupancy grid and the occupancy grid map. These large differences may be caused by the inaccuracies in the pose estimate which would cause inaccuracies in the conversion of the laser scans to the laser return occupancy grid. In other words skew in the alignment of the laser scan onto the occupancy grid map due to errors in the robot s pose estimation from rotation or translation may cause a large number of differences. If this is the case control transfers to operation block to update the position abstraction in an attempt to get a more accurate pose estimate. After receiving a new pose estimate from the position abstraction the ROCA process begins again at decision block .

If decision block evaluates true or decision block was entered from decision block decision block test to see if there are more data points in the laser scan to process. If so control transfers back to decision block to process the next element in the laser scan array.

If decision block evaluates false all the data in the laser scan array has been processed and decision block again tests to see if the change occurrence counter is still below a predetermined threshold. As discussed earlier if the change occurrence counter is not below the predetermined threshold operation block updates the position abstraction in an attempt to get a more accurate pose estimate the ROCA process begins again at decision block .

If decision block evaluates true then processing for this laser scan is complete and operation block updates a change vector and information regarding change occurrences and change clusters is made available to other robot attributes robot behaviors and cognitive conduct modules.

By way of example and not limitation the ROCA results may be sent to the user interface used by a tracking behavior and combinations thereof. For example ROCA results may be used with additional geometric calculations to pan a visual camera a thermal camera or combination thereof to fixate on one or more of the identified changes. Similarly a manipulator such as for example a weapon may be panned to acquire a target identified as one of the changes. If the detected change is moving tracking position updates may arrive in near real time the actual rate may depend on the speed and latency of the communication channel allowing various sensors to continuously track the target. If desired the robot may also continuously move to the new location identified by the change detection system to provide a mobile tracking capability.

When coupled with an operator interface the tracked entity s movements may be indicated to an operator in near real time and visual data from a camera can be used by the operator to identify the tracked entity.

As with other behaviors the ROCA behavior operates on the global timing loop. Consequently the ROCA behavior will be re entered and the process repeated on the next time tick.

One representative cognitive conduct module enabled by the RIK and GRA is a virtual rail system for robots. Many industrial and research applications involve moving a vehicle or target at varying speeds along a designated path. There is a need to follow physical paths repeatably either for purposes of transport security applications or in order to accurately record and analyze information such as component wear and tear e.g. automotive testing sensor responsiveness e.g. sensor characterization or environmental data e.g. monitoring . Such applications require both accuracy and repeatability.

Conventional practice methods have required the building of physical or actual tracks along which a vehicle can be moved. Drawbacks of such an approach include the significant limitations of the configuration of paths that may be created and the feasibility of building permanent tracks. Also for characterization and other readily modifiable tasks reconfiguration of physical track networks quickly becomes cost and time prohibitive.

Although it has long been known that physical tracks or rails are problematic mobile robots have not had a means by which to maintain accurate positioning apart from such fixed track methods. For some tasks absolute positioning can be achieved by various instrumented solutions such as visual laser based tracking systems or radio frequency positioning systems that triangulate distance based on beacons placed in the environment. Each of these systems is costly to implement in fact the cost for purchasing and installing such a positioning system is often more than the total cost of the robot itself.

Moreover the utility of visual or laser tracking systems is limited by occlusions within the environment. For example RF beacons are only appropriate for environments where the beacons can be fixed in a static known location. The physical properties of a remote sensing environment are constantly changing. In fact walls are often shifted within the building to model different operational environments. Accordingly absolute positioning is sometimes less feasible impractical and frequently impossible to implement. Therefore there is a need to provide a method and system for configuring a virtual track or rail system for use by a robot.

The present invention includes various embodiments including a robot system configured to follow pre planned routes forming a virtual rail or virtual track and may include defined speeds for traversing various segments of the pre planned routes. One application of a virtual rail system includes the repeated testing of a sensor or system to characterize the device. Due to the accuracy and repeatability of the virtual rail system sensors and systems may be tested with data collected that conforms to a sufficient comparable data standard. Such a data collection standard requires acceptance of data only when consistent and comparable data is generated in response to repeatable tests carried out under the same conditions. For example the virtual rail system may be used in a laboratory research facility or manufacturing environment to characterize a vast number of sensors. Accordingly characterization tests that may previously have required a significant amount of time for execution may now be characterized in a fraction of the time.

Sensor characterization is only one example of a specific application. Other applications include automated mail carts and other delivery systems security and surveillance systems manufacturing and monitoring systems. In particular the technology is useful for parts handling as well as replacement of current railed robotic systems especially within the manufacturing and defense industries.

The user interface provides an environment for the generation of a desired path comprised of at least one segment representative of the virtual track for the robot. The user interface may take the form of a Computer Aided Design CAD program for the formation of the desired path. The desired path comprised of one or more segments representative of the virtual track for the robot may take the form of lines arcs or any of a number of design shapes known by those of ordinary skill in the art and are collectively referred to herein as segments. By way of example a desired path includes a plurality of line segments with line segment illustrated as being selected. Line segments may be generated using any of a number of commercially available CAD systems that may generate file formats that are readily convertible and parsable. By way of example and not limitation the CAD file format may be directly saved or converted into a file format such as Drawing Exchange Format .dxf .

A path plan process receives the CAD generated drawing file and processes the one or more segments of the desired path into a waypoint file that includes instructions that are capable of being executed by robot . The processing of drawing file includes the assignment process of input velocities to the segments or vertices of the desired path segments or elements. A verification process analyzes the desired input velocities by comparing the velocities with the mobility capabilities of robot . Discrepancies or incompatibilities between the desired path and input velocities as compared with the execution capabilities of robot are reported and or resolved.

Path plan process further includes a waypoint generation process for generating waypoint file that precipitates from the original drawing file undergoing assignment process followed by verification process for determining the compatibilities of the desired path and the robot capabilities. Waypoint file includes a listing of waypoints as well as any modified velocities which may be different than the originally specified input velocities .

As stated the one or more line segments with the assigned motion qualities is compared or verified through a verification process with the performance capabilities of a specific robot which compares the requested desired path with mobility limitations and capabilities of robot . In one embodiment of the present invention an algorithm analyzes the path including traversal of the segments at various speeds including velocity transitions between line and arc segments and determines the turn gain to ensure minimal oscillations during traversal of the one or more line segments . Furthermore the algorithm is capable of carving smooth arcs by adjusting the turn gain based on an analysis of the arc shape and the commanded forward velocity. This algorithm provides the ability to arbitrate between waypoint following and motor schema control as speed and point types change.

After resolution of any inconsistencies or incompatibilities a waypoint file is generated by path plan process with waypoint file being transferred over communication interface to robot for execution. Robot executing the various waypoints and specified velocities associated therewith traces out or follows a virtual track or virtual rail as specified and or modified by a user through the control generation system .

The user interface for controlling path plan process enables a user to generate commands in the form of waypoint file for execution by robot which results in the formation of a virtual rail or track that is followed or traced by robot . The virtual track or rail may be created from an abstraction or may be generated with reference to an available map or other boundary designations of the operating environment. Furthermore accurate positioning of the robot may be maintained by application of Markov localization techniques that may combat problems such as odometry drift. Generation of waypoint file allows a robot given accurate position data to traverse a trace of arcs and lines at various speeds. The various embodiments of the present invention may utilize various mapping or localization techniques including positioning systems such as indoor GPS outdoor GPS and differential GPS a theodolite system as well as others that may be devised in the future.

As stated in the desired path includes a plurality of line segments . Through the use of the user interface the start point and end point may be selected with each individual line segment being individually selected thereby allowing the association of a velocity therewith. By way of example line segment is illustrated as being selected with a representative speed of 0.5 meters per second being associated therewith. The path plan process through user interface uses the properties of each segment within drawing file to spatially locate each segment e.g. line or arc and then creates a default path based on the initial order of segments found in the drawing file .

Path plan process through user interface can be used to manipulate various properties of the initial desired path . For example when segment is selected the segment is highlighted in the user interface . Once a segment is highlighted its properties are displayed and can be edited if desired. The order of segments can be changed for example either by using the Move Up and Move Down buttons or by selecting and dragging a segment to its new position. Each segment can be included or excluded for example from the path by appropriately marking the Include this entity in the path checkbox. This allows additional features that are not part of the path to be included in the drawing file without the requirement that they be a part of the virtual track or rail. Additional input boxes may be provided to set the initial speed the final speed or constant acceleration and provide for comments for each segment.

Once motion characteristics such as velocity have been associated with each of the line segments other processing may be performed such as an estimation of run time as well as verification of velocity transitions . Once velocities have been associated therewith and verification of compatibility with the capabilities of the target robot have been performed a waypoint file may be generated by activating generate waypoint process within the user interface .

By way of example and not limitation waypoint file assumes one or more formats an example of which is illustrated with respect to . Waypoint file may include an estimated traversal time identifying a summation of the traversal times of each segment of the virtual track. By way of example waypoint file includes a listing of ordered vertices identifying the waypoints for traversal by the robot . Each waypoint includes a waypoint number indexed according to order as previously described X and Y coordinate values a velocity value and an arc continuation flag for associating a set of waypoints for short line segments that comprise an arc traversal.

The localization process of the robot allows the robot to accurately and repeatedly trace the waypoints forming the virtual rail or track. The navigation process responds to the localization process and sensor data from sensor process to generate controls to the robot motion process . Additionally the robot uses sensor data from sensor process to determine surrounding features. The robot control process does not need to necessarily identify the composition or identity of the features but only the fact that they are part of the environment that forms boundaries for the robot. Robot may utilize one or more sensors for providing feedback to the localization process. Sensors may include wheel measuring devices laser sensors ultrasonic sensors and the like.

Waypoint navigation process generates commands or control signals to a robot motion process . Robot motion process generates controls to actuators for generating motion rotation etc. as well as velocities associated therewith. Waypoint navigation process further receives from sensor process sensor information in the form of feedback for determining when traversal of one or more segments of the virtual rail has been accomplished. Sensor process may also provide information to waypoint navigation process in the form of changes to environmental parameters which enables waypoint navigation process to protect or guard against unforeseen changes to the environment. Additional details with respect to waypoint navigation are described below with respect to .

As stated a drawing file or other illustration of a desired path is generated and includes at least one segment representative of the virtual track to be configured for the virtual track which the robot will traverse. Generation of a desired path results in the creation of a specific file format representing the illustrated segments of the desired path. The file format in one embodiment of the present invention is converted into a standardized file format an example of which is the .dxf format. Generation and converting steps may be accomplished through the use of one or more applications which are made usable through a user interface such as user interface .

Through path plan process and as further illustrated with respect to a user interface the drawing file is imported and the start points end points and segment ordering may be assigned to the various segments of the desired path . Through verification process continuity may be checked or verified and input velocities may be assigned to the various segments of the desired path . Further checking and reporting of inconsistencies or incompatibilities may also be performed.

Once the desired path has been illustrated and start and end points as well as velocities have been associated therewith as well as a successful completion of verification processes a waypoint list is generated and stored in a waypoint file. Upon completion of the generation of waypoint file by control generation system the waypoint file is sent via a communication interface to a robot . Thereafter robot may execute a first waypoint from waypoint file and subsequently execute a second and subsequent waypoints using waypoint navigation process .

The waypoint handler illustrated in starts with decision block to test whether path planning is active and the time since the achieving the last waypoint is greater than a threshold. In the representative embodiment of the threshold is set at three seconds. If sufficient progress has not been made toward a waypoint within the threshold there may be a barrier blocking the robot s progress toward the waypoint. For example perhaps a door was closed that the waypoint planning had assumed was open or perhaps a new obstacle was placed in the environment such that the robot cannot find a way around the obstacle to achieve the next way point. In these types of circumstances it may be appropriate to plan a new path with a new waypoint list. Thus if path planning is active and the threshold is exceeded operation block performs a routine to delete the current waypoint list and plan a new waypoint list then control transfers to decision block .

If decision block evaluates false or operation block completes decision block test to see if the current waypoint is defined as part of an arc. If the current waypoint is part of an arc operation block sets a variable named Waypoint Radius as the current speed times one half of the robot s length. This Waypoint Radius variable is used later as a test threshold when determining how close the robot is to the waypoint. If the current waypoint is not part of an arc operation block sets Waypoint Radius to one half the robot s length plus one half the length of the arm extension. Thus the waypoint radius is defined as the physical extent of the robot from the robot s center.

With the Waypoint Radius variable set decision block tests to see if the angle to the target waypoint is currently less than 90 degrees to the left or right. If so operation block sets the range to the target as the closest range within plus or minus 15 degrees of the current angle to the target. If the waypoint is not less than 90 degrees away operation block sets the range to target is set as Min Front Distance which as explained earlier is the range to the nearest object within plus or minus 90 degrees of the robot s forward direction. The current angle to the target defines the angle toward the target relative to straight ahead. However Range To Target defines a range i.e. distance from the robot to an obstacle in the direction of the waypoint.

After setting the Range To Target variable decision block tests to see if the distance to the current waypoint is less than the waypoint radius defined previously. If so the waypoint is considered to be achieved so operation block iterates to the next waypoint in the waypoint list and the process exits.

If decision block evaluates false a more specific test is performed to see if the waypoint has been achieved. In some instances it may not be possible to actually place the center of the robot over the waypoint. For example the waypoint may have been placed to close to a wall or perhaps even behind the wall. However if the robot can get close enough it may be sufficient to say the waypoint has been achieved. Thus if decision block evaluates true operation block iterates to the next waypoint in the waypoint list and the process exits. However if decision block evaluates false the process exits and continues on with the current waypoint.

A representative evaluation of a test for close enough to a waypoint is illustrated in block . Of course those of ordinary skill in the art will recognize that other parameters distances and decisions may be made within the scope of the present invention to define whether a waypoint has been achieved. In block the first test checks to see if the Range to Target variable is less than the arm extension plus the larger of a forward threshold or a side threshold. If not there may still be room to move forward or rotate toward the waypoint so the process may exit and continue on with the current waypoint. Otherwise the second test checks to see if the distance to the waypoint is less than the sum of the arm extension and the robot length. If not there may still be room to move forward or rotate toward the waypoint so the process may exit and continue on with the current waypoint. Otherwise the third test checks to see if the distance to the closest object on the front side of the robot i.e. Min Front Distance is less than the arm extension plus twice the forward threshold. If not there may still be room to move forward or rotate toward the waypoint so the process may exit and continue on with the current waypoint. Otherwise the final check tests to see if the angle to the target is less than 45 degrees or the range to the nearest obstacle is less than the turn threshold. If not there may still be room to move or rotate toward the waypoint so the process may exit and continue on with the current waypoint. Otherwise operation block iterates to the next waypoint in the waypoint list and the process exits.

If operation block evaluates false or after the pose is updated decision block tests to see if the range to the closest object in front is less than twice a predefined threshold. If not control transfers to decision block . However if the range to the closest object in front is less than twice a predefined threshold the robot may be approaching close to an obstacle so decision block tests to see if the robot is blocked in the direction of the target. If so operation block performs a backup procedure and the process exits. If the robot is not blocked in the target direction decision block tests to see if the angle to the target is greater than 60 degrees. If so the robot may not be able to achieve the target without backing up so operation block performs a backup procedure and the process exits. If the angle to the target is not greater than 60 degrees a backup procedure may not be needed and control transfers to decision block .

Decision block tests to see if the angle to the target is greater than 45 degrees. If so operation block sets the translational speed to zero enabling the robot to stop making forward progress while it rotates to face more directly toward the target. After setting the speed to zero the process exits.

If the angle to the target is not greater than 45 degrees new translational velocity determination continues by decision block testing to see if a detection behavior is in progress. As stated earlier when describing the obstacle avoidance behavior a detection behavior may be a behavior where the robot is using a sensor in an attempt to find something. For example the countermine conduct is a detection behavior that is searching for landmines. In these types of detection behaviors it may be desirable to approach much closer to objects or to approach objects with a much slower speed to allow time for the detection function to operate. Thus if a detection behavior is active operation block sets a desired speed variable based on detection parameters that may be important. By way of example and not limitation in the case of the countermine conduct this desired speed may be set as Desired Speed Max passover rate Scan amplitude Scan Speed . In this countermine conduct example the Max passover rate may indicate a maximum desired speed for passing over the landmine. This speed may be reduced by other factors. For example the Scan amplitude Scan Speed term reduces the desired speed based on a factor of how fast the mine sensor sweeps an area. Thus the Scan amplitude term defines a term of the extent of the scan sweep and the Scan Speed defines the rate at which the scan happens. For example with a large Scan amplitude and a small Scan Speed the Desired Speed will be reduced significantly relative to the Max passover rate to generate a slow speed for performing the scan. While countermine conduct is used as an example of a detection behavior those of ordinary skill in the art will recognize that embodiments of the present invention may include a wide variety of detection behaviors such as for example radiation detection chemical detection and the like.

If a detection behavior is not in progress decision block tests to see if a velocity limit is set. In some embodiments of the invention it may be possible for the operator to set a velocity limit that the robot should not exceed even if the robot believes it may be able to safely go faster. For example if the operator is performing a detailed visual search the robot may be performing autonomous navigation while the operator is controlling a camera. The operator may wish to keep the robot going slow to have time to perform the visual search.

If a velocity limit is set operation block sets the desired speed variable relative to the velocity limit. The equation illustrated in operation block is a representative equation that may be used. The 0.1 term is a term used to ensure that the robot continues to make very slow progress which may be useful to many of the robot attributes behaviors and conduct. In this equation the Speed Factor term is a number from one to ten which may be set by other software modules for example the guarded motion behavior to indicate a relative speed at which the robot should proceed. Thus the desired speed is set as a fractional amount of the Max Limit Speed.

If a velocity limit is not set operation block sets the desired speed variable relative to the maximum speed set for the robot i.e. Max Speed with an equation similar to that for operation block except Max Speed is used rather than Max Limit Speed.

After the Desired Speed variable is set by block or decision block determines if the distance to the current waypoint is less than the current velocity plus a small safety factor. If not operation block sets the new translational speed for the robot to the Desired Speed variable and the process exits. However if the current waypoint is getting close as determined by decision block evaluating true decision block determines if the current waypoint is part of an arc. If so operation block sets the translational speed such that the robot can smoothly traverse the arc. Thus operation block is a representative equation that sets the new translational speed as a function of the larger of either the angle to the target or the turn angle to the next waypoint. In other words the translation velocity will be reduced by setting the new speed to the current speed multiplied by a fractional change factor. This fractional change factor may be defined as the cosine of the larger of either the angle to the target or the turn angle to the next waypoint.

If the current waypoint is not part of an arc it may still be desirable to slow the robot s translational speed down in preparation for turning toward the next waypoint. Thus operation block is a representative equation for setting the new translational speed for the robot by multiplying the current speed by a different fractional change factor. This fractional change factor may be set as about 0.7 0.3 COS Next Turn Angle . In other words the new speed will be set somewhere between 70 and 100 of the current speed based on the angle toward the next waypoint. If the angle is small for example zero degrees there may be no need to slow down and the new speed can be set at 100 of the current speed. Conversely if the angle is large for example 90 degrees it may be desirable to slow down significantly in preparation for a turn to the new waypoint. Thus the new translational velocity is set at 70 of the current speed. Of course the next time through the global timing loop presents another chance to adjust the translational speed if the angle to the next waypoint is still large.

This sets the translational speed based on the severity of the turn that will be negotiated to achieve the next waypoint.

If waypoint following is enabled decision block tests to see if the robot is blocked in front. If not rotational velocity determination can continue at decision block . However if the robot is blocked in front decision block determines whether the current waypoint is to the left of the robot. If so decision block tests the area to the left of the robot where the robot may want to turn toward and find the range to the nearest object in that area. If the range is larger than a turning threshold as tested by decision block there is room to turn so operation block sets the rotational velocity to the left at 30 of a predefined maximum rotational velocity. After setting the rotational velocity the process exits.

If the waypoint is not on the left decision block tests the area to the right of the robot where the robot may want to turn toward and find the range to the nearest object in that area. If the range is larger than a turning threshold as tested by decision block there is room to turn so operation block sets the rotational velocity to the right at 30 of a predefined maximum rotational velocity. After setting the rotational velocity the process exits.

If the robot is blocked in front and there is not room to turn i.e. either decision block or evaluates false then the process exits to a get unstuck behavior in an effort to find a way to get around the obstacle in front so that the robot can continue to pursue the current waypoint.

If the robot is not blocked in front decision block tests to see if the angle to the waypoint target is less than ten degrees. If so the robot is close to pointed in the correct direction and only minor corrections may be useful. Thus in a representative method for determining an appropriate change to the rotational velocity operation block sets a Waypoint Turn Gain as the angle to the target divided by 100. Conversely if the waypoint target is equal to or greater than ten degrees a larger correction to the rotational velocity may be appropriate to get the robot pointed toward the current waypoint. Thus in a representative method for determining an appropriate change to the rotational velocity operation block sets a Waypoint Turn Gain as the base 10 logarithm of the angle to the target minus one. As a result the larger the angle to the waypoint target the larger the value will be for the Waypoint Turn Gain.

With the Waypoint Turn Gain set decision block tests to see if the waypoint is on the left. If so operation block sets the turn velocity to the left by multiplying the current turn velocity by the Waypoint Turn Gain and the process exits. If the waypoint is not on the left operation block sets the turn velocity to the right by multiplying the current turn velocity by the Waypoint Turn Gain and the process exits.

As with other behaviors the waypoint algorithms and in and respectively operate on the global timing loop. Consequently the decision of whether a waypoint has been achieved to move on to the next waypoint adjustments to the translational velocity and adjustments to the rotational velocity may be repeated on each time tick of the global timing loop.

One representative cognitive conduct module enabled by the RIK is a robotic follow capability wherein one or more robots are sent to a map location of the most recent change in the environment or directed to follow a specific moving object. is a software flow diagram illustrating components of an algorithm for performing the follow conduct .

This relatively autonomous conduct may be useful for a fast moving robot with perceptors that may be used by robot attributes and robot behaviors to detect and track changes in the environment. It would be difficult for conventional robots under direct operator control to avoid obstacles track where the robot is going and track the object of pursuit at the same time. However with the relatively autonomous conduct and collaborative tasking enabled by the RIK a high speed chase may be possible.

The RIK may include a tracking behavior that allows the robot to track and follow an object specified by the operator with the camera or other tracking sensors such as thermal infrared sonar and laser. Consequently the tracking behavior is not limited to visual tracking but can be used with any tracking system including a thermal imaging system for tracking human heat signatures.

In visual tracking for example the operator may specify an object to be tracked within the operator s video display by selecting a pursuit button on the interface and then manipulating the camera view so that the object to be tracked is within a bounding box. The camera can then track the object based on a combination of for example edge detection motion tracking and color blob tracking. Furthermore the camera can track the motion of the target independently from the motion of the robot which allows the robot to follow the optimal path around obstacles even if this path at first may take the robot in a direction different from the direction of the target.

Thus the robotic follow conduct effectively blends robot behaviors such as for example tracking obstacle avoidance reactive path planning and pursuit behaviors. To begin the follow conduct operation block illustrates that the conduct queries or receives information regarding the present bearing to a target. This present bearing may be generated by a tracking behavior such as for example the ROCA behavior discussed above or from an updated map location from the operator or other robot. In addition the bearing may be converted to a robot relative coordinate system if needed. Both the tracking behavior and follow conduct operate on the global timing loop. As a result the follow conduct will be re entered each timing tick and be able to receive an updated bearing to the target from the tracking behavior or other defined location each timing tick.

Decision block tests to see if the robot has reached the target. If so the follow conduct exits. If not the follow conduct transitions to decision block . In this representative embodiment reaching the target is defined as 1 The closest obstacle in a 30 region in which the tracked object lies is closer than the closest obstacle in a 30 region on the opposite side 2 both L front and R front are obstructed 3 the angle to the object lies in the front region and 4 the distance to the object in front is less than the distance on the right and left.

Decision block tests to see if the front is blocked. If so control transfers to operation block to attempt to get around the obstacle. If not control transfers to decision block . The front blocked decision may be based for example on a flag from the guarded motion behavior discussed previously.

Decision block begins a process of attempting to get around a perceived obstacle. To begin this process decision block checks the current speed. If the speed is not greater than zero control transfers to decision block . If the speed is greater than zero operation block sets the speed to zero before continuing with decision block .

Decision block tests to see if the robot is blocked on the left or right. If not control transfers to decision block . If the robot is blocked on the left or right the robot may not have an area sufficient to make a turn so operation block sets the robot to begin backing up with an angular velocity of zero and a linear velocity that is 20 of the presently specified maximum then the follow conduct exits.

Decision block tests to see if the robot is blocked in the direction of the target. If so control transfers to decision block . If the robot is not blocked in the direction of the target operation block sets the robot is set to turn toward the target with a linear velocity of zero and an angular velocity that is 60 of the presently specified maximum then the follow conduct exits.

Decision block tests to see if the target is positioned substantially in front of the target. If so the target is in front of the robot but the robot is also blocked by an obstacle. Thus operation block attempts to move forward slowly but also turn around the obstacle by setting the linear velocity to 10 of the presently specified maximum and the angular velocity to 60 of the presently specified maximum and away from the obstacle. Then the follow conduct exits.

If decision block evaluates false then the direction directly in front of the robot is blocked and the direction toward the target is blocked. Thus operation block attempts to find a clear path to the target by setting the linear velocity to 20 of the presently specified maximum i.e. backing up and the angular velocity to 30 of the presently specified maximum and in the direction of the target. Then the follow conduct exits.

Returning to decision block if decision block evaluates false then decision block begins a process of attempting to progress toward the target since the front is not blocked. Thus decision block tests to see if the robot is blocked in the direction of the target. If so operation block attempts to move forward while gradually turning away from the target in an effort to try to find a clear path to the target by setting the linear velocity to 20 of the presently specified maximum and the angular velocity to 20 of the presently specified maximum. Then the follow conduct exits.

If decision block evaluates false then the target is not in front of the robot and the robot is free to move forward. Thus operation block attempts to move forward and turn toward the target. In this representative embodiment the robot is set with an angular velocity toward the target that is determined by the current bearing toward the target divided by a predetermined turn factor. Consequently the speed at which the robot attempts to turn directly toward the target may be adjusted by the turn factor. In addition the robot is set to move forward at a safe speed which may be set as 10 of the maximum to ensure the robot keeps moving plus a safe speed adjustment. The safe speed adjustment may be defined as Front forward threshold 2. Wherein Front defines the distance to the nearest object in the vicinity of directly in front as defined by the range attribute discussed earlier and forward threshold defines a distance to which the robot may be relatively certain that objects are outside of its time horizon. Thus the robot makes fast but safe forward progress while turning toward the target and the speed may be adjusted on the next time tick based on new event horizon information.

As with other robot behaviors and cognitive conduct the robotic follow conduct operates on the global timing loop. Consequently the ROCA behavior will be re entered and the process repeated on the next time tick.

Embodiments of the present invention include methods and systems that provide a more intuitive approach for the user to direct a robot s activities without having to understand the available robot behaviors and autonomy levels. This intuitive approach provides a bridge between a user s intentions and a robot s behaviors by creating a new user interface and adjustments to the robot s initiative and behavior that do not need to be directly controlled by the user.

This approach provides a means to control a remote robot via a continuous range of autonomy levels without requiring the operator to explicitly change the autonomy level on the robot. Conventional approaches have distinct autonomy modes or levels and the operator generally chooses a mode and then the robot navigates according to the dictated level of autonomy.

In contrast in embodiments of the present invention the operator is no longer responsible for directly choosing the mode of autonomy. Rather the operator places a navigational target and the robot moves toward the target. This target may also be referred to herein as a hotspot a task designator and a task oriented target. 

Efficient use of the robot often depends on the interactions between the operator and the robot as interacting members of a human robot team. As tasks change the requirements of the user and the robot should also change in order to ensure efficiency. The responsibilities of the robot are often referred to as the autonomy of the robot and when multiple levels of autonomy are available conventionally the operator has the responsibility of choosing the appropriate level of autonomy. The challenge with this user selected approach is that operators often do not realize when they are in a situation where the autonomy of the robot should be changed in order to maintain high performance. Furthermore as new behaviors and intelligence are added to the robot the traditional approach requires the operator to maintain appropriate mental models of what all the modes are and how they should all be used.

Leaving the autonomy switching and mode switching responsibility to the operator means that mistakes will be made from not recognizing the need to switch or switching to the wrong autonomy level. In general requiring the operator to have an understanding of the appropriate context for each autonomy mode is cognitively challenging and prone to error because of the requirement for the operator to make the decision about autonomy modes. Despite this most solutions to adjustable autonomy are to create various levels of autonomy from which the user must choose. Even when the robot is given the ability to automatically switch between the discrete autonomy modes based on its own assessment of the situation the operator often feels frustrated and confused about what the robot is doing and why it usurped control which usually results in a fight for control between the operator and the robot. This fight for control generally emanates from poor communications between the robot and the human as to why a decision was made and how the decision is being put into action.

Embodiments of the present invention solve many of these problems by supporting seamless levels of interaction based on the operator indirectly controlling the robot. This indirect control is performed by allowing the operator to specify the intent of the user from a human centered reference frame. Instead of selecting discrete levels of autonomy the operator uses defined intentions in the form of the task oriented targets. By using indirect control through specifying intent the fight for control problem present in direct control approaches may be eliminated because issues with communications latency and bandwidth requirements are mitigated and the user may experience a more intuitive feel for what the robot is doing.

As a non limiting example seamless autonomy changes using task oriented targets may be incorporated within the GRA RIK robot behaviors and robot conduct described herein. However embodiments of the present invention are not so limited. Task oriented targets and seamless autonomy according to embodiments of the present invention may be practiced in other robot architectures that define specific behaviors for the robot to accomplish and include a general rubric of varying autonomy levels.

Using the seamless autonomy and task oriented targets embodiments of the present invention allow the operator to specify a goal for the robot to achieve with the use of a target icon. In some instances the robot is then free to determine on its own how to get to the destination. However in other instances such as for example when the task oriented target is placed closer to the robot when the task oriented target is moving relatively frequently or when the task oriented target is inside some event horizons the robot is more directly controlled. In these direct control situations where the robot has a low robot initiative the user more directly controls the robot s operation by placement movement or modification of the task oriented target.

In a very general sense when the target is placed farther from the robot it is as if there is lots of play in a leash between the user and the robot and the navigational responsibilities fall more on the robot. In this approach the operator is less concerned with how the robot will get to the destination. Consequently the user s attention can be focused more on managing the overall task or mission as opposed to the actual movement of the robot. The end effect is that the operator s cognitive capabilities are freed from the navigation responsibility and allowed to concentrate on tasks more appropriate for the human to consider.

The environment map depicts the portions of an interior space that the robot comprehends up to this point in time. As the robot progresses through the environment it may fill out more details of the environment map . In addition details of the environment map may be filled in from other sources such as for example GPS information or other information that may be supplied by the user interface. Occupancy grid based maps may be a good choice of map because they provide a higher resolution for tasking however any map representation can work as long as the robot is localized with respect to the map. Furthermore the map may depict any environment that the robot is in not just an interior space as depicted in and .

In the robot s current position and pose A is shown in the upper right corner of the environment map . A task oriented target is shown in the lower left corner of the environment map . The task oriented target is placed by the user to designate a specific task that the user wants the robot to achieve. In this case the task oriented target is a navigation target that the robot should go to. With this placement of the navigation target the distance between the robot and the navigation target is large enough that the user interface directs the robot to achieve the target with a high degree of autonomy. In other words the user interface may send instructions to develop a robot plan. The robot plan may include planning a path to the navigation target and to begin moving toward the navigation target along the planned path . The robot reports the planned path back to the user interface and the user interface window displays the planned path as a series of waypoints in the environment map . Once planned and displayed the user may adjust the waypoints if the user believes there may be a better way to approach the navigation target . Thus the robot can still achieve the navigation target with a high degree of autonomy and the user is free to concentrate on other operations.

Also illustrated in is another task oriented target positioned near the center of the environment map . This task oriented target is an imaging target . Thus the user interface sends instructions to the robot to carry out robot behaviors to point an imaging device e.g. a camera toward the imaging target and to keep pointing at the imaging target . As the robot moves the imaging device will automatically develop a robot plan to adjust the imaging device e.g. pan tilt focus to stay on the imaging target with a high degree of autonomy requiring little or no intervention from the user. Also illustrated in is an imaging window A. The imaging window A shows the current view of the camera which is pointed at or moving to achieve the imaging target . C and D illustrate the same user interface window with the environment map navigation target planned path and imaging target .

The robot plans for achieving the various task oriented targets may include using a variety of robot conduct and robot behaviors available within the robot software architecture. As non limiting examples the robot s conduct and behaviors may include focus behaviors manipulation behaviors go to points waypoints path planning search region patrol region retro traverse and laser tracking.

The instruction from the user interface for achieving the task oriented target in these lower initiative modes may include using a variety of robot conduct and robot behaviors available within the robot software architecture. As non limiting examples the robot s conduct and behaviors may include focus behaviors manipulation behaviors go to points waypoints path planning search region patrol region retro traverse and laser tracking.

With these task oriented targets a user with less trust in the robot can keep direct control of the robot by keeping the target relatively close to the robot and moving the target as the robot moves similar to leading the robot on a tight leash. If the user has more trust in the robot s ability to achieve the target on its own or the user does not care how the robot achieves the target the user can place the target farther away from the robot and move the target less often. Consequently the changes in autonomy level for the robot are indirectly controlled by the user based on where a task oriented target is placed how often it moves and whether it is inside an event horizon. Event horizons were explained earlier and also will be discussed later when discussing the robot s actions in the seamless autonomy system.

Navigation targets and visual targets have been illustrated and discussed. However other task oriented targets are contemplated within the scope of the present invention. As non limiting examples the task oriented target may be a navigation target an imaging target an artillery target a sensor target a manipulator target and the like.

In addition as illustrated in and multiple task oriented targets may be used at any given time. As a non limiting example of multiple task oriented targets a user may use a navigation target to direct the robot toward a certain point in the environment a sensor target to direct the robot to sense for a parameter of interest near the robot and an imaging target to direct the robot to look behind so the user can determine if anything is approaching near the robot.

Task oriented targets may be placed in a two dimensional map or a three dimensional map and may include two dimensional or three dimensional coordinates. In addition task oriented targets may include task attribute information beyond just location within an environment map. As a non limiting example a task oriented target may be associated with a manipulator as a manipulator target. This manipulator target may be placed for example on a door handle. Thus the user interface will direct the robot to use the manipulator to achieve the manipulator target on the door handle. Other task attribute information assigned to the manipulator target may include information such as which way to turn the door handle which way to push the door and similar attributes associated with the manipulator in question. As another non limiting example a bomb sniffing element may be capable of detecting different types of bombs based on metal content chemical content and the like. The specific types of attributes that the bomb sniffer is searching for may be defined as attributes attached to a bomb sniffer target. As another non limiting example a navigation target may be defined as a center point around which the robot should circle. Thus task attributes for a navigation target may include an orbit attribute and a radius attribute for the orbit. As yet another example offsets and pose information may be defined relative to a navigation target. The offset may define where the robot should stop when it has achieved the target. For example an offset attribute may define that the robot should stop one meter in front of the target to the side of the target or centered on the target. As an example the robot may include a manipulator or sensor that extends one meter in front of the robot. Thus if the offset attribute is set to one meter in front of the robot the robot stops in an optimal position for using the manipulator without additional requirements from the user to fine tune the robot s position before using the manipulator.

In addition task oriented targets may be used in a wide variety of robot platforms such as for example Unmanned Ground Vehicles UGVs Unmanned Air Vehicles UAVs Unattended Ground Sensors UGSs and Unmanned Water Vehicles such as Unmanned Surface Vehicles USVs and Unmanned Underwater Vehicles UUVs .

As an example of the strength of task oriented targets in a UAV an image target may be placed on a specific location on the ground. Then the user can direct the UAV to circle or directly navigate the UAV using a navigation target. All the while the user is directing navigation possibly with high user intervention and low robot initiative the user interface and robot are cooperating to keep the camera directed at and focused on the image target with low user intervention and high robot initiative.

A common operating picture block may receive information from a variety of sources to develop an understanding of the robot and its environment. Non limiting examples of these sources are UGV data UAV data UGS data Geospatial Information System GIS information and sensor data either directly or via a sensor map . The common operating picture includes the GUI that is presented to the user as well as all the collected information from the various data sources and information about the robot and the various task designators i.e. task oriented targets .

A common operating picture also receives information about the task designators in the form of intended tasks commanded tasks and confirmed tasks as is explained more fully below.

Operation block indicates user input in the form of a mouse a joystick a keyboard haptic devices and other suitable input devices for conveying a user s intentions to the user interface. The user s intentions may be actions for the robot to perform such as for example where to go what to do what to look for what to detect and the like. The user interface algorithm illustrated herein is a simplified flowchart intended to show general actions performed as a result of the user placing moving and modifying task oriented targets. Those of ordinary skill in the art will recognize that there may be many other inputs from the user not directly related to achieving task oriented targets.

Based on the user input and other information gathered by the common operating picture the user interface determines the appropriate tasks and autonomy level for the robot. As was explained above in a very general sense the autonomy level will have a low robot initiative if the task oriented target is close to the robot moving relatively frequently or within an event horizon.

Operation block tests to see if the task oriented target meets certain filtering requirements and filters information about the task oriented target to meet those filtering requirements. These filtering requirements may be time based and distance based. As a non limiting example if the user is dragging a task oriented target around in the environment map it may not be advisable to flood the communication channel between the user interface and robot with many small changes in position. Rather the user interface may attempt to filter out many of these small changes to present fewer position changes to the robot based on communication capabilities. After creating the filtered information operation block indicates that the intended task based on autonomy and task switching and filtering is communicated back to the common operating picture for display to the user. The intended task indicates what the user interface intends to instruct the robot to do.

In parallel with generating the intended task decision block tests to see if the map is geo referenced. In other words can the map be tied to Global Positioning System GPS coordinates Universal Transverse Mercator UTM coordinates or other suitable geo referenced coordinate system If the map is geo referenced operation block determines the current geo referenced coordinates for the task oriented target. If the map is not geo referenced operation block determines the current coordinates for the task oriented target in terms of another suitable map such as environment map coordinates.

Operation block determines the task coordinates from whichever map coordinates are provided. As non limiting examples these coordinates may be relative to the robot s current position and pose relative to a specific sensor relative to a specific manipulator or relative to the robot s understanding of the environment.

The decision block string of and are example decisions that may be made for various task oriented targets. Along with operation blocks and these decisions and operations are intended to be exemplary and not to cover the details of each possible task oriented target or all possible operations for each task oriented target.

Decision block determines if a navigation task needs to be addressed. If so operation block as depicted by dashed lines determines what type of navigation commands should be sent to the robot. As a non limiting example details of the type of decisions and commands that may be performed are illustrated for operation block . Decision block tests to see if the robot is a UGV. If so decision block tests to see if the current task is a high intervention task or a high initiative task. As explained earlier if the target is close to the robot or the target is moving relatively frequently the user interface decides that high intervention should be used.

In addition if the target is within an event horizon high intervention may be appropriate. As a non limiting example the distance between the target and the robot may be close enough that the time to communicate instructions to the robot perform a path plan on the robot and communicate the path plan back to the user interface may be too long. In other words at its present speed the robot may reach the target before the planning operations can be performed. In these scenarios high intervention is more appropriate because there is not enough time to allow high initiative.

If high intervention is appropriate operation block sends a waypoint command to the robot. The waypoint command indicates to the robot where in its environment it should immediately head toward.

If high initiative is appropriate operation block sends a goto command to the robot. The robot can then use its initiative to attempt to achieve the navigation target. The robot then plans a path to the navigation target and communicates the planned path back to the user interface for display to the user.

If the robot is not a UGV decision block tests to see if the robot is a UAV. If not operation block indicates that other vehicle navigation commands should be sent to the robot. As a non limiting example the robot may be an underwater vehicle and a decision should be made about the underwater vehicle s capabilities relative to the navigation target. Then appropriate navigation commands at various autonomy levels can be sent to the underwater vehicle.

If the robot is a UAV decision block tests to see if the current navigation task is to orbit around the navigation target. If the task is not an orbit task operation block sends a goto command as was described above for the UGV. If the task is an orbit task operation block sends an orbit command.

Those of ordinary skill in the art will recognize that operation block for handling navigation targets is a simplified block diagram. Many other types of robot vehicles may be involved as well as many other navigation decisions and navigation commands.

Returning to decision block if the current task is not a navigation task decision block tests to see if the current task is a camera task. If so operation block sends appropriate high robot initiative or low robot initiative commands to control a camera to the robot. As non limiting examples commands may be related to operations such as pan tilt focus and zoom.

If the current task is not a camera task decision block tests to see if the current task is a payload task. If the current task is a payload task operation block sends appropriate high robot initiative or low robot initiative commands to control a payload. Non limiting examples of payloads are bomb sniffers manipulators chemical sensors and the like.

If the current task does not appear to relate to tasks for the robot to perform operation block tests to see if the user wishes to change the map perspective and if so changes map views based on user instructions.

Operation blocks and all are operations sending instruction to the robot and are indicated as such by using a bold box. As part of sending the instruction to the robot the user interface also defines the commanded task and sends it to the common operating picture for display to the user.

Operation block sets the commanded task indicating that the robot has received the commanded task and has acknowledged the instructions. The acknowledgement may be a simple reply or a complex reply that may include information about what the robot intends to do such as for example a waypoint list for a planned path. As part of the acknowledgement the user interface reports a confirmed task to the common operating picture for display to the user. Thus the common operating picture and the user if appropriate is aware of tasks to be performed at three levels an intended task not yet sent to the robot a commanded task indicating what the user interface wants the robot to perform and a confirmed task indicating that the robot acknowledges that the task is being attempted.

As non limiting examples task oriented targets may be related to manipulators artillery navigation and cameras . As non limiting examples cameras may include still and video images generated for visual images thermal images and gamma images.

Robot instructions related to task oriented targets may call a focus entity abstraction on the robot. Other robot instructions such as for example navigation targets and possibly manipulator targets may go directly to operation block .

The focus entity abstraction generally may be used in connection with visual hotspots for various visual tracking behaviors such as for example determining line of sight tracking determining distance and the like.

Operation block determines the hotspot position relative to the robot s local pose and position. This may include conversion to a robot coordinate system or an environment map coordinate system. In other words the current hotspot position is put in robot understandable coordinates.

Decision block tests for whether the current hotspot being analyzed is a navigation target. If so decision block tests to see if the hotspot is within a planning event horizon. As discussed earlier the distance between the target and the robot may be close enough that the robot may not have time to perform a path planning algorithm. In other words at its present speed the robot may reach the target before the planning operations can be performed. If the navigation target is within the planning event horizon the robot may use a real time reactive pursuit behavior such as for example manipulation behaviors guarded motion obstacle avoidance as well as the robotic follow conduct discussed above. During performance of the pursuit behavior dynamic re planning may be requested because the navigation target may move outside of the event horizon enabling the possibility to generate a path plan. After dynamic re planning control returns to decision block to see if the target is still within the planning event horizon.

If the navigation target is outside the planning event horizon decision block tests to see if the navigation target is within the map boundaries. The navigation target placed by the user may actually be outside the boundaries of the map the robot has of its environment. If the navigation target is not within the map boundary operation block finds the nearest point on the map boundary based on where a vector between the robot and the target crosses the map boundary. As the robot moves toward this new point on the map boundary it may discover more about its environment on its own or through the user interface enabling the robot to re plan toward the navigation target. After finding the perimeter point control returns to operation block to determine robot position relative to the perimeter point.

If the navigation target is within the map boundary operation block determines a path plan and operation block filters out some of the waypoints in the path plan if appropriate. For example the filter may remove multiple waypoints in a substantially straight line or reduce the number of waypoints on a curve. With the path plan determined and filtered a waypoint following conduct such as for example the virtual rail conduct discussed above is performed to achieve the navigation target.

Returning to decision block if the current hotspot is not a navigation target the target may be related to a payload such as a sensor manipulator or camera. Operation block calculates the angle of error relative to a payload offset . A payload offset may indicate a payload s position relative to the robot. As a non limiting example a camera may be mounted on the back of a robot and compensation may be needed relative to the center of the robot.

Operation block calculates the needed pan tilt zoom movement or other appropriate movements for manipulators or sensors needed to achieve the hotspot. Information about the payload state may be used in the calculations of operation block . Decision block tests to see if the calculated movements are possible. For example a manipulator may extend beyond the robot and movement of the manipulator in the desired manner may cause it to meet an obstacle. If the payload movement is possible an appropriate payload actuation behavior is executed.

If the payload movement is not possible operation block activates a navigation task in an effort to move the robot to a position pose or combination thereof that will allow the desired payload movement.

In some instances the robot may have better knowledge of the environment than the user. In these cases the robot may move the navigation target or modify waypoints in a path plan. As a non limiting example the hotspot may be on an opposite side of a wall that the user was not aware of or did not see. In such cases the robot may move the hotspot or create a new path plan to get around the wall. Similarly a new obstacle may be placed in the robot s current path causing the robot to re plan a path or move the hotspot.

As with the simplified flowchart of is intended to illustrate only some of the operations and decisions that the robot may need to perform to achieve a task oriented target with an emphasis on navigation decisions and operations. Those of ordinary skill in the art will recognize that many other navigation decisions and operations are included within the scope of the present invention. Similarly those of ordinary skill in the art will recognize that for the sake of brevity decisions and operation related to other task oriented targets would be similar to those for navigation and need not be described in detail herein.

Although this invention has been described with reference to particular embodiments the invention is not limited to these described embodiments. Rather the invention is encompassed by the appended claims and their legal equivalents.

