---

title: Estimation of global position of a sensor node
abstract: Methods and apparatuses for locating a sensor node are disclosed. A representative apparatus, among others, includes a processing unit that receives sensor node data and object trajectory information data for an object. The sensor node data is related to the object's trajectory, and a data point in the object trajectory information data comprises a time stamp and the coordinates of a position. The position corresponds the location of the object at the given time. The processing unit is adapted to correlate at least a portion of the sensor node data with at least a portion of the object trajectory information data to determine an absolute position of the sensor node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08060338&OS=08060338&RS=08060338
owner: The United States of America as represented by the Secretary of the Army
number: 08060338
owner_city: Washington
owner_country: US
publication_date: 20080204
---
This is a continuation in part application of pending patent application Ser. No. 11 094 145 filed Mar. 31 2005 now abandoned.

The invention described herein may be manufactured and used by or for the United States Government for governmental purposes without the payment of any royalties thereon.

The present disclosure is generally related to determining the absolute position of a sensor node and more specifically to determining the absolute position of a sensor node that has been disposed in a haphazard manner.

The present disclosure pertains to the field of localization of sensors. Sensors are usually deployed in a region to observe the activity in an area of interest. Different sensors are deployed to observe different activity. Acoustic sensors are deployed in a battlefield or in an urban area to monitor acoustic activity for example the presence of tanks in the field to track vehicles and identify the type of vehicles etc. In urban settings acoustic sensors can be deployed to detect congregation of people and the general nature of such a congregation that is whether the crowd is hostile or peaceful. The deployed acoustic sensors record the sound waves emanated in the vicinity and transmit to a remote hub or a central observation center where the data sound waves is analyzed. The type of analysis employed depends on the type of activity for which one is looking. Tracking vehicles in a field may comprise use of a tracking algorithm while detecting people in the field may require an entirely different algorithm. In any case the data analyzed has to be referenced to the location of the sensor. Without the knowledge of the sensor location the data has no relevance. In a battlefield or in covert operation situations the sensors may be deployed using an airborne vehicle such as a helicopter or airplane or the sensors can be deployed by rockets. Whatever mechanism is used for deploying the sensors the location of the deployed sensors may not be available. It is possible to guess at the general vicinity where the sensors land based on the location of the deploying vehicle the speed at which the vehicle was flying the angle at which the sensors were hurled general wind speeds in the vicinity the direction in which the vehicle was traveling etc. The present disclosure presents a robust and elegant means in determining the location of the sensors.

At present localization methods e.g. R. L. Moses R. M. Patterson and W. Garber Self localization of Acoustic sensor networks in Proc. of MSS meeting of the specialty group on Battlefield acoustic and seismic sensing September 2002 include use of beacons to estimate the time of arrival TOA between two sensor nodes which translates into range after multiplying the TOA with the propagation velocity of the signal in the medium. This range corresponds to the radius of a circle or a sphere depending on whether the sensor nodes are on a plane or in three dimensional space. If there are three such circles corresponding to three different TOAs with respect to a sensor node whose coordinates we would like to determine then these circles would intersect at a common point giving the location of the sensor whose coordinates are desired. In this approach the location of the sensor is obtained relative to the three other sensors. In order to know the global positioning of the sensor one must have knowledge of the global position of the three sensors. If there is a large number of sensors deployed in a field whose coordinates must be determined one can use the above technique iteratively to determine their locations. However that approach may result in cumulative error that grows as a function of distance from one sensor to the other.

An apparatus and method for locating a sensor node are provided. Briefly described one embodiment of an apparatus includes a processing unit that receives sensor node data and object trajectory information data for an object. The sensor node data is related to the object from which the bearings of the object can be estimated which in turn are related to the trajectory of the object and a data point in the object trajectory information data comprises a time stamp and the coordinates of a position. The position corresponds to the location of the object at the given time. The processing unit is adapted to use several bearing estimates from the sensor node data and the corresponding position information of the object to determine an absolute position of the sensor node. An embodiment of a method can be broadly summarized by the following collecting object trajectory information data for an object wherein the object trajectory information data is collected over a time span acquiring sensor node data at the sensor node wherein the sensor node data is acquired over the same time span and is related to the object trajectory information data determining several direction indicators each direction indicator providing an approximate bearing from the sensor node to the object at a given instant of time during the time span the data is collected and determining coordinates of the sensor node using a portion of the collected trajectory information data and the corresponding direction indicators bearings determined by the sensor node data.

Other apparatuses systems methods features and or advantages will be or may become apparent to one with skill hi the art upon examination of the following drawings and detailed description. It is intended that all such additional systems methods features and or advantages be included within this description and be protected by the accompanying claims.

It should be emphasized that the above described embodiments are merely possible examples of implementations. Many variations and modifications may be made to the above described embodiments. All such modifications and variations are intended to be included herein within the scope of this disclosure and protected by the following claims.

Referring to a detection system includes a central observation center COC and a sensor array which is disposed over a region . The sensor array includes a plurality of sensor nodes which are adapted to detect objects including natural objects and man made objects. The sensor nodes communicate data regarding detected objects to the COC via a wireless communication link . The data typically includes a sensor node identifier and a direction indicator i.e. the angular direction of the detected object from the sensor node . In some embodiments data includes raw data or partially processed data and generally includes the sensor node identifier. The COC includes a processing unit that processes the data to determine among other things the absolute or global coordinates of the sensor node and or to identify the detected object .

Typically the sensor nodes passively detect energy that is emitted from the object . Non limiting examples of the types of energy that the sensor nodes are adapted to detect include electromagnetic energy and acoustic energy. It should be noted that the spectrum of electromagnetic energy includes both visible light and non visible light such as infrared and that acoustic energy is not limited to sounds carried by a gaseous medium such as the Earth s atmosphere and is medium independent. In other words acoustic energy can be carried by a gas solid or a liquid medium.

Typically the sensor nodes are disposed over the region in a haphazard or essentially random unpredictable manner. The sensor nodes can be delivered among other methods of deployment to the region by an artillery shell and or mortar shell by dropping from one or more aerial vehicles by dropping from ships and or submarines by dropping from a ground vehicle and or by dropping by personnel.

Sensor node includes three sensors which are disposed on the ground in an approximate equiangular distribution about the central unit . In the case of acoustic sensors sensors are microphones.

Sensor node includes four sensors disposed on the ground in an approximate equiangular distribution about the central unit and one sensor which is elevated off of the ground by a support member . The support member extends generally upward from the central unit .

Sensor node includes a plurality of sensors that are approximately linearly aligned. It should be remembered that sensor nodes through are non limiting examples and that in other embodiments greater or fewer sensors can be employed and or the sensors can be deployed about the central unit in other configurations such as essentially random non equiangular etc.

Typically the filter is an anti aliasing filter low pass filter at half the sampling frequency that filters unwanted noise from the electrical signals and provides the electrical signals to the A D converter which digitizes the electrical signals.

The direction estimator receives and processes the digitized electrical signals. In some embodiments the direction estimator includes a high resolution direction of arrival angle estimator namely a minimum variance distortionless response MVDR Van Veen B. D. and K. M. Buckley Beamforming a versatile approach to spatial filtering IEEE ASSP Magazine vol. 5 pp. 4 24 1988 and or a multiple signal classification MUSIC Adaptive Filter Theory by S. Haykin published by Prentice Hall 1991 module for determining a direction of arrival DOA angle also called the bearing angle of the target of the signal from the object . The direction indictor includes a bearing. The bearing is the angular difference between a local reference axis and a vector extending from the sensor node towards the detected object . In some embodiments the direction estimator also includes a source identifier module which analyzes received energy to identify the detected object . In other embodiments modules such as the direction estimator and the source identifier module are included in the processor .

In some embodiments the direction indicator includes a single azimuthal angle corresponding to azimuth used in two dimensional polar coordinates and the time stamp. In some embodiments the direction indicator includes two angles corresponding to azimuth and zenith elevation used in three dimensional spherical coordinates and the time stamp.

A module such as the direction estimator and the source identifier module and any sub modules may comprise an ordered listing of executable instructions for implementing logical functions. When a nodule and or any sub modules are implemented in software it should be noted that the modules can be stored on any computer readable medium for use by or in connection with any computer related system or method. In the context of this document a computer readable medium is an electronic magnetic optical or other physical device or means that can contain or store a computer program for use by or in connection with a computer related system or method. The direction estimator and the source identifier module and any sub modules can be embodied in any computer readable medium for use by or in connection with an instruction execution system apparatus or device such as a computer based system processor containing system or other system that can fetch the instructions from the instruction execution system apparatus or device and execute the instructions.

In the context of this document a computer readable medium can be any appropriate mechanism that can store communicate propagate or transport the program for use by or in connection with the instruction execution system apparatus or device. The computer readable medium can be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus device or propagation medium. More specific examples a nonexhaustive list of the computer readable medium would include the following an electrical connection electronic having one or more wires a portable computer diskette magnetic a random access memory RAM electronic a read only memory ROM electronic an erasable programmable read only memory EPROM EEPROM or Flash memory electronic an optical fiber optical and a portable compact disc read only memory CDROM optical . Note that the computer readable medium could even be paper or another suitable medium upon which the program is printed as the program can be electronically captured via for instance optical scanning of the paper or other medium then compiled interpreted or otherwise processed in a suitable manner if necessary and then stored in a computer memory.

In some embodiments the sensor node may include a data recorder on which data from the sensors is recorded. Typically recorded data includes both sensor data and a time that is associated with the sensor data. In some embodiments the direction estimator then processes recorded data to determine a direction indicator. In other embodiments the sensor node transmits recorded data to the COC .

The transmitter receives the direction indicator from the direction estimator and transmits a direction indicator message to the central observation center . Typically the direction indicator message includes a sensor node identifier and a direction indicator. However in some embodiments the transmitter sends partially processed or digitized raw data to the COC and the COO determines among other things the direction from the sensor node to the object . Furthermore in some embodiments the COC can use the data from the sensor node to identify the object.

As previously described above the sensor nodes are usually disposed ill a haphazard or essentially random manner. Consequently the absolute position of the sensor nodes is usually not known when the sensor nodes are deployed.

In some situations especially when the topography of the region is relatively flat the absolute position of the sensor node can be represented by two coordinates. Frequently two coordinates are used when the objects that are to be detected are objects such as infantry vehicles etc. which move in the same general plane as the sensor nodes in the sensor array . In other situations absolute sensor positions are determined in three dimensional coordinates x y z in Cartesian coordinate system and r in polar coordinate system etc.

Referring to in step a known source object moves in the neighborhood of the sensor array . illustrates an exemplary footprint or path of an object over the sensor array . Typically the object traverses the neighborhood of the sensor array in two back and forth motions. The first back and forth motion includes x axis legs and the second back and forth motion includes y axis legs . Preferably the x axis legs are approximately perpendicular to the y axis legs . For the sake of simplicity assume the x axis legs are approximately parallel to East West direction and the y axis legs are approximately parallel to North South direction.

In one embodiment the object is a helicopter which could also have been used to deploy the sensor nodes . As the helicopter flies in the neighborhood of a sensor node the sensor node receives the sound waves acoustic signals generated by the helicopter. The acoustic signals which are mainly due to the rotor rotating and the tail fan have distinctive frequency components that correspond to the speed of the rotors which can be extracted by signal processing techniques. illustrates typical time domain data of the acoustic signal of a helicopter and illustrates the frequency components as given by the fast Fourier transform FFT of the acoustical energy emitted from the helicopter. In the time domain data is normalized and multiplied by the hamming window to reduce the effect of noise.

In step the sensor nodes acquire source data as the object commences on path . In step direction indicators are determined using MVDR or MUSIC as mentioned earlier. In one embodiment each sensor node determines multiple direction indicators from the source data and relays the direction indicators to the COC . In other embodiments source raw data is communicated to the COC and the processing unit determines the direction indicators. Referring back to dashed lines represent nine different direction indicators which include bearings that are determined as the object traverses the neighborhood of sensor node . Note that several hundred direction indicators can be used for sensor localization that is to determine the location s of the sensor s .

In step the absolute position of object as it traverses path is determined. In one embodiment the object receives global positioning system GPS signals which the object uses to determine its absolute position. In that case the object provides its position to the COC along with the time stamp. Typically the object receives GPS information for determining its absolute position and the absolute position of the object is determined and recorded every second or so as the object traverses path . In some embodiments the object transmits trajectory information time position as the object traverses the path . The coordinates of the position can be casting northing and altitude. Sometimes GPS systems give the longitude latitude and altitude instead of casting northing and altitude. However the conversion from longitude and latitude to casting and northing is straightforward.

In addition in some embodiments the direction estimator for the sensor node is time synchronized with the GPS time so that the direction indicators generated from the sensor node have the same time reference. Generally the coordinates of the helicopter are determined or recorded every second.

In some embodiments the object includes a recorder not shown that records the trajectory information. The recorded trajectory information includes the time and position of the object as it traverses in the neighborhood of the sensor array and the trajectory information is recorded every second or so.

Alternatively the position of the object can be determined using methods other than GPS. In yet another embodiment the object can be detected by a detector not shown such as a radar station sonar detector laser station etc. which could then determine the global position of the object. The detector can then provide the global position of the object to the COC .

In step absolute sensor node positions are determined using object trajectory information i.e. the position as a function of time of the object . The processing unit correlates trajectory information of object position time with direction indicator data direction time to determine the absolute position and orientation of a sensor node . Generally the absolute sensor node position is determined using an iterative process. First an approximate location of the sensor node is determined using three or more trajectory information data points and well known trigonometric relationships. More than three data points are used if the absolute sensor node position is to be determined in three coordinates. After the initial absolute position has been determined then the absolute sensor node position is refined using additional trajectory information data points.

In some embodiments the COC also receives GPS signals and then uses them to among other things synchronize a clock with the timing information included in the trajectory information of the object . The COC associates the time of reception of the data from a sensor node and then correlates the data with the trajectory information. The direction indicator data includes a time stamp. The time stamp corresponds to the time at which the sensor node determined the direction indicator or the time at which the sensor node detected the object . If the data from the sensor node includes a time stamp then the time stamp can be used to among other purposes correlate the trajectory information of the object . In order to determine the coordinates of a sensor node one has to perform the steps and listed in the for each sensor node while step and are common for all the sensor nodes. To recap step implies that the sensor node collects signals emitted by the object step determines the bearing angles of the detected object with respect to some reference axis at the sensor node using the data signals collected in step . Step determines the coordinates of the object for the same time stamps of the data collected in step . Step uses the coordinates of the object and the bearing estimates generated in steps and respectively to determine the coordinates of the sensor node .

In step the sensor node collects data. Typically the sensor node passively collects the data by detecting energy emitted from the detected object or known object .

In step data from each one of the sensors in the sensor node is processed. Typically the processing includes among other things filtering and digitizing the data. The data can be processed at the sensor node or at the processing unit .

In step the data from all of the sensors is used to determine the bearing angle of the object using techniques such as MVDR Van Veen B. D. and K. M. Buckley Beamforming a versatile approach to spatial filtering IEEE ASSP Magazine vol. 5 pp. 4 24 1988 and or a MUSIC Adaptive Filter Theory by S. Haykin published by Prentice Hall 1991 which are well known techniques.

In step a determination is made as to whether there is more data i.e. whether the sensors of the sensor node continue to detect the object or have detected another object. If there is more data then the process returns to step otherwise the process ends at step .

Step in pertains to recording the global position and time information of the object from GPS. More is said about this later. The final step in is where the coordinates of each sensor node is determined. Detailed steps involved in step of are given in . The process of determining the coordinates of the sensor node has four major steps as listed below 

In step for a set of data points from the sensor node direction indicators or bearings of the object are calculated using well known and well documented techniques in the literature such as MVDR or MUSIC. The direction indicators are measured relative to a local reference axis. This local reference axis may be different from the global reference axis normally global reference axis is true north . The offset between the two references is denoted by . The set of sensor node direction indicators or object bearing angles are denoted by A . . . where is the angle between a local reference axis and a ray extending from the sensor node towards the detected object and where the s are calculated using collected sensor node data using either MVDR or MUSIC or ally other angle estimator.

Steps and are done essentially concurrently. In step the trajectory information of the object which is traversing the neighborhood of the sensor node is collected. The trajectory information of the object is provided to the COC . Typically trajectory information associates the position the coordinates of the object at a given instance of time. The object s trajectory information is normally recorded every second or so and an object trajectory information data point comprises a time stamp and a given position where the time stamp corresponds to when the object was located at the given position. In some embodiments the object s trajectory information is determined by a GPS system. In other embodiments the object s trajectory information is determined using other detectors that can determine the absolute or global position of the object while the object traverses the neighborhood of the sensor node.

In step a subset of trajectory points of the object is selected to be used in determining the sensor node coordinates as some trajectory points result in more error in the sensor node coordinates than the other points. In step the source trajectory information data points are analyzed such that selected data points are used to determine the coordinates and orientation of the sensor node. The accuracy of the sensor node localization position and orientation depends in part on the estimation accuracy of angles in the direction indicator. When the object is near the closest point of approach CPA the direction indicator may change rapidly if the relative distance between the sensor node and the object is small. When the angles are changing rapidly it can be difficult to estimate the sensor node bearing angle accurately and the sensor node localization accuracy suffers when inaccurate bearing angles are used. In order to improve the sensor node localization accuracy the points on the object trajectory information at which the rate of change of direction angle from one second to another second is greater than a predetermined angle are excluded. In most of the situations is approximately tell degrees 10 . Although an angle of approximately ten degrees has been explicitly identified as an example other angles are possible.

The points at which the direction indicator angles change fast can be determined by computing the difference between the adjacent direction indicator angles estimated using sensor node data. Those points with a difference greater than 10 can be eliminated from the list of trajectory information data points used to calculate the sensor node coordinates.

In this description the coordinates of the sensor node are given in two dimensions. Consequently the coordinates of the sensor node are initially calculated from the intersection of two circles. If the unknown coordinates of the sensor node were in three dimensions then the unknown coordinates would be calculated from the intersection of three spheres.

Estimation of the approximate coordinates of a sensor node Initial estimation of the approximate coordinates of a sensor node is done by first selecting three points on the trajectory of the object and corresponding same time stamps bearing angles determined from the data collected at the sensor node . Selection of the three points on the trajectory and the estimation of the approximate coordinates of the sensor node is elaborated further. illustrates a portion of a path of a known object not shown and a sensor node . Points on the path are used to determine the coordinates of the sensor node . Specifically points and correspond to the position of the object at times tand t respectively. The trajectory information for point is denoted by Pi x y t and for point is denoted by Pj x y t . Line denotes the distance d between the points and .

In order to reduce errors object trajectory information data points are selected based upon angular separation as measured at the sensor node and the distance between data points. Because the location of the sensor node is unknown the distance d between Pand Pis used as a parameter for selecting triangulation points. Generally it is found that when the distance between Pand Pis greater than 300 meters a reasonable accuracy for the sensor node location is obtained.

In step a first estimate of the coordinates of the sensor node is determined using object trajectory information data points and sensor node direction indicators. Referring to a sensor node is disposed at a location . The absolute but unknown coordinates of the sensor node are denoted by S X Y . Line represents the local reference axis and the local reference axis is offset from a global reference axis by an amount epsilon . In points and represent three object trajectory information data points which are denoted by x y t x y t and x y t respectively.

Triangulation cannot be used to find the actual coordinates X Y of the sensor node because the orientation of the local reference axis is unknown. Consequently the coordinates of position are determined using geometric and trigonometric relationships. So instead of working with the angles and directly work with the differences in angles effectively nullifying the effect of orientation of the reference axis. Construct two circles with one of the intersection point giving the coordinates of the sensor node using the differences in angles and the known distances between the points and . The description of construction of the two circles to determine the coordinates of the sensor node follows.

A first circle whose center is denoted by Cand whose radius is denoted by Ris computed using positions and and 2 where is the absolute value of the difference between and . The circumference of the first circle includes points and .

A second circle whose center is denoted by Cand whose radius is denoted by Ris computed using positions and and 2 where is the absolute value of the difference between and . The circumference of the second circle includes points and .

The coordinates of the center C and the radius R of each circle is determined using known quantities. Specifically two known positions and the difference in the bearing angles for those two known positions are used to determine each circle. Referring to a simple case of finding the coordinates of the center of a circle is illustrated. In this example the two known positions and have coordinates x y and x y respectively and are separated by a distance of d d x x . Point represents the midpoint of the line connecting positions and . The coordinates of point are 0.5 x x y . The radius of the circle is found from the relationship R d 2 sin . The distance h from point to the center C is are found from the relationship h R cos . Thus in this example the coordinates of the center C are 0.5 x x y R cos . It is a simple procedure to find the center of a circle or a sphere when the known points do not lie in the same horizontal plane.

Once the coordinates of the centers and are known and the radii Rand Rare known the intersection point point of the circles and is calculated and the calculated coordinates of the intersection point are regarded as the first estimated location of the sensor node . With an estimated location of the sensor node a first estimation of the offset angle between the local reference axis and the global reference axis is determined.

Referring back to in step the coordinates of the sensor node are estimated using object trajectory information data and the sensor node direction indicators to triangulate upon the sensor node. Triangulation is now possible because a first approximation of the offset between the local reference axis and the global reference axis has been calculated.

The initial approximate estimate of the coordinates of the sensor node is refined by averaging the estimates of the coordinates of the sensor node using triangulation. The procedure is illustrated. Proper selection of points for triangulation results in minimizing triangulation errors. For the purposes of this disclosure the position of a sensor node is denoted by S and P identifies the recorded object trajectory information data i.e. P is a list of positions P P . . . P at times t t . . . t.

As illustrated in the sensor node direction indictor or the estimate of the bearing angle given by MVDR MUSIC is an estimate of angle which is given by

The distance between the two points Pand Pis needed for triangulation. This distance is the Euclidean distance and is given as square root over square root over Equation 3 where P x y and P x y respectively. Once the distance dbetween two points on the trajectory and the angles and are determined triangulation can be performed using the principles of trigonometry to find the location of the sensor node. Let P P P represent the pairs of points such that the distance d 300 meters. Note that Pis the location of object at some instant of time tin terms of casting and northing.

Let be the corresponding pair of direction indicators where is the bearing angle estimated by the direction estimator at the instant of time t. The corresponding translated angles with respect to the points Pand P are calculated using the sensor node bearing angles and the offset angle which was determined in step . For each pair of points P P in P the corresponding pair of angles are used to estimate the location of the sensor.

Let S S S . . . S be the set of estimates given by the elements of P where S X Y and Xis the easting x coordinate and Yis the northing y coordinate of the sensor location as estimated using pairs of data points Pand P. The initial estimate of the sensor location based upon triangulation is the mean value of the set S and is given by

In step object bearing angles which are denoted by A . . . are calculated using the estimated position S X Y of the sensor node and the object trajectory information data.

In step the object trajectory information and the sensor node data are synchronized such that the offset angle between the local reference angle and the global reference angle is eliminated.

The trajectory information of the object consists of the position information along with the time stamps. These recorded time stamps are accurate to 1 sec. However since the object is moving the actual position information and the corresponding time stamps may be off by few milliseconds. Hence synchronization of the bearing angles of the object trajectory points computed using the estimated sensor coordinates and the bearing angles determined from the data at sensor node need to be synchronized in order to estimate the coordinates of the sensor node . Moreover there is propagation delay of the signals from the object to the sensor node . This delay depends on the separation distance between the sensor node and the object . This process of synchronization is presented here. In step the object bearing angles A . . . are synchronized with the sensor node bearing angles A . . . . Provided below is exemplary code for synchronizing the sensor node data with the object trajectory information. In the exemplary code the object bearing angles A . . . are denoted as hel angls and the sensor node bearing angles A . . . which are generated from the data of sensor node are denoted as luk angls . If there is a time offset between the two sets of data then it is corrected such that they are synchronized while adjusting for the orientation of the sensor node.

Exemplary Algorithm for Time Synchronizing hel angls and luk angls and Determining the Orientation of Sensor Array 

In the above algorithm a coarse time delay in a number of seconds is determined. The following exemplary algorithm is then used to time synchronize the luk angls with the hel angls to a fraction of a second.

A better estimated sensor node location can be determined now that the offset has been removed from the sensor node bearing angles A . . . and now that the sensor node data is synchronized with the object trajectory information. The refinement of the sensor node location is done in iterations. The refinement process consists of searching a 10 m by 10 m local area around the initial estimated location of the sensor node that gives minimum weighted mean squared error between the A . . . and A . . . . The reason for using weighted mean squared error instead of mean squared error is that the error in determining the angle of bearing increases with the distance between the sensor node and the object as the signal strength decreases hence signal to noise ratio with distance. The weights are selected so that they are inversely proportional to the distance. The new location that gives the minimum weighted mean squared error is declared as the new estimated location of the sensor node . The process is repeated until the weighted mean squared error no longer decreases. The description of this process follows.

In step the data is weighted and the mean square error between the sensor node bearing angles and the object bearing node angles is determined. In some embodiments the mean square error is calculated as follows 

Let d where and are the sensor node bearing angles and object bearing angle respectively and then the weighted mean bearing angle difference is given by

In step a calculation grid is established around the estimated sensor node coordinates. An example of such a calculation grid is identified in with reference numeral . The calculation grid can be of arbitrary size. As a non limiting example the calculation grid has one meter m increments and extends ten meters in both positive and negative directions for both the x axis and y axis from the estimated sensor node position X Y . This generates a grid of 21 by 21 points . However it should be noted that the grid need not be square.

Returning to the calculated error is incremented and stored as old error and an iteration counter is set to zero as indicated in step . An iteration counter is used so that the process of refinement does not continue without bounds.

In step the calculated error is compared with the stored old error and the iteration counter is compared to a maximum number of iterations. If the iteration count is greater than a maximum number of iterations or if the old error is less than the calculated error then the process proceeds to step and ends. In the first iteration the conditions always fail and the procedure continues through steps through .

In step the stored old error is updated such that it is equal to the calculated error and the iterations counter is incremented.

In step the error is calculated for every point in the defined calculation grid see . With reference to the error can be calculated by actually considering a finer grid at each selected point and computing the averaged weighted mean squared error of the finer grid. Computing the error at each point in this manner is not required however and one can ignore the finer grid if desired. However use of the liner grid increases the accuracy of the calculation.

Returning to a new sensor location is established based on the minimum error as indicated in step moreover the minimum error is stored as the calculated error. Whichever point gives the minimum error is designated as estimated sensor location point K L . In step we use this location to generate another coarse grid around it. The procedure then returns to step .

It should be emphasized that the above described embodiments are merely possible examples of implementations. Many variations and modifications may be made to the above described embodiments. All such modifications and variations are intended to be included herein within the scope of this disclosure and the present disclosure and protected by the following claims.

