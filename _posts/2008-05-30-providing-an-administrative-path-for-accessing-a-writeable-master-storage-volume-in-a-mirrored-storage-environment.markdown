---

title: Providing an administrative path for accessing a writeable master storage volume in a mirrored storage environment
abstract: A technique provides an administrative path for accessing a writeable master storage volume in a mirrored storage environment. Illustratively, a writeable master storage volume stores a master set of data addressable by a corresponding pathname, and zero or more read-only (e.g., load-balancing) mirrored storage volumes are configured to store a mirrored set of the master set of data, the mirrored set also addressable by the corresponding pathname. Clients may read data from either the master storage volume or one of the mirrored storage volumes (e.g., according to a configured access location, such as a local mirrored volume if one exists) by issuing read requests having the corresponding pathnames. Also, each client may specifically access the master set of data from the master storage volume by issuing an access (e.g., read/write) request having a specified master storage volume pathname prefix prepended to the corresponding pathname.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08429368&OS=08429368&RS=08429368
owner: NetApp, Inc.
number: 08429368
owner_city: Sunnyvale
owner_country: US
publication_date: 20080530
---
This application claims the benefit of U.S. Provisional Application Ser. No. 60 941 293 entitled PROVIDING AN ADMINISTRATIVE PATH FOR ACCESSING A WRITEABLE MASTER STORAGE VOLUME IN A MIRRORED STORAGE ENVIRONMENT filed by Michael Eisler et al. on Jun. 1 2007 the teachings of which are expressly incorporated by reference.

The present invention is directed to storage systems and in particular to providing an administrative path for accessing a writeable master storage volume in a mirrored storage environment.

A storage system typically comprises one or more storage devices into which information may be entered and from which information may be obtained as desired. The storage system includes a storage operating system that functionally organizes the system by inter alia invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage NAS environment a storage area network SAN and a disk assembly directly attached to a client or host computer. The storage devices are typically disk drives organized as a disk array wherein the term disk commonly describes a self contained rotating magnetic media storage device. The term disk in this context is synonymous with hard disk drive HDD or direct access storage device DASD .

The storage operating system of the storage system may implement a high level module such as a file system to logically organize the information stored on volumes as a hierarchical structure of data containers such as files and logical units. For example each on disk file may be implemented as set of data structures i.e. disk blocks configured to store information such as the actual data for the file. These data blocks are organized within a volume block number vbn space that is maintained by the file system. The file system may also assign each data block in the file a corresponding file offset or file block number fbn . The file system typically assigns sequences of fbns on a per file basis whereas vbns are assigned over a larger volume address space. The file system organizes the data blocks within the vbn space as a logical volume each logical volume may be although is not necessarily associated with its own file system.

A known type of file system is a write anywhere file system that does not overwrite data on disks. If a data block is retrieved read from disk into a memory of the storage system and dirtied i.e. updated or modified with new data the data block is thereafter stored written to a new location on disk to optimize write performance. A write anywhere file system may initially assume an optimal layout such that the data is substantially contiguously arranged on disks. The optimal disk layout results in efficient access operations particularly for sequential read operations directed to the disks. An example of a write anywhere file system that is configured to operate on a storage system is the Write Anywhere File Layout WAFL file system available from Network Appliance Inc. Sunnyvale Calif.

The storage system may be further configured to operate according to a client server model of information delivery to thereby allow many clients to access data containers such as files and logical units stored on the system. In this model the client may comprise an application such as a database application executing on a computer that connects to the storage system over a computer network such as a point to point link shared local area network LAN wide area network WAN or virtual private network VPN implemented over a public network such as the Internet. Each client may request the services of the storage system by issuing file based and block based protocol messages in the form of packets to the system over the network.

In order to improve reliability and facilitate disaster recovery in the event of a failure of a storage system its associated disks or some portion of the storage infrastructure it is common to mirror or replicate a data set comprising some or all of the underlying data and or the file system that organizes the data. A data set comprises an area of defined storage which may have a mirroring relationship associated therewith. Examples of data sets include e.g. a file system a volume or a persistent consistency point image PCPI described further below.

In one example a mirror of a file system is established and stored at a destination making it more likely that recovery is possible in the event of a true disaster that may physically damage a source storage location or its infrastructure e.g. a flood power outage act of war etc. . The mirror is updated at regular intervals typically set by an administrator in an effort to maintain the most recent changes to the file system on the destination. The storage systems attempt to ensure that the mirror is consistent that is that the mirror contains identical data to that of the source.

In addition to being used for improved reliability and to facilitate disaster recovery mirrors may also be used for load balancing data access requests. In particular a distributed storage system environment may provide access to a data set e.g. a volume for a large number of clients. As such the large number of corresponding access requests for that data set may become a bottleneck where generally one particular storage system maintaining the data set services each of the requests. For instance where a root volume is stored at the particular storage system each access request to the root volume is serviced by that storage system as will be understood by those skilled in the art . To pre vent that storage system from becoming a bottleneck one technique is to provide read only load balancing mirrors of the data set stored on one or more of the storage systems of the distributed storage system environment. In particular a data set that is accessed often yet that is not modified often e.g. the root volume is a good candidate for mirroring. In this manner any read only access request from a client for the mirrored data set e.g. the root volume may be serviced from any storage system having a mirrored copy thus alleviating the bottleneck at the storage system maintaining the original version of the mirrored data set the master data set or volume .

By creating multiple read only replicas of a data set and or volume across distributed storage systems a mirrored storage environment may advantageously provide read only load balancing. However one problem associated with mirrored storage environments is how to easily provide access to the writeable master storage volume for any client to update the data yet still have the benefits of load balanced read access from the read only mirrored storage volumes. There remains a need therefore for an efficient and straightforward technique for specifically accessing a writeable master storage volume in a mirrored storage environment particularly for both read and write access requests.

The present invention overcomes the disadvantages of the prior art by providing an administrative path for accessing a writeable master storage volume in a mirrored storage environment. Illustratively a writeable master storage volume is maintained to store a master set of data addressable by a corresponding pathname for the data. Also zero or more read only mirrored storage volumes e.g. load balancing mirrors are maintained for the master storage volume the mirrored storage volumes configured to store a mirrored set of the master set of data the mirrored set also addressable by the corresponding pathname. In particular clients may read the data from either the master storage volume or one of the mirrored storage volumes e.g. according to a configured access location such as a local mirrored volume by issuing read requests having the corresponding pathnames. Also each client may specifically access the master set of data from the master storage volume by issuing an access e.g. read or write request having a specified master storage volume pathname prefix e.g. .admin prepended to the corresponding pathname of the data.

Illustratively nodes of the storage environment in which the master and mirrored storage volumes are maintained may each be generally organized as a network element and or a disk element wherein the network element generally interfaces with a network and may be used to direct client requests to one or more disk elements each of which generally interfaces with storage and communicates with e.g. accesses data in the storage e.g. on one or more disks such as the mirrored or master set of data. Each element includes a cluster fabric interface module adapted to implement a network protocol. In particular in accordance with the present invention a network element may receive an access request e.g. a read request without the novel master storage volume pathname prefix mentioned above and may correspondingly direct the request to a local volume e.g. a disk element maintaining a local mirror storage volume if a mirror exists . Conversely in response to receiving an access request e.g. a read or write request with the novel master prefix the network element accordingly directs the request to the master storage volume e.g. the disk element maintaining the master storage volume.

Advantageously the novel technique provides an administrative path for accessing a writeable master storage volume in a mirrored storage environment. By directing all requests with a master storage volume pathname prefix to the writeable master storage volume the novel technique ensures that the requests are directed to a proper storage volume accordingly. In particular by utilizing a master prefix both read and write requests may be specifically directed to the master storage volume while read requests not utilizing the master prefix may be directed to a local e.g. mirrored storage volume.

One or more embodiments of the present invention provide an administrative path for accessing a writeable master storage volume in a mirrored storage environment. By directing all access requests with a master storage volume pathname prefix to the writeable master storage volume the novel techniques ensure that the requests are directed to a proper storage volume accordingly. In particular by utilizing a master prefix both read and write requests may be specifically directed to the master storage volume while read requests not utilizing the master prefix may be directed to a local e.g. mirrored storage volume.

The clients may be general purpose computers configured to interact with the node in accordance with a client server model of information delivery. That is each client may request the services of the node and the node may return the results of the services requested by the client by exchanging packets e.g. access requests described below over the network . The client may issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the client may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

Each node is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on the disks. However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor system. Illustratively one processor executes the functions of the N module on the node while the other processor executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention e.g. MSID to DSID table described herein . The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage service implemented by the node. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

The network adapter comprises a plurality of ports adapted to couple the node to one or more users clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the computer network may be embodied as an Ethernet network or a Fibre Channel FC network. Each client may communicate with the node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disks of array . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Storage of information on each array is preferably implemented as one or more storage volumes that comprise a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each logical volume is generally although not necessarily associated with its own file system. The disks within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data. An illustrative example of a RAID implementation is a RAID 4 level implementation although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

In an exemplary embodiment MSID to DSID table may comprise one or more table entries having a master data set identifier MSID that indicates a volume family and a data set identifier DSID that uniquely identifies a particular instance of a volume of the volume family. In particular the MSID identifies a volume family that may be identically replicated e.g. mirrored in the clustered storage system across a plurality of volumes each particular volume identified by a corresponding DSID. The MSID to DSID table therefore provides a mapping for the storage system of where the particular volumes of a volume family are located. Notably in the event of replicated mirrored volumes one particular DSID may represent the writeable master storage volume of the volume family accordingly.

Also contents of a volume location database VLDB which are illustratively stored in configuration table may be used to map volume identifiers to a D module that owns services maintains a data container volume within the clustered storage system . Thus the VLDB is capable of tracking the locations of volumes and aggregates of nodes within the clustered storage system. Illustratively determining the location of a D module to which an N module transmits a CF message described below is further described in commonly owned U.S. Pat. No. 7 743 210 entitled SYSTEM AND METHOD FOR IMPLEMENTING ATOMIC CROSS STRIPE WRITE OPERATIONS IN A STRIPED VOLUME SET filed by Richard P. Jernigan IV et al. and issued on Jun. 22 2010.

To facilitate access to the disks the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named data containers such as directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of data containers such as blocks on the disks that are exported as named logical unit numbers luns .

In the illustrative embodiment the storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term WAFL is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

In addition the storage operating system includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on the disks of the node . To that end the storage server includes a file system module a RAID system module and a disk driver system module . The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the SCSI protocol.

The file system also implements processes such as a redirection process in an exemplary embodiment of the present invention. The redirection process includes one or more computer readable instructions that manage redirection of access requests to appropriate storage locations or volumes. For instance within the clustered storage system with multiple D modules multiple volumes may be associated with a single D module or multiple volumes may be allocated among multiple D modules. For example volumes distributed among multiple D modules may be implemented with striped volumes of data e.g. round robin allocation of data among the striped volumes. Commonly owned U.S. Pat. No. 7 987 167 entitled ENABLING A CLUSTERED NAMESPACE WITH REDIRECTION filed by Kazar et al. and issued on Jul. 26 2011 the contents of which are hereby incorporated by reference as though fully set forth herein. In particular as described herein a volume family e.g. identified by an MSID may be distributed among a plurality of nodes and D modules such as in accordance with a mirrored storage environment. As such redirection process may be used to redirect access requests to particular volumes e.g. identified by a DSID within the volume family accordingly.

Further the file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The vdisk module enables access by administrative interfaces such as a user interface of a management framework in response to a user system administrator issuing commands to the node . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as files.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store meta data describing the layout of its file system these meta data files include among others an inode file. A file handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

Broadly stated all inodes of the write anywhere file system are organized into the inode file. A file system fs info block specifies the layout of information in the file system and includes an inode of a file that includes all other inodes of the file system. Each logical volume file system has an fsinfo block that is preferably stored at a fixed location within e.g. a RAID group. The inode of the inode file may directly reference point to data blocks of the inode file or may reference indirect blocks of the inode file that in turn reference data blocks of the inode file. Within each data block of the inode file are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally a request from the client is forwarded as a packet e.g. access request over the computer network and onto the node where it is received at the network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Upon receiving the request packet the file system generates operations to access the requested data e.g. identified by a pathname described herein from disk if it is not resident in core i.e. in memory . If the information is not in memory the file system indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by node in response to a request issued by client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the node. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows NT or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In the illustrative embodiment the storage server is embodied as D module of the storage operating system to service one or more volumes of array . In addition the multi protocol engine is embodied as N module to i perform protocol termination with respect to a client issuing incoming data access request packets over the network as well as ii redirect those data access requests to any storage server of the cluster . Moreover the N module and D module cooperate to provide a highly scalable distributed storage system architecture of the cluster . To that end each module includes a cluster fabric CF interface module adapted to implement intra cluster communication among the modules including D module to D module communication for e.g. data container striping operations.

The protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N module function as protocol servers that translate file based and block based data access requests from clients into CF protocol messages used for communication with the D module . That is the N module servers convert the incoming data access requests into file system primitive operations commands that are embedded within CF messages by the CF interface module for transmission to the D modules of the cluster . Notably the CF interface modules cooperate to provide a single file system image across all D modules in the cluster . Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster.

Further to the illustrative embodiment the N module and D module are implemented as separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as pieces of code within a single operating system process. Communication between an N module and D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism.

The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from Network Appliance Inc. The SpinFS protocol is described in the above referenced U.S. Pat. No. 6 671 773. To that end the CF protocol is illustratively a multi layered network protocol that integrates a session infrastructure and an application operation set into a session layer. The session layer manages the establishment and termination of sessions between modules in the cluster and is illustratively built upon a connection layer that defines a set of functionality or services provided by a connection oriented protocol. The connection oriented protocol may include a framing protocol layer over a network transport such as TCP or other reliable connection protocols or a memory based IPC protocol. An example of a session layer that may be advantageously used with the present invention is described in commonly owned U.S. Pat. No. 7 443 872 entitled SYSTEM AND METHOD FOR MULTIPLEXING CHANNELS OVER MULTIPLE CONNECTIONS IN A STORAGE SYSTEM CLUSTER filed by Peter F. Corbett et al. and issued on Oct. 28 2008 the contents of which are hereby incorporated in their entirety as though fully set forth herein.

The CF interface module implements the CF protocol for communicating file system commands among the modules of cluster . Communication is illustratively effected by the D module exposing the CF API to which an N module or another D module issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface on N module encapsulates a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster . In either case the CF decoder of CF interface on D module de encapsulates the CF message and processes the file system command.

As noted above mirrors may be used for improved reliability as well as for load balancing data access requests. In particular a distributed storage system environment e.g. a cluster may provide access to a data set e.g. a volume for a large number of clients . As such the large number of corresponding access requests for that data set may become a bottleneck where generally one particular storage system maintaining the data set must service each of the requests. By providing read only load balancing mirrors of the data set stored on each storage system of the distributed storage system environment a load balancing mirrored storage environment may be established. In particular a data set that is accessed often yet that is not modified often e.g. the root volume is a good candidate for mirroring. In this manner any read only access request from a client for the mirrored data set e.g. the root volume may be serviced from any storage system having a mirrored copy thus alleviating the bottleneck at the storage system maintaining the original version of the mirrored data set the master data set or volume .

Illustratively a writeable master storage volume may be maintained e.g. by node to store a master set of data addressable by a corresponding pathname. For example data corresponding to a file named data.txt may be organized within a subdirectory bar of directory foo thus at a location in the master storage volume having a corresponding pathname of foo bar data.txt accordingly. Also in accordance with one or more embodiments of the present invention zero or more read only mirrored storage volumes e.g. load balancing mirrors are maintained for the master storage volume. For instance one example mirrored storage volume may be maintained by node . Each mirrored storage volume is correspondingly configured to store a mirrored set of the master set of data the mirrored set also addressable by the corresponding pathname. In other words the file located at foo bar data.txt of the master storage volume may also be found at foo bar data.txt of the mirrored storage volume s . In addition should data of the master data set at the master storage volume be modified updated the mirrored data set of the mirrored storage volume s may also be updated e.g. replicating the master set to the mirrored sets .

One common form of establishing updating mirrors involves the use of a snapshot process in which the content of the master storage volume is captured and transmitted over a network to one or more mirrored storage volumes. Note that the term snapshot is a trademark of Network Appliance Inc. It is used for purposes of this patent to designate a persistent consistency point image PCPI . A persistent consistency point image is a point in time representation of the storage system and more particularly of the master storage volume stored on a storage device or in other persistent memory and having a name or other unique identifier that distinguishes it from other PCPIs taken at other points in time. A PCPI can also include other information metadata about the active file system at the particular point in time for which the image is taken. The terms PCPI and snapshot may be used interchangeably through out this patent without derogation of Network Appliance s is trademark rights. The PCPI process is described in further detail in U.S. Pat. No. 4 454 445 entitled INSTANT SNAPSHOT by Blake Lewis et al. and issued on Nov. 18 2008 TR3002File System Design for an NFS File Server Appliance by David Hitz et al. published by Network Appliance Inc. and in U.S. Pat. No. 5 819 292 entitled METHOD FOR MAINTAINING CONSISTENT STATES OF A FILE SYSTEM AND FOR CREATING USER ACCESSIBLE READ ONLY COPIES OF A FILE SYSTEM by David Hitz et al. which are hereby incorporated by reference.

An exemplary PCPI based mirroring technique e.g. a SnapMirror generation technique typically provides for remote asynchronous replication or mirroring of changes made to a source file system PCPI e.g. of the master storage volume in a destination replica file system e.g. a mirrored storage volume . The mirroring technique typically scans via a scanner the blocks that make up two versions of a PCPI of the source file system to identify latent divergence i.e. changed blocks in the respective PCPI files based upon differences in vbns further identified in a scan of a logical file block index of each PCPI. Trees e.g. buffer trees of blocks associated with the files are traversed bypassing unchanged pointers between versions to identify the changes in the hierarchy of the trees. These changes are transmitted to the destination replica mirror . This technique allows regular files directories inodes and any other hierarchical structure of trees to be efficiently scanned to determine differences latent divergence between versions thereof. A set number of PCPIs may be retained both on the source and the destination depending upon various time based and other criteria.

Illustratively in accordance with one or more embodiments of the present invention nodes of the storage environment in which the master and mirrored storage volumes are maintained e.g. cluster may each be generally organized as a network element and or a disk element wherein the network element generally interfaces with a network and may be used to direct client access requests to one or more disk elements. Each disk element generally interfaces with storage and communicates with e.g. accesses data in the storage e.g. on one or more disks such as the master set of data master storage volume and zero or more mirrored sets of data mirrored storage volume . For example node comprises network element and disk element which maintains disks containing the master storage volume . Also node comprises a network element and disk element which maintains disks containing an illustrative mirrored storage volume . Notably while each of the master and mirrored storage volumes are illustratively shown as being maintained within a single disk those skilled in the art will appreciate that storage volumes may be distributed among a plurality of disks such as disks of an array or as disks maintained by different nodes . Accordingly the location of the master and mirrored storage volumes and references thereto as used herein are merely representative examples and are not meant to be limiting on the scope of the present invention.

Notably by having read only load balancing mirrors of a particular data set e.g. of a root volume on each disk element in the mirrored storage environment e.g. in the namespace container any read only external client access requests e.g. for CIFS NFS etc. that access the particular data set e.g. the root volume may be serviced from any disk element. Since external client requests illustratively arrive at network elements and since each network element has a local disk element on the same node the external request can be serviced locally rather than being serviced by a disk element that is likely to be on another node. In this manner a potential bottleneck associated with having data available on only one storage volume e.g. from one disk element may be avoided.

As used herein a namespace is a collection of files and pathnames to the files while a global namespace is the collection of clients and clusters sharing the namespace. Also the set of volumes maintained by load balancing mirrors and the writeable master volume i.e. in a mirrored storage environment of a corresponding global namespace is collectively referred to as a volume family . Each volume of a volume family shares the same MSID but to distinguish among the set of volumes each volume also has its own unique DSID. In this manner access requests may be directed to any available load balancing mirror for an MSID that appears in the request s filehandle not shown as will be appreciated by those skilled in the art. For instance a client e.g. may request data from a volume family having an MSID and the receiving node e.g. network element N module may redirect the request to an appropriate e.g. local volume using a corresponding DSID accordingly.

As noted by creating multiple read only replicas of portions of a data set and or volume across distributed storage systems a mirrored storage environment may advantageously provide read only load balancing e.g. reading from a local PCPI . In particular however one problem associated with mirrored storage environments is how to easily provide access to the writeable master storage volume for any client to update the data yet still have the benefits of load balanced read access from the read only mirrored storage volumes. For instance if a network element receiving a request for a data of a particular MSID is configured to select a configurable e.g. local disk element corresponding to a selected DSID to load balance access to the data a technique is needed that may specifically indicate whether the writeable master version of the data is requested or if a read only mirrored version may be used.

The present invention provides a path e.g. an administrative path for accessing a writeable master storage volume in a mirrored storage environment. In particular clients may read the data from either the master storage volume or one of the mirrored storage volumes e.g. according to a configured access location such as a local mirrored volume if one exists by issuing read requests having the corresponding pathnames for the data. Also each client may specifically access the master set of data from the master storage volume by issuing an access e.g. read or write request having a specified master storage volume pathname prefix e.g. .admin prepended to the corresponding pathname of the data.

In particular a specified master storage volume pathname prefix a master prefix may be provided such that if a pathname an access path of an access request from a client comprises the master prefix e.g. .admin then the writeable master storage volume is always accessed. In particular the master storage volume is accessed regardless as to whether there are any load balancing mirrored storage volumes for the volume family. Notably the specified master storage volume pathname prefix is not a portion of a path to reach the data corresponding to the pathname but is rather an indication to access the master storage volume . That is there is no directory named by the master prefix e.g. no .admin directory in the file system. The master prefix simply indicates to a receiving storage system node that the writeable version of the data is to be accessed. Those skilled in the art will appreciate an example meaning significance of the characters . and .. where applicable. For example . may be a reference to a current directory while . implies a prefix to a filename that has special visibility properties in the directory. Also .. is a reference to a parent directory as will be understood. 

Illustratively however the master prefix may be treated as a directory in certain respects. For example where an illustrative client file system operates in accordance with NFS mentioned above and as will be understood by those skilled in the art mount operations may see the master prefix directory .admin while file and lock operations do not. Thus if a client e.g. an NFS client mounts the root directory then tries to change directories to the master prefix directory or view the contents of the root directory the lookup to the master prefix directory will fail. However if the client mounts a pathname starting with the master prefix e.g. .admin then all access from that pathname e.g. all volumes below .admin is to the writeable master storage volume for the volume family.

Operationally a client e.g. a user and or client process application may determine a need to generate an access request for data that is stored on a configured one of either the master storage volume or one of the mirrored storage volumes. In other words the client may wish to read data that is mirrored from the master storage volume onto one or more mirrored storage volumes . Accordingly as described above for load balancing purposes the client may transmit send a read request to a configured one of either the master storage volume or one of the mirrored storage volumes e.g. to a local N module of a local node . The read request illustratively comprises a pathname corresponding to the data of the read request without the prepended specified master storage volume pathname prefix . For example client may wish to read data located at a pathname of foo bar data.txt that is a file named data.txt located at a subdirectory bar of a parent directory foo .

In particular in accordance with the present invention a network element e.g. N module may receive the read request without the novel master storage volume pathname prefix and may correspondingly direct the request to a local volume e.g. disk element maintaining a local mirror storage volume if one exists . For instance if the disk element maintains a mirrored copy of the requested data in accordance with load balancing mirrors the receiving network element may transmit the read request to the disk element of the same node storage system . Alternatively if the disk element does not maintain a mirrored copy of the data the network element may forward the request to an appropriate disk element e.g. with redirection process and through cluster switching fabric that does maintain a copy e.g. a mirrored storage volume on another node or the master storage volume .

Notably the network elements may be configured to always direct the requests to a mirrored storage volume i.e. where at least one mirrored storage volume exists such that the master storage volume never services read requests. In this manner without using the specified master storage volume pathname prefix with the pathname i.e. using a conventional pathname a client is directed to a read only version copy of the requested data.

Conversely a client may determine a need to generate an access request e.g. read and or write for the data particularly for the master set of the data of the master storage volume . For example a client e.g. an administrator or other user may wish to update a master copy of the data such as updating the file data.txt located at a pathname of foo bar data.txt as mentioned above. Accordingly the client e.g. may transmit send the access e.g. write request to the master storage volume with the specified master storage volume pathname prefix e.g. .admin prepended to a pathname corresponding to the data of the access request e.g. data.txt e.g. .admin foo bar data.txt .

In particular in response to receiving the access request with the novel master prefix a network element e.g. redirection process of N module accordingly directs the request to the master storage volume that is to the disk element e.g. D module maintaining the master storage volume e.g. on disk . The corresponding disk element may then access the master set of data from the master storage volume for the data located at the pathname e.g. foo bar data.txt and may perform the access operation e.g. write the data.

Notably once data is written to the master storage volume the mirrored storage volumes containing that data are out of date until subsequently updated e.g. as described above . Accordingly in order to access the newly written data at the master storage volume the client continues to access the data from the master set of the master storage volume with read requests having the prepended prefix until the master set of the master storage volume has been replicated onto the one or more mirrored storage volumes of the mirrored set. In other words in addition to writing to the writeable master storage volume read requests may need to be sent to the master storage volume with the master prefix for instance until the master data set is replicated to the mirrors thus allowing load balancing to resume.

In step a client e.g. determines a need to generate an access request for the master set of data of the master storage volume e.g. a read or write request for data or alternatively for data that is stored on a volume not necessarily the master storage volume i.e. certain read requests . If the access request is for the master set in step then the client may transmit the access request to the master storage volume in step the access request having a specified master storage volume pathname prefix prepended to a pathname corresponding to the data of the access request e.g. .admin foo bar data.txt . In response to receiving the access request having the preprended prefix a corresponding node accesses the master set of data from the master storage volume in step to access e.g. read or write the data e.g. to read or write foo bar data.txt of the master storage volume . For instance as described above depending upon the particular receiving node e.g. a particular receiving N module the redirection process of the corresponding D module determines the location of the master storage volume and may redirect the request to the appropriate node i.e. if the corresponding D module does not maintain the master storage volume itself .

If on the other hand the access request is not necessarily for the master set in step i.e. is a read request then the client may transmit the read request without the prepended prefix to a configured one of either the master storage volume or one of the mirrored storage volumes e.g. a local volume in step e.g. simply having the pathname such as foo bar data.txt . In response to receiving the read access request having no prepended prefix a corresponding node accesses the configured e.g. local set of data from either the master storage volume or mirrored storage volume in step to access i.e. read the data. For instance as also described above depending upon the particular receiving node e.g. a particular receiving N module the redirection process of the corresponding D module determines the location of a locally configured volume that stores the requested data such as a local mirrored storage volume or the master storage volume if local and so configured . Accordingly if a local volume is maintained by the corresponding D module the corresponding D module may handle the request itself with its locally attached volume e.g. disks which may contain either a mirrored or master data set . If a local volume is not maintained then redirection process of the corresponding D module redirects the request to an appropriate configured node as described above. The procedure ends in step notably with the option of additional access requests and further maintenance of the master and mirrored volumes accordingly.

Advantageously the novel technique provides an administrative path for accessing a writeable master storage volume in a mirrored storage environment. By directing all requests with a master storage volume pathname prefix to the writeable master storage volume the novel technique ensures that the requests are directed to a proper storage volume accordingly. In particular by utilizing a master prefix both read and write requests may be specifically directed to the master storage volume while read requests not utilizing the master prefix may be directed to a local e.g. mirrored storage volume. In other words the novel technique provides a simple consistent manner to always locate and address the writeable master version of data e.g. of a file which may or may not have a load balanced read only mirror replica.

While there have been shown and described illustrative embodiments that provide an administrative path for accessing a writeable master storage volume in a mirrored storage environment it is to be understood that various other adaptations and modifications may be made within the spirit and scope of the present invention. For example the embodiments have been shown and described herein with using clustered storage systems particularly network elements and disk elements and the associated cluster interface. However the embodiments of the invention in its broader sense are not so limited and may in fact be used with any devices nodes that may redirect access requests between master and mirrored storage volumes in a similarly applicable manner as will be understood by those skilled in the art. Also any volume data set may have a load balancing mirrored data set such as a first mirrored set of the master set of data being mirrored as a second data set by a second mirrored storage volume with corresponding pathnames . As such when locally accessing the second data set with a master storage volume pathname prefix prepended to the request the writeable master storage volume is accessed thus bypassing the first e.g. read only mirrored set.

Further in accordance with an alternative embodiment of the present invention the master prefix may be dynamically prepended to all write requests for a mirrored storage environment thus directing all write requests to the writeable master volume . However one problem associated with this alternative embodiment is that it would be difficult to correspondingly dynamically read from the master storage volume as necessary. For instance a network element transmitting a write request to the master storage volume may determine e.g. through a session protocol that subsequent reads for the written data should be sent to the same volume location i.e. to the master storage volume. If all network elements write data to the master storage volume at some point however then all network elements may direct read requests to the master storage volume thus nullifying the purpose of the load balancing mirrored storage environment. Conversely in the event that the network element that sent the write request to the master storage volume fails then the client utilizing that failed network element may be redirected to a new network element e.g. through network . That new network element however may be unaware of the previously written data at the master storage volume and may direct any read requests to a local mirrored storage volume accordingly. In response to these and other concerns the alternative embodiment for dynamically sending write requests to the master storage volume may still particularly benefit from specifically prepending the master prefix to pathnames of read requests that are meant to be directed to the master storage volume as described above.

In addition according to one or more embodiments of the present invention other illustrative features may be available in accordance with providing an administrative path for accessing a writeable master storage volume in a mirrored storage environment as described above. For example the .admin prefix space may be a virtual extension of the namespace such that the .admin prefix may appear everywhere in any directory e.g. is omnipresent . The .admin path component thus may appear anywhere in a pathname not just as a prefix thus prepending is also illustrative . Also access to the writeable copy the .admin pathname may be limited by enforcing mount options as may be appreciated by those skilled in the art such that a client with write access to the data may be allowed to enter the writeable master copy while a client without write access may not. Further particular levels of the pathname may be restricted to certain clients e.g. clients with write access to mount the top level writeable directory e.g. to those with root access and other clients with write access to mount only certain writeable directories. In particular it is not necessary that writeable directories be available for each directory pathname. In other words the prepended prefix .admin provides a gateway to the writeable master data and the number of gateways to the roots of the namespace or to volume roots for example may be limited.

Moreover in accordance with one or more embodiments of the present invention it may be beneficial to monitor keep track of whether a client is in the writeable .admin copy or the normal e.g. local configured namespace e.g. such as by marking a flag in the filehandles that are given to the client. In this manner when the client subsequently changes directories through a namespace this flag may be used to remember that the client has access to the writeable .admin copy of the data. Note that the filehandle is generally opaque to the clients thus the presence or absence of the flag may be obscure to the client if so configured. By tracking whether a client is in the writeable namespace allows for the construction of complex namespaces from volumes that are mirrored junctioned to volumes that are not mirrored junctioned again to other mirrored volumes in any combination. That is as the client traverses the namespace the namespace may be moved to either read only mirrors or writable master volumes according to whether the client has entered the writeable .admin namespace at some higher level directory. Also in one or more embodiments it may be beneficial to configure the transition to the writeable .admin namespace to occur only once. In other words once in the .admin namespace changing directories again to a writeable .admin directory is idempotent and the .admin namespace remains the active namespace once it has been entered. Further changing directories to the parent directory e.g. to .. may lead to a different parent directory depending on whether the client is in the writeable .admin namespace or not.

The foregoing description has been directed to particular embodiments of this invention. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. Specifically it should be noted that the principles of the present invention may be implemented in non distributed file systems. Furthermore while this description has been written in terms of N and D modules network elements and disk elements the teachings of the present invention are equally suitable to systems where the functionality of the N and D modules are implemented in a single system. Alternately the functions of the N and D modules may be distributed among any number of separate systems wherein each system performs one or more of the functions. Additionally the procedures processes and or modules described herein may be implemented in hardware software embodied as a computer readable medium having program instructions firmware or a combination thereof. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention.

