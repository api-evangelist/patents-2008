---

title: Systems and methods for storing object and action data during media content development
abstract: Embodiments provide data storage of object and action data for use in media content development applications. For example, a data storage scheme may make information available to a user interface specifying, for a particular object type, which actions can be performed by objects of that type, as well as default values for objects of that type. Embodiments involve data storage techniques that are adaptable for use with new and changing object and media types.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08918758&OS=08918758&RS=08918758
owner: Adobe Systems Incorporated
number: 08918758
owner_city: San Jose
owner_country: US
publication_date: 20080530
---
This application claims priority to U.S. Provisional Patent Application No. 61 123 861 filed Apr. 11 2008 entitled SYSTEMS AND METHODS FOR DEVELOPING OBJECTS AND ACTIONS IN MEDIA CONTENT AND FOR CREATING AND STORING OBJECT AND ACTION DATA DURING MEDIA CONTENT DEVELOPMENT and which is hereby incorporated herein by this reference.

Embodiments of the present disclosure relate generally to the development of media content including moving and interactive media content and to backend data storage in content development environments.

Various software applications and other tools facilitate the creation of media content such as web banners site openers showcase sites and concept pitches. An action is something that happens when content runs including what a page or object within a page does. An action can describe motion move face etc. or interaction play go to link etc. . Media content can involve various types of objects including displayed objects such as squares circles other shapes buttons text web links images movies and objects that are not displayed. Some media content development tools display a development canvas that provides a visual preview of the content being developed and allows adjustment of the size or position of a displayed object. However generally such tools require significant user effort particularly in defining movement interactivity and other actions which are typically user coded. While such tools are often powerful robust and useful they may be ill suited for some users such as those unfamiliar with the particulars of the development environment coding syntax and parameters including the available objects actions and triggers.

Content development tools must also keep track of data about the objects and actions being created on the backend for example using storage mechanisms to facilitate the development application. Since the attributes of the various types of objects and the various types of actions are not constant and new objects and action types may be created in the future it may be desirable in some embodiments to provide a content development tool that is adaptable to changing and new types of objects and or actions.

Certain embodiments of the present disclosure provide an intuitive interactive content development environment that simplifies assigning an action for example assigning an action to an object. In one embodiment the user interface provides the user with a list of available actions for example for a particular object based on the developer s selections on a development canvas. For example when a user mouse clicks on an action menu item and selects a rectangle object displayed on the canvas a list of the available options for that object e.g. rotate slide into view etc. are presented to the user for selection. The options may be displayed in a window that is adjacent to or proximate the object. Global objects which do not relate to a particular object may also be presented based on a developer selection.

In certain embodiments of the present disclosure the user interface of the development tool simplifies creating interactions between a trigger object and a target object. An exemplary method involves a receiving a selection of a trigger object for example by a user positioning a mouse curser over a button and clicking and holding the mouse button b receiving a selection of a target object for example by the user dragging the mouse curser from the button the trigger to a green oval the target and c providing a menu of available actions for the target object and d receiving a selection of an action that will be performed by the target object when the trigger action occurs. In certain embodiments the method is initiated by a user selecting an action tool for example from a menu on the user interface. In certain embodiments the triggering event may be identified by default for example it may default to the down click of a displayed button that is selected as a trigger. The triggering event may also be specified by the user. For example the method may involve providing a menu of available trigger actions and receiving a selection of a trigger action. In one embodiment the trigger object is a video and the trigger action is a cue point in the video.

Certain embodiments of the present disclosure provide data storage of object and action data for use in media content development. Such data may include among other things a list of editable properties along with their types and default values for the object or action a list of available triggers that an object can respond to and a list of actions that an object supports.

Certain embodiments of the present disclosure provide adaptable data storage of object and action data for use in media content development. One exemplary system comprises a database a data engine and one or more component factories. In one embodiment a component factory produces information about an object type that is provided to the data engine and used to populate tables that include data about the object type. A development interface may access such tables for example through the data engine to obtain the information about an object type. For example when a user creates a new square the user interface may access the database to determine shape attributes and default attributes e.g. initial color position and size for the square. The database may also contain information about the newly created instance of the square. Upon modification any changes to the attributes are sent to the data engine for storage in the database. In certain embodiments an intermediate is used between the component factory and the data engine to among other things convert the information about the object type into a different format such as an eXtensible Markup Language XML format that may be parsed by the data engine.

In other embodiments a computer readable medium such as for example random access memory or a computer disk comprises code for carrying out these methods.

These embodiments are mentioned not to limit or define the disclosure but to provide examples of embodiments to aid understanding thereof. Embodiments are discussed in the Detailed Description and further description is provided there. Advantages offered by the various embodiments may be further understood by examining this specification.

Certain embodiments of the present disclosure provide an intuitive interactive media content development environment that simplifies the creation of actions for example allowing users to easily specify that a shape will move when the media is run or that a web page will be opened when a button is clicked within the media. The assignment of an action may involve one or more displayed objects including as examples trigger objects associated with an event that triggers the action and target objects that move or otherwise perform the specified action s . Embodiments provide a visual and intuitive interface that minimizes the experience required of a user and in some cases provide a primarily mouse click driven development environment for both objects and actions including for specifying interactions between objects. Embodiments also provide data storage of object and action data for use in these and other media content development applications. For example a data storage scheme may make information available to a user interface specifying for a particular object type which actions can be performed by objects of that type as well as default values for objects of that type. Embodiments involve data storage techniques that are adaptable for use with new and changing object and action types.

The following specific illustrative embodiment is provided to illustrate some of the applications uses and benefits of providing a media development tool having a user interface that facilitates the creation of interactivity.

Referring now to the drawings in which like numerals indicate like elements throughout the several Figures are illustrations of a user interface according to certain embodiments. As shown in the user interface comprises two modes a design mode selectable with the DESIGN button and a preview mode selectable with the PREVIEW button . While in design mode the user interface comprises a stage or canvas area upon which a user can visually position and manipulate objects and specify actions.

This exemplary user interface further comprises a variety of tools including a selection tool for selecting objects displayed and actions represented on the canvas area an action tool for specifying actions an import tool for importing graphics video and other ingredients a rectangle tool for drawing a rectangle object an ellipse tool for drawing an ellipse object and a button tool for drawing a button object. Other tools for example for different object types may of course be provided.

The user interface shown in further comprises a publish button which allows a user to publish a created document for example by creating an executable or otherwise usable file or collection of files that can be distributed or made available to an audience. The user interface also displays a Layers area for providing information about the components placed on the canvas area .

The following example illustrates how a user is able to simply and easily create interactivity between objects using this interface. On the user interface a user mouse clicks on the button tool and is presented with a displayed button on the canvas . The button is given default attributes including its position size color label normal skin among others as shown in . The button appears in the default position. The user is also presented with a properties window that displays attributes of the button that may be edited. In this example the properties window is displayed over the canvas area and proximate the object i.e. button to which it relates. The user is able to change these attributes by typing in new values or can change the attributes by manipulating the displayed button . For example the user might use the mouse to resize the button on the canvas or click on the button and drag it to a new position using a mouse.

After repositioning the button as shown in the user next clicks on the rectangle tool and is presented with a displayed rectangle on the canvas . The rectangle is given default attributes including its position width height fill color stroke corner radius and opacity among others. The user is also presented with a properties window that displays attributes of the rectangle that may be edited. The user is able to resize and reposition the rectangle using only the mouse. The resized rectangle at a new position and repositioned button are shown in

Next the user mouse clicks on the action tool with the intention of identifying the button as an action trigger object that will target the rectangle to perform an action i.e. at runtime when the button is clicked the rectangle will do some action. As shown in and the user presses the mouse clicker down when the action cursor is positioned above the button on the canvas holds the mouse clicker down while moving the curser from the button to the rectangle and then releases the mouse clicker. As shown in the user is then presented with an available actions window that displays actions available to be performed by the rectangle including as examples Go To Link Slide Resize Fade Spin and Front Back. The user selects the Slide action from the available actions window using the mouse curser.

As shown in when the user selects the rectangle on the canvas timing information about the interaction that has been defined between the button and the rectangle is displayed. In addition upon creation of an action or when an action is selected by a user an actions window may be displayed that provides general information Slide when fl.controls.Button is clicked as shown in . Here fl.controls.Button is the representation of button used by the user interface and clicked is the action that will trigger the rectangle to perform the action i.e. when the button is clicked during runtime the rectangle will slide.

By clicking on the listed action in the actions window the user is able to bring up an action attribute window as shown in . Exemplary action attributes for the Slide action include x coordinate start and finish positions y coordinate start and finish positions delay duration and ease among others. These attributes can be edited in action attribute window or changed using the mouse pointer. For example as shown in the user presses the mouse clicker down when the cursor is positioned above the rectangle on the canvas holds the mouse clicker down while moving the curser until a shadow image of the rectangle is in a desired position and then releases the mouse clicker. Based on this action by the user the action attributes are defined. The user can adjust the final position of the rectangle by clicking it and moving it or by manually changing the values displayed in the action attribute window .

The user is also able to change the triggering event. Upon creation of the action the triggering event was defaulted to Click however other events associated with the button can trigger the action. As shown in the user is presented with a list of the available trigger events including as examples Click Double Click Mouse Entrance Mouse Exit Mouse Pressed and Mouse Released. In this example the user leaves the trigger event at its default value of Click. 

To preview the interaction the user can select the preview button from the user interface as shown in . Upon selection of the preview button a preview panel is presented displaying the button and the rectangle in its initial position. The user is able preview the media being developed as if the media were playing. If the user mouse clicks on the button the user preview panel shows the rectangle moving to its ending position as shown in and

The user is also able to publish a media file by selecting publish button of the user interface. For example the user may use this tool to generate a file or collection of files that can be executed by one or more applications. and show the media created in the preceding example running outside of the user interface development environment.

These illustrative examples are given to introduce the reader to the general subject matter discussed herein and are not intended to limit the scope of the disclosed concepts. The following sections describe various additional embodiments and examples of methods and systems for using media and associated metadata.

Certain embodiments of the present disclosure relate to specifying an action trigger interactivity model which defines how a user is able to add behavior to content. Generally interactivity relies on the events a given object component can receive as well as global events not related to a particular object. For example events may be messages from a sender object to a receiver object for example a button sends a message of I was clicked to a video object which understands from the message that it should start playing its video etc. Object components thus will generally have actions they can execute and events they can recognize. Certain embodiments facilitate development of actions by providing a visual interface that displays components and advertises the events i.e. trigger events a given component can recognize and the actions it can execute. The source of an event can be from the user mouse keyboard touch screen voice etc. internally generated the same component or other components in the document and externally timer network activity . For example mouse events include down up click double click mouse enter mouse exit etc.

There is a wide breath of actions a given component may be able to perform. Like objects actions may be installed with the development program or may be available from imported sources. For example an action application programming interface API may be made available for third parties to define objects and actions. Thus the development application may be extensible and allow plug in object and action components. In addition the attributes of an object or action may be changed. An action may dictate how it is layered and sequenced with other actions.

Actions include but are not limited to transforms effects and behaviors. For example a transform action is one that modifies component parameters such as position rotation alpha scale skew repel from and attract to. An effect action is one that utilize a runtime player s filters and or does some type of processing of or on the content for example drop shadow blur glow bitmap manipulation etc. By their nature effects can be layered. Behavior action perform some operation but do not modify transform or effect parameters. Examples are goto URL goto page send an event to another object to cause it to play pause set a Property etc. Compound actions are those that fit into more than one of the other categories.

Actions can also be grouped depending on whether they are universal component specific or freestanding. Universal actions can be applied to all components and are automatically made available for selection e.g. set a property goto URL goto state generate an event goto page next prev etc. Component specific actions are also automatically made available for selection and are based on the component s type e.g. video components can be played. These actions can be thought of as inherent built into the component. Freestanding actions are not defined by the component but instead are applied to a component e.g. a bounce motion preset. Additional exemplary actions include but are not limited to for a document document specific actions play move generate progress event etc for a page page specific actions transition etc. for other objects object specific actions play move generate progress events filter effects blur shadow emboss glow etc. tweens for a bitmap bitmap specific actions for a vector vector specific actions for audio video audio video specific actions start playback at time generate progress events pause stop rewind fast forward start playback at cue point goto next prev cue point video specific actions audio specific actions mute change volume . Actions for freestanding components include but are not limited to motion on object mimic motion of another component with modifiers property with modifiers with modifiers including but not limited to invert do opposite delay cycle repeat repeat n times repeat forever. Other actions for freestanding components include but are not limited to repeal from mouse other objects edge of canvas stage etc. with attributes like hit radius and strength attract to mouse other objects edge of canvas stage etc. attach noise to a property with attributes like frequency and amplitude and change z order incrementally steps or smooth dissolve as you step. 

Exemplary user events include mouse events down up click double click mouse enter mouse exit hover start drag end drag keyboard camera and microphone events. Exemplary internal events include component loaded created component unloaded deleted action began completed sequence began completed. Exemplary external events include timer player network activity and file system activity. Exemplary event properties include delay repeat N times repeat forever and collision detection events.

Multi component interactivity can be thought of as a relationship or link between two or more objects that can be established terminated and or interrogated. Each action may generate action began and action ended events or other events. For example an action s end event can be used to sequence operations within and or across components.

Multiple actions can be trigged from the same event. illustrates component interactivity involving a mouse and a video object . The interactivity is specified such that when the mouse is clicked the video performs both a fade up action and a play video action. Additionally the execution of the action can be delayed from the start of the event. Therefore each event causes a timeline of actions to be executed played. In other words there is a timeline per event where actions are sequenced an Action Timeline. illustrates a timeline for the fade up and the play actions. Certain embodiments provide a user interface that displays such a time line to a user and allows the user to visually adjust the delay and duration parameters of actions. Display of is an example of a timeline for a single action a slide action.

In certain embodiments during the authoring process a user may attach actions to the object and assigns the events which trigger them to construct a web of interactivity in which multiple objects interact back and forth with one another and in loops i.e. a first object performs an action resulting in an event that triggers a second object to perform an action resulting in an event that triggers a third object to perform an action resulting in an event that triggers the first object to perform an action etc. Such a web of interactivity can grow in complexity for example even with a single initial triggering event such an event can result in multiple actions performed by a set of multiple objects as illustrated in . Certain embodiments also provide for infinite event loop checking to alert users of these and other potentially problematic interactivity relationships.

The user interface may or may not prevent a user from directly interacting with the objects programmatically for example using declarative or procedural programming directly in the tool. To assist developers unfamiliar with the programming syntax and parameters or unable to program the development tool may facilitate the development of interactivity by advertising as many events as possible for a given object type. Thus even a relatively unsophisticated user may have easy access to a rich set of actions and events available to construct interactivity on an easy to use user interface. On the other hand in some cases the available actions events objects and other components may be limited to simplify the user interface in an effort to avoid overwhelming a user.

As shown in an exemplary method involves receiving a selection of a trigger object as shown in block . For example this may involve identifying that a user has positioned a mouse curser over a button and clicked and held the mouse button.

The method shown in further comprises receiving a selection of a target object as shown in block . For example this may involve identifying that the user has dragged the mouse curser from the trigger object to a target object and released the mouse button.

The method shown in further comprises providing a menu of available actions for the target object as shown in block .

The method shown in further comprises receiving a selection of an action that will be performed by the target object when a trigger event occurs as shown in block . In certain embodiments the triggering event may be identified by default for example it may default to the down click of a displayed button that is selected as a trigger. The triggering event may also be specified by the user. For example the method may involve providing a menu of available trigger events and receiving a selection of a trigger event. In one embodiment the trigger object is a video and the trigger action is a cue point in the video.

A computer readable medium may comprise but is not limited to an electronic optical magnetic or other storage or transmission device capable of providing a processor with computer readable instructions. Other examples comprise but are not limited to a floppy disk CD ROM DVD magnetic disk memory chip ROM RAM an ASIC a configured processor optical storage magnetic tape or other magnetic storage or any other medium from which a computer processor can read instructions. A computer readable medium may transmit or carry instructions to a computer including a router private or public network or other transmission device or channel both wired and wireless. The instructions may comprise code from any suitable computer programming language including for example C C C Visual Basic Java Python Perl and JavaScript.

While the development application is shown operating on a single device alternative embodiments include applications that reside and or access data stored on more than one device and or that utilize a network to transmit information. Other alternative configurations are of course possible. The device may also comprise a number of external or internal devices such as a mouse a CD ROM DVD a keyboard a display or other input or output devices. Examples of devices that could execute a development application are personal computers digital assistants personal digital assistants cellular phones mobile phones smart phones pagers digital tablets laptop computers Internet appliances other processor based devices and television viewing devices. In general a device may be any type of processor based platform that operates on any operating system capable of supporting one or more applications.

In certain embodiments a media content development application comprises a front end and a back end . A component archive module also may be part of the application. The front end comprises an editing interface a preview engine and an application builder . The editing interface may be used to display an editing canvas to allow a user to create media content including interactivity. The editing interface may also comprise various tools that a user may use to develop interactive content on the editing interface . The preview engine may be used to mimic the runtime behavior of a media content application being developed on the editing interface . The application builder may be used to publish or otherwise generate a final piece of media based on the information specified on the editing interface .

The back end supports the operation of the front end components and the component archive module provides information about components such as object and action types that are available for use on the editing interface . The back end may comprise an intermediate component that interfaces between the component archive module and the data engine . The data engine may store data in a database such as a relational database. Such information may include component metadata including information about object and action types and associated default values. The component metadata may be extracted from the component archive module by the intermediate and provided to the data engine for storage in the database . The component metadata stored in the database may be used by the front end which will generally access such data through the data engine . For example the editing interface may ask the database engine for a list of actions that are available for a particular object type. The database may also be used to store instance data reflecting information about objects and actions instances created on the editing canvas. For example when a button is moved on the editing canvas of the editing interface information about the position of the button may be stored in the database .

The database need not include information about how these objects are implemented i.e. how the attributes relate to one another or the object or action with which they are identified. The database may view content as black boxes without storing the actual logic used to generate for example a rectangle from the rectangle attributes stored in the database. The back end may utilize bag data structures or a sets of tuples e.g. for each object or action that is on the editing canvas storing a name value and a list of attributes.

Similarly the application builder may interact with the database to get information about objects and actions while compiling an application. For example the application builder may ask the data engine for the attribute values for a rectangle that is displayed on the editing canvas from the instance data and information about rectangle type objects from the component metadata .

The editing interface and preview engine may operate using some or all of the technologies used by the runtime component. For example if the development tools facilitate development of an application that can be run on an Adobe Flash player the editing interface and preview engine can include actual Adobe Flash technologies to facilitate the display and preview of objects and actions. Accordingly a virtual machine can be used that effectively runs an Adobe Flash player.

The component archive module comprises one or more component archive files and one or more component factories . Exemplary functions for these components are described in the following sections.

Information about the objects and actions available in a media development application may be extracted at the time the development application is run. For example when an application is run for the first time a component archive module may access the installed component libraries and identify one or more component archive files . In the particular embodiment of an Adobe Flash media development application a .swc archive file may be available that includes a catalog of one or more component packages and the individual components of the packages and other information. The one or more .swc files may be used to as the component archive file .

The component archive module functions to extract information about components from the component archive file s and provide such information for storage in the database or other use. The database may not need to be rebuilt upon each initialization of the application i.e. the component metadata may persist in the application even when it is not running. To avoid unnecessary effort the component archive module intermediate or data engine may determine whether the component archive file has been changed since data was last extracted from it for example by comparing the components already available in the application and the components available from the component archive file and or using a modification date of the component archive file and a modification data associated with a component type already in the application.

If new components exist or if components have been changed a component factory which may have been part of a component archive file e.g. a .swc can extract information from the component archive file and send the information either to an intermediate or directly to the data engine . An intermediate may be used to put the information in a consistent or otherwise useable format. In one embodiment the intermediate for each component type it encounters builds an XML description of the component and passes it to the data engine. The XML may be made available to other components of the development application in addition to the data engine . The intermediate may also generate a type stamp that can be used at later application start ups to determine whether a component or component archive file has been updated.

The XML is parsed to determine the attributes for the component type including its display properties available actions available triggers and default values. The object metadata is stored in the database as component metadata . Action types may also have properties and defaults that are stored in the component metadata . Thus for a given component type the data engine can populate a table that describes everything the data engine needs to know to create and use an instance of this type of component. In certain embodiments component type information is derived in other ways. For example it may be derived from constants embedded in a component type file among many other ways.

Certain embodiments of the present disclosure provide methods for implementing a storage mechanism that is able to adapt to unknown classes types of objects that may be unknown at the time of the storage mechanism s creation or that may be changed on the fly. Such methods allow information about such classes types of objects to be used by a front end development application and or virtual machine.

In the method shown in a media development application receives a component archive such as a component archive file that comprises information about one or more component types as shown in block . The information about a component type may comprise information about the attributes of the component type information about the default values of the component type information about how the component is rendered on a canvas or other display and or other types of information about the component type.

In the method shown in the media development application uses the information about one or more component types to determine one or more component descriptions specifying how the component will be displayed on an editing canvas of the development application as shown in block . The component description may specify an attribute list for components of that type and or default values for components of that type among other things.

In the method shown in metadata is stored for the one or more component types that comprises the one or more component descriptions specifying how the component will be displayed on an editing canvas of the development application as shown in block . For example a data engine may be used to generate tables in a relational database comprising the metadata information about the component types.

When a user creates a piece of media using the front end components the front end communicates with the back end to store data related to the media being developed. For example when a user creates a button object on the editing canvas the front end for example a native code handler may send a command to create a new instance of the button to notify the data engine . The data engine queries its component metadata tables that define the button and other components and retrieves the information that is related to button type components. The data engine then goes to a table of instance data and creates one or more database entries specifying an initial state for the created button. The data engine populates all the properties with default values from the component metadata and may allocate space for actions or other related components.

The data engine also sends back a notification to the front end that there is a new object that has been created the button. The front end will retrieve information about the newly created button including its attribute values and display the button on the canvas . For example the front end may send a request to the data engine for the stored values of one or more of the attributes of the button. Thus the newly created button instance that is displayed on the canvas is in sync with what is inside the database .

As the user makes changes to the object on the canvas messages are sent to and from the back end data engine so that the changes made by the user are reflected in the instance of the object stored on the back end e.g. in instance data .

When a user creates an action on the editing interface the backend may be used both as a source of information and to store information about the newly created action instance. For example a use may specify that a mouse click on a button will cause a rectangle to slide as in the example. To create such an action the user selects the trigger object button and target object rectangle . Note that a trigger and a target may be a single object and that an action need not have a trigger object and or target object. Upon selection of a trigger object and target object a message may be sent to the back end to let the back end know that a new action instance is being created. The backend data engine may receive this message and look up in the component metadata all of the available actions that a rectangle type object can perform and returns this information to the front end where it is provided to the user. The data engine may use a similar method to return a list of available trigger events for the trigger object.

The data engine may also modify the instance data for example by creating an action instance sub component of the rectangle object and including an identification of the triggering object. Thus in certain embodiments the instance data includes data about an object instance that includes a list of actions that that component is specified to perform including for each such action properties or other attributes that specify the details of the action. In the preceding example if the action is a slide within the rectangle object there is a new slide action instance created in the instance data for the rectangle that includes the specified or default if unspecified attributes of the slide action.

In the method shown in a media development application stores metadata for one or more component types as shown in block . The metadata specifies how each component type will be displayed on an editing canvas of the development application for example by describing which attributes of an object or action may be specified by a user and how those attributes affect the display of the object or characteristics of the action.

In the method shown in the media development application provides an interface that allows a user to create an object on an editing canvas based on the stored metadata as shown in block . In certain embodiments the object will be displayed upon creation in a default location determined by the metadata stored for the object type.

In the method shown in an instance of metadata is created for the created object in a database or other storage device as shown in block .

Certain embodiments of the present disclosure provide methods for supporting a user interface of a development application that utilizes generic communications. For example a user interface is able to allow a user to create objects and actions without having to know the details of the particular type of object or action such as the available actions and triggers for a given object type. In certain embodiments the details of the objects and actions are completely extensible and able to be extended for example based on imported component class libraries. In other words the object and action schema of both the front end and back end can change over time.

The extensibility of certain embodiments allows the use of new types of objects that work seamlessly on a development application s development canvas . Certain embodiments are able to derive component information from components used in other development environments even ones that do not conform strictly with the component model. For example the application could include logic that essentially makes assumptions based on the information about an object type that can be derived about an object type used in another application.

Certain embodiments of the present disclosure facilitate undo redo commands on a media development application including the undo of the deletion of an object. Upon deletion of an object the data engine may delete all instances of the object in the back end data base. Rather than completely removing the information it may be stored permanently or temporarily in an undo table that may not be viewable to the front end of the application. If the user subsequently decides to undo the object deletion the data engine can retrieve the entries from the undo table and repopulate entries in a live table corresponding to the undeleted object.

Certain embodiments of the present disclosure provide a user interface that allows a user to specify one or more actions for an object that describe what the object does when the interactive content runs. The interface further provides the user with a timeline showing the sequence and duration of all of the actions associated with the object and the duration of each. In one embodiment the timeline is interactive. For example a user can change the delay or duration associated with an action.

When a user elects to publish a piece of media a publishing module shown as an application builder in is called. The publishing module while depicted as a portion of the front end in may be implemented as a client of the data engine on the back end . In certain embodiments the publishing module walks through the objects and other components defined on the editing interface . It may begin with a root object for example an object that corresponds to the canvas itself and then proceed to the subcomponents of the root component and so on. The publishing component may use a portion of a declarative file for example an MXML file in the Adobe Flash environment that describes the objects and the object attribute values. A similar task is performed for each action. The publishing component may also include declarative handler tags inside of the components. The publishing component may use object relational mapping to extract the data from the database. The result is a declarative lay out of all of the components and edits that the user has specified in their project. The implementation details of the actions may be pulled from a subclass for example from a class library.

The declarative code can then be converted or compiled into another format such as a media player format e.g. Adobe Flash player format. This process may involve the archive files used to specify the component types e.g. a .swc which may provide the component definitions. Accordingly in some embodiments the same dynamic library may be used to provide component type information used for an editing interface and to provide component definitions used when a media file is published.

Certain embodiments provide a media development application that includes a user interface that relies upon an Adobe Flash player for the rendering of data. The components used in an editing canvas of such embodiments may for example be implemented within the Adobe Flash runtime as ActionScript3 objects. As such they may have a set of properties that can be set within the ActionScript at runtime. These properties can affect things such as positioning appearance and other data aspects of the components. Each type or class of component can have an arbitrary set of properties although for most visual components there can be some overlap within the set of properties exposed. Since the various types of components have heterogeneous sets of properties and also since the editing canvas may be designed to support external or extended sets of components dynamically the back end data storage mechanism may be designed to be flexible enough to adapt to a large set of heterogeneous component types.

While there are some facilities within ActionScript to ascertain class metadata as well as metadata compile time information used for example by Adobe Flex Builder the level of information these provide may be insufficient for certain user interface and back end tasks. Additional metadata may be useful with respect to support manipulation of an instance of a component of a particular type e.g. for obtaining or set the current value of a specific property and support for obtaining metadata about class or type of component. Support for obtaining metadata about type components may comprise methods of enumerating the unique property identifiers within a type methods to get the action type of a property and methods of getting the default or preferred value for a specified property. The additional metadata may also contain support for actions for example by comprising methods to list the set of available actions APIs to get information about the actions e.g. parameters and default values and methods to instantiate and edit actions contained by a component. These support functions or scaffolding may comprise methods that are contained in an ActionScript interface declaration. When a component type is used an instance of that type may be registered with the main application based upon implementing or otherwise using the interface declaration.

The first time a component type is encountered by the development application an operation may be performed that readies the type for use within the canvas and within the data engine. For example an ActionScript virtual machine may be used that functions to generate a high level XML structure. As a particular example a utility class may utilize public methods exposed from an ActionScript interface declaration to generate data for the XML fragment. An exemplary XML fragment comprises the ActionScript of the class a list of all editable properties along with their types and a default value that should be set on a new instance of this type a list of available triggers trigger events that this object can respond to a list of available actions that this object supports for each action type a list of parameters that the action can take and the type and default values of these action parameters and for each component type and action type additional information for publishing the media for example information for .swf generation such as a generic identifier to be used in an MXML file . Alternative embodiments may of course not use an XML fragment or use an XML fragment with differing content.

A data engine may be used to parse the XML fragment and create entries within a relational database model. These tables may describe at a fundamental level all of the relevant information about a component type including information needed to instantiate a new component of this type within the data engine as well as manipulation and inspection of arbitrary properties within a component instance. A time stamp describing when the component type metadata was created may be embedded or otherwise included with this information. In the event that a component type is updated for example when a component library is provided that has a newer modification date the rows in the database that describe the component class in question are removed and the XML parsing and importing of the metadata process is re run.

Certain embodiments relate to an adaptable back end development application architecture and or to a user interface that provides specific functionality such as the ability to visually define interactivity. These are merely illustrative. For example certain embodiments will not involve an adaptable backend. Certain embodiments will not involve interactivity. In short the techniques for supporting the development of media and the other features described herein have uses in a variety of contexts not to be limited by the specific illustrations provided herein. It should also be noted that embodiments may comprise systems having different architecture and information flows than those shown in the Figures. The systems shown are merely illustrative and are not intended to indicate that any system component feature or information flow is essential or necessary to any embodiment or limiting the scope of the present disclosure. The foregoing description of the embodiments has been presented only for the purpose of illustration and description and is not intended to be exhaustive or to limit the disclosure to the precise forms disclosed. Numerous modifications and adaptations are apparent to those skilled in the art without departing from the spirit and scope of the disclosure.

