---

title: System, method and computing device for detecting duplicate financial documents
abstract: A method and system are disclosed for identifying in real time duplicate financial documents processed by a financial institution or check clearinghouse. A collection of hash values representative of previously processed financial documents are maintained in a memory, such as a GPU memory. When a new financial document enters the financial institution or check clearinghouse for processing, one or more features of the financial document are captured. A hash value is generated from the one or more features of the financial document. A search is performed in the collection of hash values for a matching hash value. If a match is found, a potential fraudulent event or operational error may be indicated. If a match is not found, the hash value representative of the new financial document is added to the collection of hash values.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08185459&OS=08185459&RS=08185459
owner: Symcor Inc.
number: 08185459
owner_city: Mississauga, ON
owner_country: CA
publication_date: 20080514
---
The present invention relates generally to document processing and more particularly to the detection of duplicate processing of financial documents.

Financial fraud is a problem. People have devised creative ways of defrauding financial institutions including altering financial document data. People have also been known to create counterfeit copies of financial documents and in particular financial instruments and to pass those counterfeit copies through the financial system. For example fraudsters have been known to alter checks to change the amount or the name of the payee fraudsters have also have also been known to duplicate checks and attempt to cash the duplicates.

To combat fraud financial institutions may keep records of financial documents that have already been processed. For example electronic images of cashed checks are captured and archived. However in North America alone billions of financial documents may pass through the financial system each year. Thus efficiently searching through the billions of financial documents for duplicate documents poses a significant problem.

At the same time accidental duplicate processing of documents may occur as a result of error or oversight for example on the part of a bank or financial clearing centre. For example a transaction may be passed twice through the banking clearing system. Although detectable and correctable such errors tend to become apparent after funds have been transferred making their correction time consuming labour intensive and ultimately costly.

Accordingly there remains a need for methods and systems for quickly and in real time detecting processing of duplicate documents particularly financial ones.

In accordance with the present invention a graphics processing subsystem or similar processor is used to search entries representing processed financial documents in real time as such documents are processed to assess whether or not any particular document has previously been processed.

Conveniently graphics processing subsystems may efficiently search and maintain search entries representing millions or even billions of financial documents.

In a first aspect of the present invention there is provided a method of managing financial document data comprising maintaining at least a subset of a collection of hash values in a memory of a computer graphics processing subsystem wherein the memory is partitioned into a plurality of blocks wherein the plurality of blocks is searchable by the graphics processing subsystem in parallel and wherein each of the hash values is representative of a feature or features of a financial document generating a hash value for a given financial document the hash value representative of the feature or the features of the given financial document executing graphics processing subsystem executable instructions on the graphics processing subsystem the instructions directing the graphics processing subsystem to search at least a subset of the plurality of blocks concurrently for a hash value matching the hash value representative of the given financial document and returning an indicator indicating whether a matching hash value was found in the memory of said graphics processing subsystem.

In a second aspect of the present invention there is provided a system for financial documents comprising a duplicate document detection subsystem and a capture subsystem in communication with the duplicate document detection system. The duplicate document detection subsystem comprises a first computing device. The first computing device comprises at least one graphics processing subsystem. Each of the graphics processing subsystems comprises a memory storing a plurality of hash values the memory partitioned into a plurality of memory blocks and a programmable graphics processing unit operable to search the plurality of memory blocks in parallel and return an indicator if a value matching an input hash value is found in any of the plurality of memory blocks. The capture subsystem comprises a capture device for capturing one or more features of the financial document. The capture subsystem also comprises a second computing device. The second computing device comprises a processor operable to generate at least one hash value from the one or more features of the financial document and pass the at least one hash value to the duplicate document detection subsystem as an input hash value.

In a third aspect of the present invention there is provided a computing device for detecting duplicate documents. The computing device comprises at least one graphics processing subsystem and a processor. Each of the graphics processing subsystems comprises a memory storing a plurality of hash values each of the hash values representative of a feature or features of a document the memory partitioned into a plurality of memory blocks and a programmable graphics processing unit operable to search the plurality of memory blocks in parallel and return an indicator if a value matching an input hash value is found in any of the plurality of memory blocks. The processor is operable to instruct the at least one graphics processing subsystem to search at least a subset of the plurality of memory blocks concurrently for the input hash value.

Other aspects and features of the present invention will become apparent to those of ordinary skill in the art upon review of the following description of specific embodiments of the invention in conjunction with the accompanying figures.

Check is typically given as a form of payment to a payee. Thereafter the payee or a holder in due course may submit the check to a financial institution for payment. After submission of the check to the institution funds are transferred between the payor bank and the payee s bank and payment is settled. Financial processing of the check between submission and settlement is referred to as clearing.

Check may be processed by exemplary system as part of this check clearing. System may be operated by or on behalf of one or more financial institutions. In particular it may be desired to capture and store images of checks that have already been processed for a multitude of reasons including record keeping.

System also serves as a convenient locale for detecting possible fraud. For example fraudsters could attempt to deposit the same check twice for example by making multiple copies of a check and depositing each of the copies possibly altering one or more fields e.g. the payee or courtesy amount fields during the process. In this regard a method and system that detects attempts to process a check or other financial document more than once or where an attempt is being made to process an altered financial document e.g. check at or in conjunction with system may be highly desirable. Moreover apart from fraud detection such method and system may be used to catch operational errors where a financial document may be accidentally run through system more than once.

As illustrated in system may include a capture subsystem into which check or other financial documents may be input duplicate document detection DDD system which is exemplary of an embodiment of the present invention and financial document archive . Capture subsystem may output a digital representation or representations of check which representation s is are then input to DDD system . The representation s of check may subsequently be persistently stored in financial document archive .

Exemplary capture subsystem may be designed to digitally capture data from checks. Specifically capture subsystem may include a check scanner a MICR reader and a capture software application . Check scanner may be a standalone check scanner such as those manufactured by NCR Unisys Cannon or IBM e.g. Unisys My Vision X Panini MyVision Canon CR 25 55 or CR 180 . Similarly MICR reader may be a standalone MICR reader. Alternatively and as is more typical check scanner and MICR reader may be components of a high speed check processing transport or a high speed document processor such as the NCR iTRAN 8000 Unisys DP1800 IBM 3890 or the like. In this case check scanner may be a high speed digital scanner that captures images of the front and back of check and MICR reader may read the magnetic ink data MICR line at the bottom of check . Optionally certain fields on check such as MICR line or courtesy amount may undergo optical character recognition OCR ed . Capture application may be hosted on a conventional computing device to which check scanner and MICR reader may be directly connected. Alternatively images of checks captured by scanner and MICR line data read by MICR reader may be sent to the computing device hosting capture application and thus check scanner and MICR reader may not be directly connected to the computing device hosting capture application . The function of capture application will be further described below. It may however be appreciated that capture subsystem may be designed in other ways to capture financial documents other than checks.

Financial document archive which may persistently store data representing financial documents processed by system may be a computing device hosting a database engine and in communication with persistent storage memory in the form of magnetic optical or other memory suitable for the storage of large volumes of financial data e.g. images . Exemplary archive may for example take the form of an RS600 computing device hosting a DB2 database. Conveniently exemplary archive may store images of financial documents processed by system along with ancillary data associated with each image such as MICR data date processed and source of image e.g. the name of the financial institution from which the image was sent or the location where the financial document was scanned .

An exemplary DDD system exemplary of an embodiment of the present invention is depicted in . In particular DDD system may be hosted on computing device . Computing device may be a conventional computing device including for example a CPU a network interface one or more optional user input devices e.g. keyboard mouse one or more persistent data storage devices and system memory hosting an operating system such as Linux Windows XP Vista Unix MacOS or the like.

Computing device includes a graphics subsystem and may optionally include a second graphics subsystem . Graphics subsystem may be a graphics processor with a graphics processing unit . As may be appreciated by those of ordinary skill graphics processing unit may be a dedicated graphics rendering device designed to manipulate and display graphics on display device . Notably GPU may be a GPGPU such as the Nvidia Tesla graphics card that is compatible with Nvidia s CUDA Compute Unified Device Architecture technology. Second graphics subsystem may include a conventional graphics processing unit connected to display interface which in turn may be interconnected to optional display device . Display device may be a conventional computer monitor. To this end display interface may be a conventional DVI interface RAMDAD or the like. Both graphics subsystem and may communicate with other components of computing device such as CPU and system memory by way of bus . Bus may be a high speed system interface or a high speed peripheral interconnect bus such as the PCI PCI express or the like.

Conveniently the high degree of parallel processing on GPUs developed specially for parallel computation intensive graphics rendering also makes them conducive for use as general purpose processors. Used in this manner calculations and algorithms traditionally designed to execute only on a CPU may be executed on a GPU. Specifically calculations and algorithms may be designed to be more efficient by coding them in a multi threaded manner to take advantage of multi threaded execution on a GPU. Moreover multiple sections of code may be written so that they are executable in parallel on a GPU. As a result the overall execution time of a program i.e. calculation or algorithm may be reduced. See e.g. 2004 the contents of which are incorporated herein by reference.

In the past in order to use the GPU as a general purpose processor software programmers may have abused shader functions e.g. vertex shaders and pixel shaders of the GPU. Shader programs were written in languages such as Assembly Cg GLSL and HLSL shader languages consequently programmers were required to write code even for general purpose calculations in a shader language. Additionally programmers were required to manipulate both data and algorithms such that the data could be input in a format understandable by a shader program to produce a desired output e.g. a desired transformation calculation on the input data . These difficulties limited the extent to which GPUs could be used for general purpose programming.

However GPUs today have moved towards general purpose computing. In particular such GPUs known as GPGPUs may be more flexibly programmed. Examples of GPGPUs include Nvidia s Tesla processor Intel s Larrabee processor and AMD ATI s CTM close to metal initiative.

GPU API may be for example the CUDA Compute Unified Device Architecture API for Nvidia s GPUs. As may be appreciated by a person of ordinary skill the CUDA API is an extension of the C programming language that allows software programmers to write programs for execution by a GPU. Moreover using the CUDA API a programmer may in a given body of code specify blocks of code that are to be executed by the CPU e.g. CPU i.e. code that compiles into instructions executable on the CPU and blocks of code that are to be executed on the GPU e.g. GPU i.e. code that compiles into instructions executable on the GPU . More information regarding CUDA may be found in the 1.0 the contents of which are incorporated herein by reference.

Moreover GPU may interface with GPU memory . GPU memory may be GPU accessible memory which may be accessed by instructions including executing threads executing on GPU . GPU memory may be memory on the GPU processor or may be memory interconnected with GPU for relatively fast access. GPU memory may for example be interconnected to GPU by way of a local memory bus separate from the memory bus of CPU . GPU memory may for example be DDRAM forming part of a graphics subsystem formed on a peripheral interface card. In any event GPU memory may be memory local to GPU . DDD application data may be stored in GPU memory .

GPU memory may also contain software for execution by GPU referred to as kernel . As may be known to those of ordinary skill in the GPGPU art a portion of an application that is executed many times but independently on different data may be isolated into a function. The function may be executed on GPU by a plurality of different threads. The function may be compiled into the GPU instruction set so that it may execute natively on GPU and the resulting program known as the kernel may be downloaded to GPU . As will be further explained below program execution may be distributed between GPU and CPU to increase the overall efficiency of the program.

Beginning with capture application a representation of check may be captured by one or both of scanner i.e. an image representation and MICR reader e.g. a string representation of MICR line to form financial document data . Financial document data may then be input into a hash function to form financial document hash value .

As may be appreciated by those of ordinary skill different hash functions exist. The choice of a hash function dictates the likelihood that two different inputs may produce the same hash value i.e. the likelihood of producing a false collision . It may be appreciated that depending upon the technical application a suitable hash function may be chosen to output a desired expected number of unique hash values. In this particular technical application a hash function that outputs a very small number of collisions may be employed so as to provide a large set of uniquely identifiable financial documents. To further clarify if two representations of two different financial documents were to hash to the same hash value i.e. collide the two financial documents would no longer each be uniquely identifiable by its hash value. However if the two representations hash to different hash values then the financial documents may be uniquely identified by hash value. For this reason it is helpful if hash function produces as many unique hash values as is practicable. For example a hash value may be computed based on for example the MD5 or Haval hash functions.

The financial document data or a portion thereof should be consistently ascertainable for any particular financial document. In this way a hash value of this document data would be unique and reproducible for any particular document. For example MICR data on a check including the check serial number routing transit number account number and other transaction specific data e.g. courtesy amount is unique and may be unambiguously and consistently read no two cheques include the exact same MICR data yet each read of the MICR data will result in the same data being read.

In the depicted embodiment as illustrated in document data allows calculation of two hash values MICR hash value and image hash value . As will be further explained below image hash value may be generated from the image data itself. It may however be appreciated that only one of hash values and need be generated that is check may be represented by a single hash value e.g. MICR hash value . For the remainder of this document hash values and may be individually and collectively referred to as hash value s . As noted the hash value however should be easily reproducible from a given document. In an exemplary embodiment of the present invention two hash values are kept for reasons that will be further explained below.

As noted DDD system may be a component of a check processing system utilized by for example a check clearinghouse or a financial institution. In overview when a new financial document e.g. a check is received by the clearinghouse the check may be input into capture subsystem . Check may be scanned and its MICR line read. An image hash value may be produced from the image of check and an MICR hash value may be produced from the MICR line of check . It may be appreciated that each of the hash values is representative of a feature of check i.e. image and MICR line . Image data may be provided in a conventional image format such as a TIFF file an image bitmap a jpeg file or the like. Alternatively an image hash could be calculated upstream as the document image is being converted into a suitable image format. Both hash values may then be input into DDD system which may in turn return an indication of whether one or both of the hash values have previously been stored. The significance of one or both of the hash values being previously stored will be further explained below.

Specifically once generated by capture application hash value and or hash value may next be input to DDD application . DDD application may direct GPU to search for hash value or hash value collectively more typically hash value in hash table . As will be further detailed below hash table may contain a collection of hash values each representative of an already processed financial document. If hash value is found in hash table this may signify that the financial document represented by the hash value i.e. check has previously been processed by system . This may be an indicator of fraud or error. Conversely if hash value is not found in hash table this may indicate that check has not previously been processed by system . In particular search result output by DDD application may indicate whether hash value was found in hash table . Hash value may also then be added to hash table to signify that check has been processed by system .

Conveniently hash table may be held in GPU memory thus allowing quick searching of a large number of hash values i.e. in hash table . To this end hash table may continuously be held in GPU memory i.e. DDD system may be up and running at all times . Thus conveniently as a new financial document enters system for processing duplicate document detection may occur in real time. For contingency purposes however hash table may be periodically written out to disk e.g. persistent data storage device s .

As will become apparent in the presence of multiple GPUs hash table may be distributed across the GPU memories of the multiple GPUs. That is each individual GPU may host in its memory a subset of the collection of hash values making up hash table . Each of the multiple GPUs may search the subset of hash values of hash table in its local GPU memory i.e. its subset of the collection of hash values for the presence of the hash value of a document being processed. As well the hash value may be added to local memory of one of the multiple GPUs to indicate that the financial document from which hash value was generated has been processed.

To allow searching for hash value kernel may also include a search function which is executed by GPU to effect the search of hash table for a given hash value . Kernel may also include a sort function which is executed by GPU to sort the hash values or a subset of hash values in hash table . Conveniently because search function and sort function form part of kernel multiple execution instances of search function and sort function may be executed in parallel on different sets of hash values held in hash table in GPU memory by threads executing concurrently on a single GPU as detailed above.

At the outset GPU may be initialized for use by DDD system as detailed in . Specifically flow diagram S depicts initialization. S may be performed for example by CPU executing DDD application . Specifically in S kernel may be compiled and downloaded to GPU S . Memory space to hold hash table may also be allocated in GPU memory in S.

In an exemplary embodiment implemented using the CUDA API code for search function and for sort function kernel code may be specified in S by encapsulating the code in a  global  function. An example function declaration for search function may be  global void Search . . . and the body of the function may contain code for searching a block of GPU memory . An example function declaration for sort function may be  global void Sort . . . and the body of the function may contain code for sorting the values contained in a block of GPU memory .

Kernel may be compiled to GPU for execution on GPU using for example the nvcc Nvidia C compiler. Memory space may be allocated in GPU memory for hash table by e.g. calling the cudaMalloc function which allocates a user specified number of bytes of linear memory on GPU S .

Flow diagram S provides a high level overview of the operation of DDD application executing on CPU . As previously described in conjunction with DDD application may receive an input hash value e.g. hash value S . DDD application may direct GPU to search in hash table S for hash value . In the presence of multiple GPUs DDD application may direct multiple GPUs to concurrently search for hash value S . DDD application may then return an indication e.g. to display device or to a calling application of whether the input hash value was found search result S .

DDD application may further direct GPU to add the input hash value and possible ancillary data to hash table S . In the presence of multiple GPUs DDD application may direct a single GPU typically selected round robin to add hash value to its local GPU memory . DDD application may continue to execute as long as it continues to receive input hash values S .

Flow diagram S depicts operation of capture application in the context of operation of system to form financial document hash value from a currently processed financial document such as check . Operation of capture application is also described in conjunction with .

More specifically capture application may receive image data and MICR data of check from scanner and MICR reader respectively S . Capture application may also receive or get an associated timestamp of image and MICR capture at scanner and MICR reader S .

Capture application may then generate an image hash value from the image data by inputting image data or a subset of the image data into a hash function e.g. hash function . Conveniently the full byte array of the image file may be input into hash function . Other methods of generating a hash value from an image may be known to those of ordinary skill. For example some methods are discussed in US Application 2007 0239756 A1. One such method may include generating a feature vector for the image and inputting the feature vector into a hash function. The feature vector may be based on e.g. a color histogram of the image.

Capture application may also generate a MICR hash value from the MICR data by e.g. inputting the numbers on MICR line into the hash function . As noted that MICR line will be unique for each check as it is formed from among other things the payor s account number and the payor s bank number. Next a timestamp and may be generated from the time of image and MICR capture received or obtained at step S S . The image hash value and timestamp may be concatenated i.e. joined S and similarly the MICR hash value and timestamp may be concatenated S . Of course the generation of a timestamp and the concatenation of the timestamp to MICR hash value and image hash value is optional but may be useful for reasons to be explained below.

Conveniently hash values and may each be represented in memory e.g. system memory or GPU memory as a 64 bit hash value. Consequently for reasons that will also be further explained below image hash value may be divided into MICR hash value high portion the most significant 32 bits and MICR hash value low portion the least significant 32 bits . Similarly image hash value may be treated as image hash value high portion and image hash value low portion . Timestamp and may be a 32 bit value.

Now in order to take advantage of the parallel execution capabilities of GPU GPU memory may be logically partitioned into a plurality of memory blocks . . . as depicted in . As will become apparent use of multiple partitions allows distribution of hash values across the partitions sorting within each partition and concurrent searching of multiple partitions.

Insertion of a new hash value and ancillary data e.g. hash value for in process documents and timestamp s by DDD application and kernel to hash table is explained in conjunction with flow diagram S . Initially hash value may reside in system memory . In the single GPU embodiment the hash value to be added may be copied to GPU memory from system memory or wherever else the hash value may be stored S . Conveniently the CUDA API provides functions for copying a value from system memory to GPU memory e.g. cudaMemcpy .

In the depicted embodiment memory storing the collection of hash values contained in hash table is filled block by block. So for example new hash values may be added to a memory block e.g. Block until that memory block fills up at which time a subsequent new hash value may be added to the next memory block e.g. Block . Specifically with each new hash value to be added to the collection of hash values i.e. hash table the current memory block may be determined S . Next a determination is made whether the current memory block is full S . If not the new hash value may be added to the current memory block S e.g. after the last added hash value. If the memory block is full then the next memory block may be marked as the current memory block S . If the current memory block has been previously used and is therefore full as determined in S the hash values in the current memory block may be cleared in S by for example overwriting all memory locations in the current memory block with 0 s. The new hash value may then be copied to the current memory block e.g. at the beginning S . If the first cleared block fills the subsequent second block may be cleared and so on. In this way the n blocks of GPU memory maintain a rolling window in time of hash values . Conveniently a pointer may be kept to the current insert location for hash table .

Once the new hash value has been added to a particular memory block of GPU memory and hence added to the collection of hash values stored in hash table DDD application may optionally instruct GPU to sort the hash values stored in that particular memory block S . Algorithms for sorting values stored in a memory may be known to those of ordinary skill. As the bitonic sort algorithm is effective in a parallel execution environment it may be especially selected for the present purpose. While it is not necessary that hash values within a given memory block e.g. blocks . . . be sorted maintaining sorted hash values within each memory block may increase the efficiency of the search operation of DDD application as detailed below. In some instances however depending on the characteristics of the collection of hash values e.g. the statistical distribution of the hash values a mixture of sorted and non sorted hash values may in reality provide better performance than a fully sorted collection of hash values or a fully non sorted collection of hash values.

In the depicted embodiment hash values are sorted in ascending order based on their high parts e.g. portion of MICR hash value and portion of image hash value . In the event system includes multiple GPUs DDD software may first select one of the multiple GPUs S to which a new hash value is to be added. Thereafter the new hash value is added at that GPU to the next non full memory block as described above.

As should now be appreciated hash table will be stored in memory blocks . . . of the one or more GPUs. Hash table contains a collection of hash values each representative of one or more features of an already processed financial document i.e. its MICR data or image data . In the single GPU embodiment each memory block of the GPU will store a subset of hash table . Within each block hash values and ancillary data in the form of timestamps are stored in sorted order based on the high parts of the hash values. In the multiple GPU embodiment each hash table of each GPU maintains in its hash table a subset of the collection of hash values representative of already processed financial documents.

Now for each newly received hash value DDD application also checks table to determine whether or not that hash value has already been added to table signifying possible duplicate document processing.

Flow diagram S depicts operation of DDD application specifically of scatter gather function . Flow diagrams A and B depict corresponding operation of kernel . As illustrated scatter gather function may receive an image hash value and MICR hash value to search S . In the single GPU embodiment scatter gather function may instruct a single GPU to search for the image hash value and MICR hash value in hash table S . In the presence of multiple GPUs scatter gather function may instruct the multiple GPUs to concurrently search for the image and MICR hash values S .

In the depicted embodiment both MICR hash values e.g. MICR hash value and image hash values e.g. image hash value are stored in hash table . As such for any document both the MICR hash value and the image hash value may be searched.

If MICR hash value is found by GPU i.e. matches a value already stored in hash table as determined in S the time stamp associated with the matching MICR hash value may be returned S . Conveniently the return of the timestamp associated with the matching hash value provides information regarding when the duplicated document was first captured or previously processed by system . Also conveniently as noted above a timestamp and may be a 32 bit value and may be for example a date time value thus providing more accurate information regarding when the duplicated document was first captured.

Alternatively the 32 bit value may signify other information for example a memory location where an image of the associated document is stored thus allowing retrieval of the image. Conveniently if the 32 bit value is a key for example in financial document archive ancillary information associated with the suspect image could be retrieved from financial document archive and further decisions made in accordance with the retrieved ancillary information. For instance If the ancillary information indicates that the suspect documents were processed on the same day or originated from the same financial institution this may indicate an innocent error or no error at all in contrast to a fraudulent event. Thus depending on the ancillary information associated with a duplicated document different categories of potential errors may be identified and appropriate error handlers may be invoked.

If image hash value is found by GPU S the timestamp or another indicator associated with the matching image hash value may be returned in S. Of course image hash and MICR hash could be concurrently searched.

The result of the operation described by flow diagram may be returned as search result . Additionally upon identifying a duplicated document i.e. a positive search result a suspect message could be sent to an external handling system along with information regarding the suspect financial document. For example images of the suspect financial document s could be presented to a human operator for review along with an indication of the nature of the match e.g. match on image hash value and or match on MICR hash value .

Each search of hash table may be performed by GPU and more specifically by search function of kernel . is a flow diagram SA depicting one version of the search function performed by GPU . GPU may receive a hash value e.g. hash values or to search for target value in SA. GPU may execute a plurality of threads that execute search function on all GPU memory blocks in parallel. For instance referring to GPU memory may be partitioned into m memory blocks. At step SA m threads may be spawned by GPU . Each of the m threads may search within a memory block. More specifically search thread may search for the target value in block search thread may search for the target value in block and so on. All m threads may execute search function in parallel i.e. concurrently . Conveniently the CUDA API allows a calling program to specify how many threads should be launched and the memory space accessible by each thread. Conveniently more than one thread may also operate on a given memory block i.e. more than m threads may be spawned such that more than one thread may operate on each of the m memory blocks.

Returning to flow diagram A if the target hash value is found in any one of memory blocks . . . SA an indication may be sent to DDD application that the target value was found SA e.g. the timestamp associated with the matching hash value may be returned . Otherwise no indication may be sent alternatively an indication that the target value was not found may be sent. Conveniently if the hash values within each of memory blocks . . . are sorted then search function of kernel may be a binary search algorithm.

Flow diagram B depicts operation of an alternate search function which takes advantage of the fact that hash values within a given memory block may be sorted. Specifically to reduce the time required for sorting the hash values and ancillary data within a memory block may be sorted solely by their most significant bits e.g. MICR hash value high of MICR hash value and image hash value high at S . As such after receiving an indication of a hash value to search for target value SB an initial search for a matching value in memory may quickly locate hash values with identical most significant bits for example using a binary search SB . Once located SB a linear search of the least significant bits of hash values with identical most significant bits may be performed SB . Upon finding a hash value which matches the target value i.e. with identical most significant bits and identical least significant bits SB a notification may be sent to DDD application that the target value was found SB .

It may now be appreciated that whereas previously if there were 1 000 000 hash values in hash table to be searched and the search was to be performed by a process running on CPU in the worst case scenario 1 000 000 hash values may have to be visited. This problem may be diminished somewhat by using a more efficient search algorithm i.e. better than linear search . However by offloading the search to GPU and taking advantage of multi threading on GPU the 1 000 000 hash values may be divided up into blocks e.g. 1 000 blocks of 1 000 hash values and the 1 000 blocks may be searched in parallel. It may be appreciated that this provides an increase in efficiency.

Moreover even greater efficiency may be achieved by using more than one GPU. This is the aforementioned multiple GPU embodiment of DDD system . That is GPUs . . . may be added to DDD system as depicted in . In this embodiment previously identified GPU may considered to be GPU . Operation of each of GPUs . . . may be as previously described in relation to GPU . Notably each of GPUs . . . hosts a hash table . . . and its own copy of kernel . Conveniently additional GPUs . . . may be added to computing device or may be part of other computing devices networked to and possibly geographically remote to computing device .

In the multiple GPU embodiment of system and as depicted at step S of flow diagram S of before copying a new hash value to GPU memory from system memory DDD application may first select the GPU hosting the GPU memory to which the new hash value is to be added. The GPU may be selected in a round robin fashion e.g. if the last new hash value was added to hash table on GPU then the next new hash value may be added to hash table on GPU .

Also in the multiple GPU embodiment of system the scatter gather function of DDD application may receive a hash value to search for target hash value and may concurrently instruct each of GPUs . . . e.g. via a multicast message to each search for the target hash value in its memory . The results of the search by each of GPUs . . . may be compiled by scatter gather function to produce search result . Specifically if the target hash value matches a hash value stored in any of hash tables . . . then search result may indicate that a match was found.

Significantly the use of multiple GPUs provides additional gains in efficiency as compared to using even a single GPU. Taking the example in which 5 GPUs are used the previously discussed 1 000 000 hash values may be distributed across the 5 GPU memories so that for example each GPU stores 200 000 hash values in memory. Then within each GPU the 200 000 hash values may be divided across for example 1 000 blocks with 200 hash values in each block. It may therefore be appreciated that any single GPU thread may be responsible for searching only 200 hash values and all the threads may be running in parallel. This may be contrasted with the scenario of one process running on a CPU e.g. CPU searching through 1 000 000 hash values.

Further efficiency may be achieved by collecting a batch of hash values e.g. 512 input hash values to search for and sending the entire batch at once to the GPU for searching. In particular preparatory search initialization code may be run on CPU prior to calling kernel . Thus by batching input hash values the initialization code only has to be run once for the batch. In this instance kernel may search for example for all 512 values and return 512 search results. Of course the batch size could be smaller or larger depending on the number of search requests received at a point in time.

Moreover the ability to add additional GPUs to system allows system to scale up gracefully as the space of financial document data i.e. the number of hash values representative of processed financial documents to be searched increases.

In any event if DDD system indicates that the MICR hash value representative of check was found in its set of stored hash values this signifies that a previously processed check may have had an identical MICR line to MICR line of check . Because MICR lines of checks are supposed to be unique this may be suggestive of a potential fraudulent event i.e. that check was deposited twice .

A similar reasoning applies to image hash value. For instance if image hash value generated from check matches a stored image hash value this may indicate that check or at least the same image of check had previously been processed.

As discussed above it is not necessary to generate both an image hash value and a MICR hash value for a given check however generating both may provide an additional level of confidence and further may provide information about the type of error that may have resulted in a duplicated financial document. For example depending on how image hash value is generated from an image of check two images of check scanned at different times may result in two different image hash values because of variations in the scanning process. Moreover an altered image of check may also generate two different hash values. Therefore storing image hash values alone may not provide an adequate degree of confidence. However the MICR hash value of check taken e.g. via OCR from both images may be expected to be the same.

Thus if for a suspected duplicated document the image hash and the MICR hash match the image hash and MICR hash of a previously processed document this may indicate that the same image file representing a financial document or instrument was processed twice. However if for example the MICR hashes match but the image hashes do not this may indicate that two different instances of an image of the check has somehow entered the processing system. As previously discussed a positive search result may be presented to a human operator for review the operator may therefore be provided with information regarding the nature of the match e.g. MICR hashes match image hashes do not etc. and on this basis the operator may be able to conclusively identify the source of the error.

In a second embodiment of the present invention features of a financial document other than image and or MICR line data may be used to generate a hash value. For example one or more features of a financial document such as the name and address of the payor name of payee date courtesy amount may be combined and represented as an array of bytes and that array of bytes may be input into hash function to generate a hash value that is representative of the financial document.

In a third embodiment of the present invention instead of adding a new hash value to a GPU in round robin fashion as described above each GPU . . . may store hash values within a certain range. As such a search for a given hash value may be directed e.g. by scatter gather function to the GPU storing hash values in that range. Thus in this embodiment only the memory blocks on that particular GPU may be searched possibly concurrently by multiple threads for the given hash value there is no need to run a search on each of GPUs . . . . Similarly when a new hash value is to be added to hash table it may be added into the hash table i.e. one or more of which stores hash values in the appropriate range.

A variation of the third embodiment is to further store values within a given GPU memory block by range. Therefore within a given GPU instead of having to search all memory blocks of that GPU only the memory block storing hash values in the appropriate range need be searched. Alternatively each GPU may not store values in any particular range however each memory block of each GPU may.

Another variation of the third embodiment may be to distribute hash values upon inserts across GPUs in another fashion. For example the first byte of the hash value to be inserted may be input into a function which function outputs which of GPUs . . . the hash value will be added to. Conveniently the function may be chosen to achieve a desired distribution pattern of inserts across the GPUs. Moreover conveniently upon receiving a hash value to search for the first byte of that hash value may be input into the same function to determine which GPU the matching hash value would be stored in if a match exists. Thus a search for a matching hash value need only be directed to a particular GPU and not all GPUs.

Conveniently if the memory blocks store hash values within a particular range and batch searching of input hash values is employed then a further batch search could be run on a given memory block i.e. for a given memory block search for all input hash values in the search batch which fall into that range .

In a fourth embodiment of the present invention hash values and ancillary data within a given GPU memory block may be fully sorted. That is instead of sorting on the high part of the hash values alone the low parts of the hash values may be sorted followed by a sort by timestamps. Thus all zero entries i.e. 0 in the high part of the hash value 0 in the low part of the hash value and 0 in the timestamp will be located at the beginning of a memory block. As such empty memory locations into which a new hash value may be added e.g. at S may be quickly and easily identified.

In a fifth embodiment of the present invention images of financial documents may be created e.g. scanned at locations remote to DDD system e.g. at branches of financial institutions and the originals of the financial documents discarded. These images may then be input into DDD system . Thus in this embodiment capture subsystem may be geographically remote from DDD system . Equally financial document archive may be geographically remote from DDD system and capture subsystem .

In a sixth embodiment instead of being geographically remote as in the previously described alternate embodiment capture subsystem and DDD system may be hosted on the same computing device such that MICR reader and scanner may be connected to computing device and capture application may be hosted on computing device .

Of course the above described embodiments are intended to be illustrative only and in no way limiting. The described embodiments of carrying out the invention are susceptible to many modifications of form arrangement of parts details and order of operation. The invention rather is intended to encompass all such modification within its scope as defined by the claims.

