---

title: Method and apparatus for maintaining performance monitoring structures in a page table for use in monitoring performance of a computer program
abstract: A method and apparatus in a data processing system for measuring events associated with the execution of instructions are provided. Instructions are received at a processor in the data processing system. If a selected indicator is associated with the instruction, counting of each event associated with the execution of the instruction is enabled. In some embodiments, the performance indicators, counters, thresholds, and other performance monitoring structures may be stored in a page table that is used to translate virtual addresses into physical storage addresses. A standard page table is augmented with additional fields for storing the performance monitoring structures. These structures may be set by the performance monitoring application and may be queried and modified as events occur that require access to physical storage.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08191049&OS=08191049&RS=08191049
owner: International Business Machines Corporation
number: 08191049
owner_city: Armonk
owner_country: US
publication_date: 20080403
---
This application is a continuation of application Ser. No. 10 757 250 filed Jan. 14 2004 now U.S. Pat. No. 7 526 757 issued Apr. 28 2009.

The present invention is related to the following applications entitled Method and Apparatus for Counting Instruction Execution and Data Accesses Ser. No. 10 675 777 filed on Sep. 30 2003 Method and Apparatus for Selectively Counting Instructions and Data Accesses Ser. No. 10 674 604 filed on Sep. 30 2003 Method and Apparatus for Generating Interrupts Upon Execution of Marked Instructions and Upon Access to Marked Memory Locations Ser. No. 10 675 831 filed on Sep. 30 2003 Method and Apparatus for Counting Data Accesses and Instruction Executions that Exceed a Threshold Ser. No. 10 675 778 filed on Sep. 30 2003 Method and Apparatus for Counting Execution of Specific Instructions and Accesses to Specific Data Locations Ser. No. 10 675 776 filed on Sep. 30 2003 Method and Apparatus for Debug Support for Individual Instructions and Memory Locations Ser. No. 10 675 751 filed on Sep. 30 2003 Method and Apparatus to Autonomically Select Instructions for Selective Counting Ser. No. 10 675 721 filed on Sep. 30 2003 Method and Apparatus to Autonomically Count Instruction Execution for Applications Ser. No. 10 674 642 filed on Sep. 30 2003 Method and Apparatus to Autonomically Take an Exception on Specified Instructions Ser. No. 10 674 606 filed on Sep. 30 2003 Method and Apparatus to Autonomically Profile Applications Ser. No. 10 675 783 filed on Sep. 30 2003 Method and Apparatus for Counting Instruction and Memory Location Ranges Ser. No. 10 675 872 filed on Sep. 30 2003 Method and Apparatus for Counting Instruction Execution and Data Accesses to Identify Hot Spots Ser. No. 10 757 248 filed on Jan. 14 2004 Method and Apparatus for Autonomically Initiating Measurement of Secondary Metrics Based on Hardware Counter Values for Primary Metrics Ser. No. 10 757 269 filed on Jan. 14 2004 Method and Apparatus for Generating Interrupts Based on Arithmetic Combinations of Performance Counter Values Ser. No. 10 757 212 filed on Jan. 14 2004 and Method and Apparatus for Optimizing Code Execution Using Annotated Trace Information Having Performance Indicator and Counter Information Ser. No. 10 757 197 filed on Jan. 14 2004. All of the above related applications are assigned to the same assignee and incorporated herein by reference.

The present invention relates generally to an improved data processing system. In particular the present invention provides a method and apparatus for obtaining performance data in a data processing system. Still more particularly the present invention provides a method and apparatus for hardware assistance to software tools in obtaining performance data in a data processing system.

In analyzing and enhancing performance of a data processing system and the applications executing within the data processing system it is helpful to know which software modules within a data processing system are using system resources. Effective management and enhancement of data processing systems requires knowing how and when various system resources are being used. Performance tools are used to monitor and examine a data processing system to determine resource consumption as various software applications are executing within the data processing system. For example a performance tool may identify the most frequently executed modules and instructions in a data processing system or may identify those modules which allocate the largest amount of memory or perform the most I O requests. Hardware performance tools may be built into the system or added at a later point in time.

One known software performance tool is a trace tool. A trace tool may use more than one technique to provide trace information that indicates execution flows for an executing program. One technique keeps track of particular sequences of instructions by logging certain events as they occur so called event based profiling technique. For example a trace tool may log every entry into and every exit from a module subroutine method function or system component. Alternately a trace tool may log the requester and the amounts of memory allocated for each memory allocation request. Typically a time stamped record is produced for each such event. Corresponding pairs of records similar to entry exit records also are used to trace execution of arbitrary code segments starting and completing I O or data transmission and for many other events of interest.

In order to improve performance of code generated by various families of computers it is often necessary to determine where time is being spent by the processor in executing code such efforts being commonly known in the computer processing arts as locating hot spots. Ideally one would like to isolate such hot spots at the instruction and or source line of code level in order to focus attention on areas which might benefit most from improvements to the code.

Another trace technique involves periodically sampling a program s execution flows to identify certain locations in the program in which the program appears to spend large amounts of time. This technique is based on the idea of periodically interrupting the application or data processing system execution at regular intervals so called sample based profiling. At each interruption information is recorded for a predetermined length of time or for a predetermined number of events of interest. For example the program counter of the currently executing thread which is an executable portion of the larger program being profiled may be recorded at each interval. These values may be resolved against a load map and symbol table information for the data processing system at post processing time and a profile of where the time is being spent may be obtained from this analysis.

Creating tools such as these to find answers related to specific situations or problems can take much effort and can be very difficult to calibrate as the software tools themselves affect the system under test. The present invention recognizes that hardware assistance for tool development and problem analysis can significantly ease the amount of effort needed to develop software performance tools. Further with the increasing density of processors hardware assistance can be included to provide additional debug and analysis features.

Therefore it would be advantageous to have an improved method apparatus and computer instructions for providing hardware assistance for performance tools to analyzing the performance of data processing systems.

The present invention provides a method apparatus and computer instructions in a data processing system for processing instructions. Instructions are received at a processor in the data processing system. If a selected indicator is associated with the instruction counting of each event associated with the execution of the instruction is enabled.

In some embodiments of the present invention the counts associated with the indicators may be checked to determine if the counts are above a threshold. If a count is above a threshold the associated instruction data address may be identified as a hot spot and optimization of the execution of the code may be performed based on the identification of the hot spot.

In further embodiments of the present invention arithmetic combinations of counter values generated based on the encountering of performance indicators may be generated and compared to threshold values to determine whether to generate interrupts to the monitoring application. In such embodiments the microcode of the processor is programmed to check the counter values of counters specified by the monitoring application combine the counter values in a manner specified by the monitoring application and then compare the combined value to a threshold value supplied by the performance monitoring application. In this way more complex conditioning of interrupts may be provided within the hardware of the processor.

In other embodiments of the present invention functionality is provided in the performance monitoring application for initiating the measurement of secondary metrics with regard to identified instructions data addresses ranges of identified instructions or ranges of identified data addresses based on counter values for primary metrics. Thus for example when a primary metric counter or a combination of primary metric counters meets or exceeds a predetermined threshold value an interrupt may be generated. In response to receiving the interrupt counters associated with the measuring of secondary metrics of a range of instructions data addresses may be initiated. In this way areas of particular interest may first be identified using the primary metric performance counters with more detailed information being obtained through the use of secondary metric performance counters directed to measuring metrics associated with the particular area of interest.

In additional embodiments of the present invention the performance indicators and counter values may be used as a mechanism for identifying cache hits and cache misses. With such an embodiment performance indicators are associated with instructions for selected routines of interest in the computer program. Performance counters are incremented each time the instructions of the routines are executed and each time the instructions must be reloaded into the cache. From the values of these counters the cache hit miss ratio may be determined.

When the cache hit miss ratio becomes less than a predetermined threshold i.e. there is a greater number of cache misses than cache hits the present invention may determine that a problem condition has occurred. One contributor to such a small cache hit miss ratio may be the chase tail condition. A chase tail condition occurs when a block of instructions data must be loaded into cache but there is not enough available room in the cache to store the entire block of instructions data. In such a case the instructions data are written to the available space in the cache and any overflow is written over the least recently used portion of the cache. This may cause cache misses on the instructions data overwritten thereby increasing the number of cache misses.

When a problem condition is detected due to the values of the performance counters indicating a low cache hit miss ratio the present invention may set a mode bit in a mode register indicating that the processor should implement a chase tail operation within the microcode of the processor. With this chase tail operation upon processing a reload operation for reloading a block of instructions data into the cache the processor checks to determine if there is available space in the cache for the entire block of instructions data. If there is available space in the cache then the block of instructions data are stored in the cache in a normal manner. However if there is not sufficient space in the cache to store the block of instructions data that is to be reloaded then the block of instructions data or at least the overflow portion of the block of instructions data is loaded into a reserved portion of cache which is reloaded using a different algorithm than that of the instructions data into a non reserved area of the cache and overwriting instructions data already present in the cache.

In addition a performance indicator may be associated with the block of instructions indicating that when an instruction in this block of instructions is again executed or when a data address in the block of data addresses is again accessed the processor should look for the instruction data in the reserved area of the cache.

Thus by invoking the chase tail operation of the present embodiment when the cache hit miss ratio is below a predetermined threshold the present invention avoids the chase the tail situation by causing any reloads of instructions data that cannot be accommodated by the available space in the cache to be stored in a reserved area of the cache rather than overwriting existing cache entries in the non reserved area of the cache. In this way the domino affect with regard to overwriting and reloads caused by overwriting the least recently used entries in the cache may be avoided.

In even further embodiments of the present invention the performance indicators of the present invention may be utilized to obtain information regarding the nature of the cache hits and reloads of cache lines within the instruction or data cache. These embodiments of the present invention for example may be used to determine whether processors of a multiprocessor system such as a symmetric multiprocessor SMP system are truly sharing a cache line or if there is false sharing of a cache line. This determination may then be used as a means for determining how to better store the instructions data of the cache line to prevent false sharing of the cache line.

The determination of true or false cache line sharing may be beneficial in determining the manner by which data and instructions are stored in a cache. That is if it is determined that cache lines are being falsely shared and thus cache line reloads are often being performed due to writes to areas of the cache line by a first processor that are not being accessed by the second processor then appropriate measures may be taken to minimize the amount of false cache line sharing.

For example in a further embodiment of the present invention when it is determined that a cache line is being falsely shared using the mechanisms described above the data or instructions being accessed may be written to a separate memory area dedicated to false cache line sharing data.

The code may then be modified by inserting a pointer to this new area of memory. Thus when the code again attempts to access the original area of the memory the access is redirected to the new memory area rather than to the previous area of the memory that was subject to false sharing. In this way reloads of the cache line may be avoided.

In a further embodiment of the present invention a compiler may obtain this performance profile data along with the instructions data of the computer program and use this information to optimize the manner by which the computer program is executed instructions data are stored and the like. That is the compiler may take extra time during initial application load to optimize the application and instruction data storage so that the runtime component of the application is optimized.

The manner by which the compiler optimizes the runtime aspects of the computer program may vary depending on the particular performance profile data obtained which is annotated by the output obtained from the use of performance indicators counters flags and the like previously described. The optimizations may be to optimize the instruction paths optimize the time spent in initial application load the manner by which the cache and memory is utilized and the like.

In yet other embodiments of the present invention the performance indicators counters thresholds and other performance monitoring structures may be stored in a page table that is used to translate virtual addresses into physical storage addresses. A standard page table is augmented with additional fields for storing the performance monitoring structures. These structures may be set by the performance monitoring application and may be queried and modified as events occur that require access to memory.

Logically the page table must be consulted for every instruction fetch and data access to translate the program address or virtual address into a physical address. To improve performance recently used page table entries are kept in a cache a Translation Look aside Buffer or an Effective to Real Address look aside buffer providing fast access to the information needed to translate a program address to a physical address. The performance tracking indicators contained in a page table entry can also be cached in the same look aside buffers.

During the process of translating a program address to a physical address it can be determined from the performance monitoring structures whether the instruction data has an associated performance indicator counter values threshold and the like. The same functionality provided by the performance indicators and hardware counters described in other embodiments of the present invention may be provided via the augmented page table according to this embodiment of the present invention.

These and other features and advantages of the present invention will be described in or will become apparent to those of ordinary skill in the art in view of the following detailed description of the preferred embodiments.

With reference now to a block diagram of a data processing system is shown in which the present invention may be implemented. Client is an example of a computer in which code or instructions implementing the processes of the present invention may be located. Client employs a peripheral component interconnect PCI local bus architecture. Although the depicted example employs a PCT bus other bus architectures such as Accelerated Graphics Port AGP and Industry Standard Architecture ISA may be used. Processor and main memory are connected to PCI local bus through PCI bridge . PCI bridge also may include an integrated memory controller and cache memory for processor . Additional connections to PCI local bus may be made through direct component interconnection or through add in boards.

In the depicted example local area network LAN adapter small computer system interface SCSI host bus adapter and expansion bus interface are connected to PCI local bus by direct component connection. In contrast audio adapter graphics adapter and audio video adapter are connected to PCI local bus by add in boards inserted into expansion slots. Expansion bus interface provides a connection for a keyboard and mouse adapter modem and additional memory . SCSI host bus adapter provides a connection for hard disk drive tape drive and CD ROM drive . Typical PCI local bus implementations will support three or four PCI expansion slots or add in connectors.

An operating system runs on processor and is used to coordinate and provide control of various components within data processing system in . The operating system may be a commercially available operating system such as Windows XP which is available from Microsoft Corporation. An object oriented programming system such as Java may run in conjunction with the operating system and provides calls to the operating system from Java programs or applications executing on client . Java is a trademark of Sun Microsystems Inc. Instructions for the operating system the object oriented programming system and applications or programs are located on storage devices such as hard disk drive and may be loaded into main memory for execution by processor .

Those of ordinary skill in the art will appreciate that the hardware in may vary depending on the implementation. Other internal hardware or peripheral devices such as flash read only memory ROM equivalent nonvolatile memory or optical disk drives and the like may be used in addition to or in place of the hardware depicted in . Also the processes of the present invention may be applied to a multiprocessor data processing system.

For example client it optionally configured as a network computer may not include SCSI host bus adapter hard disk drive tape drive and CD ROM . In that case the computer to be properly called a client computer includes some type of network communication interface such as LAN adapter modem or the like. As another example client may be a stand alone system configured to be bootable without relying on some type of network communication interface whether or not client comprises some type of network communication interface. As a further example client may be a personal digital assistant PDA which is configured with ROM and or flash ROM to provide non volatile memory for storing operating system files and or user generated data. The depicted example in and above described examples are not meant to imply architectural limitations.

The processes of the present invention are performed by processor using computer implemented instructions which may be located in a memory such as for example main memory memory or in one or more peripheral devices .

Turning next to a block diagram of a processor system for processing information is depicted in accordance with a preferred embodiment of the present invention. Processor may be implemented as processor in .

In a preferred embodiment processor is a single integrated circuit superscalar microprocessor. Accordingly as discussed further herein below processor includes various units registers buffers memories and other sections all of which are formed by integrated circuitry. Also in the preferred embodiment processor operates according to reduced instruction set computer RISC techniques. As shown in system bus is connected to a bus interface unit BIU of processor . BIU controls the transfer of information between processor and system bus .

BIU is connected to an instruction cache and to data cache of processor . Instruction cache outputs instructions to sequencer unit . In response to such instructions from instruction cache sequencer unit selectively outputs instructions to other execution circuitry of processor .

In addition to sequencer unit in the preferred embodiment the execution circuitry of processor includes multiple execution units namely a branch unit a fixed point unit A FXUA a fixed point unit B FXUB a complex fixed point unit CFXU a load store unit LSU and a floating point unit FPU . FXUA FXUB CFXU and LSU input their source operand information from general purpose architectural registers GPRs and fixed point rename buffers . Moreover FXUA and FXUB input a carry bit from a carry bit CA register . FXUA FXUB CFXU and LSU output results destination operand information of their operations for storage at selected entries in fixed point rename buffers . Also CFXU inputs and outputs source operand information and destination operand information to and from special purpose register processing unit SPR unit .

FPU inputs its source operand information from floating point architectural registers FPRs and floating point rename buffers . FPU outputs results destination operand information of its operation for storage at selected entries in floating point rename buffers .

In response to a Load instruction LSU inputs information from data cache and copies such information to selected ones of rename buffers and . If such information is not stored in data cache then data cache inputs through BIU and system bus such information from a system memory connected to system bus . Moreover data cache is able to output through BIU and system bus information from data cache to system memory connected to system bus . In response to a Store instruction LSU inputs information from a selected one of GPRs and FPRs and copies such information to data cache .

Sequencer unit inputs and outputs information to and from GPRs and FPRs . From sequencer unit branch unit inputs instructions and signals indicating a present state of processor . In response to such instructions and signals branch unit outputs to sequencer unit signals indicating suitable memory addresses storing a sequence of instructions for execution by processor . In response to such signals from branch unit sequencer unit inputs the indicated sequence of instructions from instruction cache . If one or more of the sequence of instructions is not stored in instruction cache then instruction cache inputs through BIU and system bus such instructions from system memory connected to system bus .

In response to the instructions input from instruction cache sequencer unit selectively dispatches the instructions to selected ones of execution units and . Each execution unit executes one or more instructions of a particular class of instructions. For example FXUA and FXUB execute a first class of fixed point mathematical operations on source operands such as addition subtraction ANDing ORing and XORing. CFXU executes a second class of fixed point operations on source operands such as fixed point multiplication and division. FPU executes floating point operations on source operands such as floating point multiplication and division.

As information is stored at a selected one of rename buffers such information is associated with a storage location e.g. one of GPRs or CA register as specified by the instruction for which the selected rename buffer is allocated. Information stored at a selected one of rename buffers is copied to its associated one of GPRs or CA register in response to signals from sequencer unit . Sequencer unit directs such copying of information stored at a selected one of rename buffers in response to completing the instruction that generated the information. Such copying is called writeback. 

As information is stored at a selected one of rename buffers such information is associated with one of FPRs . Information stored at a selected one of rename buffers is copied to its associated one of FPRs in response to signals from sequencer unit . Sequencer unit directs such copying of information stored at a selected one of rename buffers in response to completing the instruction that generated the information.

Processor achieves high performance by processing multiple instructions simultaneously at various ones of execution units and . Accordingly each instruction is processed as a sequence of stages each being executable in parallel with stages of other instructions. Such a technique is called pipelining. In a significant aspect of the illustrative embodiment an instruction is normally processed as six stages namely fetch decode dispatch execute completion and writeback.

In the fetch stage sequencer unit selectively inputs from instruction cache one or more instructions from one or more memory addresses storing the sequence of instructions discussed further hereinabove in connection with branch unit and sequencer unit .

In the dispatch stage sequencer unit selectively dispatches up to four decoded instructions to selected in response to the decoding in the decode stage ones of execution units and after reserving rename buffer entries for the dispatched instructions results destination operand information . In the dispatch stage operand information is supplied to the selected execution units for dispatched instructions. Processor dispatches instructions in order of their programmed sequence.

In the execute stage execution units execute their dispatched instructions and output results destination operand information of their operations for storage at selected entries in rename buffers and rename buffers as discussed further hereinabove. In this manner processor is able to execute instructions out of order relative to their programmed sequence.

In the completion stage sequencer unit indicates an instruction is complete. Processor completes instructions in order of their programmed sequence.

In the writeback stage sequencer directs the copying of information from rename buffers and to GPRs and FPRs respectively. Sequencer unit directs such copying of information stored at a selected rename buffer. Likewise in the writeback stage of a particular instruction processor updates its architectural states in response to the particular instruction. Processor processes the respective writeback stages of instructions in order of their programmed sequence. Processor advantageously merges an instruction s completion stage and writeback stage in specified situations.

In the illustrative embodiment each instruction requires one machine cycle to complete each of the stages of instruction processing. Nevertheless some instructions e.g. complex fixed point instructions executed by CFXU may require more than one cycle. Accordingly a variable delay may occur between a particular instruction s execution and completion stages in response to the variation in time required for completion of preceding instructions.

A completion buffer is provided within sequencer to track the completion of the multiple instructions which are being executed within the execution units. Upon an indication that an instruction or a group of instructions have been completed successfully in an application specified sequential order completion buffer may be utilized to initiate the transfer of the results of those completed instructions to the associated general purpose registers.

In addition processor also includes processor monitoring unit which is connected to instruction cache as well as other units in processor . Operation of processor can be monitored utilizing performance monitor unit which in this illustrative embodiment is a software accessible mechanism capable of providing detailed information descriptive of the utilization of instruction execution resources and storage control. Although not illustrated in performance monitor unit is coupled to each functional unit of processor to permit the monitoring of all aspects of the operation of processor including for example reconstructing the relationship between events identifying false triggering identifying performance bottlenecks monitoring pipeline stalls monitoring idle processor cycles determining dispatch efficiency determining branch efficiency determining the performance penalty of misaligned data accesses identifying the frequency of execution of serialization instructions identifying inhibited interrupts and determining performance efficiency.

Performance monitor unit includes an implementation dependent number e.g. 2 8 of counters labeled PMC1 and PMC2 which are utilized to count occurrences of selected events. Performance monitor unit further includes at least one monitor mode control register MMCR . In this example two control registers MMCRs and are present that specify the function of counters . Counters and MMCRs are preferably implemented as SPRs that are accessible for read or write via MFSPR move from SPR and MTSPR move to SPR instructions executable by CFXU . However in one alternative embodiment counters and MMCRs may be implemented simply as addresses in I O space. In another alternative embodiment the control registers and counters may be accessed indirectly via an index register. This embodiment is implemented in the IA 64 architecture in processors from Intel Corporation

Additionally processor also includes interrupt unit which is connected to instruction cache . Additionally although not shown in interrupt unit is connected to other functional units within processor . Interrupt unit may receive signals from other functional units and initiate an action such as starting an error handling or trap process. In these examples interrupt unit is employed to generate interrupts and exceptions that may occur during execution of a program.

The present invention provides an ability to monitor the execution of specific instructions as well as the access of specific memory locations during the execution of a program. Specifically a spare field may be used to hold an indicator that identifies the instruction or memory location as one that is to be monitored by a performance monitor unit or by some other unit in a processor. Alternatively the indicator may be stored in another location in association with the instruction or memory location. In the case in which the indicator is placed in the instruction a spare field is typically used but in some cases the instruction may be extended to include the space needed for the indicator. With this case the architecture of the processor may require changes. For example a 64 bit architecture may be changed to a 65 bit architecture to accommodate the indicator. With respect to accesses of data an indicator may be associated with the data or memory locations in which the data is located.

Turning now to a diagram illustrating components used in processing instructions associated with indicators is depicted in accordance with a preferred embodiment of the present invention. Instruction cache receives bundles . Instruction cache is an example of instruction cache in . A bundle is a grouping of instructions. This type of grouping of instructions is typically found in an IA 64 processor which is available from Intel Corporation. Instruction cache processes instructions for execution.

As part of this processing of instructions instruction cache determines which instructions are associated with indicators. These indicators are also referred to as performance indicators in these examples. Signals have been associated with performance indicators. As a result signals for the instructions are sent to performance monitor unit . Performance monitor unit is an example of performance monitor unit in .

When instruction cache determines that an instruction associated with an indicator is present a signal is sent to indicate that a marked instruction is being executed. In these examples a marked instruction is an instruction associated with a performance indicator. Alternatively a performance indicator may indicate that all items or instructions in a bundle are marked to be counted. Additionally signals for these instructions are sent by instruction cache to the appropriate functional unit. Depending on the particular implementation a functional unit other than performance monitor unit may count execution of instructions. In the case that the performance indicators are in the instructions or in the bundles the cache unit instruction cache detects the indicators and sends signals to performance monitor unit .

When signals for these instructions are received by performance monitor unit performance monitor unit counts events associated with execution of instructions . As illustrated performance monitor unit is programmed only to count events for instructions associated with performance indicators. In other words an indicator associated with an instruction or memory location is used to enable counting of events associated with the instruction or memory location by performance monitor unit . If an instruction is received by instruction cache without a performance indicator then events associated with that instruction are not counted. In summary the performance indicators enable the counting on a per instruction or per memory location basis in a processor.

Performance monitor unit counts events for instructions associated with performance indicators if performance monitor unit is set in a mode to count metrics enabled for these types of marked instructions. In some cases performance monitor unit may be set to perform some other type of counting such as counting execution of all instructions which is a currently available function.

With respect to the accessing of data in memory locations the data and indicators are processed by a data cache such as data cache in rather then by an instruction cache. The data cache sends signals indicating that marked memory locations are being accessed to performance monitor unit . Marked memory locations are similar to marked instructions. These types of memory locations are ones associated with a performance indicator.

Turning next to a diagram illustrating one mechanism for associating a performance indicator with an instruction or memory location is depicted in accordance with a preferred embodiment of the present invention. Processor receives instructions from cache . In this example the indicators are not stored with the instructions or in the memory locations in which data is found. Instead the indicators are stored in a separate area of storage performance instrumentation shadow cache . The storage may be any storage device such as for example a system memory a flash memory a cache or a disk.

When processor receives an instruction from cache processor checks performance instrumentation shadow cache to see whether a performance indicator is associated with the instruction. A similar check is made with respect to accesses of memory locations containing data. In one embodiment a full shadow word is provided for each corresponding word that does not affect the actual data segments. In other words processor allows for the architecture or configuration of cache to remain unchanged. In these examples the mapping described is word for word. However some other type of mapping may be used such as a shadow bit per data word in which a bit in performance instrumentation shadow cache corresponds to one word of data.

With respect to this type of architecture the compilers using this feature create the debug information in a separate work area from the data area themselves in a manner similar to debug symbols. When a module is loaded the extra information performance indicators is prepared by the loader so that it will be available to incorporate into performance instrumentation shadow cache when instructions are loaded into cache . These cache areas may be intermingled and either marked as such or understood by the mode of operation. Processor uses the performance indicators to determine how the related data accesses and instruction executions are to be counted or made to take exceptions. In these examples the process is programmed by a debugger or a performance analysis program to know whether to use the shadow information while it is executing instructions.

Turning next to a diagram illustrating a bundle is depicted in accordance with a preferred embodiment of the present invention. Bundle contains instruction slot instruction instruction slot and template . As illustrated bundle contains 128 bits. Each instructions slot contains 41 bits and template contains 5 bits. Template is used to identify stops within the current bundle and to map instructions within the slots to different types of execution units.

Spare bits within bundle are used to hold indicators of the present invention. For example indicators and are located within instruction slots and respectively. These indicators may take various forms and may take various sizes depending on the particular implementation. Indicators may use a single bit or may use multiple bits. A single bit may be used to indicate that events are to be counted in response to execution of that instruction. Multiple bits may be used to identify a threshold such as a number of processor or clock cycles for instruction execution that may pass before events should be counted. Further these bits may even be used as a counter for a particular instruction. A similar use of fields may be used for indicators that mark data or memory locations.

Alternatively template may be used to contain a bundle of related indicators so that one bit is used to identify all the instructions in a bundle. Also the bundle itself could be extended to be 256 bits or some other number of bits to contain the extra information for the performance indicators.

Turning next to diagrams of a subroutine containing performance indicators and data containing performance indicators are depicted in accordance with a preferred embodiment of the present invention. In this example subroutine in includes a number of instructions in which instructions and are associated with performance indicators. These instructions also are referred to as marked instructions. When these instructions are executed events associated with those instructions are counted to obtain data for software tools to analyze the performance of a data processing system executing a subroutine .

Data or memory locations containing data may be marked with indicators in a similar manner. These indicators are used in counting accesses to the data or memory locations in these examples. In data includes data associated with performance indicators. Data and data are sections of data that are associated with performance indicators. These sections of data which are associated with performance indicators also are referred to as marked data.

Turning now to a flowchart of a process for processing instructions containing performance indicators is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in an instruction cache such as instruction cache in .

The process begins by receiving a bundle step . In these examples each bundle has a format similar to bundle in . An instruction in the bundle is identified step . A determination is made as to whether a performance indicator associated with the instruction is present step . This determination may be made by examining an appropriate field in the instruction or bundle. Alternatively a performance instrumentation shadow cache such as performance instrumentation shadow cache in may be checked to see if a performance indicator is associated with the instruction.

If a performance indicator is present a signal is sent to a performance monitor unit step . Upon receiving this signal the performance monitor unit will count events associated with the execution of the instruction. Additionally the instruction is processed step . Processing of the instruction includes for example sending the instruction to the appropriate functional unit for execution.

Thereafter a determination is made as to whether additional unprocessed instructions are present in the bundle step . If additional unprocessed instructions are present in the bundle the process returns to step as described above. Otherwise the process terminates. Turning back to step if the performance indicator is not present the process proceeds directly to step .

Turning now to a flowchart of a process for selectively sending signals to an interrupt unit is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in an instruction cache such as instruction cache in . This process is employed in cases in which monitoring events using a performance monitor unit may miss certain events. For example a performance monitor unit counts events. When a cache miss occurs a signal is sent to the performance monitor unit. When the meta data for a corresponding cache line is loaded into the cache the appropriate signal or signals also are raised. If the metadata indicates that an exception is to be raised then a signal is sent to the interrupt unit in which the signal indicates that an exception is to be raised.

The process begins by receiving a bundle step . An instruction in the bundle is identified step . A determination is made as to whether a performance indicator associated with the instruction is present step . The signal sent to the interrupt unit to indicate an exception is to be raised is different from the signal sent to the performance monitor unit. For example an instruction may be associated with a specific performance indicator having a first value that causes a signal to be sent to the interrupt unit. A second value for a performance indicator may be used to send a different signal to the performance monitor unit. If a performance indicator having the first value is present the signal is sent to an interrupt unit step . Upon receiving this signal the interrupt unit initiates appropriate call flow support to process this interrupt. The call flow support may for example record cache misses that may be missed by a functional unit trying to access instructions or data in a cache.

Additionally the instruction is processed step . Processing of the instruction includes for example sending the instruction to the appropriate functional unit for execution.

Thereafter a determination is made as to whether additional unprocessed instructions are present in the bundle step . If additional unprocessed instructions are present in the bundle the process returns to step as described above. Otherwise the process terminates. Turning back to step if the performance indicator is not present the process proceeds directly to step .

With reference now to a flowchart of a process for generating an interrupt in response to an access of a memory location associated with a performance indicator is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in a data cache such as data cache in .

The process begins by identifying a request to access a memory location step . In response to identifying this request a determination is made as to whether a performance indicator is associated with the memory location step . If a performance indicator is associated with the memory location an interrupt is generated by sending a signal to the interrupt unit step . Thereafter the access to the memory location is processed step with the process terminating thereafter.

In a flowchart of a process for counting events is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in a performance monitor unit such as performance monitor unit in .

The process begins by receiving a signal from an instruction cache indicating that an instruction with a performance indicator is being processed step . Next events associated with the instruction being processed are counted step with the process terminating thereafter. The counting of events may be stored in a counter such as counter in .

With reference next to a flowchart of a process for selective counting of instructions is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in an instruction cache such as instruction cache in .

The process begins by determining whether an instruction associated with a performance indicator has been received step . In this example the indicator causes counting of events for this instruction and all subsequent instructions executed by the processor. Alternatively the indicator could be an instruction itself which indicates the new mode of counting is to be started. If an instruction with an indicator has been received a flag is set to start counting events for instructions step . This flag indicates that counting events for instructions should start.

Next a determination is made as to whether an instruction with an indicator has been received step . Alternatively the indicator could be an instruction itself which indicates the new mode of counting is to be stopped. If an instruction with an indicator is received the flag is unset to stop counting the events step with the process terminating thereafter.

The indicator in step and step may be the same indicator in which the indicator toggles the setting and unsetting of the flag. In another implementation two different indicators may be used in which a first indicator only sets the flag. A second indicator is used to unset the flag. Communication between a cache unit such as an instruction cache or a data cache and the performance monitor unit to indicate a mode of counting may be implemented simply with a high signal when counting is to occur and a low signal when counting is no longer enabled.

With reference next to a flowchart of a process for selective counting of instructions is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in an instruction cache such as instruction cache in .

The process begins by checking a flag step . A determination is made as to whether the flag is set step . If the flag is set a signal is sent to the performance monitor unit to enable this unit to count events step with the process terminating thereafter. Otherwise a signal is sent to the performance monitor unit to disable the counting of events step with the process terminating thereafter.

The processes illustrated in count events for all instructions after an instruction is associated with a performance indicator. In this manner fewer bits may be used to toggle counting of events. Further with the counting of all instructions events associated with calls to external subroutines may be counted.

Turning now to a flowchart of a process for identifying instructions exceeding a threshold is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in an instruction cache such as instruction cache in .

The process begins by receiving an instruction associated with a performance indicator step . A threshold is identified for the instruction step . In these examples the threshold relates to a number of processor or clock cycles needed to complete an instruction. If the cache latency or amount of time needed to access the cache exceeds the threshold value that event is counted. The threshold value is set within the indicator in these examples.

For example three bits may be used to set eight different values for the threshold. For example xx1 10 cycles x1x 50 cycles and 1xx 100 cycles. Some combination of these three bits may be used to set values for the threshold. More or fewer bits may be used and different values may be assigned to the bits depending on the specific implementation. The meaning of the bits may also be controlled through an interface such as a set of registers that may be used to set the meaning of each of the bits. These registers are ones that are added to the processor architecture for this specific purpose.

Cycles for executing the instruction are monitored step . A determination is made as to whether the threshold has been exceeded for this instruction step . If the threshold has been exceeded then a selected action is performed step . This selected action may take different forms depending on the particular implementation. For example a counter may be incremented each time the threshold is exceeded. Alternatively an interrupt may be generated. The interrupt may pass control to another process to gather data. For example this data may include a call stack and obtaining information about the call stack. A stack is a region of reserved memory in which a program or programs store status data such as procedure and function call addresses passed parameters performance monitor counter values and sometimes local variables.

A determination is made as to whether monitoring is to end step . Step may be implemented one instruction at a time. When an instruction is executed or the threshold is exceeded a signal is sent. In this example execution of a single instruction results in one signal being sent. In the case in which multiple instructions may be executed at the same time multiple signals may be needed to indicate the execution of each instruction. In some embodiments a sampling approach may be supported where the threshold is only supported for one instruction at a time. This may be done by only supporting thresholds for those instructions that are in a particular position in the processor s instruction queue. In other embodiments one signal may be sent if at least one of the marked instructions exceeds the threshold. For each instruction in which a threshold is exceeded a separate signal is raised or generated for that instruction.

If the monitoring is to end the collected information is sent to a monitoring program step with the process terminating thereafter. Otherwise the process returns to step as described above. In step if the threshold is not exceeded for the instruction the process proceeds directly to step .

A similar process may be implemented in a data cache such as data cache in to monitor accesses to memory locations. The process illustrated in may be adapted to identify the cycles needed to access data in a memory location. As with the execution of instructions counting occurs or an interrupt is generated when the amount of time needed to access the data in a memory location exceeds a specified threshold.

As with the other examples these indicators may be included as part of the instruction or with the data in a memory location. Alternatively these indicators may be found in a performance instrumentation shadow cache or memory in association with the instruction or data.

With reference to a flowchart of a process for monitoring accesses to a memory location is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in a data cache such as data cache in . This process is used to count accesses to data in a memory location.

The process begins by receiving data associated with a performance indicator step . A determination is made as to whether a memory location for the data has been accessed step . If the memory location has been accessed then a counter is incremented step . A determination is made as to whether monitoring is to end step . If monitoring of the memory location is to end the process terminates. Otherwise the process returns to step . In step if the memory location is not accessed then the process proceeds to step .

Turning to a block diagram illustrating components used for generating metadata such as performance indicators is depicted in accordance with a preferred embodiment of the present invention. The compiler supports directives embedded in the source that indicate the metadata to be generated. Compiler may generate instructions for execution and metadata for monitoring. As instruction or data cache pages are loaded into memory the operating system program loader linker and or the performance monitoring program reads the metadata generated by compiler and loads the metadata into memory such as performance monitor section in these examples. The section itself is marked as metadata . The processor may accept metadata in the format of the compiler generated section data in performance monitor section and populate processor s internal performance instrumentation shadow cache with the data. A block oriented approach is described with reference to below.

In one embodiment the format simply has a performance instrumentation shadow cache entry for each of its block or sector references and moves metadata to its corresponding shadow entry or entries. Instead of having a performance instrumentation shadow cache the internal format of the cache itself may be modified to contain metadata . In embodiments where the instruction stream itself is modified to contain the metadata then either the loader updates the instruction stream to contain the appropriate indicators and work areas or compiler has generated the code to contain metadata . In either case after the code is loaded the processor receives the metadata

In addition metadata may be placed into performance instrumentation shadow memory in association with instructions . Compiler produces information in a table or debug data section. The performance monitoring program loads this information into shadow data areas in performance instrumentation shadow memory . Alternatively the debug areas may be automatically populated by the operating system and the processor working together.

Instructions may then be executed by processor . Compiler may set a register such as mode register in processor . When this register is set processor looks at metadata in performance instrumentation shadow memory when executing instructions to determine whether performance indicators in metadata are associated with instructions that are being executed in instructions . These performance indicators are handled using processes such as those described above with reference to . If mode register is not set then metadata is ignored when instructions are executed.

A similar process may be performed with respect to data in memory location . Depending on the particular implementation metadata may be placed within the instruction or within the data rather than in performance instrumentation shadow memory . However by placing metadata in performance instrumentation shadow memory the generation of metadata may be performed dynamically when metadata is placed in performance instrumentation shadow memory .

This feature allows for selection and monitoring of instructions to occur without having to modify the program. In other words compiler may generate metadata after instructions have been compiled for execution by processor . Setting mode register causes processor to look for metadata in performance instrumentation shadow memory without having to modify instructions . In these examples metadata take the form of performance indicators that tell processor how to handle the execution of instructions and or data accesses to memory location .

Turning next to a diagram illustrating metadata is depicted in accordance with a preferred embodiment of the present invention. Metadata is an example of metadata in . This metadata is generated by a compiler such as compiler .

In this example metadata includes 5 entries entry and as indicated by line in metadata . Each of these entries includes an offset a length and a flag for describing the instrumentation of code in this example.

Entry has an offset of 0 with an entry length of 120 bytes. Flag indicates that all instructions within the range indicated by entry length need to be counted. In these examples each instruction has a length of 4 bytes. Entry has an entry length of 4 bytes which corresponds to an instruction. Flag indicates that an exception should be generated upon execution of this instruction.

In entry an instruction beginning at an offset of 160 bytes is associated with flag . This flag indicates that the instruction should be counted if the threshold 100 cycles is exceeded.

Flag in entry indicates that tracing should start at the instruction having an offset of 256 bytes. Tracing stops as indicated by flag in entry which has a flag for the instruction at an offset of 512 bytes.

These flags are used to generate the performance indicators that are associated with the instructions. The operating system moves this metadata generated by the compiler and processes the metadata into a performance instrumentation shadow memory such as performance instrumentation shadow memory in . Alternatively this metadata may be placed into fields within the instructions depending on the particular implementation.

With reference now to a diagram illustrating components involved in loading and maintaining a performance instrumentation shadow cache are depicted in accordance with a preferred embodiment of the present invention. In this example existing cache contains primary segment . Primary segment includes blocks and . Translation table is used to provide a mapping for blocks in primary segment to blocks in perfinst segment . The data in this segment is placed into new performance instrumentation shadow cache .

At program compile time the compiler generates a new performance instrumentation data section as previously described. At program load time the loader queries the processor to determine cache line size. The loader parses perfinst segment and constructs a shadow segment in the format required by the processor for any text or data segment that the loader loads. This shadow segment is placed into new performance instrumentation shadow cache .

Each block in the shadow segment contains metadata for instructions or data in the corresponding primary cache block. This metadata includes for example flags tag fields threshold and count fields for each tagged item in a block in primary segment . This metadata also may include a flag that represents all the instructions or data in the block.

The loader constructs a table mapping translation table for each block in primary segment to a corresponding perfinst block such as block and in perfinst segment . Further the loader registers the head of this table translation table and the location and size of primary segment with the processor.

At page replacement time paging software provides a new interface to associate perfinst segment with the corresponding primary segment primary segment . When primary segment pages in or out perfinst segment pages in or out as well.

At cache line replacement time the processor contains new performance instrumentation shadow cache with cache frames directly associated with the frames in the existing data and instruction caches such as existing cache . When the processor s instruction or data cache loads a new line it must also load the corresponding perfinst block into the performance instrumentation shadow cache new performance instrumentation shadow cache . The processor sees from the registration data given by the loader at program load time that the processor is bringing a block into its cache that has an associated perfinst segment perfinst segment . The processor looks in translation table associated with this segment finds a reference to the perfinst block corresponding to the block it is about to load and loads the perfinst block into new performance instrumentation shadow cache . In these examples cache misses associated with metadata are not signaled or are treated differently from cache misses associated data in a primary cache block such as in primary segment .

With reference now to a flowchart of a process for generating metadata for instructions is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented by a performance monitoring program.

The process begins by identifying an instruction for profiling step . This instruction may be for example one that has been executed more than a selected number of times. Metadata is generated for the identified instruction step . This metadata takes the form of a performance indicator. The performance indicator may for example increment a counter each time the instruction is executed increment a counter if the number of cycles needed to execute the instruction exceeds a threshold value toggle counting of events for all instructions for all events after this instruction or count events occurring in response to executing the instruction. In a preferred embodiment the counters are in the associated performance instrumentation shadow cache and take some number of bits to allow for a one to one correspondence between the data or instructions in the cache and the bits reserved for counting.

The metadata is then associated with the instruction step . Next a determination is made as to whether more instructions are present for processing step . If additional instructions are present the process returns to step . Otherwise the process terminates. A similar process may be used to dynamically generate metadata for data in memory locations.

With reference now to a flowchart of a process for generating metadata for memory locations is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in a compiler such as compiler in .

The process begins by identifying a memory location for profiling step . Metadata is generated for the identified memory location step . This metadata takes the form of a performance indicator. The performance indicator may for example increment a counter each time the memory location is accessed increment a counter if the number of cycles needed to access the memory location exceeds a threshold value or toggle counting of all accesses to memory locations. The metadata is then associated with the memory location step . Next a determination is made as to whether more memory locations are present for processing step . If additional memory locations are present the process returns to step . Otherwise the process terminates.

Turning now to a flowchart of a process for counting execution for particular instructions is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in an instruction cache such as instruction cache in .

The process begins by executing an instruction step . A determination is made as to whether a counter is associated with the instruction step . The counter may be included in a field within the instruction or may be in a performance instrumentation shadow memory. If a counter is associated with the instruction the counter is incremented step with the process terminating thereafter. Otherwise the process terminates without incrementing the counter. The counter may be reset if the counter exceeds a threshold value.

When the counter is implemented as part of the instructions the counter may be of limited size. In this case a threshold value for the counter may be set to indicate when the counter is in danger of overflowing. A value of the counter prior to the counter exceeding the threshold value or when the value is reached. The counter may then be reset after the value has been read. This value may be read by a performance monitor unit or by a program used to analyze data. APIs may be implemented to access this data.

Turning now to a flowchart of a process for counting accesses to a particular memory location is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in a data cache such as data cache and instruction cache in .

The process begins by detecting access to a memory location step . A determination is made as to whether a counter is associated with the memory location step . The counter may be included within the memory location or may be in a performance instrumentation shadow memory. If a counter is associated with the memory location the counter is incremented step with the process terminating thereafter. Otherwise the process terminates without incrementing the counter.

With reference next to a diagram illustrating components used in accessing information collected with respect to the execution of instructions or the access of memory locations. In this example instruction unit executes instruction and increments counter . This counter is incremented each time instruction is executed. In this example instruction unit may be implemented as instruction cache in .

When the instruction or data cache pages are loaded into memory the operating system program loader linker and or the performance monitoring program reads the metadata generated by the compiler and determines that counting is associated with instruction or data access then the loading process allocates data areas to maintain the counters as part of its perfinst segment. The size of the counters and the granularity of the data access determine the amount of work area to be allocated.

In a simple case the granularity of the data or instruction access could be word size so that an access to any byte in the word is considered an access and the counts could also be a word size. In this case one to many mapping is present between the primary segment and the perfinst segment a full word to contain the counts or threshold is not required . The loading process allocates a shadow page or pages and tells the processor to use the shadow page s to contain the counts. Details of this mapping are described above with reference to . The cache unit in the processor maintains a shadow block entry to indicate the corresponding page to contain the count information. Different mapping and different levels of support could be provided.

In an alternative embodiment the compiler allocates the work areas to maintain the counts and indicates the placement of these work areas in its generated data areas. An entry in the meta data could indicate the start of the data the number of bytes of data granularity of the data the start of the count area and the granularity of each counting unit. In either case the metadata is loaded into the processor and the processor populates its internal shadow cache with the metadata. In embodiments in which the instruction stream itself is modified to contain the metadata then either the loader updates the instruction stream to contain the appropriate indicators and work areas or the compiler has generated the code to contain the metadata. In either case after the code is loaded the processor receives the metadata.

Data unit may be implemented as data cache in . In this example each time data is accessed counter is incremented. Data and counter are both located in a particular memory location. In these examples a new instruction may be employed in which the instruction is called ReadDataAccessCount RDAC that takes a data address and a register and puts the count associated with that data address in the register.

Each of these events instruction execution and data access results in incrementing of a counter. The mechanism of the present invention provides an interface hardware interface to access this collected data. In these examples hardware interface takes the form of an application programming interface API for operating system . In this way analysis tool may obtain data from counter and counter .

Although the examples in illustrate providing an interface to an instruction unit and a data unit hardware interface may be implemented to provide access to information from other units in a processor. For example APIs may be created for hardware interface that allows for accessing information located in counters in a performance monitor unit such as counter and in performance monitor unit in .

In a block diagram of components used in autonomically modifying code in a program to allow selective counting or profiling of sections of code in accordance with a preferred embodiment of the present invention. In this example profiler is a program such as tprof that may be used to identify routines of high usage in a program such as program . In these examples tprof is a timer profiler which ships with the Advanced Interactive Executive AIX operating system from International Business Machines IBM Corporation. This program takes samples which are initiated by a timer. Upon expiration of a timer tprof identifies the instruction executed. Tprof is a CPU profiling tool that can be used for system performance analysis. The tool is based on the sampling technique which encompasses the following steps interrupt the system periodically by time or performance monitor counter determine the address of the interrupted code along with process id pid and thread id tid record a TPROF hook in the software trace buffer and return to the interrupted code.

Alternatively a fixed number of counts of a performance monitor counter may be used instead of a timer. This program profiles subroutines that are used to indicate where time is spent within a program. A program having usage over a certain threshold also is referred to as being hot . By using information from profiler routines of interest such as subroutine in program may be identified.

With this information the instructions in subroutine may be autonomically modified by analysis tool to allow counting of the execution of subroutine . Additional routines may be identified for modification by analysis tool . For example subroutine also may be identified as a routine of interest with the instructions of this routine being modified to allow counting of the execution of subroutine . The modification of the code in these routines includes associating performance indicators with one or more instructions within each of these subroutines.

After the instructions in these routines have been modified by analysis tool program is then executed by processor . Processor executes program and provides counts for these routines. For example the counting of instructions executed and the number of cycles used in executing a routine may be performed by processor using the mechanisms described above.

With reference to a flowchart of a process for dynamically adding or associating performance indicators to an instruction is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in a program such as analysis tool in .

The process begins by identifying instructions of interest using data from a profiler step . This profiler may be for example a timer profiler found in AIX. An instruction from the identified instructions is selected for modification step . Thereafter a performance indicator is dynamically added to the selected instruction step .

In step the instruction may be added in a manner such that the instructions do not need to be modified for execution. A performance instrumentation shadow memory such as performance instrumentation shadow memory in may be employed to hold the performance indicators. In this situation a register is set in the processor to indicate that the performance instrumentation shadow memory should be checked for performance indicators when executing instructions.

A determination is then made as to whether additional identified instructions are present for modification step . If additional instructions are present for modification the process returns to step . Otherwise the process terminates.

Turning next to a diagram illustrating components used to scan pages through associating performance indicators with instructions in a page is depicted in accordance with a preferred embodiment of the present invention. The mechanism of the present invention uses performance indicators to allow instrumenting or modifying of instructions in a program one page at a time.

In this example program contains three pages page page and page . Scanning daemon associates performance indicators with instructions in program one or more pages at a time. For example the instructions in page may be associated with performance indicators by scanning daemon . Program is then executed by processor . Data from the execution of program may then be collected. This data includes for example counts of events occurring in response to instructions in page counting the number of times each instruction in page is executed and or identifying the number of visits to page .

Next scanning daemon may remove the performance indicators from instructions in page and associate performance indicators with instructions in page . Program is then executed again by processor and data from execution of this program is collected. Then instructions in page may be modified in program executed to collect data on that page.

In this manner usages of routines typically not recorded by programs such as a timer profiler may be identified. A timer profiler may not record some usages of routines because interrupts may be inhibited or the timing of samples may cause synchronous non random behavior. By modifying instructions in program counting a routine or other modules may be obtained in which the counts are unbiased and the system is unperturbed. In this manner interrupt driven counting is avoided. Further although the instrumenting of code is one page at a time other groupings of instructions may be used in scanning a program such as modules that form the program. For example the grouping may be a single executable program a library a group of selected functions and a group of selected pages.

Turning next to a flowchart of a process for adding indicators to instructions in a page is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in a program such as scanning daemon in .

First a selection of pages is identified step . In this example the pages are those in the program that are to be scanned or instrumented. Next a page within the selection of pages is selected for modification step . Indicators are then associated with all of the instructions in the selected page step . The program is then executed step . Next a determination is made as to whether all the pages with the selection have been scanned step . If all of the pages have been scanned the process terminates thereafter. However if not all pages have been scanned the next page to be scanned is selected step with the process returning to step as described above.

The process illustrated in shows scanned groupings of instructions as pages. Depending on the particular implementation other types of groupings of instructions such as modules that form a program may be scanned or instrumented in this manner.

A program is employed to identify a caller from a routine from the information found in a call stack. This program allows for an identification of what has occurred in a routine and provides a summary of what has occurred in a program by identifying function calls that have been made. This program however requires instructions inserted in the code to obtain this information.

The mechanism of the present invention allows for identifying calls and returns without having to perform special code instrumentation. In particular the function of generating an interrupt on a specific set of instructions may be used to gather information about the system and applications. In these examples instructions for calls and returns are associated with a performance indicator that generates an interrupt.

By walking back up the call stack a complete call stack can be obtained for analysis. A stack walk may also be described as a stack unwind and the process of walking the stack may also be described as unwinding the stack. Each of these terms illustrates a different metaphor for the process. The process can be described as walking as the process must obtain and process the stack frames step by step or frame by frame. The process may also be described as unwinding as the process must obtain and process the stack frames that point to one another and these pointers and their information must be unwound through many pointer dereferences.

The stack unwind follows the sequence of function method calls at the time of an interrupt is generated in response to execution of an instruction associated with a performance indicator. A call stack is an ordered list of routines plus offsets within routines i.e. modules functions methods etc. that have been entered during execution of a program. For example if routine A calls routine B and then routine B calls routine C while the processor is executing instructions in routine C the call stack is ABC. When control returns from routine C back to routine B the call stack is AB. For more compact presentation and ease of interpretation within a generated report the names of the routines are presented without any information about offsets. Offsets could be used for more detailed analysis of the execution of a program however offsets are not considered further herein.

Thus during interrupt processing or at post processing initiated by execution of an instruction associated with a particular performance indicator the generated sample based profile information reflects a sampling of call stacks not just leaves of the possible call stacks as in some program counter sampling techniques. A leaf is a node at the end of a branch i.e. a node that has no descendants. A descendant is a child of a parent node and a leaf is a node that has no children.

With reference now to a diagram depicting call stack containing stack frames is depicted in accordance with a preferred embodiment of the present invention. A stack is a region of reserved memory in which a program or programs store status data such as procedure and function call addresses passed parameters and sometimes local variables. A stack frame is a portion of a thread s stack that represents local storage arguments return addresses return values and local variables for a single function invocation. Every active thread of execution has a portion of system memory allocated for its stack space. A thread s stack consists of sequences of stack frames. The set of frames on a thread s stack represent the state of execution of that thread at any time. Since stack frames are typically interlinked e.g. each stack frame points to the previous stack frame it is often possible to trace back up the sequence of stack frames and develop the call stack . A call stack represents all not yet completed function calls in other words it reflects the function invocation sequence at any point in time.

Call stack includes information identifying the routine that is currently running the routine that invoked it and so on all the way up to the main program. Call stack includes a number of stack frames and . In the depicted example stack frame is at the top of call stack while stack frame is located at the bottom of call stack . The top of the call stack is also referred to as the root . The interrupt found in most operating systems is modified to obtain the program counter value pcv of the interrupted thread together with the pointer to the currently active stack frame for that thread. In the Intel architecture this is typically represented by the contents of registers EIP program counter and EBP pointer to stack frame .

By accessing the currently active stack frame it is possible to take advantage of the typical stack frame linkage convention in order to chain all of the frames together. Part of the standard linkage convention also dictates that the function return address be placed just above the invoked function s stack frame this can be used to ascertain the address for the invoked function. While this discussion employs an Intel based architecture this example is not a restriction. Most architectures employ linkage conventions that can be similarly navigated by a modified profiling interrupt handler.

When an interrupt occurs the first parameter acquired is the program counter value. The next value is the pointer to the top of the current stack frame for the interrupted thread. In the depicted example this value would point to EBP in stack frame . In turn EBP points to EBP in stack frame which in turn points to EBP in stack frame . In turn this EBP points to EBP in stack frame . Within stack frames are EIPs which identify the calling routine s return address. The routines may be identified from these addresses. Thus routines are defined by collecting all of the return addresses by walking up or backwards through the stack.

Obtaining a complete call stack may be difficult in some circumstances because the environment may make tracing difficult such as when an application having one call stack makes a call to a kernel having a different call stack. The hardware support provided by the mechanism of the present invention avoids some of these problems.

Turning next to a flowchart of a process for identifying events associated with call and return instructions in which data is collected from a performance monitor unit is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may also be implemented for an analysis tool such as analysis tool in .

The process begins by identifying call and return instructions step . The instructions for calls and returns are ones of interest for determining when a routine has been called and when a routine completes. This may be accomplished for interrupts interrupt returns system calls and returns from system calls.

Next performance indicators are associated with the identified call and return instructions step . The program is then executed step and data is collected from the performance monitor unit step with the process terminating thereafter. This information may be collected through interfaces such as hardware interface illustrated in in which APIs are employed to obtain data collected by the different functional units in a processor.

With this data identifications of callers of routines may be made. This information may be used to generate data structures such as trees to track and present information regarding the execution of the program. This generation of data structures may be implemented using processes similar to those provided in analysis tools.

Turning next to a flowchart of a process for identifying routines that have been executed more than a selected number of times is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in a functional unit within a processor such as instruction cache in . This process is used to identify counts of instructions that are executed and to generate an interrupt when these instructions have occurred more than some selected number of times.

First a determination is made as to whether an execution of a selected instruction is detected step . This determination is made by examining each instruction that is executed to see whether a performance indicator is associated with the instruction. These performance indicators may be associated with the instructions through different tools such as compiler in or analysis tool in .

If execution of an instruction containing a performance indicator is not identified the process returns to step until a selected instruction is detected. If a selected instruction is identified as being executed a counter with a set threshold is incremented for that selected instruction to count how often that particular instruction is executed step . In these examples each instruction identified for monitoring is assigned a counter.

Next a determination is made as to whether the set threshold has been reached step . Threshold values are initially determined by using documented cache miss times for each of the cache levels. However increasing times are used to determine problems caused by cache interventions accesses from other processors . Repeated runs with different values may be made to identify the areas with the worst performance.

In these examples the instruction may be associated with an indicator that includes an indication that execution of the instruction is to be monitored as well as providing a counter. Further count criteria may be included to identify when an interrupt is to be generated. For example an interrupt may be generated when the instruction has been executed more than thirteen times.

If the threshold has not been reached the process returns to step as described above. If the set threshold has been reached an interrupt is sent to the monitoring program step with the process terminating thereafter. This interrupt may be sent to an interrupt unit such as interrupt unit in which passes control to the appropriate procedure or process to handle the interrupt.

This process may be especially useful for routines with many branches. In this case all branch instructions would be flagged for counting. Information derived by this type of counting may be useful for identifying improvements for compiler and just in time JIT code generation by minimizing branches or adjusting hint flags supported in the instruction architecture of the processor that is used.

Turning next to a flowchart of a process for examining a call stack and identifying a caller of a routine when a particular instruction is executed more than some selected number of times is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be initiated by an interrupt unit such as interrupt unit in . This process is used to identify a call in routine and may be used to recursively obtain information for callers.

First a call stack is examined and the caller of a routine is identified step . Next a count of the number of instructions executed is captured from the instruction cache step . The count is for a counter used in step in . The counter is then reset step with control thereafter returned from the interrupt step . The information obtained in the process in may be used to identify additional routines for monitoring to recursively identify callers of routines.

Turning next to a diagram illustrating ranges of instructions and data that has been selected for monitoring is depicted in accordance with a preferred embodiment of the present invention. In this example program includes instruction range and . Each of these ranges have been identified as ones of interest for monitoring. Each of these ranges is set within an instruction unit such as instruction cache in . Each range is used by the processors to count the number of instructions executed in a range as well as the number of times a range is entered during execution of program .

Instruction cache uses range registers to define instruction ranges. These registers may be existing registers or instruction cache may be modified to include registers to define instruction ranges. These ranges may be based on addresses of instructions. Additionally range registers may be updated by various debugger programs and performance tools.

If an instruction is executed in a range such as instruction range or instruction range a counter is incremented in instruction cache . Alternatively the instruction may be sent to a performance monitor unit such as performance monitor unit in . The performance monitor unit tracks the count of the number of instructions executed within the range and the number of times the instruction range is entered in these examples.

Data accesses may be monitored in a similar fashion. For example data includes data range . Data accesses to data range may be counted in a similar fashion to execution of instructions within instruction range or instruction range . These ranges may be defined in registers within a data unit such as data cache in . These ranges for data may be defined in the register as a range of memory locations for the data.

Turning next to a flowchart of a process for counting the number of visits to a set range as well as the number of instructions executed within a set range is depicted in accordance with a preferred embodiment of the present invention. The process illustrated in may be implemented in an instruction unit such as instruction cache in .

First an instruction is identified for execution step . Next a determination is made as to whether the instruction is within a set range of instructions step . The range may be identified by examining registers defining one or more instruction ranges. If the instruction is not within a set range of instructions the process returns to step as described above. If the instruction is within a set range of instructions a determination is made as to whether the previous instruction was within the set range step . If the previous instruction was not within the set range of instructions a visit counter is incremented to tell the processor how many times the instruction range is entered step . Additionally an execution counter is incremented to count the number of instructions executed within the set range of instructions step with the process terminating thereafter.

With reference again to step if the previous instruction was within the set range of instructions the process proceeds to step as described above.

A similar process to the one illustrated in may be implemented for access to data. In this case the process would typically be implemented in a data unit rather than in an instruction unit.

As discussed above there are many possible applications of the hardware assistance offered through the counting mechanisms of the present invention as well as the performance indicators associated with instructions data addresses. The following descriptions are intended to provide additional embodiments of the present invention in which the performance indicators and counting mechanisms described above are utilized in different ways to achieve improved profiling ability with regard to computer programs. The embodiments described above and hereafter may be utilized separately or in various combinations without departing from the spirit and scope of the present invention.

Context switching needs to update the pointers to point to the appropriate metadata. The context may change from one thread to another or from one routine to another or to a library. Any of these transfers of control may have a new context. The registers that set up as part of the calling sequence may include registers that indicate the new shadow cache data.

As previously described above with regard to and the performance indicators and counters of the present invention may be used to determine the number of times an instruction is executed a data address is accessed a routine is executed and the like. In addition a determination may be made as to whether the instruction data area or routine is executed accessed more than a threshold number of times in order to determine whether to perform a subsequent action. In a further embodiment of the present invention these mechanisms are utilized to determine hot spots within a cache or memory in order to improve the performance of the computer program being profiled by the mechanisms of the present invention.

That is instructions and or data areas of the computer program cache or memory are instrumented by the addition of performance indicators in the manner previously described above. When counts of the instructions data area accesses exceed established thresholds this may be an indication of a hot spot area of the cache or memory i.e. an area that consumes a relatively larger amount of processor time than other areas.

With this embodiment of the present invention upon receiving the interrupt an interrupt handler of the monitoring program recognizes the interrupt and determines the instruction to be associated with a hot spot area of the cache or memory. The routine method in which the instruction is located is determined to be the hot spot. As a result the cache or memory addresses of the instructions of the routine method determined to be a hot spot are identified.

The interrupt handler copies the metadata associated with these instructions at the cache memory addresses of the routine method to a storage location that is designated for use by an analysis engine to analyze the metadata to determine an optimization scheme for the routine method. For example the metadata may be stored to a trace file for later use in trace analysis. The detection of hot spots in this manner may be continued during the profiling of the computer program. If the same area is again detected to be a hot spot area the information in the storage area may be updated with a new version of the metadata for the hot spot instructions data areas.

Thereafter such as during post processing of the trace data obtained during profiling of the execution of the computer program the metadata for the hot spots may be analyzed to determine performance improvement methodologies that may be used to increase the performance of the computer program. For example particular instructions within the hot spot may be identified as being executed more often than others. The code for the routine method may then be modified such that the execution of these instructions is optimized.

For example if the instructions within a hot spot routine method that are executed more than other instructions are associated with a particular branch the code may be optimized by reorganizing the instructions to achieve a contiguous execution of the code flow. That is computing cycles may be saved by reducing the amount of speculative processing by reorganizing the instructions of the routine such that the code that has the most often taken branches is repackaged into a set of instructions that are executed in a more contiguous manner with the other instructions of the hot spot routine method thereby reducing the amount of branching of the code flow and cache misses.

In still another embodiment of the present invention the range of cache or memory addresses associated with the hot spots are determined and the instructions data associated with these cache or memory addresses are copied to a hot spot shadow data structure . A mapping of the old address in the cache or memory to the new address in the hot spot shadow data structure is generated. The mapping may be implemented by a pointer associated with the old memory address location a mapping table or the like. Thereafter when accesses to the old address are attempted the access attempt is mapped to the new data structure. Alternatively the code itself may be modified such that the instructions that access the old address are changed to access the new address in the hot spot data structure.

By locating the hot spots of the cache in a shadow data structure the hot spot data is centrally located. This allows for a reduction in the cache flushing and fetching that would otherwise be needed. As a result machine cycles are saved. If multiple processors are sharing the same cache line for different data then the data could be separated out by processor access. This will again prevent frequent cache flushes which results in saving machine cycles. Furthermore at some point all of the addresses will be mapped to the cache or shadow cache data structure. This will result in faster memory accesses.

Other methods of optimizing code based on hot spot detection are generally known in the art. The present invention may make use of any known hot spot optimization technique. One of the principle differences between the known hot spot optimization techniques and the present invention is that the present invention identifies hot spots based on the performance indicators and counters described previously.

That is those instruction addresses data addresses of the cache or memory that have been instrumented with performance indicators in the manner described above and that are accessed more than a threshold number of times are identified based on the values stored in the counters associated with these instruction data addresses. The routines methods associated with these instrumented instructions whose counter values exceed a predetermined threshold are identified based on the code of the computer program. The instruction data addresses for these routines methods in the cache or memory are then identified.

The metadata associated with these instruction data addresses of the identified hot spot is copied to a storage location such as a trace data file designated by the performance monitoring application step . A determination is made as to whether continued monitoring of the execution of the computer program is to be performed step . If so the operation returns to step . If not post processing of the data obtained during the performance monitoring is performed step .

As part of this post processing the hot spot metadata stored in the designated storage location is analyzed to determine how the processing of the hot spot metadata may be optimized step . That is characteristics of the metadata may be identified and compared to optimization criteria associated with different optimization techniques. An optimum optimization technique may then be selected and the code data storage may be modified to implement the selected optimization technique step . Thereafter the operation terminates.

As mentioned above the optimization of the code data storage may take many different forms. In some cases optimizing of the code may include repackaging the instructions in the code to provide contiguous execution of the hot spots with the other instructions in the computer program. illustrates such a method. As shown in the range of cache addresses corresponding to the hot spot are identified step . Thereafter the instructions are repackaged to provide contiguous execution of the hot spots step . Examples of ways in which instructions and data may be repackaged to provide contiguous access of hot spots are provided in U.S. Pat. No. 5 212 794 entitled Method for Optimizing Computer Code to Provide More Efficient Execution on Computers Having Cache Memories and U.S. Pat. No. 5 689 712 entitled Profile Based Optimizing Post Processors for Data References both of which are hereby incorporated by reference.

As shown in the operation starts by identifying the range of cache addresses either instruction or data corresponding to the hot spot step . A hot spot shadow data structure is then created step . The instructions data from the identified cache or memory addresses are then copied to the new addresses in the hot spot shadow data structure step . A mapping from the current cache or memory address to the new address in the shadow data structure is established step and the operation terminates. Thereafter when there is an access of a cache or memory address that has been mapped to the new shadow data structure the access is redirected to the new address in the new shadow data structure. This mapping may be implemented by causing an interrupt whenever the old data area is accessed. The interrupt handler may then modify the code that accesses the old data area so that it will now access the new data area.

Thus the present invention in addition to the previously described embodiments provides embodiments in which hot spots within caches or memories may be identified using the performance indicators hardware counters and established thresholds. In addition the present invention provides embodiments in which code and or data storage may be optimized based on the detection of hot spots.

In a further embodiment of the present invention the performance indicators and counters may be used in a more complex fashion to determine when an interrupt is to be sent to an interrupt handler of a monitoring application for processing. That is the previous embodiments of the present invention have been described in terms of the counter values individually being used as a basis for determining whether to send an interrupt to the interrupt handler of the monitoring application. Thus for example when one counter associated with a particular instruction range of instructions data address or range of data addresses exceeds a given threshold an interrupt may be sent to the interrupt handler of the monitoring application to thereby perform hot spot detection processing as previously described.

In a further embodiment of the present invention an arithmetic combination of counter values may be utilized to determine if an interrupt is to be sent to the interrupt handler of the monitoring application. The performance monitor unit may periodically check the counter values for a predetermined set of counters and combine them in an arithmetic manner as specified in the microcode of the performance monitor unit to determine whether a condition exists requiring an interrupt to be sent to the monitoring application.

Register stores a threshold value Z against which the arithmetic combination of the values X Y A and B is to be compared in order to determine whether to send an interrupt or not. In the present invention the performance indicators may be associated with instructions or portions of data. For example the performance indicators may be associated with addresses in an instruction cache data cache or memory i.e. instruction addresses or data addresses.

Periodically or upon the occurrence of an event such as incrementing of one of the counters or the microcode of the performance monitor unit checks the current values X Y A and B of the counters and registers and against the threshold value Z of the register . The particular counters and and registers and whose values are to be combined to determine whether to generate an interrupt are identified based on information passed to the performance monitor unit by the monitoring application. That is the monitoring application upon initialization may inform the performance monitor unit that the counters associated with particular types of instructions ranges of instructions data addresses or ranges of data addresses are to be combined along with particular register values and compared to a particular threshold value. In addition the monitoring application may instruct the performance monitor unit in the manner by which the counter and register values are to be combined. This information may then be stored in the performance monitor unit for use when combining the values of the counters to determine if an interrupt is to be generated. Alternatively the performance monitor unit may be hard coded with the particular combination of counters and registers that will always be checked.

For example the performance monitoring application may interface with a device driver that initializes the counters in the hardware. The performance monitoring application may inform the device driver regarding what is to be counted e.g. instructions cache misses memory accesses etc. what thresholds to use the vector e.g. pointer to a portion of code that is to be executed when the threshold is met or exceeded and other miscellaneous control information. In addition the performance monitoring application through the device driver may set the multipliers i.e. the register values for the original counter values in order to scale the events being counted by each counter and the like. The device driver may then set appropriate bits and register values in the hardware to indicate which counters are to be combined and which register values are to be compared against. For example a bit mask or the like may be used to identify which counters and registers are to be combined as well as the manner by which these counters are to be combined.

Once the device driver has initialized the base event counters and the combined event counters execution of the computer program is started. Every time the base counters are incremented the hardware will update the combined counters based on the values of the base counters and the multipliers i.e. the register values and check if the thresholds have been reached or exceeded. When any of the thresholds have been reached or exceeded the hardware may initiate an interrupt and transfer control to the interrupt handler of the performance monitoring application. At this point the interrupt handler executes to perform desired actions. For example an event may be logged to the performance monitoring application buffer or log a log daemon process may be notified that an event has occurred or the like.

The microcode performs the check of the designated counters by first generating an arithmetic combination of the counter values to generate a combination counter value. The combination counter value may be stored in a combination counter or register and may then be compared against the value stored in register to determine if a predetermined relationship exists.

In the depicted example the value X of register is multiplied by the value A of the counter and the value Y of register is multiplied by the value B of counter . The products of these operations are then added to generate the combined counter value that is equal to X A Y B. The combined counter value is then compared to the register value Z to determine if it is greater than Z. If not performance monitoring is continued without generating an interrupt. If the combined counter value is greater than Z then an interrupt is sent to the monitoring application for processing.

Thus for example the performance monitoring application may inform the processor that a particular routine is of particular interest and that the number of cycles per instruction for the routine is to be monitored and used as a basis for determining if an interrupt is to be generated. For example it may be determined that when the number of cycles per instruction is greater than 3 an interrupt should be generated and sent.

In this example when making the determination as to whether to send an interrupt counter values may be associated with the routine for counting the number of instructions and the number of processor cycles. A multiplier value of 3 may be stored in a first register and a multiplier value of 1 may be stored in a second register. The threshold value may be designated to be zero and stored in a threshold value register. The resulting equation obtained from the combination of the register values and the counters may be of the type 3 number of instructions 1 number of cycles 0

When this relationship is satisfied the number of cycles per instruction is greater than 3 and thus an interrupt is generated and sent. This relationship may be checked for example every time the number of instructions counter is incremented or the number of cycles counter is incremented.

Thus rather than merely checking to see if the number of instructions is more than a predetermined number or if the number of cycles is more than a predetermined number the combination of counter values according to the present embodiment allows for a complex condition in which a combination of the scaled number of instructions executed and the scaled number of cycles is used as a basis for determining whether to generate an interrupt.

While illustrates a specific exemplary combination of two counter values and two register values being compared to a single register value the present invention is not limited to such. Rather any combination of counter and register values may be made by establishing the proper combination within the microcode of the performance monitor unit. Moreover various combinations of counter and register values may be compared to various register values without departing from the spirit and scope of the present invention. The primary concept of this embodiment of the present invention being the ability of the present invention to combine the values of multiple performance monitor counters and register values in any suitable manner to determine whether to generate an interrupt.

If the counter values are to be checked the counter values for the designated range of instructions data addresses are retrieved along with the threshold and multiplier values step . The counter values and register values are then arithmetically combined and the result compared to the threshold value step . A determination is then made as to whether the threshold value is met or exceeded step . If not the operation returns to step to continue monitoring of the execution of the computer program. If the threshold value is met or exceeded then an interrupt is generated and sent to the monitoring application for processing step . The operation then terminates. The steps may be repeated until the monitoring of the execution of the computer program is completed.

Thus in this further embodiment of the present invention arithmetic combinations of counter values and register values may be used to determined when to send an interrupt to a monitoring application. In this way more complex conditions may be selected as the basis for determining when interrupts are to be generated.

In a further embodiment of the present invention functionality is provided in the performance monitoring application for initiating the measurement of secondary metrics with regard to identified instructions data addresses ranges of identified instructions or ranges of identified data addresses based on counter values for primary metrics. Thus for example when a primary metric counter or a combination of primary metric counters meets or exceeds a predetermined threshold value an interrupt may be generated. In response to receiving the interrupt counters associated with the measuring of secondary metrics of a range of instructions data addresses may be initiated. In this way areas of particular interest may first be identified using the primary metric performance counters with more detailed information being obtained through the use of secondary metric performance counters directed to measuring metrics associated with the particular area of interest.

With this exemplary embodiment instructions data addresses in memory cache or the like are instrumented with performance indicators and counters and are initialized in the manner previously described above. The performance of the computer program is monitored in the manner previously discussed with performance indicators being encountered and counter values being incremented. The metrics being monitored by the use of the performance indicators and the associated counters are considered to be the primary metrics i.e. the metrics for which the computer program is initially instrumented for monitoring. As previously discussed above these counter values may be compared against threshold values to determine if certain conditions have occurred e.g. entry into a routine more than a predetermined number of times.

When the comparison of the counter values to the thresholds results in a threshold value being met or exceeded an interrupt is generated. The interrupt handler of the performance monitoring application receives the interrupt and performs appropriate processing based on the received interrupt.

In this present embodiment of the present invention the processing may involve instrumenting the same or other instruction data addresses with performance indicators and initializing counters for counting secondary metrics. The instrumenting of the instruction data addresses involves storing performance indicators in association with the instruction data addresses identified in a manner such as that used to instrument the instruction data areas for the primary metrics described above. In one exemplary embodiment as described above this may involve storing performance indicators in a shadow cache data structure for example.

The other instruction data addresses to instrument for the monitoring of secondary metrics may be determined based on the particular implementation. For example the interrupt handler of the performance monitoring application may be programmed such that when an interrupt is received in response to a threshold being exceeded a particular class of instructions within the routine associated with the instruction whose counter value exceeded the threshold value may be instrumented by the storing of performance indicators in association with the instruction addresses. Thereafter when performance monitoring continues by returning from the interrupt handler the newly instrumented class of instructions within the routine will begin accumulating counts of executions of these instructions in a manner described previously with regard to the primary metric counters.

For example as previously described above the counter values for instruction data addresses that have been instrumented with performance indicators may be used as a mechanism for identifying hot spots within the computer program being monitored. That is for example the number of times a routine is entered may be used as a means for determining whether that routine is a hot spot i.e. uses more computing cycles relative to other portions of code. In determining whether the routine is a hot spot or not the counter value for the count of the number of times the routine is entered may be compared to a threshold that is established for identifying hot spots. If the threshold is met or exceeded the routine is considered a hot spot.

With the present embodiment of the present invention this hot spot may be further instrumented within the routine to determine which instructions of interest are hot spots within the hot spot routine. Alternatively the secondary metric may be a measure of how many times during execution of the computer program the routine is identified as a hot spot i.e. how many times during execution of the computer program do the counter values associated with the routine exceed the threshold defined for hot spots. Other secondary metrics may be initiated in a similar manner without departing from the spirit and scope of the present invention.

Thus different levels of granularity of metric measurements may be made by the use of a first set of counters to measure primary metrics and a second set of counters to measure secondary metrics in accordance with this embodiment of the present invention. Of course the ability to initiate measurement of new metrics based on the previous metric measurements exceeding a threshold may be extended to even further iterations rather than stopping at only a primary set of metrics and a secondary set of metrics.

In response to receiving the interrupt a determination is made as to whether secondary metrics for the instruction data address or related instruction data addresses have already been initiated step . If not then monitoring of secondary metrics is initiated step . This step involves determining which instruction data addresses associated with the instruction data address that instigated the interrupt are to be instrumented with performance indicators and storing the performance indicators with the identified instruction data addresses. The determination of which instruction data addresses associated with the instruction data address that instigated the interrupt is implementation specific and may be performed in any suitable manner as previously discussed. The actual instrumenting of the identified instruction data addresses may be performed in a similar manner to that described previously with regard to the performance indicators being stored for the primary metrics.

The counters associated with the performance indicators for the secondary metrics are then initiated step . Thereafter or if monitoring of secondary metrics has already been initiated for the instruction data address instigating the interrupt step the counters for the secondary metrics are incremented in accordance with execution of the computer program step . A description of the incrementing of counters in association with performance indicators has been previously provided above.

A determination is then made as to whether the counter values for the secondary metric exceed a threshold step . If so then an interrupt may be sent to the interrupt handler of the performance monitoring application step . This may cause the operation shown in to be repeated in which case the primary metric is now the secondary metric and secondary metric would be a tertiary metric etc. The operation then terminates.

Thus with this embodiment of the present invention performance monitoring of a computer program execution may be initially performed at a first granularity to identify areas of interest. Once these areas of interest are identified based on the monitoring of primary metrics the computer program may be dynamically instrumented during execution of the computer program with regard to the identified areas of interest. This dynamic instrumentation involves instrumenting the instruction data addresses associated with the identified areas of interest and initiating monitoring of secondary metrics within these areas of interest. Thus a dynamic modification in the granularity at which the computer program may be monitored is achievable through the use of performance identifiers hardware based counters and thresholds in accordance with the present invention.

In an additional embodiment of the present invention the performance indicators and counter values may be used as a mechanism for identifying cache hits and cache misses. With such an embodiment performance indicators are associated with instructions for selected routines or portions of code of interest in the computer program. For example as discussed above with regard to and performance indicators may be associated with instructions data addresses and may be stored in the instructions data in a performance indicator shadow cache or other type of metadata data structure.

Performance counters are incremented each time the instructions of the routines or portions of code which have been instrumented with performance indicators are executed. That is as described previously when an instruction is executed or a data address is accessed and it has an associated performance indicator the performance monitor unit increments a counter in the shadow cache the performance monitor unit or the like. In addition as described with regard to above a signal may be sent to the performance monitor unit when a cache miss occurs and an instruction or block of instructions must be reloaded into the cache. When the signal is received by the performance monitor unit a counter may be incremented indicating a number of times a cache miss occurs on an instruction or portion of code using the counts in the shadow cache performance monitor unit or the like. From the values of these counters that indicate the number of times an instruction is executed and the number of times a cache miss occurs the cache hit miss ratio may be determined.

The cache hit miss ratio may be stored in a metadata data structure associated with the cache. When the cache hit miss ratio becomes less than a predetermined threshold i.e. there is a greater number of cache misses than cache hits an interrupt may be sent to the performance monitoring application indicating that a problem condition has occurred. An interrupt handler associated with the performance monitoring application may then handle the interrupt by initiating appropriate support to process this interrupt. Alternatively the condition may be determined by periodically examining the information via a sampling approach.

As discussed above one contributor to such a small cache hit miss ratio may be the chase tail condition. A chase tail condition occurs when a block of instructions data must be loaded into cache but there is not enough available room in the cache to store the entire block of instructions data. In such a case the instructions data are written to the available space in the cache and any overflow is written over the least recently used portion of the cache. This may cause cache misses on the instructions data overwritten thereby increasing the number of cache misses causing more reloads of the cache and more overwriting of instructions data in the cache.

To avoid this chase tail condition the present invention employs support instigated in response to a determination that the cache hit miss ratio is falling below a threshold that stores the instructions data such that instructions data that would have been overwritten in the cache are maintained in the cache and the instructions data being reloaded are stored in a dedicated or reserved portion of the cache. This support in one exemplary embodiment may involve setting a mode bit in a mode register such as mode register indicating that chase tail operation is to be followed by the processor.

This chase tail operation may involve determining upon processing a reload operation on the cache whether the cache has sufficient available space to store the block of instructions data that are to be reloaded into the cache. If there is available space in the cache then the block of instructions data are stored in the cache in a normal manner. However if there is not sufficient space in the cache to store the block of instructions data that is to be reloaded then the block of instructions data or at least the overflow portion of the block of instructions data is loaded into a reserved portion of cache rather than reloading the instructions data into a non reserved area of the cache and overwriting instructions data already present in the non reserved area of the cache.

In addition a performance indicator may be associated with the block of instructions indicating that when an instruction in this block of instructions is again executed or when a data address in the block of data addresses is again accessed the processor should look for the instruction data in the reserved area of the cache. As with the other performance indicators described above these performance indicators may be stored in a performance indicator shadow cache with the instruction or portion of code itself or the like.

The dedicated or reserved portion of the cache to which the block of instructions data or at least the overflow portion of the block of instructions data is written may itself be overwritten with subsequent operations of the present invention. However a separate algorithm may be utilized to determine how to overwrite the instructions data in the reserved portion of the cache. For example a least recently used algorithm for the reserved portion of the cache may be used to determine which instructions data in the reserved portion of the cache are to be overwritten. This approach allows for speculatively loading the reserved portion of the cache with new data and still allows access to the data recently loaded into the reserved area of the cache.

Alternatively when a portion of the reserved area of the cache is to be overwritten a more complex algorithm in which the comparison of instructions data in the reserved area of the cache and the non reserved area of the cache may be utilized in determining how to handle the reload of the instructions data. For example when it has been determined that a reload operation will result in instructions data in the reserved area of the cache being overwritten a comparison of the least recently used instructions data in both the reserved area of the cache and the non reserved area of the cache may be made. Whichever portion of the cache has the oldest instructions data that have not been used recently may be determined to be the area where the reload operation will load the instructions data. Other similar types of determinations may be made by weighing the affects of overwriting instructions data in the non reserved and reserved areas of the cache.

Thus by invoking the chase tail operation of the present embodiment when the cache hit miss ratio is below a predetermined threshold the present invention avoids the chase tail situation by causing any reloads of instructions data that cannot be accommodated by the available space in the non reserved portion of cache to be stored in a reserved portion of the cache rather than overwriting existing cache entries in the non reserved portion of the cache. In this way the domino effect with regard to overwriting and reloads caused by overwriting the least recently used entries in the non reserved portion of cache may be avoided. Furthermore the cache hit miss ratio will increase above the threshold since the block of instructions data are guaranteed to be stored in a reserved area of the cache.

As shown in the operation starts by receiving a request for a block of instructions which are to be retrieved from cache or memory step . A determination is made as to whether instructions in the block of instructions have associated performance indicators step . If so counters associated with the instructions that have performance indicators are incremented step .

A determination is made as to whether the block of instructions are present in the instruction cache step . If not a reload of the block of instructions into the cache is performed and the instructions are executed step .

In addition in response to the reload operation a reload counter for the instruction cache is incremented step . The values for the instruction counters and the value for the reload counter are used to determine the cache hit miss ratio step . The cache hit miss ratio is then compared to a threshold that is established by the performance monitoring application step .

A determination is then made as to whether the cache hit miss ratio meets or is below the threshold step . If so an interrupt is sent to the interrupt handler of the performance monitoring application in order to initiate the chase tail operation of the processor step . If not or if the block of instructions does not include an instruction having a performance indicator the operation terminates. This operation may be repeated for each block of instructions requested from the cache.

As shown in the operation starts with receiving an interrupt indicating that the cache hit miss ratio for the instruction cache meets or falls below an established threshold step . A mode bit in a mode register of the processor is then set step . The operation then waits for a reload operation to be executed by the processor step .

A determination is made as to whether a reload operation is executed by the processor step . If not the operation returns to step and continues to await a reload instruction. If a reload operation is executed by the processor a determination is made as to whether the instruction cache has sufficient available space to load the block of instructions without overwriting instructions already present in the cache step . If so the reload operation is executed in a normal manner step . If not the block of instructions are stored in a reserved portion of the cache step . A performance indicator is then associated with the block of instructions indicating that upon subsequent execution of an instruction in the block of instructions the instruction should be retrieved from the reserved portion of the cache step . The operation then ends.

Thus with this embodiment of the present invention the performance indicators and counters may be used to determine when a cache hit miss ratio falls to or below a predetermined threshold thus indicating a problem with the execution of the computer program. In addition this embodiment of the present invention includes the ability for the processor to operate in a chase tail mode of operation in which the microcode of the processor determines whether subsequent reloads of the cache may be performed without overwriting existing entries in a non reserved portion of the cache. If not then the entries that need to be written to the cache may be written to a reserved portion of the cache and a performance indicator may be associated with the instructions data of these entries indicating that the processor should look to the dedicated cache for these instructions data.

It should be noted that while the above embodiment has been described in terms of the entire block of instructions data being written to the dedicated memory or cache area the present invention is not limited to such. Rather in some exemplary embodiments a portion of the block of instructions data of the same size as the available space in the non reserved portion of the cache may be written to the non reserved portion of the cache while the remainder i.e. the overflow is written to the reserved portion of the cache. In such embodiments the performance indicators directing the processor to the reserved portion of the cache will be associated with only those instructions data within the block of instructions data that are written to the reserved portion of the cache.

As an example of the benefit of this invention consider a repetitive sequential read of data. If the block being read is long enough to overflow the cache then on the second iteration of the read it is possible that the data at the head of the block has been evicted from the cache and must be reloaded. This reloading of cache data evicts more data from the cache potentially immediately prior to its being read . In this way it is possible for this repetitive read to never derive advantage from the cache.

With the invention described herein some portion of the data consumes the stable portion of the cache and the remainder of the block overflows into the less stable overflow region. Then on subsequent reads data at the head of the block is still available in the stable portion of the cache without reloading. It is only the portion of the block beyond the extent of the cache that must be reloaded on subsequent reads.

Similar advantage may be derived from this technique for the case of four processors operating upon four independent blocks of code with a cache only large enough to support three of those blocks. This invention allows the system to identify this condition and split for example two of the instruction blocks into the volatile overflow region of the cache leaving two of the blocks undisturbed in the stable portion of the cache.

In even further embodiments of the present invention the performance indicators of the present invention may be utilized to obtain information regarding the nature of the cache hits and reloads of cache lines within the instruction or data cache. These embodiments of the present invention for example may be used to determine whether processors of a multiprocessor system such as a symmetric multiprocessor SMP system are truly sharing a cache line or if there is false sharing of a cache line. This determination may then be used as a means for determining how to better store the instructions data of the cache line to prevent false sharing of the cache line.

False cache sharing is a result of the cache operating at a greater granularity than the processors of the system operate at. That is processors operate on individual instruction data areas e.g. blocks of instructions data within a cache line. However the cache operates on a cache line granularity. Thus if there is any change to any portion of the cache line and an access request is received for another portion of the cache line the cache line must be reloaded before the access request is permitted.

This may lead to the case where one processor of the system writes to a first area of the cache line and a second processor reads data from a second area of the cache line that is not modified by the write to the first area of the cache line yet the cache line as a whole must be reloaded by the cache prior to the access to the second area being permitted. Thus even though the data or instructions in the second area have not been modified by the write to the first area and thus the read could be completed without having to reload the cache line because of the granularity at which the cache operates the cache line is reloaded. This causes a performance degradation due to having to process the reload of the cache line.

This is often referred to as false sharing of a cache line or a dirty cache hit. It would be beneficial to be able to identify when such situations are present in the cache. Embodiments of the present invention provide a mechanism for identifying such situations.

With these embodiments of the present invention individual instruction data areas or portions of code within cache lines are instrumented with performance indicators and processor write and read flags. The performance indicators and or processor write and read flags may be stored within the cache line itself in association with their instruction data areas in a performance indicator shadow cache or other metadata data structure. The performance indicators operate in a similar manner as discussed above with regard to the previous embodiments of the present invention.

With the present embodiment of the present invention upon an access request to the instruction data area a determination is made as to whether there is a performance indicator associated with the instruction data area. If so the processor that issued the access request is identified i.e. which processor of the multiprocessor system is reading from or writing to that instruction data area of the cache. In a preferred embodiment the access request includes header information or metadata that identifies the processor from which the access request was received. From this information it can be determined which processor sent the access request.

Thereafter a processor access flag bit associated with the instruction data area in the cache line and associated with the identified processor is set Depending on whether the access request is a read or a write either a read processor access flag bit or a write processor access flag bit is set. That is each instruction data area or portion of code that is instrumented by a performance indicator has both a read processor access flag bit and a write processor access flag bit for each processor of the multiprocessor system associated with it and stored in a corresponding portion of the cache line.

When the instruction data area is written to and the instruction data area has an associated performance indicator a write processor access flag bit corresponding to the processor that sent the write access request is set. Similarly when the instruction data area is read from and the instruction data area has an associated performance indicator a read processor access flag bit corresponding to the processor that sent the read access request is set. In this way it can be determined which processors have written to which instruction data areas which processors have read from which instruction data areas and whether reloads of the cache line are due to true sharing of the cache line between processors or false sharing of the cache line.

When a reload of a cache line is to be performed for example due to an access request to an area of the cache line but a previous change to the cache line having been performed an interrupt is generated and sent to an interrupt handler of the performance monitoring application. The interrupt handler obtains the write and read processor access flag bit values for the instruction data areas of the cache line that are being reloaded. The values of these write and read processor access flag bits are then compared to determine if false cache line sharing occurred. That is a determination is made as to whether there were data areas in the same cache line being written to by at least one processor and different data areas being accessed by another processor. True cache line sharing occurs when the same data area is written to by one processor and then accessed by one or more other processors.

As shown in each data area has associated write flag bits and read flag bits . While the write flag bits and read flag bits are illustrated as being stored in the data cache in association with their respective data areas the present invention is not limited to such. Rather the write flag bits and read flag bits may be stored in the performance indicator shadow cache or other metadata data structure outside the data cache without departing from the spirit and scope of the present invention.

As previously described either all or certain ones of the data areas may be provided with performance indicators in accordance with the present invention. These performance indicators may be stored in the data areas write flag bit areas read flag bit areas a separate performance indicator shadow cache or other metadata data structure.

When the processor processes an access request to a data area the processor determines if there is a performance indicator associated with the data area. If so an interrupt is sent to the interrupt unit that causes the performance unit to operate to determine which processor sent the access request. A determination is then made as to whether the access request is a read access request or a write access request and an appropriate read or write flag bit in the read or write flag bit area associated with the data area is set indicating that the processor has written to the data area. Thus for example if the access request is a write to data area and data area has an associated performance indicator a write flag bit in the write flag bit area may be set for a processor that sent the access request.

As shown in the write flag bit area includes a write flag bit for each processor of the multiprocessor system. Thus for example write flag bit is for a first processor P write flag bit is for a second processor P write flag bit is for a third processor P and write flag bit is for a processor Pn. These write flag bits may be present in a portion of a cache line in association with their associated data instructions in a shadow cache data structure or other metadata data structure. These write flag bits may be set in response to a determination that the corresponding processor P Pn has written to the data area associated with the write flag bit area . These write flag bits may be reset paged out to memory or the like after the processing of the present invention is performed in response to a reload of the cache line.

Similar structures and functionality may be provided for read access requests. That is a read flag bit area similar in structure to write flag bit area may be provided in which read flag bits are set in response to a read access request being processed for a particular processor. These read flag bit areas may be reset paged out or the like after the processing of a reload operation of a cache line in accordance with the present invention.

Thereafter the processor monitors for accesses to the cache step . For each cache access request a determination is made as to whether the access to the cache references a cache area that has been instrumented with a performance indicator step . If not the operation processes the access request in a normal fashion and then goes to step to determine whether to continue to monitor for access to the cache by returning to step .

If the access request references a cache area that has an associated performance indicator then the processor that issued the access request is identified step . A determination is then made as to whether the access request is a write access request step . If the access request is a write access request then an appropriate write flag bit for the cache area and the identified processor is set step . If the access request is not a write access request then it must be a read access request. As a result an appropriate read flag bit for the cache area and the identified processor is set step . Thereafter a determination is made as to whether continued monitoring of cache access requests is to be performed step . If so the operation returns to step . If not the operation terminates.

A determination is then made as to whether the write flag bits and or read flag bits for other processors i.e. processors other than the one that sent the access request that initiated the cache line reload for the cache area that is being accessed by the present access request are set step . This determination is basically one that determines whether the reload may be due to another processor having previously written to the same data area being accessed which is an indication of real sharing of the cache line. If so then the reload is determined to be due to a different processor writing to the cache area being accessed and thus the cache line is truly being shared by the processors of the system step .

If other processors have not previously written to the cache area being accessed by the present access request then a determination is made as to whether the write flag bits or read flag bits for any of the other processors are set for the other data areas of the cache line step . If so then a determination is made that other processors of the multiprocessor system have accessed other areas of the cache line but have not accessed the area of the cache line being accessed by the present processor. This is an indication of false cache line sharing. As a result the reload is determined to be due to false cache line sharing step . If the write or read flag bits for other processors have not been set for any of the other areas of the cache line then a determination is made that the reload of the cache line is due to true sharing of the cache line step .

An indication of the determined basis for the cache line reload may be output for use in performance analysis step . The cache line is then reloaded and the write flag bits and read flag bits may be reset step . The operation then terminates.

While the above embodiments illustrate the present invention performing the check of false cache line sharing with each reload of a cache line the present invention is not limited to such. Rather in an alternative embodiment the check for false cache line sharing may be performed periodically. That is after the computer program has run for a predetermined amount of time a check to see if there are false cache line sharing may be performed. The periodic nature of this check may be associated with a predetermined amount of execution time since a previous check for false cache line sharing the occurrence of an event such as a determination that a particular thread or threads are no longer active and the like.

In addition while the present invention has been described in terms of the read and write flag bits being reset when the processing of a reload operation is completed the present invention is not limited to such. Rather than resetting the read and write flag bits or in addition to resetting the read and write flag bits the present invention may page out the values of the read and write flag bits to memory in order to preserve a copy of the state of the read and write flag bits for later processing.

Thus the mechanisms of the present invention provide an ability to monitor performance within a cache line. More specifically the mechanisms of the present invention allow for the identification of reloads of cache lines as being due to true or false cache line sharing amongst processors of a multiprocessor system.

The determination of true or false cache line sharing may be beneficial in determining the manner by which data and instructions are stored in a cache. That is if it is determined that cache lines are being falsely shared and thus cache line reloads are often being performed due to writes to areas of the cache line by a first processor that are not being accessed by the second processor then appropriate measures may be taken to minimize the amount of false cache line sharing.

For example in a further embodiment of the present invention when it is determined that a cache line is being falsely shared using the mechanisms described above an interrupt may be generated and sent to the performance monitoring application. An interrupt handler of the performance monitoring application will recognize this interrupt as indicating false sharing of a cache line. Rather than reloading the cache line in a normal fashion the data or instructions being accessed may be written to a separate area of memory dedicated to avoiding false cache line sharing.

The code may then be modified by inserting a pointer to this new area of cache or memory. Thus when the code again attempts to access this area of the cache the access is redirected to the memory area rather than to the previous area of the cache that was subject to false sharing. In this way reloads of the cache line may be avoided.

Thus through the operation of the mechanisms outlined in the storage of instructions data in the cache may be modified to avoid false cache line sharing and thereby minimize the amount of cache line reloading that is performed during the execution of the computer program. In this way the performance of the computer program may be increased.

The above descriptions of the various embodiments of the present invention have been focused on the use of the performance indicators counters flags and the like to provide information about the manner by which instructions are executed data areas are accessed and the like. Information regarding the counter values determinations made based on the counter values and flag values and the like may be used to annotate performance profile data that is obtained by the performance monitoring application during a trace of the execution of the computer program. This annotated performance profile data hereafter simply referred to as the performance profile data may be stored to a performance profile data storage area for use in analyzing the performance of the computer program.

In a further embodiment of the present invention a compiler may obtain this performance profile data along with the instructions data of the computer program and use this information to optimize the manner by which the computer program is executed instructions data are stored and the like. That is the compiler may optimize the application and instruction data storage so that the runtime component of the application is optimized.

The manner by which the compiler optimizes the runtime aspects of the computer program may vary depending on the particular performance profile data obtained which is annotated by the output obtained from the use of performance indicators counters flags and the like previously described. The optimizations may be to optimize the instruction paths optimize the time spent in initial application load the manner by which the cache and memory is utilized and the like. The following are intended to only be example ways in which the runtime aspects of a computer program may be optimized based on the information obtained through the use of the performance indicators counters flags and the like and are not intended to limit the application of the present invention in any manner.

As a first example of the manner by which the compiler may optimize the runtime components of the computer program based on the performance profile data an optimization of the storage of instructions data in a cache will first be described. For example the mechanisms of the present invention have been described as including an embodiment in which false cache line sharing may be detected. As described above this false cache line sharing may be identified and an indication of the false cache line sharing may be output for later analysis. This indication may be output and stored in the performance profile data and may be utilized by the compiler to determine whether an alternative approach to storing data instructions in the cache is needed.

That is in one exemplary embodiment the compiler may determine from the performance profile data that there is a problem with false cache line sharing in the execution of the computer program and as a result the instructions data should be arranged in memory in such a manner that cache line sharing is minimized. For example the compiler may determine that blocks of instructions data are to be written 64 bytes away from each other. In this way each block of instructions data is allocated a 64 byte area of the cache and blocks of instructions data are guaranteed to be on separate cache lines in a 64 byte cache. Thus each cache line is accessed by only one processor of the system and false cache line sharing is eliminated.

In a further example of how the compiler may optimize the runtime components of the computer program the performance profile data may indicate that certain paths of execution are followed more often than others. That is at a branch instruction the same path of execution tends to be taken more than 50 of the time. This may be determined based on for example hot spot detection or the like as previously described above. From the performance profile data obtained the compiler may determine when compiling the code of the computer program into an executable to make the path that is executed more often at the branch contiguous with the branch instruction. That is the branch checks may be reordered so that they are more contiguous.

Alternatively the information regarding how the code may be optimized may be provided to the programmer such that the programmer may perform these optimizations offline. Thus rather than actually modifying the code or the storage of the data instructions in cache recommendations regarding the manner by which the runtime components of the computer program may be optimized may be provided to the programmer for use in modifying the code or operation of the computing system. The programmer may then decide whether to implement the recommended optimizations.

The optimizations that may be performed by the compiler or recommended by the compiler may be done automatically upon detection of the possible optimization based on the performance profile data. Alternatively the compiler may provide an alert to the programmer indicating the identified optimizations that may be performed and allow the programmer to select the optimizations that are to be performed. For example a graphical user interface may be provided that includes a listing of the optimizations with check boxes and virtual buttons that allow the programmer to select the particular optimizations to be performed and then initiate those optimizations through the compiler. Alternatively the optimizations may be completely left up to the programmer such that the present invention provides only the alert of the possible optimizations and leaves it to the programmer to decide whether to actually implement those optimizations or not.

The code for the computer program is then obtained step and determinations are made regarding the manner by which the compilation of the code may be performed to optimize the execution of the computer program based on the performance profile data step . These optimizations may then be presented to a programmer via one or more graphical user interfaces step . The optimizations selected by the system programmer are then received step and the code is compiled using the selected optimizations step . The operation then ends. Of course as noted above the optimizations may be performed automatically without contacting the programmer.

While the above embodiments have been described in terms of a single source of the performance profile data the present invention is not limited to such. Rather the performance profile data from a plurality of sources may be compiled into a single performance profile data set that may be used to optimize the compilation of the computer program. For example various traces of the computer program execution from a plurality of customers may be compiled into a single performance profile data set in order to address the various problems with the execution of the computer program on the different customer platforms in a single optimization of the computer program compilation.

Therefore using the mechanisms of the present invention the results of the use of the performance indicators counters flags and the like of the various embodiments of the present invention may be used to optimize the compilation of a computer program in order to obtain an optimum runtime execution of the computer program.

The above embodiments of the present invention are described in terms of the performance indicators being stored in the instructions themselves in a performance indicator shadow cache or the like. Moreover the above embodiments are described in terms of the counters being hardware counters. The present invention is not limited to such embodiments. In a further embodiment of the present invention elements of a page table may be used to store performance indicators and or counts of events.

A page table is a data structure in memory that provides a means of mapping virtual memory addresses to physical memory addresses permitting the virtualization of program memory. A page is a block of memory with which attributes e.g. read only read write cacheable can be associated. When instructions or data are to be retrieved from memory the processors uses the values stored in the page table to translate the address specified by the program e.g. the address computed by a load instruction or the address of the next sequential instruction into the physical address of the desired location in physical memory. Since the page table must be referenced to translate each program address to a physical address the page table is an ideal place in which to store performance indicators and or event counts.

In a further embodiment of the present invention the page table is expanded to include additional fields for each entry for storing performance monitoring structures such as performance indicators event counts thresholds ranges of addresses within the corresponding page that are of interest and the like. When a process accesses the page table to perform virtual to physical page address mapping these additional fields may be queried values from these fields retrieved and values in these fields updated based on the particular event causing the access to the page table.

Alternatively to avoid any degradation of performance the performance indicator information in these fields may be cached in processor resources similar to a Translation Look aside Buffer TLB or an Effective to Real Address Translation Buffer ERAT . For example a Performance Indicator Look Aside Buffer PILAB may be provided in which the virtual to real address translation information and the performance indicator information provided in the above fields of the page table may be cached. When an instruction or data address access request is received a lookup of the program or virtual address may be performed in the PILAB to obtain both the address translation information and the performance indicator information. If the program or virtual address is not present in the PILAB the page table may be consulted to obtain this information.

For example in a similar manner that performance indicators are associated with instructions and or portions of data as described above the performance indicators may be associated with these instructions and or data portions within the page table. Thus when determining whether an instruction or data portion has an associated performance indicator the virtual address of the instruction or data portion may be used to identify an entry in the page table and the values stored in the additional field and may be checked to see if a performance indicator is associated with the physical page or a portion of the physical page. That is if the offset associated with the virtual address falls within an offset range identified in field and the field has a performance indicator stored therein then the instruction corresponding to the virtual address has an associated performance indicator.

Similar to the hardware counters discussed above the field may be used to store an event count and may be incremented when certain events occur. For example in the above embodiments where cache misses result in the incrementing of a counter the count field may be used to store this count rather than or in addition to a physical counter. Thus for example when an instruction or portion of data must be retrieved from physical storage the page table is consulted to identify the physical storage location of the instruction or portion of data. At the same time the fields may be queried and the counter value in field may be incremented indicating the number of times the page must be fetched from physical storage and loaded into the memory or cache.

The field may be used to store threshold information for determining when to send interrupts to an interrupt handler of the performance monitoring application. As discussed above when an event occurs that results in the fields being accessed in the page table the value in the count field or a plurality of count fields may be compared against the threshold stored in the field to determine if the threshold has been met or exceeded. If so then an interrupt may be generated and sent to an interrupt handler of the performance monitoring application.

It should be appreciated that while shows only a single field for storing a performance indicator a single field for storing a count a single field for storing a threshold and a single field for storing a range of offsets into the page the present invention is not limited to such. Rather any number of fields for storing a plurality of performance indicators thresholds event counts ranges of offsets and the like associated with the physical page may be used without departing from the spirit and scope of the present invention.

A determination is made as to whether an event requiring access to physical storage occurs step . If so a determination is made as to whether the performance indicator field in the page table entry for the virtual address is set step . If so the counter field value or counter field values are incremented for the range of offsets in which the offset of the virtual address falls step .

A comparison of the counter field value or counter field values may then be made against corresponding threshold values in the threshold fields of the page table entry step . A determination may then be made as to whether the threshold is met or exceeded by the counter field value or counter field values step . If any of the threshold values are met or exceeded an interrupt may be generated and sent to the interrupt handler of the performance monitoring application step .

Of course in parallel to this operation the retrieval of the instructions data from the physical storage address location is performed. A determination is then made as to whether the operation is to terminate step . If not the operation returns to step otherwise the operation terminates.

Thus in this further embodiment of the present invention rather than requiring separate data structures or hardware devices the present embodiment allows for an extension of an already existing page table to include additional fields for storing performance monitoring structures. Since many events that are to be monitored by the performance monitoring application are closely tied to the accessing of physical storage the use of the page table to store these performance monitoring structures provides a less invasive solution to assisting the performance monitoring application in determining the performance of an execution of a computer program.

Thus the present invention provides an improved method apparatus and computer instructions for providing assistance in monitoring execution of programs and using the information obtained through monitoring the execution of the program to optimize the execution of the programs. The mechanism of the present invention includes employing an indicator that is recognized by the processor to enable counting the execution of an instruction associated with the indicator. Various types of counting as described above are enabled through this mechanism. Further with the information provided through the use of associating indicators with particular instructions the mechanism of the present invention also provides for various types of adjustments to programs in monitoring and analyzing performance of programs. Further as described above programs may be automatically adjusted to allow for monitoring of selected instructions and even routines and modules without having to modify the program.

It is important to note that while the present invention has been described in the context of a fully functioning data processing system those of ordinary skill in the art will appreciate that the processes of the present invention are capable of being distributed in the form of a computer readable medium of instructions and a variety of forms and that the present invention applies equally regardless of the particular type of signal bearing media actually used to carry out the distribution. Examples of computer readable media include recordable type media such as a floppy disk a hard disk drive a RAM CD ROMs and DVD ROMs. The computer readable media may take the form of coded formats that are decoded for actual use in a particular data processing system.

The description of the present invention has been presented for purposes of illustration and description and is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art. For example instead of using a field in an instruction or in a bundle a new instruction or operation code may be used to indicate that a subsequent instruction or a subsequent set of instructions are marked instructions. Also the architecture of a processor may be changed to include additional bits if spare fields for performance indicators are unavailable in the case in which it is desirable to include performance indicators within fields in the instructions. Also although examples of events such as execution of the instruction time such as clock or processor cycles needed to execute an instruction time to access data entry into a section of code have been given these examples are not meant to limit the present invention to the types of events that can be counted. Any event relating to execution of an instruction or access to a memory location may be counted using the mechanisms of the present invention.

The illustrative embodiments were chosen and described in order to best explain the principles of the invention the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

