---

title: Interactive transcription system and method
abstract: A method and system which seamlessly combines natural way of handwriting (real world) with interactive digital media and technologies (virtual world) for providing a mixed or augmented reality perception to the user is disclosed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08358320&OS=08358320&RS=08358320
owner: National University of Singapore
number: 08358320
owner_city: Singapore
owner_country: SG
publication_date: 20081103
---
The present application claims the benefit of and priority of U.S. Provisional Application No. 61 001 764 filed Nov. 2 2007 the entire contents of which is hereby incorporated by reference.

This invention relates generally to a multi sensory interactive learning environment and more particularly to a method and system which seamlessly combines natural way of handwriting real world with interactive digital media and technologies virtual world thus providing a mixed reality perception to the user.

Teachers play an important role in the social and cognitive development of children. However traditional teaching approaches put the focus on teachers and consider students as secondary as discussed in New media learning for children interact with color and tempo Liu K. R. Chen Y. S. Chen K. T. Chen H. S. Proc. IEEE International Conference on Sensor Networks Ubiquitous and Trustworthy Computing. Volume 2 Taichung Taiwan 2006 124 129. This results in lower learning efficiency since students may become passive and not fully attentive to the material. To improve learning among the diverse group of children the initiative must come from the learners rather than the teachers. This can be achieved by the use of novel teaching approaches integrating intuitive design and multimedia technologies. Advances in sensors and media technologies have made it possible to design real time human computer interface HCI applications suitable for the task. Such applications are receptive to the user s needs easy to use and offer multi sensory experiences.

Although the importance of human tutors cannot be denied in the cognitive and social development of children it can be enhanced with the use of interactive media technology to enrich the learning process. Emerging interactive media technologies have the potential to not only help young people learn but also engender a true love of learning as discussed in Children and interactive media a compendium of current research and directions for the future Wartella E. O Keefe B. Scantlin R. Markle Foundation 2000. Smart toys have been designed to stimulate creative and imaginative abilities in children as discussed in TSU.MI.KI Stimulating children s creativity and imagination with interactive blocks Itoh Y. et al. Proc. 2nd International Conference on Creating Connecting and Collaborating through Computing Kyoto Japan 2004 pages 62 70. Researchers have been working on making the learning process for children more intuitive by exploring new paradigms of interaction as discussed in Technologies for lifelong kindergarten. Educational Technology Research and Development Resnick M. 46 4 1998. There are also some haptics enabled learning environments as discussed in Haptic desktop for assisted handwriting and drawing Portillo O. Avizzano C. Raspolli M. Bergamasco M. Proc. IEEE International Workshop on Robot and Human Interactive Communication Nashville Tenn. USA 2005 512 17 but they require more expensive equipment. In the inventors previous work the inventors strived to bring the imaginative fantasy world of children in to physical reality by using augmented reality and tangible user interface TUI technologies as discussed in Magic cubes for social and physical family entertainment Zhou Z. Cheok A. D. Li Y. Kato H. CHI Extended Abstracts on Human Factors in Computing Systems Portland Oreg. USA 2005 1156 1157. This encouraged creative thinking and social interaction among children and their families.

Additionally attempts have been made as disclosed in existing patents claiming to teach and improve student s handwriting skills. These are U.S. Pat. No. 6 215 901 Pen Based Computer Handwriting Instruction and U.S. Pat. No. 5 730 602 Computerized Method and Apparatus for Teaching Handwriting . In U.S. Pat. No. 6 215 901B for example a pen based computing system is used to acquire the handwriting sample and claims to capture writing instrument or implement movement see patent file page 9 selection 65 over the sensor board tablet which is capable of translating the pen stroke information in to electronic signals. The tablet also transmits these signals to connected computer so that it can display the similar stroke on the computer display panel for user to view. Furthermore to properly display the strokes inventor clamps the paper in special position so as to match the orientation on the computer display panel. see patent page 9 1column of summary of the invention first five paragraphs .

Thus there is a need for a method and a system which addresses the limitations of conventional system and methods and seamlessly combines natural way of handwriting real world with interactive digital media technologies virtual world for providing a mixed reality perception to the user.

An aspect of the invention provides an interactive system for providing a mixed or augmented reality experience to a user the system comprising an object having a transcription surface the transcription surface for receiving a user s transcription a tracker having a known position and orientation relative to the transcription surface an image capturing device to capture an image of the tracker and the transcription surface in a first scene a microprocessor configured to track the position and orientation of the tracker within the image and to calibrate the tracker with the image capturing device and to track the transcription surface in the first scene by identifying the tracker the microprocessor configured to identify within the image any transcription made by the user on the transcription surface and retrieve multimedia content associated with the transcription made by the user the microprocessor configured to generate a second scene having the associated multimedia content superimposed on the first scene in a relative position to the tracker the microprocessor configured to provide a mixed or augmented reality experience to a user using the second scene.

In an embodiment the user s transcription may be a drawing a symbol a written character of the alphabet a plurality of written characters of the alphabet and or representative of a color. The user s transcription may form a word and the transcription may be in different forms such as printing cursive writing or the like. The object may be a whiteboard and the user transcribes the transcription with a pen. The system may further comprise a display to display the second scene at the same time the second scene is generated. The system may further comprise a device comprising the processor and display wherein the device is a computer mobile phone personal digital assistant PDA or a handheld device. The system may further comprise the camera integrated within a single device. The display and camera may be integrated together with the computer in a single device. The tracker may include a gyroscope. The microprocessor may be configured to generate a second scene having the associated multimedia content superimposed on the first scene in a relative position to the user s transcription.

In an embodiment the tracker may be in a different plane than the transcription surface relative to the image capturing device. The tracker and the transcription surface may be on the same plane relative to the image capturing device. The system may further comprise at least two trackers on the transcription surface each tracker having known position and orientation relative to the transcription surface. The at least two trackers may be tracked to identify the transcription surface for tracking the position and orientation of the transcription surface and one of the at least two trackers may continue to be tracked to identify the transcription surface for tracking the position and orientation of the transcription surface when another of the at least two trackers is occluded in the first scene. The system may comprise at least two transcription surfaces each surface having a tracker having known position and orientation to the transcription surface. The transcription surface may be planar or non planar.

An aspect of the invention provides a method for providing a mixed or augmented reality experience to a user the method comprising providing an object having a transcription surface the transcription surface for receiving a user s transcription providing a tracker having known position and orientation relative to the transcription surface capturing a first scene with the transcription surface calibrating an image capturing device in the first scene comprising an image of the transcription surface and the tracker tracking the position and orientation of the transcription surface for receiving a user s transcription and the tracker for positioning and orientating the transcription surface in the first scene by identifying the tracker identifying within the first scene any transcription made by the user on the transcription surface and the tracker for positioning orientating the transcription surface and the transcription retrieving multimedia content associated with the transcription made by the user and generating a second scene having the associated multimedia content superimposed on the first scene in a relative position to the marker and the user s transcription to provide a mixed or augmented reality experience to a user using the second scene.

In an embodiment the method may further comprise displaying the second scene on a display. The method may further comprise tracking at least two trackers on the transcription surface each tracker having known position and orientation relative to the transcription surface. The method may further comprise tracking the at least two trackers to identify the transcription surface for tracking the position and orientation of the transcription surface. The method may further comprise continuing to track one of the at least two trackers to identify the transcription surface for tracking the position and orientation of the transcription surface when another of the at least two trackers is occluded in the first scene. The method may further comprise providing at least two transcription surfaces each surface having a tracker having a known orientation to the transcription surface and capturing the at least two surfaces in the first scene.

An aspect of the invention provides a computer program product comprising of a computer readable medium for carrying computer executable instructions which when executed on a computer cause a system to perform operations comprising capturing a first scene with an image capturing device the first scene having the tracker and the transcription surface in the first scene image the transcription surface is on an object and the transcription surface is for receiving a user s transcription and the tracker has a known position and orientation relative to the transcription surface calibrating an image capturing device in the first scene comprising an image of the transcription surface and the tracker tracking the position and orientation of the transcription surface for receiving a user s transcription and the tracker for positioning and orientating the transcription surface in the first scene by identifying the tracker identifying within the first scene any transcription made by the user on the transcription surface and the tracker for positioning orientating the transcription surface and the transcription retrieving multimedia content associated with the transcription made by the user and generating a second scene having the associated multimedia content superimposed on the first scene in a relative position to the marker and the user s transcription to provide a mixed or augmented reality experience to a user using the second scene.

Opposed to the conventional inventions discussed above embodiments of the present invention strive to employ a cheap and more advanced solution to the given problem of handwriting instruction. In the present embodiments of the invention the inventors continue their efforts to develop tools more specifically tailored to children s education. In embodiments of the invention this is taken further by using mixed reality in conjunction with tangible user interfaces TUI to make the learning process a fun and exciting experience as discussed in the inventors previous work Magic cubes for social and physical family entertainment Zhou Z. Cheok A. D. Li Y. Kato H. CHI Extended Abstracts on Human Factors in Computing Systems Portland Oreg. USA 2005 1156 1157 a mixed reality system is discussed as mentioned above and the entire contents of which are incorporated herein by reference. In mixed reality the user has complete presence in the real world while he or she interacts with objects in the virtual world that overlap with the real world. A table showing the mixed reality real world and virtual world components a user experiences is shown in .

In embodiments of the invention by using everyday common tangible objects like white board and marker pen and other cheap handwriting scripting equipment may be used in embodiments of the invention. Moreover an ordinary webcam may be used in embodiments of the invention to capture and communicate the handwriting information supplied by the user to a connected computer system for handwriting analysis. Furthermore connected computer system uses mixed reality scenario to give feedback in terms of audio and visual display. Mixed reality as stated earlier is overlapping of the virtual world over the real world. Furthermore the system s visual response is not limited to animated characters or words but the system may also display corresponding three dimensional 3D visual illustration of the word. For example if user writes 3 green cars the system will respond by displaying and overlaying three green car models over user s physical work space along with the textual display of the written characters or words as shown in the flow chart figure of and the third person view of the system of an embodiment of the invention in on the connected display panel. Furthermore in an embodiment the system may respond by dynamically synthesizing the recognized text to speech for teaching the proper pronunciation of the written word. In the third person view of the system and the camera view of the second scene image is shown. The threshold and warped image in this embodiment is shown as hand written 3 green cars and the following character segmentation CCA and intelligent pre grouping is shown with segmented characters followed by recognition . Comparisons based on intelligent pre grouping and word compendium for example is shown with visual illustrative visual descriptive and the like of secondary scene image on display with any other output such as auditory or the like.

A discussion of the various aims and goals which embodiments of the present invention are intended to achieve is provided. Considerable portion of the children s daily activities is comprised of playing different games. The varying dynamics of the game play keeps them indulged in the activity for hours. In contrast practicing the writing skills in traditional way of character template tracking seems less exciting and more routine. To overcome these shortcomings of traditional approaches embodiments of the invention provide a system that augments the learning time with play time. More specifically the proposed design of an embodiment of the invention provides improved teaching scenarios in which children feel enthusiastic and take initiative towards learning. To achieve this stated goal it is necessary for novel teaching approaches integrating intuitive design and multimedia technologies. An embodiment of the invention is a novel media rich text graphics video audio system which brings the virtual world to children s perceptive reality by merely interpreting handwritten words. In this embodiment of the system users are encouraged to hand write the words for the system to respond. In an embodiment the user can write the name of a physical object to see corresponding computer generated model in mixed reality. The user such as a child can then color these models with different handwritten commands. In an embodiment the directive is given in the form of handwritten color name. The user can also create multiple copies of the model by writing numerical identities. A term for this interface is What you write is what you get WYWIWYG as a counterpart to the inventors previous work What you say is what you get WYSIWYG as discussed in Multi sensory musical entertainment systems Zhou Z. Cheok A. Liu W. Chen X. Farbiz F. Yang X. Haller M. IEEE Transactions on Multimedia 11 3 2004 88 101 which is incorporated herein by reference.

The camera takes an image of an image area shown by dashed oval to capture a first image or scene. The object within the image area has a transcription surface with a marker or tracker . The calibration of the camera with respect to the tracker is performed as shown by dashed line . The image area may take any configuration other than oval such as square rectangular etc. and may not necessarily include the entire surface of the transcription surface the object . The display shows a second image or scene that includes the first image or scene with descriptive and illustrative model elements superimposed over the first scene.

The tracker or marker is shown as a fiducial marker. Such a marker and calibration process is described in the inventors previous works such as US patent application publication nos. 2005 0276444 2005 0288078 and 2005 0264555 the entire contents of each are all incorporated herein by reference. The tracker may also incorporate other positioning systems such as global positioning system GPS tracking system gyroscope based systems infrared tracking systems and the like to measure and maintain orientation and position between the tracker and the camera.

In an embodiment the transcription surface may be any surface of an object capable of capturing a user s transcription. For example a transcription surface may be any writing surface that may be marked or surface that can sense that allows a user to input a transcription. For example a touch screen stylus screen may sense the input to display the transcription. In such an embodiment the image capture device captures the display of the transcription on the screen. Of course other means of marking a transcription that may be captured by an image capture device may be implemented such as for example by a marker such as a white writing board and marker ink pens black board and chalk paper and pen paper and brushes with water paint or the like.

In an embodiment the system may be responsive to basic three categories noun color and digit and may be extended by incrementing the database with more categories such as for example actions. In addition to visual response users such as children may also receive auditory response from the system thus teaching the user pronunciation of the word digit and color they have written. An embodiment of the invention is referred to as What you write is what you get WYWIWYG which aims at improving the way users such as children learn. Accepting simple handwritten directives by children the system may be configured to respond with multimodal visual and auditory feedback in mixed reality space. In an embodiment regarding auditory feedback proper pronunciation of the written words is synthesized in to digital format. It is then outsourced to the user through connected speaker. In an embodiment of the invention there are two different visual feedbacks that the system can offer namely illustrative and descriptive. For illustrative feedback system overlay s 3D animated graphics over user s physical work space. For descriptive feedback computer generated text corresponding to recognized words is displayed in virtual space augmenting the real world work space. shows WYWIWYG realm over real and virtual horizons. The system takes the physical handwriting written on the transcription surface and registers multimedia responses with user s physical work space thus providing mixed reality perception. WYWIWYG offers pen based interaction in mixed reality environment. By using every day tangible objects like a maker pen and a white board the system offers an intuitive interface that provides a multi sensory response. A user handwrites the words by using the pen. These words are interpreted by the system as a written directive by the user. The system may accordingly responds by auditory digitized pronunciation of the word written and visual feedback 3D animated graphics and recognized text in digital format . The user can then move the writing board to view the 3D graphics object from different angular perspectives. The user can also interact with the object by handwriting different color names to change the color of an object. The user can also clone the object by writing digits on the board. shows an embodiment of the actual system setup and practical interaction scenario from a third person view of the system.

With this motivation a media rich text graphics video system and application is provided which combines the flexibility of the virtual world with a handwriting recognition engine to create a tangible mixed reality learning environment. In an embodiment WYWIWYG aims on improving the writing skills of children by focusing on handwriting improvements noun to 3D physical representation of the object and proper pronunciation of the words and digit. Unlike other conventional handwriting applications where aim is to achieve higher recognition rate by avoiding cursive writing our approach encourages users such as children to write properly for the system to interpret and translate the writings to multimedia responses. An embodiment of the WYWIWYG system has been compared against traditional medium like books writing coloring and word books tablet PC legacy applications like Microsoft s Paint as shown in the table of . gives the breakdown of the comparisons in multicolumn chart format. WYWIWYG offers highly intuitive interface using affordance of marker pen and common writing board. In contrast legacy applications on desktop PCs like Microsoft Paint use input peripherals for example keyboard or mouse and the like to interact with the writing drawing canvas. The WYWIWYG system also offers dynamic interactivity with multimedia contents for example like changing colors cloning objects and 3D view of the graphical contents and the like in contrast to other mediums. In case of tablet the PC only recognizes the text that is displayed. While in case of books and legacy applications interactions are performed in terms of passively writing or drawing on the canvas. WYWIWYG gives superior feedback and high reusability.

In traditional approaches for practicing handwriting students are often asked to trace the character templates in the book. Once they track these templates there is no automatic feedback positive or negative to the writing. In contrast WYWIWYG gives an auditory response and a textual feedback. Also character templates can only be used for limited number of times corresponding to how many copies of single character are in the book to trace and how many time it can be re traced after erasing last trace . In contrast WYWIWYG offers high re usability. By using white board and marker pen children can write words as many times as they like without the need of replacing the board. By giving 3D graphics and animation response coupled with digital audio preliminary user studies of an embodiment of the invention have shown significant potential of the system to aggrandize concentration and incitement towards learning to write properly. In an embodiment of the invention users such as children may use normal writing surfaces such as a white writing board and marker pens black board and chalk paper and pen and the like to write the name of an object. The physical object then appears as a 3D model in a virtual world. The virtual world is augmented with a real world scene to give a consistent mixed reality perception. Now users such as children can play with the virtual object existing in perceived reality by coloring it with different colors and making multiple copies of the object using handwritten commands. Embodiments of the invention offer cost effective solution and can be deployed on a desktop PC or a notebook without the need for special equipment.

Possible industrial applications include next generation teaching systems extended versions of WYWIWYG can serve as multiplayer gaming platforms and the like. Other applications may also include a communication aid and interface for a disabled user search and retrieve interface for accessing information on the screen by writing the desired information within a database or network such as the internet a command interface for controlling systems and methods by writing the desired command for the system or method an idea generation interface for designers to sketch and share 2D 3D information or the like.

Since WYWIWYG is a classifier based system the response of the system depends on the handwriting of the user. In one embodiment the classifier has been trained with handwriting samples from different writers. Of course it will be appreciated that the classifier may be trained with commercially available handwriting sample database which in turn will give better results in terms of recognition.

In embodiments more than one tracker may be provided to aid in the occurrence that one of the trackers is covered up by the user for example with the user s hand. In this instance the system continues to track the tracker and the transcription surface that is not covered up and that is still visible in the first scene captured by the image capture device. Similarly in embodiments there may be more than one transcription surface . In embodiments one tracker may have a known relationship such as position or orientation with each transcription surface or the system may be configured with a tracker or at least one tracker for each transcription surface. show the known position and orientation of the transcription surface relative to the tracker configured with a single tracker multiple trackers single transcription surface or multiple transcription surfaces on the same or different planes in accordance with embodiments of the invention. It would be apparent to have different embodiments with different configurations of trackers and transcription surfaces the position and orientation of which would be tracked in real time within the system.

In an embodiment the first step is capturing image frames of 640 480 resolutions with a webcam. The captured image is in 24 bit 3 channel red green and blue RGB format. It is converted to binary image using dynamic threshold 9 for subsequent processing. Since the writing area is not clearly visible in the acquired image perspective view from camera a perspective warp is applied to the image to get the orthogonal view of the writing area. This is done by locating the writing area corners four points relative to fiducial marker and warping the image area bounded by a rectangle comprised of these four corners. It will be appreciated that this may be achieved with less or more points than four points as discussed in this embodiment. For example at least three points may be used to define the transcription surface or plane of the transcription surface. Additionally in other embodiments more than one tracker

Text detection is triggered using simple hand occlusion mechanism. Since hand color is considerably different then the white board and in threshold image its darker than back ground. The standard deviation of the writing area is then computed. If the standard deviation is large enough greater than defined threshold we trigger the recognition sequence as shown in otherwise tracking continues for the hand occlusion.

Once the text detection is triggered image undergoes a series of pre processing steps before input to the recognition engine. The image is forwarded to character level segmentation. This segmentation is done by Connected Component Analysis CCA as discussed in A threshold selection method from grey level histograms Otsu N. IEEE Transactions on Systems Man and Cybernetics 9 1 1979 62 66 which is incorporated herein by reference followed by the intelligent Pre Grouping of characters. Characters may be grouped under the alphabet and digit categories by measuring the distance between the centers of gravity of adjacent character areas. This Pre Grouping of characters along with the word compendium word database corresponding to number of different models that system can load and display builds up the knowledge base which is used in the post processing stage to get the final recognition results in terms of digit color and noun. Part of knowledge base Pre Grouping is updated each time a new word or digit is written by the user. The word compendium part of knowledge base remains static unless updated in offline mode by incrementing the word database and adding corresponding 3D model for display .

Once the characters are extracted they are normalized to 32 32 pixels using bilinear interpolation. Normalized characters are then formatted saved in text file for processing by the recognition engine. Since we already have the Pre Grouping of character identification of the possible digit and alphabets we save alphabets and digits in separate files The mode of saving is writing all the 1024 pixels 32 rows 32 columns of character in linear sequence as follows

First column is to explain the format and is not part of the saved text file. It shows how each character image is saved in row by row fashion. X is total number of characters in any given file Where category is by default set to 1 for all segmented characters images. This is supplied dummy category to recognition engine see recognition . A is the pixel value either 0 or 1 since it s a binary image .

A C SVC type vector machine may be used in an embodiment of the invention for example as disclosed in LIBSVM a library for support vector machines Chang C. Lin C. 2001 which is incorporated herein by reference. In an embodiment the C SVC type vector machine is with RBF kernel support as a recognition engine. The performance of the SVM model depends on the fine tuning of its parameters C error cost and kernel parameter . These parameters are selected with the help of fold cross validation technique where v denotes the number of subsets the training data is divided into. First a coarse grid search is performed with parameters C and having wide range of values with exponential steps 2 2 . . . 2 2 . The parameter values are selected with the highest cross validation rate and perform a finer grid search in the neighborhood of these values. This ensures the best possible recognition rates in online recognition mode. Once the best parameters are found the recognition engine is trained with the complete training data set. The recognition engine may be trained for digits and alphabets separately. For example shows a table of characteristics and performance of text recognition rates for both alphabets and digits. In an embodiment two trained model files for the recognition engine to load may be trained when in online mode recognition mode . Recognition engine SVM loads the saved character image files and stores the corresponding classification either on disk in file or in array in program.

The output of the recognition engine is a 1 N prediction matrix corresponding to each character detected in the input text. To achieve good recognition results at the word level we cross reference the prediction matrix with the knowledge base. First the prediction matrix is divided into digit and alphabet groups using the Pre Grouping from before. Then the alphabets are further classified into colors and nouns by comparing them with the available word compendium. This results in the final recognition in terms of number color and noun.

Once the final recognition is done corresponding system responses are invoked. In case of auditory response system translates the recognized text to digitized audio and plays the audio through connected speakers. There are two kinds of visual responses illustrative and descriptive. For descriptive feedback computer generated text corresponding to recognized words is displayed in virtual space augmenting the real world work space.

For illustrative response system overlay s 3D animated graphics model over user s physical work space the writing board . If only the color is present the color of the default model a tea pot is changed. If only the noun is present the corresponding 3D model is loaded in default color white . Digit recognition clones the virtual model according to the number recognized. The 3D models are in wavefront geometry definition format as discussed in On visual similarity based 3D model retrieval Chen D. Y. Ouhyoung M. Tian X. P. Shen Y. T. Ouhyoung M. Proc. Eurographics Granada Spain 2003 223 232 which is incorporated herein by reference and are rendered using MXR Toolkit as discussed in a website having a hypertext transfer protocol http of mxrtoolkit.sourceforge.net which is the inventors open source mixed reality application programming interface API .

In an embodiment of the invention an objective of the mixed reality environment may be to serve as an interactive learning tool and input in terms of handwritten characters is encouraged thus facilitating the goal of learning while playing. In an embodiment the system displays multiple instances of a plurality of models corresponding to a plurality of nouns in the word compendium and also a plurality of colors. Recognition and display are done in real time which is necessary for the user to have a consistent mixed reality perception. Real time online recognition results of an embodiment is shown in . The tracker is shown in the display scene shows default model of a teapot and the transcription is shown in display scene with real time change of color of multimedia content or change in a default parameter i.e. the teapot changes color to purple after the transcription purple is written by a user on the transcription surface shown in . The multimedia content changes or the model changes on the display screen to tank after the transcription tank is hand written by a user on the transcription surface shown in . In the media content changes or number and model changes to nine yellow cars on the display scene after 9 yellow cars is hand written by a user on the transcription surface. The processing speed of the system for example may be 10 frames per second. The average recognition time in an embodiment of the system is 400 msec on a current Pentium D. The system may be developed in C using MXR Toolkit and OpenCV as discussed in Open Computer Vision Library Open CV in a website having a hypertext transfer protocol http of www.opencvlibrary.sourceforge.net.

In other modes of practice of embodiments of the invention it will be appreciated that more categories may be added to the WYWIWYG system. In an embodiment three categories may include namely noun color and digit. Other categories may be added including action such as 3 green cars running . Furthermore it can be extended to multiuser interaction. In an embodiment the system supports single user interface. In another embodiment a multi user scenario system may be used as a gaming platform where each contender has power to design their own contents by merely handwriting.

It will be appreciated that the transcription may be by writing with a pen on the transcription surface or plane such as a marker board as described above however any mode of transcription may be implemented in embodiments of the invention. For example instead of a writing implement such as a pen transcription may be performed by any device that marks the surface of the transcription surface. Such transcriptions devices may include markers stamps or the like. Likewise transcription surfaces may be configured to have different shapes as long as the shape is known. The transcription surface may be planar curved and the like. The material of the surface may be different materials that are capable of capturing a transcription such as a white marker board black board paper and the like and the material of the transcription device is a material that is compatable and or response to the material of the transcription surface that allows the image capturing device to capture the transcription i.e. any surface that is transcriptable. The act of transcribing may be by writing stamping marking and the like a transcription for example a symbol character letters words numbers or the like onto the surface of the transcription surface which is recognized by the recognition module of the system to access and display the desired multi media content that is associated with the transcription.

In the embodiment discussed above the system is a personal computer with a stand alone image capturing device a stand alone display and stand alone transcription surface however it will be appreciated that any combination of the system components may be configured as stand alone and or integrated in a single device. Additionally the system components may be located remotely from each other and even connected via a network such as a local area network LAN or a public network such as the Internet. The system components may be connected to each other in a wired or wireless means. In embodiments of the invention the image capturing device is arranged to be in sight of the transcription surface. The processing computer system the display and the stand alone image capturing device may be in a single integrated device such as a mobile phone personal digital assistant PDA handheld devices and the like. In embodiments the display may be formed integral to the processing computer device and the image capture device and the transcription surface may be remote to the display and computer device. The image capture device and the transcription surface are calibrated and the image capture device is configured to capture an image of the transcription on the transcription surface. The image capture device is calibrated relative the tracker and the transcription surface within an image area of the first scene and the transcription is also within the image area of the first scene.

In an embodiment the transcription surface and the tracker may be located on different planes relative to the image capture device as shown in . The calibration of the image capture device and the transaction surface may be made with respect to the tracker and as long as the relative orientation and other parameters such as distance shape and like are known to determine the position of the transcription surface relative the tracker. The transcription surface may be different configurations such as planar as shown in non planar as shown in or the like. Once the tracker is calibrated with the image capture device the image capture is also calibrated with the image capture device as long as the components are known to determine the orientation and position of the transcription surface relative to the tracker. If the position of the transcription surface is known then the position of the transcription surface relative to the image capture device is known once the image capture device is calibrated with the tracker.

While embodiments of the invention have been described and illustrated it will be understood by those skilled in the technology concerned that many variations or modifications in details of design or construction may be made without departing from the present invention.

