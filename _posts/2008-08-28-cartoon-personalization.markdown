---

title: Cartoon personalization
abstract: Embodiments that provide cartoon personalization are disclosed. In accordance with one embodiment, cartoon personalization includes selecting a face image having a pose orientation that substantially matches an original pose orientation of a character in a cartoon image. The method also includes replacing a face of the character in the cartoon image with the face image. The method further includes blending the face image with a remainder of the character in the cartoon image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08831379&OS=08831379&RS=08831379
owner: Microsoft Corporation
number: 08831379
owner_city: Redmond
owner_country: US
publication_date: 20080828
---
This application claims priority to U.S. Provisional Patent Application No. 61 042 705 to Fang et al entitled Cartoon Personalization System filed Apr. 4 2008 and incorporated herein by reference.

Computer users are generally attracted to the idea of personal identities in the digital world. For example computer users may send digital greeting cards with personalized texts and messages. Computer users may also interact with other users online via personalized avatars which are graphical representations of the computer users. Thus there may be growing interest on the part of computer users in other ways of personalizing their digital identities.

This Summary is provided to introduce a selection of concepts in a simplified form that is further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Described herein are embodiments of various technologies for integrating facial features from a photographic image into a cartoon image. This integration process is also referred to as cartoon personalization. Cartoon personalization may enable the creation of cartoon caricatures of real individuals. Generally speaking the process of cartoon personalization involves the selection of a photographic image and replacing facial features of a cartoon character present in the cartoon image with the facial features from the photographic image.

In one embodiment cartoon personalization includes selecting a face image having a pose orientation that substantially matches an original pose orientation of a character in a cartoon image. The method also includes replacing a portion of the character in the cartoon image with the face image. The method further includes blending the face image with a remainder of the character in the cartoon image. Other embodiments will become more apparent from the following detailed description when taken in conjunction with the accompanying drawings.

This disclosure is directed to embodiments that enable personalization of a cartoon image with features from real individuals. Generally speaking cartoon personalization is the process in which facial features of a cartoon character in a cartoon image are replaced with the facial features of a person as captured in a photographic portrait. Nevertheless the process may also involve the replacement of other parts of the cartoon image with portions of photographs. For example additional content of a cartoon image such as but not limited to bodily features clothes cars or even family pets may be replaced with similar content captured in actual photographs. The integration of a cartoon image and a photograph may enable the generation of custom greeting material and other novelty items.

The embodiments described herein are directed to technologies for achieving cartoon personalization As described the cartoon personalization mechanisms may enable the automatic or semi automatic selection of one or more candidate photographs that contains the desired content from a plurality of photographs dynamic processing of the selected candidate photograph for the seamless composition of the desired content into a cartoon image and integration of the desired content with the cartoon image. In this way the embodiments described herein may reduce or eliminate the need to create cartoon personalization by using manual techniques such as but not limited to cropping adjusting rotating and coloring various images using graphics editing software. Thus the embodiments described herein enable the creation of cartoon personalization by consumers with little or no graphical or artistic experience and ability. Various examples of cartoon personalization in accordance with the embodiments are described below with reference to .

The cartoon personalization system may receive one or more source cartoon images from cartoon providers . Cartoon providers may include professional and amateur graphic artists and animators. The one or more source cartoon images are generally artistically created pictures that depict cartoon characters or subjects in various artificial sceneries and backgrounds. In various embodiments the cartoon image may be a single image or a still image from a video clip.

In the exemplary system the source images and the source cartoon images may be transferred to a cartoon personalization engine via one or more networks . The one or more networks may include wide area networks WANs local area networks LANs or other network architectures. However in other embodiments at least one of the source images or the source cartoon images may also reside within a memory of the cartoon personalization engine . Accordingly in these embodiments the cartoon personalization engine may access at least one of the source images or the source cartoon images without using the one or more networks .

The cartoon personalization engine is generally configured to integrate the source images with the cartoon images . According to various embodiments the cartoon personalization engine may replace the facial features of a cartoon character from a cartoon image with a face image from a source image . The integration of content from the source image and the cartoon image may produce a personalized cartoon .

The memory may store program instructions. The program instructions or modules may include routines programs objects components and data structures that perform particular tasks or implement particular abstract data types. The selected program instructions may include a pre processor module a selection module a fitter module a user interface module and a data storage module .

In turn the pre processor module may include a face detection engine a face alignment engine and face tagging engine . The selection module may include a face pose engine and a face filter engine . The fitter module may include a geometry engine and a blending engine . In some embodiments the exemplary cartoon personalization engine may include other components associated with the proper operation of the engine.

The face detection engine may implement various techniques to detect images of faces in photographs. In various embodiments the face detection engine may implement an integral image technique for the detection of faces in photographs.

The aforementioned integral image technique implemented by face detection engine includes the use of integral images that enables the computation of features for face detection. The integral image technique also involves the use of a met algorithm known as the AdaBoost learning algorithm to select a small number of critical visual features from a very large set of potential features for the detection of faces. The integral image technique may be further implemented by combining classifiers in a cascade to allow background regions of the photographic images that contain the faces to be quickly discarded so that the face detection engine may spend more computation on promising face like regions.

In some embodiments the integral image technique may include the computation of rectangular features. For example the integral image at location x y contains the sum of the pixels above and to the left of x y inclusive 1 where ii x y is an integral image and i x y is an original image. Using the following pair of recurrences 1 2 1 3 where s x y is the cumulative row sum s x 1 0 and ii 1 y 0 the integral image may be computed in one pass over the original image. Using the integral image any rectangular sum may be computed in four array references. Moreover the difference between two rectangular sums may be computed in eight references. Since the two rectangle features as defined above involve adjacent rectangular sums they can be computed in six array references eight in the case of the three rectangle features and nine for four rectangle features.

The implementation of the AdaBoost algorithm may involve the restriction of a weak learner that is a simple learning algorithm to a set of classification functions where each function depends on a single feature. Accordingly the weak learner may be designed to select the single rectangle feature which best separates the positive and negative examples. For each feature the weak learner may determine the optimal threshold classification function such that the minimum number of examples that are misclassified. Thus a weak classifier h x f p may consist of a feature f a threshold and a polarity p indicating the direction of the inequality 

Furthermore the weak classifiers may be implemented in a cascade in an algorithm to achieve increased detection performance while reducing computation time. In this algorithm simpler classifiers may be used to reject the majority of sub windows before more complex classifiers are called upon to achieve low false positive rates. Stages in the cascade may be constructed by training classifiers using the AdaBoost learning algorithm. For example starting with a two feature strong classifier an effective face filter can be obtained by adjusting the strong classifier threshold to minimize false negatives. The initial AdaBoost threshold 

Accordingly the overall form of the face detection process is that of a degenerate decision tree also referred to as a cascade. A positive result from the first classifier triggers the evaluation of a second classifier which has also been adjusted to achieve very high detection rates. A positive result from the second classifier triggers a third classifier and so on. A negative outcome at any point leads to the immediate rejection of the sub window.

In other embodiments the face detection engine may employ a multiview technique for the detection of faces in photographs. The multiview technique includes the use of a three step face detection approach in combination with a two level hierarchy in plane pose estimator to detect photographs containing facial features.

The three step detection approach may include a first step of linear filtering. Linear filtering as implemented in the multiview technique may include the use of the AdaBoost algorithm previous described. For example given x y . . . x yas the training set where y 1 1 is the class label associated with example x the decision function may be the following 5 where b and r 1 1 are the coefficients which could be determined during the learning procedure. The first term in decision function 5 is a simple decision stump function which may be learned by adjusting threshold according to the face non face histograms of this feature. The parameters in the second term could be acquired by a linear support vector machine SVM . The target recall could be achieved by adjusting bias terms bin both terms.

The three step detection approach may also include a second step of implementing a boosting cascade. During the face detection training procedure windows which are falsely detected as faces by the initial classifier are processed by successive classifiers. This structure may dramatically increase the speed of the detector by focusing attention on promising regions of the image. In some embodiments efficiency of the boosting cascade may be increased using a boosting chain. In each layer of the boosting cascade a classifier is adjusted to a very high recall ratio to preserve the overall recall ratio. For example for a 20 layer cascade to anticipate overall detection rates at 96 in the training set the recall rate in each single layer may be 99.8 square root over 0.96 0.998 on average. However such a high recall rate at each layer may result in decreasing sharp precision. Accordingly a chain structure of boosting cascades may be implemented to remedy the decreasing sharp precision.

Further during each step of the boosting chain false rates may be reduced by optimization via the use of a linear SVM algorithm. The optimization is obtained by the linear SVM algorithm that resolves the following quadratic programming problem 

The three step detection system may also include a third step of post filtering. The posting filtering may include image processing and color filtering. Image processing may alleviate background lighting and contrast variations. It consists of three steps. First a mask which is generated by cropping out the four edge corner from a window is applied to the candidate region. Second a linear function may be selected to estimate the intensity distribution on the current window. By subtracting the plane generated by this linear function the lighting variations could be significantly reduced. Finally histogram equalization is performed. With this nonlinearly mapping the range of pixel intensities is enlarged and thus improves the contrast variance which caused by camera input difference.

Color filtering may be achieved by adopting a YCCspace where the Y mainly represents image grayscale information that is relevant to skintone color and the CCcomponents are used for false alarm removal. Since the color of face and non face images is distributed as nearly Gaussian in the CCspace a two degree polynominal function may be used as an effective function. For any point c c in the space C C the decision function can be written as sign 8 which is a linear function in the feature space with dimension c cc ccc . Consequently a linear SVM classifier is constructed in this five dimensional space to separate skin tone color from the non skin tone color.

For each face training sample the classifier F c c may be applied to each pixel of face image. Statistics results can therefore be collected where the grayscale value of each pixels corresponding to its ratio to be skin tone color in the training set. Thus the darker the pixel the less possible it may be a skin tone color.

In some embodiments the three step face detection may be further combined with a two level hierarchy in plane pose estimator to detect photographs containing facial features. The two level hierarchy in plane pose estimator may include an in plane orientation detector to determine the in plane orientation of a face in an image with respect to the upright position. The pose estimator may further include an upright face detector this is capable of handling out plane rotation variations in the range of 45 45 .

The in plane pose estimator includes the division of into three sub ranges 45 15 15 15 15 45 . Further the input image is in plane rotated by 30 . In this way there are totally three images including the original image and each corresponds to one of the three sub ranges respectively. Third in plane orientation of each window on the original image is estimated. Finally based on the in plane orientation estimation the upright multiview detector is applied to the estimated sub range at the corresponding location. In various embodiments the pose estimator may adopt the coarse to fine strategy. The full range of in plane rotation is first divided into two channels and each one covers the range of 45 0 and 0 45 . In this step only one Haar like feature is used and results in the prediction accuracy of 99.1 . Subsequently a finer prediction based on AdaBoost classifier with six Haar like features may be performed in each channel to obtain the final prediction of the sub range.

It will be appreciated that while some of the face detection techniques implemented by the face detection engine have been discussed the face detection engine may employ other techniques to detect face portions of photographic images. Accordingly the above discussed face detection techniques are examples rather than limitations.

The face alignment engine may be configured to obtain landmark points for each detected face from a photographic image. For example in one embodiment the face alignment engine may obtain up to 87 landmarks for each detected face. These landmarks may include 19 landmark points for a profile 10 landmark points for each brow eight landmarks for each eye 12 landmarks for a nose 12 landmark points for outer lips and 8 for inner lips.

In various embodiments the face alignment engine may make use of the tangent shape approximated algorithm referred to as the Bayesian Tangent Shape Model BTSM . BTSM makes use of a likelihood P y x that is a probability distribution of the grey levels conditional on the underlying shape where represents the pose parameters x represents the tangent shape vector and y represents the observed shape vector. Assume yis the shape estimated in the last iteration by updating each landmarks of ywith its local texture y the observed shape vector may be obtained. The distance between observed shape y and the rue shape may be modeled as an adaptive Gaussian as 9 where y is the observed shape vector s is the scale parameter. Moreover 

Subsequently the posterior of model parameters b s c may be computed given the observed shape of y where b represents the shape parameters. If the tangent shape x is known the face alignment engine may implement an expectation maximization EM based parameters estimation algorithm.

For example given a set of complete data x y the complete posterior of the model parameters is a product of the following two distributions exp 1 2 10 exp 1 2 11 where X x x e e and s sin s sin c c . Accordingly by taking the logarithm and the conditional expectation the following equation may be obtained 

Subsequently the face alignment engine may maximize the Q function over model parameters. The computation of the derivations of the Q function may provide 

It will be appreciated that while some of the face alignment techniques implemented by the face alignment engine has been discussed the face alignment engine may employ other techniques to obtain landmark points for each detected face. Accordingly the above discussed face alignment techniques are examples rather than limitations.

The face tagging engine may be configured facilitate the selection of detected and aligned faces from photographic images by grouping similar face images together. For example the face tagging engine may group face images of the same individual as present in the different photographic images into one group for identification and labeling.

In various embodiments the face tagging engine may use a three stage technique to group similar face images. The stages may include offline pre processing online clustering and online interactive labeling.

In the pre processing stages facial features from detected and aligned face images may be extracted. The extraction may be based on two techniques. For example the facial features may be extracted from various face images using the local binary pattern LBP feature a widely used feature for face recognition. In another example facial feature extraction may be based on the recognition of scene features as well as cloth contexture features and Color Correlogram features extracted from the human body areas present in the photographic images.

In the online clustering stage the face tagging engine may apply a spectral clustering algorithm that handles noise data to the LBP features scene features and cloth features to group similar face images into groups. In some embodiments the face tagging engine may create a plurality of groups for the same person to account for face diversity.

In the online interactive stage the face tagging engine may ranking the face images in each group according to a confidence that each face image belongs in the group. For example the face tagging engine may use a first clustering algorithm that reorders the faces so that the faces are ordered from the most confident to the least confidence in a group. In another example the face tagging engine may use a second clustering algorithm that orders different groups of clustered face images according to confidence level.

The face tagging engine may interact with the user interface module . In some embodiments the face tagging engine may have the ability to dynamically rearrange the order of face images and or the groups of face images according to user input provided via the user interface module . The face tagging engine may implement the rearrangement using methods such as linear discriminate analysis support vector machine SVM or simple nearest neighbor. Moreover the face tagging engine may be further configured to enable a user to annotate each face image and or each group of face images with labels to facilitate subsequent retrieval or interaction.

It will be appreciated that while some of the face grouping and ranking techniques implemented by the face tagging engine has been discussed the face tagging engine may employ other techniques to create and rank groups of similar face images. Accordingly the above discussed face tagging techniques are examples rather than limitations.

The face pose engine may be configured to determine the pose orientation also commonly referred to as face orientation of the face images. Pose orientation is an important feature in face selection. For instance if the pose of the selected face is distinct from that of the cartoon image the synthesized result may look unnatural.

In order to calculate the pose orientation the face pose engine may first estimate the symmetric axis in a 2D face plane of the face image. Suppose is the in plane rotation of a face and C is the shift parameter then and C may be estimated by optimizing the following symmetric criteria circumflex over circumflex over argmin cos sin 18 where L is the set of landmarks in the left nose and mouth p x y 1 is a point in L and p is the corresponding point of pin the right part of the face. The points in the nose and mouth to estimate the symmetric axis may be especially ideal for the calculation of pose orientation as points far away from the symmetric axis tend increase the amount of effect caused by 3D rotation.

Following the calculation of circumflex over from criteria 18 the distance between the symmetric axis and points in the left and right face are calculated as 19 where cos circumflex over sin circumflex over and L and R are the set of points excluding nose and mouth in left and right respectively. The final pose orientation may be calculated as 

The face filter engine may be configured to filter out face images that are not suitable for transference to a cartoon image. In various embodiments the face filter engine may filter out face images that exceed predetermined brightness and or darkness thresholds. Faces that exceed the predetermined brightness and or darkness thresholds may not be suitable for integration into a cartoon image. For example when a face from a photographic image that is too dark is inserted into the head of a character in a cartoon image the face may cause artifacts. Moreover since faces with size much smaller than that of the cartoon template may cause blurring in the synthesis result the face filter engine may also filter out face images that are below a predetermined resolution.

In some embodiments given that ldenotes an illumination of point i on a face image and the mean value of illumination of all points in the face may be denoted as l the brightness of the face may be expressed as 

In other embodiments given that the width and height of a cartoon image are wand h and the width and height of the face are wand h. For a given ratio s face filtering based on resolution may be implemented as follows if w w s h h s the face filter engine may designate the face as having an acceptable resolution. Otherwise the face filter engine may be filter out due to the lack of proper resolution.

The geometry engine may be configured to geometrically fit a selected face image to a cartoon image. In other words the geometry engine may integrate a face image into a cartoon image by changing the appearance of the face image.

The geometry engine may implement a two part algorithm to blend a selected face into a cartoon image. The first part may include determine the place in the cartoon image that the face should be positioned. The second part may include estimating the transformation of the face that is warp the face image to matched the cartoon image.

In various embodiments the geometry engine may estimate the affine transform matrix via the following equation argmin 22 where A is a 3 3 affine matrix M is the set of landmarks in the face image pis a landmark in the face and p is the corresponding point in the cartoon image where pand p are represented in a homogenous coordinates system. After estimating the affine transformation for a point p in the selected face the corresponding coordinate in the cartoon image p p.

It will be appreciated that while some of the geometric fitting techniques implemented by the geometry engine have been discussed the geometry engine may employ other techniques to geometrically fit a face image into a cartoon image. Accordingly the above discussed face alignment techniques are examples rather than limitations.

The blending engine may be configured to blend a face image and a host cartoon image following the insertion of face image into the cartoon image. To conduct appearance blending the blending engine may calculate a face mask indicating the region where the face is inserted. The face mask may be obtained by calculating a convex hull of face image landmarks. In order to accomplish appearance blending the blending engine may perform three operations. The first operation includes the application of shading to the integrated image. The second operation includes illumination blending which may make the illumination appear natural in the integrated image. The third operation includes color adjustment which adjusts the real face color to match the face color in the original cartoon image.

In various embodiments the blending engine may convert a color RGB image into a L a b color space. The L channel is the illumination channel and a b are the color channels. In these embodiments shading application and illumination blending may be performed on the L channel while color adjustment is performed on the a b channels.

Since photographs may be taken in different environments the face images in those photographs may have different shading than the faces in cartoon images. In order to match the shading of a face image from a photograph with the shading of a face in a cartoon image as well as make the overall integrated image look natural the blending engine may implement cartoon shading on the face image. In one embodiment the blending engine may sample the cartoon image to obtain skin color information. For example illumination of the cartoon image as depicted by the cartoon image creator implies shading information as regions with shading are darker than others. Accordingly the blending engine may use this illumination information to provide shading to the face image.

In one embodiment suppose that the shading of a cartoon image in the illumination channel is u. The shading includes K uniform color regions and the illumination value for each region k is u k . Further the illuminated image of the warped face may be denoted as u and urepresents the illumination of point p. The blending engine may first find all points in the face whose corresponding points belong to the same region in the cartoon skin image. Suppose that the corresponding point of p belongs to region k. Then the new illumination value of p may be set by the blending engine as 

The blending engine may further provide illumination blending to an integrated image. The illumination blending may occur after shade matching as described above. Illumination blending naturally blends the illumination image u into the region face mask by keeping illumination smooth across the boundary of . The blending engine may perform illumination blending via Poisson Blending. In one embodiment the Poisson approach may formulate the blending as the following variational energy minimization problem min 24 where u is the unknown function in v is the vector field in and f is the cartoon image on . Generally v is the gradient field of the image u i.e. v u. Further the minimization problem may be discretized to be a sparse linear system and the blending engine may solve the system using the Gauss Seidel method. In some embodiments the blending engine may contract the face mask so that its boundary does not coincide with the boundary of the face image.

The blending engine may be further configured to perform color adjustment on the integrated image. In various embodiments the blending engine may determine the skin color of a face image and transform the real face colors into skin color present the cartoon image. As described above this process is performed in a b color channels. To estimate the skin color of the face image the face is clustered into K where K 6 components via the Gaussian Mixture Model GMM . Each component is represented as m v w where m v and ware the mean variance and weight of the kth component. Intuitively the largest component may be the skin color component denoted as m v w and the others are the non skin components.

After skin component is obtained the blending engine may label each point in the face image via a simple Bayesian inference. The posterior of point p belonging to the skin point is 

In one embodiment suppose that p is a skin point its original color is c values in a b channels and the modified color is C then the blending engine may implement the following simple linear transform to map the center of the skin component to the center of the skin color of the cartoon image 26 where mis the mean of skin component and cis the center of the skin color of the cartoon image. The colors of non skin points are kept unchanged.

It will be appreciated that while some of the image shading and blending techniques implemented by the blending engine have been discussed the blending engine may employ other techniques to shade and blend the face image with the cartoon image. Accordingly the above discussed shading and blending techniques are examples rather than limitations.

The user interface module may be configured to enable a user to provide input to the various engines in the pre preprocessor module the selection module and the fitter module . The user interface module may interact with a user via a user interface. The user interface may include a data output device such as a display and one or more data input devices. The data input devices may include but are not limited to combinations of one or more of keypads keyboards mouse devices touch screens microphones speech recognition packages and any other suitable devices or other electronic software selection methods.

In various embodiments the user interface module may enable a user to supply input to the face tagging engine to group similar images together as described above as well other engines. The user interface module may also enable the user to supply the minimum and maximum brightness threshold values to the face filter engine . In other embodiments the user interface module may enable the user to review then accept or reject the processed results produced by the various engines at each stage of the cartoon personalization as well as to add select label and or delete face images and groups of face images as stored in the cartoon personalization engine . Additional details regarding the operations of the user interface module are further described with respect to .

The data storage module may be configured to store data in a portion of memory e.g. a database . In various embodiments the data storage module may be configured to store the source photos and the source cartoon images . The data storage module may also be configured to store the images derived from the source photos and or source cartoon such as any intermediary products produced by the various modules and engines of the cartoon personalization engine .

The photo group selection interface which may include scroll control may be configured to enable a user to select a group of face images from a plurality of groups for possible synthesis with a cartoon image. The plurality of groups may have been organized by the face tagging engine with input from the user. Further the user may have provided each group with a label. For example a first group of face images are images of a particular individual and may be grouped under a label Person One. Likewise a second group of face images belong to a second individual and may be grouped under a label Person Two. Accordingly the user may use the scroll control to select the desired group of images for potential synthesis with the cartoon image if there are more groups that are capable of being simultaneously displayed by the photo group selection interface . In turn the display area may be configured to display the selected face images. It will be appreciated that the scroll control may be substituted with a suitable control that performs the same function.

The modulator interface may include a brightness control a scale control and a pose control . Each of the controls may be the form of a slider bar wherein different positions on the bar correspond to gradually incremented differences in settings. Nevertheless it will be appreciated that the controls may also be implemented in other forms such as but not limited to a rotary style control arrow keys that manipulate numerical values etc. so long as the control provides gradually incremented settings.

The brightness control may be configured to enable a user to eliminate face images that that are too dark or too bright. For example depending on the bright and dark thresholds set using the positions on the slider bar of the control face images falling outside the range of brightness may be excluded as candidate images for potential integration with a cartoon image. In turn the display area may display face images that have been selected based on the bright and dark thresholds.

The scale control may be configured to enable a user to eliminate face images that are that are too small. Face images that are too small may need scaling during synthesis with a cartoon image which may result in the blurring of the image due to inadequate resolution. Accordingly depending on the particular minimum size specified using the scale control one or more face images may be excluded. In turn the display area may display face images that have been selected based on the minimum size threshold.

The pose control may be in the form of a slider bar that represents the pose orientation. Different positions of the bar correspond to different pose orientations. The operation of the pose control may be further illustrated with respect to .

Returning to the user interface may also display the cartoon image that has been selected by a user for integration with a face image. In one embodiment the user may select the cartoon image via an affirmative action e.g. clicking dragging etc. . Moreover the user interface may also display a final integrated image that is produced from the cartoon image and one of the face images the user selected from the display area . As shown in the face of a character in the cartoon image may be replaced to generate the integrated image .

At block the cartoon personalization engine may detect one or more face images from at least one photographic image such as source images . In some embodiments a face image may be detected from photographic images via an integral image technique. In other embodiments the face images may be detected from the photographic images via a multiview technique.

At block the cartoon personalization engine may determine the suitability of the face images for integration with a cartoon image. In various embodiments the cartoon personalization engine may determine the suitability based on a determination of whether each of the face images is within acceptable illumination range. In other embodiments suitability may be determined by the cartoon personalization engine based on whether the image exceeds a minimum resolution threshold. In some embodiments the upper and lower bounds of the illumination range and or the minimum resolution threshold may be supplied to the cartoon personalization engine by a user via the user interface.

At block the cartoon personalization engine may determine the pose orientation of each of the face images. In various embodiments the pose orientation of a face image also referred to as face orientation may be determined based on landmarks extracted from the face image. In turn the landmarks may be extracted from the face image by using a Bayesian Tangent Shape Model BTSM .

At block the cartoon personalization engine may enable a user to select at least one of the detected face images as a candidate face image for integration with a cartoon image. In some embodiment the cartoon personalization engine may provide a user interface such as user interface which enables the user to select the candidate face images based on criteria such as the desired pose orientation the desired size of the image and the desired brightness for the candidate face images. In other embodiments the cartoon personalization engine may enable user to browser different groups of faces images for the selection of candidate face images. In these embodiments the cartoon personalization engine may have organized the groups based on the similarity between the face images. For example face images may be organized into a group in the descending order of similarity according to user input.

At block the cartoon personalization engine may enable a user to choose a particular candidate face image for integration with the cartoon image. In various embodiments the user may select the particular candidate face image via a user interface such as the user interface .

At block the cartoon personalization engine may replace a face of a cartoon character in the cartoon image with the chosen face image using a transformation technique.

At block the cartoon personalization engine may complete the integration of face image into the cartoon image to synthesize a transformed image such as the personalized cartoons using various blending techniques.

At block the cartoon personalization engine may determine a portion of the cartoon character in a cartoon image to be replaced via an affine transform. The portion of the cartoon character to be replaced may include the face of the cartoon character.

At block the face image chosen to replace the portion of the cartoon character may be transformed via the affine transform. In various embodiments the cartoon personalization engine may use the affine transform to warp the face image for integration with the cartoon character.

At block the cartoon personalization engine may substitute the portion of the cartoon character with the transformed face image.

At block the cartoon personalization engine may perform blending using illumination information of the cartoon character that is being transformed. The illumination information may include shading information as originally implied by a graphical creator of the cartoon character through shading.

At block the cartoon personalization engine may perform further blending by using a Poisson blending technique.

At block the cartoon personalization engine may adjust the color of the face image that is inserted into the cartoon character based on the original coloring of the cartoon character face.

In a very basic configuration computing device typically includes at least one processing unit and system memory . Depending on the exact configuration and type of computing device system memory may be volatile such as RAM non volatile such as ROM flash memory etc. or some combination of the two. System memory typically includes an operating system one or more program modules and may include program data . The operating system includes a component based framework that supports components including properties and events objects inheritance polymorphism reflection and provides an object oriented component based application programming interface API such as but by no means limited to that of the .NET Framework manufactured by the Microsoft Corporation Redmond Wash. The device is of a very basic configuration demarcated by a dashed line . Again a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.

Computing device may have additional features or functionality. For example computing device may also include additional data storage devices removable and or non removable such as for example magnetic disks optical disks or tape. Such additional storage is illustrated in by removable storage and non removable storage . Computer storage media may include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. System memory removable storage and non removable storage are all examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of device . Computing device may also have input device s such as keyboard mouse pen voice input device touch input device etc. Output device s such as a display speakers printer etc. may also be included. These devices are well known in the art and are not discussed at length here.

Computing device may also contain communication connections that allow the device to communicate with other computing devices such as over a network. These networks may include wired networks as well as wireless networks. Communication connections are some examples of communication media. Communication media may typically be embodied by computer readable instructions data structures program modules etc.

It is appreciated that the illustrated computing device is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well known computing devices systems environments and or configurations that may be suitable for use with the embodiments include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor base systems set top boxes game consoles programmable consumer electronics network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and or the like.

The ability to replace the facial features of a cartoon character with a face image from a photographic image may provide personalized cartoon images that are customized to suit the unique taste and personality of a computer user. Thus embodiments in accordance with this disclosure may provide cartoon images suitable for personalized greeting and communication needs.

In closing although the various embodiments have been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended representations is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claimed subject matter.

