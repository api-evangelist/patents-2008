---

title: Simple flow control protocol over RDMA
abstract: A method and system for directing data transfers between applications residing on different computers or devices using a simplified flow control protocol. The protocol eliminates the need for and use of flow control modes and supports all possible data transfer mechanisms. The protocol also allows source and sink applications to independently set their own local memory threshold over which data transfers are made using remote direct memory access (RDMA) or zero-copy transfers. Through adjusting its threshold value or size, a sink or receiving application or component adapts its behavior to the behavior of a sending or source application or component.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08024417&OS=08024417&RS=08024417
owner: Microsoft Corporation
number: 08024417
owner_city: Redmond
owner_country: US
publication_date: 20080604
---
Software applications residing on separate computers or devices communicate with each other over networks. Traditional network protocols such as Ethernet and Asynchronous Transfer Mode ATM are not reliable for application to application communication and provide only machine to machine datagram delivery service. Transport protocol software operating on host machines can provide more direct and reliable application to application communication.

Typically protocol software for network communication is implemented as a combination of a kernel mode driver and a user mode library. All application communication passes through these components. As a result application communication consumes a significant amount of the resources of its host processor and incurs unnecessary latency. Both of these effects degrade application communication performance. This degradation significantly limits the overall performance of communication intensive applications such as distributed databases.

Recently a new class of connectivity called System Area Networks SANs has emerged to address the performance requirements of communication intensive distributed applications. SANs provide very high bandwidth communication with relatively low latency. SANs differ from existing technologies such as Gigabit Ethernet and ATM because they implement reliable transport functionality directly in hardware. Each SAN network interface controller NIC exposes individual transport endpoint contexts and demultiplexes incoming packets accordingly. Each endpoint is usually represented by a set of memory based queues and registers that are shared by the host processor and the NIC. Many SAN NICs permit these endpoint resources to be mapped directly into the address space of a user mode process which allows application processes to post messaging requests directly to the hardware. This design consumes very little of the resources of a host processor and adds little latency to the communication. As a result SANs can deliver relatively fast communication performance to applications.

In general SAN hardware does not perform any end to end flow control. Most distributed applications are designed to communicate using a specific transport protocol and a specific application programming interface API . A large number of existing distributed applications are designed to use the Transmission Control Protocol Internet Protocol TCP IP suite and some variant of the Berkeley Sockets API such as Windows Sockets.

Some existing applications are designed to use a primary transport protocol and API such as a TCP IP or Sockets based API. In order to enable data transfer between machines in a SAN without using an existing or TCP IP protocol on each machine a new protocol must be implemented that controls the transfer of data from source memory buffers supplied by a first software application into destination memory buffers supplied by a second software application. This aspect of data transfer is known as flow control.

In SANs Sockets Direct Protocol SDP and Windows Sockets Direct WSD protocol allow network applications written using a sockets API a direct path to system hardware. SDP provides several data transfer mechanisms. Broadly there are two ways to transfer data in a SAN as small messages or via remote direct memory access RDMA transfers.

Small messages are transferred from a private and pre registered set of buffers of a source or send application to a private and pre registered set of buffers of a sink or receive application. This mechanism is referred to as a buffer copy or BCopy. Each application operating on peer computers selects its own size and number of buffers. The source application is responsible to ensure that the message fits into the buffers of the receiving application.

For large data transfers or RDMA transfers the memory buffers are dynamically registered prior to copying data. RDMA transfers are zero copy transfers and bypass the kernel. Kernel bypass allows applications to issue commands to a NIC without having to execute a kernel call. RDMA requests are issued from local user space to the local NIC and over the network to the remote NIC without requiring any kernel involvement. This reduces the number of context switches between kernel space and user space while handling network traffic.

One type of RDMA transfer is a read zero copy or Read ZCopy transfer. A Read ZCopy transfer is illustrated in . With reference to a transfer application not shown operating on a data source device sends a source available message to a transfer application not shown operating on a data sink device . Next the transfer application of the data sink device performs a Read ZCopy transfer or RDMA read by copying data directly from memory buffers of a user application operating on the data source device to one or more memory buffers of another user application operating on the data sink device . Finally the transfer application of the data sink device sends an acknowledgement or RDMA Read Complete message to the transfer application of the data source device .

Another type of RDMA transfer is a write zero copy or Write ZCopy transfer. A Write ZCopy transfer is illustrated in . With reference to A transfer application not shown of a data source device sends a source available message to a transfer application not shown of a data sink device this message is optional. The data sink application sends a sink available message to the application of the data source device indicating that one or more memory buffers are ready to receive data. The data source transfer application responds by performing a Write ZCopy transfer or RDMA write directly from user buffers of the data source device to user buffers of the data sink device . The data source transfer application then sends a write complete message to the application of the data sink device indicating that the transfer is complete.

A third type of RDMA transfer is called a transaction mechanism and is similar to a Write ZCopy transfer this mechanism is illustrated in . This type of transfer is optimum for transaction oriented data traffic where one transfer application sends relatively small commands and expects medium to large responses. With reference to a sink available message is sent from a transfer application on the data sink device to a transfer application on the data source device . However in this transfer extra information or data is appended to the message indicating where to transfer data via Write ZCopy from a data source device to a data sink device . Once the message is received the transfer application or program operating on the data source device performs an RDMA Write or Write ZCopy transfer and sends a write complete message to the transfer application of the data sink device indicating that the transfer is complete.

Existing transfer applications using SDP and WSD manage both small and large data transfers through flow control modes. For example SDP provides at least three modes Pipelined Mode Combined Mode and Buffered Mode. Each transfer application is ordinarily in a single mode at any given time. However mode is typically in reference to the transfer application which is receiving data. Mode change messages may cause the receiving application to change to a different mode.

Combined Mode corresponds to the receiving application waiting to receive an indication that data is available before it posts large receive buffers for RDMA transfers. Transfers in Combined Mode occur through BCopy if the data size is smaller than an RDMA threshold or through Read ZCopy. Since the sink user application expects a source available message before posting large RDMA buffers the message typically contains a beginning portion of the send data.

Pipelined Mode corresponds to an application which always posts large receive buffers. In this mode all types of transfers e.g. BCopy Read ZCopy Write ZCopy may be made. Since the application in this mode always pre posts receive buffers and is not waiting for any data receive information the source available message does not carry data.

With reference to once a connection is initialized between transfer applications operating on separate computers or devices each transfer application is set to the Combined Mode . In this mode if a transfer application having source data determines to change to either a Pipelined Mode or a Buffered Mode the source application initiates the change since it is the master application. The same is true if the source transfer application is in the Pipelined Mode . However the sink transfer application is the master when the transfer applications are in the Buffered Mode .

Switching between modes and managing mode changing messages makes SDP and WSD excessively complex especially since these protocols are designed for low latency high throughput environments.

Described herein is an exemplary computer program and system for implementing a simple flow control protocol to exchange data. Network interface controllers NICs which are enabled to transfer data in system area networks SANs or SAN enabled NICs and other hardware components provide means whereby remote direct memory access RDMA transfers move or exchange data between computers or devices.

Applications which implement the simple flow control protocol transfer data between two computers or devices based on the size of the data relative to a local threshold or a remote threshold. Data is transferred by one of two mechanisms a buffer copy or RDMA transfer. The protocol eliminates traditional mode switching such as that used in the Sockets Direct Protocol. The protocol functions even if no read zero copy function or mechanism is supported by using a write zero copy function. The source transfer application and the receiving or sink transfer application can independently respond to the behavior of the other by adjusting their respective threshold values or threshold sizes. The protocol is designed to maximize the communication bandwidth and minimize the communication latency observed by the communicating user applications.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

What has been needed is a flow control protocol that removes the redundant features of Sockets Direct Protocol SDP and Windows Sockets Direct WSD protocol removes any mode switching and allows user and transfer applications to take fuller advantage of the performance benefits of System Area Networks SANs and remote direct memory access RDMA .

The simple flow control protocol described herein preferably operates in distributed computing environments where tasks are performed by local and remote processing devices that are linked through a communications network. In a distributed computing environment program modules may be located in both local and remote memory storage devices. The simple flow control protocol operates without explicit mode switching as done in the Sockets Direct Protocol and as shown in . The simple flow control protocol may adjust its data transfer strategy based on the behavior of the communicating applications.

In one embodiment a SAN enabled NIC is a network interface card which allows a computer or device using the card to transfer data by remote direct memory access transfers across the system or storage area network to which the computer or device belongs. A SAN enabled NIC may have other or additional features.

With reference to the flow control layer communicates with traditional TCP IP components in the kernel. The TCP IP components in turn communicate with a SAN interface driver which then communicates with a SAN enabled NIC . On the other hand the flow control layer may alternatively communicate with a SAN management driver or directly with the SAN enabled NIC . It is the direct communication between the flow control layer and the SAN enabled NIC that provides improvements over existing protocols. The SAN enabled NIC is in communication with the network and a remote computer . Although not shown in the same or a similar environment exists on the remote computer . Although one environment is shown and described other environments may be used in which to implement the simple flow control protocol.

In other embodiments there may be an additional component not shown between the SAN interface driver and the SAN enabled NIC and between the SAN management driver and the SAN enabled NIC . Such a component may be a miniport driver which connects hardware devices to a protocol stack. A miniport driver handles the hardware specific operations necessary to manage a network adapter or other hardware device. A miniport driver is helpful to hardware manufacturers because the miniport driver may be updated to provide added functionality.

The user applications and flow control layers are shown operating in a user security level but may be implemented in any security mode or combination of modes such as in a combination of user and kernel modes or just in kernel mode. The remote computer is comprised of a SAN enabled NIC a remote flow control layer and a remote user application .

When requested to do so the flow control layers perform zero copy or RDMA transfers. For example during an RDMA or Read ZCopy transfer from the local user application to the remote user application the local flow control layer sends a source ready message to the remote flow control layer . The remote flow control layer then performs a Read ZCopy by copying the portion of the local memory buffer specified in the source ready message to the remote memory buffer posted by the remote application .

The remote flow control layer then sends a copy complete message to the local flow control layer which then communicates with the local user application indicating to the local user application that the transfer was successful. A Write ZCopy works in a similar fashion with the local flow control layer performing the RDMA transfer from the local memory buffer to the remote memory buffer . While a layer has been described as performing the protocol the protocol may be performed by any software program or application hardware device or application or other entity or any combination of entities.

If the source data is smaller than the source threshold the flow control layers perform a BCopy transfer of the source data to the sink application. This situation is referred to as making small transfers. In a preferred embodiment the flow control layers always have posted a set of memory buffers for sending and receiving BCopy transfers.

In this scenario the source application does not initially receive any indication whether the sink application has posted sink buffers for RDMA transfers. In the event that the flow control layers perform BCopy transfers source data being smaller than the source threshold the remote or sink flow control layer may send a sink available message SinkAvail Msg to the local or source flow control layer if the remote application has posted sink memory buffers. The remote or sink flow control layer or application may optionally adjust a sink threshold based on the size of the transferred data because the source data was not greater than the source threshold or based on a combination of factors. In the case of the source data being smaller than the source threshold the adjustment is typically to a higher value so that the remote or sink application does not needlessly post sink buffers and so that the remote or sink flow control layer sends out fewer sink available messages. In this way the sink application adjusts to the behavior of the source application an improvement over other protocols. The adjustment of the sink threshold may be controlled by an algorithm heuristics or other method.

With reference to if the sink application posts new or additional sink buffers when a BCopy transfer is not complete between the flow control layers the sink flow control layer may send a sink available message SinkAvail Msg to the source. In this case the source application or source flow control layer may adjust the source threshold typically this adjustment is to a lower value so that an increased number of future source data transfers may be by RDMA transfer. It may then continue data transfer using BCopy or switch to an RDMA mode. If the BCopy transfer is complete the flow control layers wait until the next transfer . Alternatively the source application or source flow control layer may check to see if the source data is greater than the source threshold and follow subsequent steps as described.

In another scenario if the size of source data is greater than the source threshold the source flow control layer sends a source available message SrcAvail Msg to the sink. In a preferred embodiment the source available message comprises a portion of data to be transferred and the overall size of the source data. At this point if the sink application has posted memory buffers of sufficient size the sink flow control layer performs a Read ZCopy transfer the data being transferred directly from the source buffers to the sink buffers.

Alternatively if the sink buffers are insufficient for the source data the sink flow control layer may notify the sink application and the sink application may post new or additional sink buffers to receive the source data. If the sink application has not or does not post sink buffers of sufficient size the sink application sends a send small message SendSm Msg to the source flow control layer requesting that the source flow control layer perform a BCopy transfer . The source upon receiving this send small message may adjust its source threshold typically to a higher value so that the source flow control layer sends fewer source available messages in the future.

If the sink application has posted or posts sink buffers of sufficient size the sink flow control layer performs a read zero copy Read ZCopy transfer either until the data is successfully transferred or until the sink flow control layer changes the mechanism of transfer to BCopy. Specifically if the Read ZCopy transfer completes the sink flow control layer sends a read complete message RdCompl Msg to the source and the flow control layers wait for the next data transfer . During the Read ZCopy transfer the sink flow control layer may change its behavior and interrupt the Read ZCopy transfer by sending a send small message to signal the source flow control layer to finish sending the data by BCopy transfer .

In a first embodiment there is no way to interrupt the BCopy transfer or to change the mechanism back to a Read ZCopy or other RDMA transfer. In another embodiment the source flow control layer may be interrupted during a BCopy transfer and the source application may be able to continue a data transfer by Write ZCopy or the sink flow control layer may be able to continue a data transfer by Read ZCopy. The penalty would be that extra mechanism change messages would be incurred.

In yet another embodiment if a sink application posts large buffers and communicates this event to the sink flow control layer before the sink flow control layer receives a source available message the sink flow control layer sends a sink available message to the source. The source flow control layer then may implement a Write ZCopy without sending a source available message. In this manner the protocol avoids the overhead of an extra or wasted zero copy message transferred between the flow control layers and the data transfer between applications is faster.

In a preferred embodiment both flow control layers post and maintain small or BCopy buffers so that BCopy transfers may be performed on demand. In another embodiment the flow control layers do not post small buffers unless requested to do. For example the sink flow control layer would post small buffers prior to or just after sending a send small message to the source.

The threshold size for using bulk or zero copy data transfers is based upon justifying the cost of initiating these types of transfers. Each zero copy operation has a cost which is a function of the number of control messages exchanged by the transport providers or flow control layers and a function of the hardware operations needed to support the protocol. In any event zero copy operations may be performed on data which is smaller than a threshold. Thus in an alternative embodiment the type of transfer or threshold may be overridden and one particular type of transfer may be enforced.

In one embodiment the source and sink thresholds are independently adjusted based on conditions transfers or any set of factors. In another embodiment or implementation a source threshold is raised to effectively infinity and thus the source or send application may be forced into operating with just one mechanism e.g. BCopy . In yet another embodiment source or sink thresholds may be adjusted by an algorithm local heuristics or other mechanism. In this manner through the use of local and remote thresholds there is no need for explicit mode change operations between source and sink flow control layers as used in the Sockets Direct Protocol SDP .

The system bus represents one or more of any of several types of bus structures including a memory bus or memory controller a point to point connection a switching fabric a peripheral bus an accelerated graphics port and a processor or local bus using any of a variety of bus architectures. By way of example such architectures comprise an Industry Standard Architecture ISA bus a Micro Channel Architecture MCA bus an Enhanced ISA EISA bus a Video Electronics Standards Association VESA local bus and a Peripheral Component Interconnects PCI bus also known as a Mezzanine bus.

An exemplary computer typically comprises a variety of computer readable media. Such media can be any available media that is accessible by computer and comprises both volatile and non volatile media removable and non removable media.

The system memory comprises computer readable media in the form of volatile memory such as random access memory RAM non volatile memory such as read only memory ROM or both. A basic input output system BIOS containing the basic routines that help to transfer information between elements within a computer such as during start up is stored in ROM . RAM typically contains data program modules or both that are immediately accessible to or processed by the processing unit .

Computer may also comprise other removable non removable volatile non volatile computer storage media. By way of example illustrates a hard disk drive for reading from and writing to a non removable non volatile magnetic media not shown a magnetic disk drive for reading from and writing to a removable non volatile magnetic disk e.g. a floppy disk and an optical disk drive for reading from and or writing to a removable non volatile optical disk such as a CD ROM DVD ROM or other optical media. The hard disk drive magnetic disk drive and optical disk drive are each connected to the system bus by one or more data media interfaces . Alternatively the hard disk drive magnetic disk drive and optical disk drive can be connected to the system bus by one or more interfaces not shown .

The disk drives and their associated computer readable media or processor readable media provide non volatile storage of computer readable instructions data structures program modules and other data for computer . Although the example illustrates a hard disk a removable magnetic disk and a removable optical disk it is to be appreciated that other types of computer readable media which can store data that is accessible by a computer such as magnetic cassettes or other magnetic storage devices flash memory cards CD ROM digital versatile disks DVD or other optical storage random access memories RAM read only memories ROM electrically erasable programmable read only memory EEPROM and the like can also be used to implement the example computing system and environment.

Any number of program modules can be stored on the hard disk magnetic disk optical disk ROM RAM or combination thereof including by way of example an operating system one or more application programs program modules and program data . Each of such operating system one or more application programs other program modules and program data or some combination thereof may implement all or part of the resident components that support the distributed file system.

A user can enter commands and information into computer via input devices such as a keyboard and a pointing device e.g. a mouse . Other input devices not shown specifically may comprise a microphone joystick game pad satellite dish serial port scanner and the like. These and other input devices are connected to the processing unit via input output interfaces that are coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB .

A monitor or other type of display device can also be connected to the system bus via an interface such as a video adapter . In addition to the monitor other output peripheral devices may comprise components such as speakers not shown and a printer which can be connected to the computer via input output interfaces .

A computer on which to practice the protocol described herein operates in a networked environment using logical connections to one or more remote computers such as a remote computing device . By way of example the remote computing device can be a personal computer portable computer a server a router a network computer a peer device or other common network node game console and the like. The remote computing device is illustrated as a portable computer that can include many or all of the elements and features described herein relative to computer .

Logical connections between a computer and a remote computer are depicted as a local area network LAN and a general wide area network WAN or Internet. It is to be understood that a computer and remote computer are connected such that the protocol described herein is enabled. In one embodiment such connection is a switched fabric communications link used in high performance computing. Such connection is preferably a point to point bidirectional serial link.

When implemented in a SAN networking environment the computer is connected to a local network via a network interface or system area network SAN adapter or SAN enabled NIC. It is to be appreciated that the illustrated network connections are exemplary and that other means of establishing communication link s between the computers and can be employed.

In a networked environment such as that illustrated with computing environment program modules depicted relative to the computer or portions thereof may be stored in a remote memory storage device. By way of example remote application programs reside on a memory device of remote computer . For purposes of illustration application programs and other executable program components such as the operating system are illustrated herein as discrete blocks although it is recognized that such programs and components reside at various times in different storage components of the computing device and are executed by the data processor s of the computer.

In the previous description the embodiments were described with reference to acts and symbolic representations of operations that are performed by one or more computers unless indicated otherwise. As such it is understood that such acts and operations which are at times referred to as being computer executed comprise the manipulation by the processing unit of the computer of data in a structured form. This manipulation transforms the data or maintains it at locations in the memory system of the computer which reconfigures or otherwise alters the operation of the computer in a manner well understood by those skilled in the art. The data structures where data is maintained are physical locations of the memory that have particular properties defined by the format of the data. However the embodiments are not meant to be limiting as those of skill in the art appreciate that various acts and operation described hereinafter may also be implemented in hardware.

Turning to the drawings wherein like reference numerals refer to like elements the embodiments are illustrated as being implemented in a suitable computing environment. Although not required the embodiments will be described in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules comprise routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. Moreover those skilled in the art appreciate that the embodiments may be practiced with other computer system configurations including for example hand held devices multi processor systems microprocessor based or programmable consumer electronics network PCs minicomputers mainframe computers and the like.

Although the description above uses language that is specific to structural features and methodological acts it is to be understood that the embodiments defined in the appended claims is not limited to the specific elements features modes or acts described. Rather the specifics are disclosed as exemplary forms.

In view of the many possible embodiments to which the principles described herein may be applied it should be recognized that the embodiment described herein with respect to the drawing figures is meant to be illustrative only and should not be taken as limiting. For example those of skill in the art recognize that the elements of the illustrated embodiment shown in software may be implemented in hardware and vice versa or that the illustrated embodiment can be modified in arrangement and detail without departing from the spirit of that described herein. Therefore the description herein contemplates all such embodiments as may come within the scope of the following claims and equivalents thereof.

