---

title: Scheduling collections in a scheduler
abstract: A scheduler in a process of a computer system includes a respective scheduling collection for each scheduling node in the scheduler. The scheduling collections are mapped into at least a partial search order based on one or more execution metrics. When a processing resource in a scheduling node becomes available, the processing resource first attempts to locate a task to execute in a scheduling collection corresponding to the scheduling node before searching other scheduling collections in an order specified by the search order.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08561072&OS=08561072&RS=08561072
owner: Microsoft Corporation
number: 08561072
owner_city: Redmond
owner_country: US
publication_date: 20080516
---
Processes executed in a computer system may include task schedulers that schedule tasks of the processes for execution in the computer system. These schedulers may operate with various algorithms that determine how tasks of a process are to be executed. In a computer system with multiple processing resources the processing resources may contend with one another in searching for tasks to execute in a scheduler. The contention tends to reduce the efficiency of the computer system in executing a process with a scheduler and the amount of contention typically increases as the number of processing resources increases in the computer system. As a result the contention of processing resources may limit the scalability of the scheduler as the number of processing resources in a computer system increases.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

A scheduler in a process of a computer system includes a respective scheduling collection for each scheduling node in the scheduler. The scheduler populates each scheduling collection with a set of schedule groups where each include schedule group includes a set of tasks of the process. The scheduling collections are mapped into at least a partial search order based on one or more execution metrics. When a processing resource in a scheduling node becomes available the processing resource attempts to locate a task to execute in a scheduling collection corresponding to the scheduling node. If the processing resource does not locate a task to execute in the scheduling collection the processing resource attempts to locate a task to execute in other scheduling collections in an order specified by the search order.

In the following Detailed Description reference is made to the accompanying drawings which form a part hereof and in which is shown by way of illustration specific embodiments in which the invention may be practiced. In this regard directional terminology such as top bottom front back leading trailing etc. is used with reference to the orientation of the Figure s being described. Because components of embodiments can be positioned in a number of different orientations the directional terminology is used for purposes of illustration and is in no way limiting. It is to be understood that other embodiments may be utilized and structural or logical changes may be made without departing from the scope of the present invention. The following detailed description therefore is not to be taken in a limiting sense and the scope of the present invention is defined by the appended claims.

It is to be understood that the features of the various exemplary embodiments described herein may be combined with each other unless specifically noted otherwise.

Runtime environment represents a runtime mode of operation in a computer system such as embodiments A and B of a computer system shown in and described in additional detail below where the computer system is executing instructions. The computer system generates runtime environment from a runtime platform such as a runtime platform shown in and described in additional detail below.

Runtime environment includes an least one invoked process a resource management layer and a set of hardware threads M where M is an integer that is greater than or equal to two and denotes the Mth hardware thread . Runtime environment allows tasks from process to be executed along with tasks from any other processes that co exist with process not shown using resource management layer and hardware threads M . Runtime environment operates in conjunction with resource management layer to allow process to obtain processor and other resources of the computer system e.g. hardware threads M .

Runtime environment includes a scheduler function that generates scheduler . In one embodiment the scheduler function is implemented as a scheduler application programming interface API . In other embodiments the scheduler function may be implemented using other suitable programming constructs. When invoked the scheduler function creates scheduler in process where scheduler operates to schedule tasks of process for execution by one or more hardware threads M . Runtime environment may exploit fine grained concurrency that application or library developers express in their programs e.g. process using accompanying tools that are aware of the facilities that the scheduler function provides.

Process includes an allocation of processing and other resources that host one or more execution contexts viz. threads . Process obtains access to the processing and other resources in the computer system e.g. hardware threads M from resource management layer . Process causes tasks to be executed using the processing and other resources.

Process generates work in tasks of variable length where each task is associated with an execution context in scheduler . Each task includes a sequence of instructions that perform a unit of work when executed by the computer system. Each execution context forms a thread that executes associated tasks on allocated processing resources. Each execution context includes program state and machine state information. Execution contexts may terminate when there are no more tasks left to execute. For each task runtime environment and or process either assign the task to scheduler to be scheduled for execution or otherwise cause the task to be executed without using scheduler .

Process may be configured to operate in a computer system based on any suitable execution model such as a stack model or an interpreter model and may represent any suitable type of code such as an application a library function or an operating system service. Process has a program state and machine state associated with a set of allocated resources that include a defined memory address space. Process executes autonomously or substantially autonomously from any co existing processes in runtime environment . Accordingly process does not adversely alter the program state of co existing processes or the machine state of any resources allocated to co existing processes. Similarly co existing processes do not adversely alter the program state of process or the machine state of any resources allocated to process .

Resource management layer allocates processing resources to process by assigning one or more hardware threads to process . Resource management layer exists separately from an operating system of the computer system not shown in in the embodiment of . In other embodiments resource management layer or some or all of the functions thereof may be included in the operating system.

Hardware threads reside in execution cores of a set or one or more processor packages e.g. processor packages shown in and described in additional detail below of the computer system. Each hardware thread is configured to execute instructions independently or substantially independently from the other execution cores and includes a machine state. Hardware threads may be included in a single processor package or may be distributed across multiple processor packages. Each execution core in a processor package may include one or more hardware threads .

Process implicitly or explicitly causes scheduler to be created via the scheduler function provided by runtime environment . Scheduler may be implicitly created when process uses APIs available in the computer system or programming language features. In response to the API or programming language features runtime environment creates scheduler with a default policy. To explicitly create a scheduler process may invoke the scheduler function provided by runtime environment and specifies a policy for scheduler .

Scheduler interacts with resource management layer to negotiate resources of the computer system in a manner that is transparent to process . Resource management layer allocates hardware threads to scheduler based on supply and demand and any policies of scheduler .

In the embodiment shown in scheduler manages the processing resources by creating virtual processors that form an abstraction of underlying hardware threads . Scheduler multiplexes virtual processors onto hardware threads by mapping each virtual processor to a hardware thread . Scheduler may map more than one virtual processor onto a particular hardware thread but maps only one hardware thread to each virtual processor . In other embodiments scheduler manages processing resources in other suitable ways to cause instructions of process to be executed by hardware threads .

Runtime environment creates scheduler with knowledge of the underlying topology of the computer system. Runtime environment provides resource management layer and or scheduler with node information of the computer system. The node information identifies hardware nodes of the computer system directly or includes sufficient information about the topology of the computer system to allow resource management layer and or scheduler to partition hardware resources into scheduling nodes based on one or more execution metrics. The execution metrics may include a speed type and or configuration of processing resources e.g. hardware threads memory resources and or other resources of the computer system.

For example in embodiments where the topology of the computer system includes a cache coherent non uniform memory access NUMA architecture the node information may identify a set of two or more NUMA nodes where each NUMA node includes a set of hardware threads and a local memory. The node information may also include information that describes memory accesses between NUMA nodes e.g. NUMA distances or memory access topologies or times .

In another example the node information may describe the speed type and or configuration of processing resources e.g. hardware threads to allow the processing resources to be grouped based on similarities or differences between the characteristics of the processing resources. These characteristics may include the type of instruction set of one or more of the processing resources to allow different nodes to be formed with sets of processing resources that have different types of instruction sets.

Runtime environment causes scheduler to include a set of two or more scheduling nodes L based on the node information. Each scheduling node includes allocated processing resources in the form of virtual processors and hardware threads . Scheduling node includes virtual processors N which map to hardware threads where Nis an integer that is greater than or equal to one and denotes the N th virtual processor and mis less than or equal to M and denotes the m th hardware thread . Scheduling node L includes virtual processors N which map to hardware threads M where Nis an integer that is greater than or equal to one and denotes the N th virtual processor and mis less than or equal to M greater than m and denotes the m th hardware thread .

Scheduler creates a scheduling collection for each scheduling node . Accordingly scheduling collections L to correspond to respective scheduling nodes L as indicated by arrows L . Scheduler maps scheduling collections into a full or a partial search order based on one or more execution metrics and uses the search order to search for tasks to execute when processing resources become available as will be described in additional detail below.

The set of execution contexts in scheduler includes a set of execution contexts with respective associated tasks that are being executed by respective virtual processors in each scheduling node and in each scheduling collection a set of zero or more runnable execution contexts and a set of zero or more blocked i.e. wait dependent execution contexts . Each execution context and includes state information that indicates whether an execution context and is executing runnable e.g. in response to becoming unblocked or added to scheduler or blocked. Execution contexts that are executing have been attached to a virtual processor and are currently executing. Execution contexts that are runnable include an associated task and are ready to be executed by an available virtual processor . Execution contexts that are blocked include an associated task and are waiting for data a message or an event that is being generated or will be generated by another execution context or .

Each execution context executing on a virtual processor may generate in the course of its execution additional tasks which are organized in any suitable way e.g. added to work queues not shown in . Work may be created by using either application programming interfaces APIs provided by runtime environment or programming language features and corresponding tools in one embodiment. When processing resources are available to scheduler tasks are assigned to execution contexts or that execute them to completion on virtual processors before picking up new tasks. An execution context executing on a virtual processor may also unblock other execution contexts by generating data a message or an event that will be used by another execution context .

Each task in scheduler may be realized e.g. realized tasks and which indicates that an execution context or has been or will be attached to the task and the task is ready to execute. Realized tasks typically include unblocked execution contexts and scheduled agents. A task that is not realized is termed unrealized. Unrealized tasks e.g. tasks may be created as child tasks generated by the execution of parent tasks and may be generated by parallel constructs e.g. parallel parallel for begin and finish . Each scheduling collection in scheduler may be organized into one or more synchronized collections e.g. a stack and or a queue for logically independent tasks with execution contexts i.e. realized tasks along with a list of workstealing queues for dependent tasks i.e. unrealized tasks as illustrated in the embodiment of described below.

Upon completion blocking or other interruption e.g. explicit yielding or forced preemption of an execution context running on a virtual processor the virtual processor becomes available to execute another realized task or unrealized task . Scheduler searches for a runnable execution context or an unrealized task to attach to the available virtual processor for execution. Scheduler continues attaching execution contexts to available virtual processors for execution until all execution contexts of scheduler have been executed.

When a virtual processor in a scheduling node becomes available the virtual processor attempts to locate a task to execute in a scheduling collection corresponding to the scheduling node . If the virtual processor does not locate a task to execute in the scheduling collection the virtual processor attempts to locate a task to execute in other scheduling collections in an order specified by the search order. In one embodiment scheduler may include a configurable delay parameter that causes available virtual processors to delay the search of other scheduling collections to attempt to minimize contention with other available virtual processors . The delay parameter may also be used to prioritize the search for work to the scheduling collection corresponding to the scheduling node of the available virtual processor .

In runtime environment and or resources management layer identify scheduling nodes based on one or more execution metrics as indicated in a block . The execution metrics may be any suitable measures of executing instructions in the computer system and may include processing speed processing throughput and memory latency characteristics of processing and other resources in the computer system. Using execution metrics determined for various sets of components of the computer system runtime environment and or resources management layer partition the processing and other resources of the computer system and use the partitions to identify scheduling nodes for scheduler . Scheduling nodes each include groups of similar or dissimilar sets of processing and other resources of the computer system.

In one example the computer system may include processors that include multiple hardware threads . In this example runtime environment and or resources management layer may partition each processor package into separate node and create a scheduling node for each node.

In another example in a NUMA system the difference in memory latencies between processors and different portions of a memory may be used as execution metrics to divide the computer system into NUMA nodes and create a scheduling node for each NUMA node. The NUMA nodes may each have a set of processing resources and a local memory where the access to the local memory by processing resources within a NUMA node is faster than access to a local memory in another NUMA node by the processing resources.

In a further example runtime environment and or resources management layer may partition arbitrary or partially arbitrary sets of processor resources in a computer system into nodes and create a scheduling node for each node.

In yet another example runtime environment and or resources management layer may partition processing resources of different types or speeds into nodes where each node includes a number of the same type or speed of processing resource. Runtime environment and or resources management layer create a scheduling node for each node.

Runtime environment resources management layer and or scheduler create a respective scheduling collection for each scheduling node as indicated in a block . As shown in scheduler creates scheduling collections L that correspond to respective scheduling nodes L . Each scheduling collection forms a data structure in the memory of the computer system for storing tasks where the data structure is searchable by virtual processors from a corresponding scheduling node and virtual processors from other scheduling nodes .

Runtime environment resources management layer and or scheduler map scheduling collections L into a full or partial search order based on one or more execution metrics as indicated in a block . Scheduler uses the execution metrics to compare execution costs between different scheduling nodes . The execution costs may be described in terms of node distances where different node distances express different execution characteristics between a given scheduling node and other scheduling nodes . With node distances scheduling nodes with lower execution costs relative to a given scheduling node are described as being closer to the given scheduling node and scheduling nodes with higher execution costs relative to the given scheduling node are described as being farther from the given scheduling node . Scheduler maps scheduling collections L into the full or partial search order using the node distances in one embodiment.

To create the search order scheduler groups the set of scheduling collections into subsets of one or more scheduling collections based on the node distances. Each scheduling collection has a node distance of zero from a corresponding scheduling node . Accordingly each scheduling collection forms the first level subset of scheduling collections e.g. a level subset for the corresponding scheduling node . For the next level subset of scheduling collections e.g. a level subset scheduler groups the set of one or more scheduling collections with a closest range of node distances from the given scheduling node . Scheduler then groups the set of one or more scheduling collections with a next closest range of node distances from the given scheduling node into the next level subset of scheduling collections e.g. a level subset . Scheduler continues grouping sets of one or more scheduling collections with successive ranges of node distances from the given scheduling node into successive level subsets of scheduling collections until all desired scheduling collections in the set of scheduling collections have been incorporated into the search order.

The search order of scheduling collections is used by available processing resources i.e. virtual processors in scheduling nodes to search for tasks to execute. The search order may specify a partial search order by grouping more than one scheduling collections in at least some of the subsets e.g. a subset of two or more scheduling collections that correspond to a subset of scheduling nodes that have the same node distance or similar node distances from the given scheduling node . Where a partial order is specified a processing resource may search the subset of scheduling collections in a round robin or other suitable order. The search order may also specify a full search order by either grouping only one scheduling collection in each subset or specifying a search order of each subset of two or more scheduling collections .

As shown in scheduling nodes share an interconnection between nodes scheduling nodes share an interconnection between nodes scheduling nodes share an interconnection between nodes and scheduling nodes share an interconnection between nodes . Interconnections are all assumed to have the same speed and bandwidth characteristics in the example of

The node distances between any two nodes that share an interconnection is less than the node distances between any two nodes that do not share an interconnection . For example node accesses node using either both interconnections and or both interconnections and . Similarly node accesses node using either both interconnections and or both interconnections and .

From node the level subset of scheduling collections includes scheduling collections which correspond to scheduling nodes and the level subset of scheduling collections includes scheduling collection which corresponds to to scheduling node .

From node the level subset of scheduling collections includes scheduling collections which correspond to scheduling nodes and the level subset of scheduling collections includes scheduling collection which corresponds to to scheduling node .

From node the level subset of scheduling collections includes scheduling collections which correspond to scheduling nodes and the level subset of scheduling collections includes scheduling collection which corresponds to to scheduling node .

From node the level subset of scheduling collections includes scheduling collections which correspond to scheduling nodes and the level subset of scheduling collections includes scheduling collection which corresponds to to scheduling node .

Referring back to scheduler populates scheduling collections M with respective sets of tasks as indicated in a block . Each set of one or more tasks presented to scheduler may be created explicitly by process or implicitly by runtime environment e.g. by creating an agent without a parent or inducting an operating system execution context into an execution context of scheduler . Scheduler inserts the sets of tasks into scheduling collections according to any suitable algorithm or according to the topology of scheduling nodes . For example scheduler may insert sets of tasks into scheduling collections in a round robin order. As another example scheduler may insert sets of tasks into scheduling collections corresponding to desired topologies of scheduling nodes .

Scheduler determines whether a virtual processor becomes available as indicated in a block . Scheduler may perform this function continuously while causing process to be executed. Upon completion blocking or other interruption e.g. explicit yielding or forced preemption of an execution context running on a virtual processor the virtual processor becomes available to execute a new task.

When scheduler determines that a virtual processor becomes available scheduler begins a search for a task for the available virtual processor to execute. Scheduler first attempts to locate a task to execute in a first subset of scheduling collections as indicated in a block . The first subset of scheduling collections is the scheduling collection corresponding to the scheduling node that includes the available virtual processor . Scheduler may search the first subset in any suitable way.

If an executable task is found in the first subset then scheduler causes the task to be executed by the virtual processor as indicated in a block . Virtual processor attempts to execute the task as a continuation of a previous execution context . If virtual processor is unable to execute the task as a continuation then virtual processor performs a full operating system context switch to the execution context represented by the task.

If an executable task is not found in the first subset then scheduler determines whether another subset of scheduling collections is specified by the search order as indicated in a block . If the first level subset is the only subset specified by the search order then scheduler continues to search the first subset until an executable task is located.

If another subset is specified by the search order then scheduler attempts to locate a task to execute in one or more scheduling collections in the next subset as indicated in a block . If an executable task is found in a scheduling collection in the next subset then scheduler causes the task to be executed by the virtual processor as indicated in a block . If an executable task is not found in the next subset of scheduling collections then scheduler repeats the function of block . Scheduler continues to search subsets of scheduling collections in the specified search order until either an executable task is found or all subsets specified by the search order have been searched.

In the above embodiments scheduler may be configured to search one or more of the above subsets of scheduling collections repeatedly before moving on to the next subset. Scheduler may also be configured to delay the search of one or more of the subsets in accordance with one or more delay parameters.

In the above embodiments scheduling nodes effectively own corresponding scheduling collections . At some point in the execution of process all processing resources of a given scheduling node may be executing tasks from scheduling collections other than the scheduling collection that corresponds to the given scheduling node . In this scenario the owned scheduling collection of the given scheduling node becomes the scheduling collection from which the most processing resources of the given scheduling node are executing tasks and the given scheduling node becomes a rambling node. If a rambling node later has a processing resource that is executing a task from the originally owned scheduling collection then the rambling node again becomes the owner of the originally owned scheduling collection .

Scheduler populates scheduling collections A with respective sets of zero or more schedule groups at any time e.g. in response to executing other tasks where each schedule group includes a set of tasks of process . In such embodiments scheduler may search each schedule group in a scheduling collection A before searching for a task in another scheduling collection or A.

Scheduler may attempt to locate a task to execute in the schedule group from which an available virtual processor most recently obtained an executable task or in the schedule group indicated by an index e.g. a round robin index . In each schedule group scheduler may search for realized tasks in the runnables collection of the schedule group before searching for a realized task in other schedule groups e.g. in a round robin order . If no realized task is found then scheduler may search for unrealized tasks in the workstealing queues of the schedule group before searching for an unrealized task in other schedule groups e.g. in a round robin order . Scheduler may update the index to identify a schedule group where an executable task was found.

Process may use schedule groups in scheduler to provide a structure for locality of work fairness and forward progress. The tasks of each schedule group may be grouped due to logically related work e.g. a collection of tasks descending from a common root task hardware topology e.g. a non uniform memory architecture NUMA or a combination thereof.

In embodiments where one or more scheduling collections which include local collections the set of execution contexts in scheduler also includes sets of runnable execution contexts N in respective local collections N . Each execution context has an associated task that was unblocked by the execution of a task where the task was executed or is currently being executed on the virtual processor corresponding to the local collection that includes the execution context .

Scheduler may first attempt to locate a task in the local collection corresponding to the available virtual processor before searching elsewhere in scheduling collection B. Local collections may allow scheduler to exploit memory locality and other effects that may occur with hardware threads . In executing process scheduler may assign each wait dependent execution context that becomes unblocked to the local collection corresponding to the virtual processor that caused the execution context to become unblocked. When a virtual processor becomes available the virtual processor may attempt to execute the most recently added execution context in the corresponding local collection to try to take advantage of data stored in the memory hierarchy corresponding to the virtual processor .

If an executable task is not found in the local collection corresponding to the available virtual processor then scheduler may attempt to locate an executable task in a local collection corresponding to another virtual processor of a scheduling node . Scheduler accesses the local collections corresponding to the other virtual processors in a round robin or other suitable order and may execute the least recently added execution context in the local collection where an executable task is found.

In other embodiments other scheduling collections may include both the schedule groups of scheduling collection A and the local collections of scheduling collection B .

As shown in computer system A includes one or more processor packages a memory system zero or more input output devices zero or more display devices zero or more peripheral devices and zero or more network devices . Processor packages memory system input output devices display devices peripheral devices and network devices communicate using a set of interconnections that includes any suitable type number and configuration of controllers buses interfaces and or other wired or wireless connections.

Computer system A represents any suitable processing device configured for a general purpose or a specific purpose. Examples of computer system A include a server a personal computer a laptop computer a tablet computer a personal digital assistant PDA a mobile telephone and an audio video device. The components of computer system A i.e. processor packages memory system input output devices display devices peripheral devices network devices and interconnections may be contained in a common housing not shown or in any suitable number of separate housings not shown .

Processor packages include hardware threads M . Each hardware thread in processor packages is configured to access and execute instructions stored in memory system . The instructions may include a basic input output system BIOS or firmware not shown an operating system OS a runtime platform applications and resource management layer also shown in . Each hardware thread may execute the instructions in conjunction with or in response to information received from input output devices display devices peripheral devices and or network devices .

Computer system A boots and executes OS . OS includes instructions executable by processor packages to manage the components of computer system A and provide a set of functions that allow applications to access and use the components. In one embodiment OS is the Windows operating system. In other embodiments OS is another operating system suitable for use with computer system A.

Resource management layer includes instructions that are executable in conjunction with OS to allocate resources of computer system A including hardware threads as described above with reference to . Resource management layer may be included in computer system A as a library of functions available to one or more applications or as an integrated part of OS .

Runtime platform includes instructions that are executable in conjunction with OS and resource management layer to generate runtime environment and provide runtime functions to applications . These runtime functions include a scheduler function as described in additional detail above with reference to . The runtime functions may be included in computer system A as part of an application as a library of functions available to one or more applications or as an integrated part of OS and or resource management layer .

Each application includes instructions that are executable in conjunction with OS resource management layer and or runtime platform to cause desired operations to be performed by computer system A. Each application represents one or more processes such as process as described above that may execute with scheduler as provided by runtime platform .

Memory system includes any suitable type number and configuration of volatile or non volatile storage devices configured to store instructions and data. The storage devices of memory system represent computer readable storage media that store computer executable instructions including OS resource management layer runtime platform and applications . The instructions are executable by computer system to perform the functions and methods of OS resource management layer runtime platform and applications described herein. Examples of storage devices in memory system include hard disk drives random access memory RAM read only memory ROM flash memory drives and cards and magnetic and optical disks.

Memory system stores instructions and data received from processor packages input output devices display devices peripheral devices and network devices . Memory system provides stored instructions and data to processor packages input output devices display devices peripheral devices and network devices .

Input output devices include any suitable type number and configuration of input output devices configured to input instructions or data from a user to computer system A and output instructions or data from computer system A to the user. Examples of input output devices include a keyboard a mouse a touchpad a touchscreen buttons dials knobs and switches.

Display devices include any suitable type number and configuration of display devices configured to output textual and or graphical information to a user of computer system A. Examples of display devices include a monitor a display screen and a projector.

Peripheral devices include any suitable type number and configuration of peripheral devices configured to operate with one or more other components in computer system A to perform general or specific processing functions.

Network devices include any suitable type number and configuration of network devices configured to allow computer system A to communicate across one or more networks not shown . Network devices may operate according to any suitable networking protocol and or configuration to allow information to be transmitted by computer system A to a network or received by computer system A from a network.

In the embodiment of each processor package R and respective set of memory devices R form a node. The nodes are interconnected with any suitable type number and or combination of node interconnections . The speed and or bandwidth of interconnections may vary between the nodes.

Each processor package includes a set of hardware threads where each hardware thread includes an L1 level one cache not shown . Each processor package also includes a set of L2 level two caches that correspond to respective hardware threads . Each processor package further includes an L3 level three cache available to the set of hardware threads a system resource interface a crossbar switch a memory controller and a node interface . System resource interface provides access to node resources not shown . Crossbar switch interconnects system resource interface with memory controller and node interface . Memory controller connects to a memory device . Node interface connects to one or more node interconnections .

Because a node includes local memory i.e. a set of memory devices the access to the local memory by processor packages in the node may be faster than access to memory in other nodes. In addition access to memory in other nodes may depend on a connection speed bandwidth cache topology and or NUMA node distance of interconnections between the nodes. For example some nodes may be connected with a relatively fast interconnection such as an Advanced Micro Devices HyperTransport bus or an Intel CSI bus while others may be connected with one or more relatively slow interconnections .

In other embodiments each processor package may include other configurations and or numbers of caches. For example each hardware thread may include two or more L1 caches in other embodiments and the L2 and or L3 caches may or may not be shared in other embodiments. As another example other embodiments may include additional caches e.g. a level four L4 cache or fewer or no caches.

With reference to the embodiments described above in the memory and interconnect latencies in computer system B provide node distances that may be considered by runtime environment resources management layer and or scheduler in forming scheduling nodes . For example runtime environment resources management layer and or scheduler may create a scheduling node for each node in computer system B along with a corresponding scheduling collection . Runtime environment resources management layer and or scheduler may map the scheduling collections into a partial or full search order based on the interconnect topology between the nodes. For example any two nodes connected with a relatively fast interconnection may be grouped into the same scheduling node and scheduling collection subset level and nodes with relatively slow interconnections may be grouped into a scheduling node and scheduling collection subset level above the scheduling node and scheduling collection subset level that includes the relatively fast interconnection .

By searching the scheduling collections for executable tasks in the search order processing resources in nodes increase the likelihood of exploiting memory locality effects in computer system B. Tasks from the same scheduling collection may be more likely to have common data that is present in the local memory hierarchy of a node than tasks from another scheduling collection .

In addition to the potential locality advantages the use of scheduling nodes and scheduling collections in the above embodiments may provide a scheduler with the ability to reduce contention between processing resources that are searching for tasks to execute. Processing resources in different scheduling nodes initiate the search for executable tasks in different corresponding scheduling collections. By doing so the number of locks or other synchronization constructs placed on task collections in the scheduler may be reduced.

The scheduler may also scale to computer systems with a large number of processing resources as a result of the localized search for executable tasks. Further the scheduler may provide locality of work while preserving fairness and forward progress using round robin searching and workstealing queues in schedule groups.

Although specific embodiments have been illustrated and described herein it will be appreciated by those of ordinary skill in the art that a variety of alternate and or equivalent implementations may be substituted for the specific embodiments shown and described without departing from the scope of the present invention. This application is intended to cover any adaptations or variations of the specific embodiments discussed herein. Therefore it is intended that this invention be limited only by the claims and the equivalents thereof.

