---

title: Process performance evaluation for Enterprise data systems
abstract: A method using integrated software and algorithms for measuring, modeling, benchmarking and validating any Enterprise Content management system, forms processing data capture system or data entry system, including, at the user's option, ingest of special engineered test materials such as a Digital Test DeckÂ®, applying data quality scoring algorithms, use of cost models, validation of downstream business processes, and implementing statistical process control.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08055104&OS=08055104&RS=08055104
owner: ExactData, LLC
number: 08055104
owner_city: Rochester
owner_country: US
publication_date: 20080302
---
This application claims the benefit of U.S. Provisional Application No. 60 892 654 filed Mar. 2 2007 which application is hereby incorporated by reference.

The invention relates to Enterprise Content Management ECM systems and other forms including checks processing particularly for data capture systems including such activities as manual data entry from image or paper recognition technologies OCR ICR OMR etc that automatically capture data from image data quality evaluations and downstream business process performance quality evaluations.

System performance in forms processing is typically measured through inside out or white box test scripting or costly 100 inspection scenarios. System level testing from ingest to population of the content management system is generally not performed nor is statistical process control employed. This is primarily due to the lack of suitable test materials having a perfectly known result or truth with which to compare system performance results particularly in sufficient quantities to allow statistically valid conclusions.

We have solved the above problems with this invention which makes possible outside in or black box testing and cost effective sampling for process quality assurance. This enables system level testing of all or portions of a forms data capture system and provides valid data for statistical process control. A preferred element of this invention is the incorporation of special engineered test materials such as a Digital Test Deck available from ADI LLC of Rochester N.Y. which simulates a collection of real forms filled out by respondents but for which the truth is already known thus rendering unnecessary the laborious and expensive truthing of large volumes of actual production data. However the use of production image snippets for which the correct answers have been determined i.e. they have been truthed is also possible in the scope of this invention if desired.

The invention is preferably practiced by incorporation of a Digital Test Deck such as described in U.S. patent application Ser. No. 10 933 002 for HANDPRINT RECOGNITION TEST DECK published on Mar. 2 2006 under the publication number US 2006 0045344 A1 and hereby incorporated by reference.

Our invention enables real time measurement and modeling of system characteristics as part of the workflow of a current Enterprise Content Management ECM system or any other forms processing data capture system. With the ingest of special engineered test materials such as a Digital Test Deck and other system operating parameters flagged test information will become part of the content repository date and time stamped with other typical content metadata. While some of the examples specifically reference the Digital Test Deck other engineered test materials can be used.

With the integration of our software into the Enterprise Service Bus or other workflow manager one can conveniently measure system performance at any point in time. One will also be able to calculate a cost per form based on measured data such as the operation point of the OCR engine as well as reduce the costs associated with traditional inspection processes through the deployment of statistical process control. System level testing can be deployed to measure and validate any changes or improvements made to the forms processing or data entry system.

A primary aspect of our invention is to enable cost effective statistically valid real time measurement and modeling of system characteristics as an integrated part of the workflow of a current Enterprise Content Management ECM system or any other forms processing data capture system.

Another aspect of our invention is that with the ingest of special engineered test materials such as a Digital Test Deck for which the truth is perfectly known along with other system operating parameters flagged test information will become part of the content repository date and time stamped with other typical content metadata. This test information consisting of the data capture system s inferences about the true contents of the forms being read may then be compared to the perfectly known truth to score the accuracy of the data capture system s inferences. While some of the examples specifically reference the Digital Test Deck any specially created test materials could be used if they are sufficiently well truthed and of sufficient quantity to allow statistically valid testing.

Another aspect is that with the integration of our invention into the Enterprise Service Bus or other workflow manager one can conveniently and cost effectively measure data capture system performance at any point in time.

Another aspect of our invention is to be able to calculate a processing cost per form based on measured data such as the operation point of the OCR engine .

A further aspect is to deploy system level testing to measure and validate any changes or improvements made to the forms processing or data entry system.

An additional aspect of our invention is to allow evaluation of the outputs of just the machine interpreting or the outputs of human interpreting or both in addition the final data capture system outputs.

Another aspect is to allow comparison of data capture system performance over time and from place to place.

Another aspect is to make possible the comparison of two different data capture systems as might be desired in a proposal and bidding process. This would a fair comparison as both systems would be inferring the contents of the same data sets.

In addition to scoring data capture accuracy another aspect of this invention is to enable evaluating data mining systems by designing the test materials to evaluate the internal logic of the systems such as testing to see if a document with certain properties is discovered by the process.

As shown in integration of the AIMED Q module seamlessly into an existing data capture system will allow users to routinely and automatically verify and track system output data quality. This may be accomplished in several ways the most basic of which is to use a Digital Test Deck as input forms to the existing data capture system . This test deck of forms by design having perfectly known truth is the most cost effective way to measure the performance of a data capture system. The forms first pass through a scanning step and the electronic images of the forms are sent to the OCR subsystem . Alternatively an electronic image deck could be sent directly to OCR if only software tests are desired . The OCR engine or ICR if you prefer as is well known by practitioners of the data processing art then examines the form image and creates an estimate of what is written in the field in question along with a confidence value. If this confidence value is equal to or greater than the confidence threshold set by the data capture system administrator then that field is accepted and is sent on to reside in a data merge server . Humans typically never see these accepted fields and their corresponding computer data is sent on to the system database where users may access the data for their downstream business processes.

If a field does not have a confidence value greater than the confidence threshold then it is rejected and sent to human Keyers who perform Key From Image KFI to capture the data from that field. The resultant keyed fields are then sent to data merge to be combined with the accepted fields and stored in the database . The combination of the accepted fields and the keyed fields are the total data captured by the existing data capture system . This total data contains two sources of error one is the error made by the OCR or ICR subsystem and the other is the error produced by the human Keyers. In order to measure these errors the OCR accepted fields are sampled and so are the keyed fields and sent to the AIMED Q ADI Scoring Module for analysis. In the case of using a Digital Test Deck as input forms one may choose to sample everything in the deck. In the case of scoring production data it is typical to use a sample size that is much smaller than the total production data population but still statistically valid . By analyzing and counting the errors coming from OCR and KFI and by counting the number of fields that are accepted relative to the number of fields processed one can also estimate the total error in the merged data as was taught in our referenced U.S. patent application Ser. No. 10 933 002 for a Handprint Recognition Test Deck.

The AIMED Q Subsystem may also be used to score production data but unlike like the above example of using a Digital Test Deck where the truth is perfectly known the truth of the sampled production data set must first be determined that is one must determine the correct answers to the fields that are sampled for analysis. This is a laborious and time consuming process usually involving double keying and verifying to get at the correct answers for the sampled fields. Fields selected to be keyed for QA purposes may then be sent to KFI for keying whether they originate from OCR accepted fields or OCR rejected fields . So if a Keyer say K keys from the image snippet of a field selected for QA sampling from the OCR accepted fields and the resultant ASCII data agrees with the result from OCR than the OCR field is deemed correct. If the result from K does not agree with the OCR result then we have an inconclusive situation regarding the correct answer and then another Keyer say K is asked to key the same snippet and if the results from K agree with the OCR then the OCR is deemed correct. If K agrees with K and not the OCR then the OCR result is incorrect and so on. Although such double keying has been practiced in the data capture art prior to this invention the use of the AIMED Q subsystem integrated with an existing data capture system makes it very easy and cost effective to implement this approach to measure the quality of production data.

If a sampled keyed field from KFI is selected for QA keyed originally say by Keyer K then another Keyer K is independently asked to key the same field image snippet and if those two resultant ASCII fields agree then K is deemed correct. If K and K disagree then we have an inconclusive situation regarding the correct answer and a third Keyer say K may be employed to attempt to determine the correct answer. If all three Keyers disagree then this field is usually called ambiguous and removed from the scoring process. Another advantage of using AIMED Q integrated with the existing data capture system is the convenient access to OCR rejected fields to assist in QA. By using the data from the OCR s rejected fields one can obtain some additional efficiency in determining the truth of production data by realizing that a substantial amount often as much as half of the OCR rejected fields is actually correct even though rejected. By first comparing the first Keyer K result with the OCR rejected field a match would indicate with no additional labor that K was correct. In the case where they do not match then the above procedure may be used as described.

If the amount of production data to be scored is sufficient the ADI Scoring module may be enhanced by the addition of another OCR engine to determine a provisional data file to use in determining the truth of the data set being scored. In this application the extra OCR engine is used in much the same way as K above except it is much faster and costs less. The field level results from the extra OCR engine may be compared with the data from the existing data capture system and if these two ASCII data fields agree one can declare the existing data capture system to be correct for that field etc. Human Keyers are brought in only in the case of a disagreement in which case they can determine which of the two fields are correct or if neither is correct key the correct entry. We call this approach Production Data Quality PDQ a product available from ADI LLC of Rochester N.Y. and because it uses automation to help get at the truth of a data file it is a very cost effective way to assess production data quality.

In many data capture systems it is common to associate a group of forms in is what is called a batch say of 300 paper forms of a given size. If it is desired to analyze production data in the existing data capture system at the batch level for purposes of performing quality assurance on each batch then the ADI Scoring Module may be enhanced to perform that function as well since all the necessary data interfaces and architectures are present. The additional data that must be used is to keep track of batches in production and to apply a sampling plan to determine if a given batch is acceptable. There are many ways to devise such sampling plans that are well known in the statistical art but to apply them they usually come down to specifying a batch sample size and a maximum number of errors in the sample that would allow batch acceptance.

The software may be run in a standalone mode as part of a desktop or distributed client service network as a shared application.

As shown in Application Programming Interfaces APIs will be exposed for each Functional Element of the Services Software Module. The API s can be exposed at three levels for each Functional Element deep middle and shallow. The middle API s will be a logical consolidation of the deep API s while the shallow API s will be a logical combination of the middle and deep. A custom negotiator can be written for the legacy system current workflow or software of an Independent Software Vendor ISV integration. For the initial functionality of AIMED Q only one API level may need to be exposed due to the simple nature of this initial version of the software.

User interfaces workflow and security will be outside the scope of this software module as they will be provided as functionality within the context of the larger document imaging or ECM system with the exception of software protection for the purpose of tracking and collecting licensing fees.

The software may be run in a standalone mode as part of a desktop or distributed client service network as a shared application. In this mode not integrated into the ESB or workflow software selected input parameters must be input or selected that are not derived from systematic testing and will not represent actual system performance. Benchmark reports will not be able to be obtained.

A simple GUI helps the user select or input parameters run the software and display outputs. This software can be both SOA and XML compliant.

A Digital Test Deck DTD would be submitted for system ingest. The DTD would be tagged DTD00000001 at submission. The flagging would occur immediately after scanning and conversion to a digital image or at the time of submission of the electronic DTD file.

The DTD would be processed utilizing current workflows. The resultant data file would be a Digital Test Actual OCR file DTAR0OCR00000001 for data processed directly after OCR. Processing would be accomplished with the OCR Reject Rate Set at zero 0 .

The DTD would be processed utilizing current workflows. The resultant data file would be a Digital Test Actual OCR file DTAOCR00000001 for data processed directly after OCR. Processing would be accomplished with the normal OCR Reject Rate R .

The DTD would be processed utilizing current workflows. The resultant data file would be a Digital Test Actual OCR file DTACORR00000001 for data processed directly after any data correction. Processing would be accomplished with the normal OCR Reject Rate.

Tagged truth information would also be submitted into the ingest process herein called Digital Test Truth DTT00000001 . This can be accomplished by bar coding of the DTD to facilitate content management workflow or workflow within the ESB. The Digital Test Truth would be converted to the Digital Test Actual file structure if not already so.

Content management and storage would be handled as normal in the ECM system with the following exception both for image and data test material files all information would be tagged with the above test material schema including date and time stamps.

User classes including but not limited to Operations Business Unit Management Quality Assurance and the Chief Information Office would be able to access AIMED Q Services through current system User Interface or Web Portal Services. Specific Test and Truth material image or data files would be accessed through the systems current content management systems.

The Enterprise Service Bus Workflow Manager would allow AIMED Q Services to also be pushed to the user community.

Open security the ISV could be expected to protect the AIMED Q software while tracking and paying licensing fees.

Negotiators could be SW protected for an annual licensing schema with the ability to allow 30 day trials.

AIMED Q application software module could be SW protected for an annual licensing schema with the ability to allow 30 day trials.

Operation on Desktop or in a Client Server Environment and Operation Integrated into Current Workflows and Content Management Systems

The user would set the following parameters in a desktop or client server environment. Users would be allowed to choose benchmark criteria or allowed to type in any numerical number in the correct format. Input will always be reference to the same date time stamp range.

The following inputs to our analysis are provided by the ESB or ISV workflow software. Input will always be reference to the same date time stamp.

With the above inputs we can do all the analysis mentioned so far and we can also do much more and at steps in the workflow beyond the recognition and keying steps. The cost model can be modified so that the cost of error is increased at each subsequent step simulating the error propagating downstream. With the timestamp it also lends itself to workflow analysis in terms of efficiency. The timestamp with history enables Statistical Process Control. With this nodal concept the workflow process steps themselves are a black box but each create a new state of the data that can be analyzed.

At a specific data time range the percentage difference is computed between Fields of the DTT and DTACORR.

X Fields of DTT are compared to the same X Fields of DTACORR. A hard match all characters are the same or both are recognized as ambiguous One 1 . A non match is if any characters or ambiguous answers are not the same Zero 0 . Hard Match Corrected SUM Fields 100 for the noted time period 

At a specific data time range the percentage difference is computed between Fields of the DTT and DTAR0OCR.

X Fields of DTT are compared to the same X Fields of DTAR0OCR. A hard match all characters are the same or both are recognized as ambiguous One 1 . A non match is if any characters or ambiguous answers are not the same Zero 0 . Hard Match 0 SUM Fields for the noted time period 

Based on looking at thousands of real data points of OCR error as a function of reject rate we sometimes find it handy to use a simple model of a straight line starting at E 0 E sloping down to a floor of E R E. The specific model for OCR error is 1 if 0 and if 1

In some cases a detailed set of data points describing error rate versus reject rate may not be readily available so this simple linear model shown above is useful if you know the error rate at zero reject and the approximate break points. If the detailed data is available from measurements using this invention then the entire data set may be used in these calculations.

In order to get a relationship for total cost C we add up the elements of cost. The first term is the fixed system cost C. Then in brackets multiplied by the total number of fields processed VF we have the cost of keying the rejects plus the cost of an error committed by the OCR plus the cost of an error produced by keying.

From the above data an OCR Error Rate vs. Reject Rate curve can be generated thus enabling the Form vs. Reject curve to be also generated as a robust guide to improvement progress.

The invention can also be arranged to allow auditing for effective management of outsourced data entry and forms processing services for both cost and quality of data. A DTD or other suitable test materials are injected into the services providers current workflow and benchmarked for cost per page and error rates coming into the client systems enabling Service Level Agreement enforcement for both cost and data quality. Benefits include 

The invention can also be arranged to technically differentiate alternative bidders for competitive request for proposals. A DTD that is a simulation of the user s current form type s with realistic respondent data is supplied to bidders. Bidders provide results from their proposed solution recognition engine or from data entry for outsourced solutions . Bidders may then be scored and ranked for the best technical solution. Benefits include 

Another variation on this invention is to use a business rules based engine that sits off the ESB or workflow software layer internal to the ECM system which typically stores or has the ability to create encapsulated information files. These files are utilized for various downstream workflow processes. This invention is to seed the ECM system with data files that would have a predetermined answer based on the requirements of the downstream workflow processes. At a statistically valid cadence flagged test paper or electronic content would be ingested into the workflow to reside in the depository. Automatic electronic auditing would be performed at many or all of the user group access points and compared to our previously known truth for validity. The statistics and content of the rules engine would be varied depending on current or assumed error rates necessary time frames to catch workflow errors desired confidence levels and other customer defined requirements. Part of the value here is that content in an ECM system is typically changed and repurposed physical storage layers are modified storage software workflow and content ownership encryption security layers and so on all change over time and result in changes to content. This auditing and data quality management system would be focused on information after the conversion process but internal to the ECM system. An advantage of this type of software system is that it can run in the background with no manual intervention providing reports trends and other information automatically and would only need to be refreshed periodically as new content types are ingested into the ECM system.

An example would be the e Discovery process for legal applications. Typically e mails are the first set of information sets to be collected as they are organized and managed and therefore can be discovered. For a Lotus Notes application a Zantac e mail server and software application serves this purpose. Encapsulated files pulled from the Zantac system are denoted as .nsf files. Encapsulated .nsf files would be created as DTD Data Files and placed on the Zantac server. The files would be created in such as way as to statistically test the 60 some odd requirements of the discovery process for e Discovery of the Zantac system. These requirements include things such as removing e mails that are privileged husband and wife or attorney communications correctly organizing the information and correctly identifying hot docs with certain predefined key words contained within the e mails or attachments. When litigation is instigated the DTD data files would be included in the discovery process. During the final quality checks before the .nsf files are produced the files would be compared to the known pre determined answers and removed from the submitted discovery package. Risk would be significantly decreased as you now have an audited process and understand if you are not producing information you are legally required to produce and or are producing information you should not either of which could cause you to lose the case.

Much of the above description deals with measuring data accuracy such as to what extent does a data capture system correctly recognize a hand printed character string It is an additional benefit of this invention that the simulated data may also be created in such a way as to test the system s internal logic and or the ability of humans associated with the process to follow the necessary business rules. Some examples of how this is done are instructive.

For example in the case of a census survey the respondent might be expected to write the number of people in a household in a two digit numerical field called say pop and then write data about each person in the household in a section of the census form called say a person block . Normally then you would expect that the number in the pop field would be the same as the number of person blocks that contained data. However it often happens that the pop field says for example 05 but there are say six person blocks actually containing data. In this case this form image is supposed to be automatically presented on a computer screen to a human analyst who attempts to resolve the possible discrepancy. If we wish to test the extent to which the data processing system is correctly presenting form images to the analysts we would intentionally put 05 in the pop field on a simulated form but with six person blocks filled out with data. In a very large test set with thousands of simulated household forms we could place say 100 forms in the test deck that intentionally have this discrepancy and test to see if all 100 get to the analysts or if only 90 forms do or even if 110 forms are presented . This sort of test would be looking for issues with the inherent system logical performance.

There are also people involved in capturing census data and in addition to keying the handprint fields accurately they are expected to follow certain keying rules . Our simulated data sets may also test to see if the keyers either individually or as groups are following these rules. For example keyers may be told that if they see someone print FIVE in the pop field where numbers are expected not letters then they are supposed to key the digits 05 assuming the intent of the respondent was clear. We can test for the extent to which keyers follow this rule by intentionally placing the character string FIVE in a pop numeric field and seeing if the resultant output ASCII data string contains a 05 as it should if the keying rule was followed correctly.

Although the above description is given with respect to preferred embodiments one skilled in the art of forms processing data capture will employ various modifications and generalizations to meet specific system needs. For example although basic forms are discussed above this invention clearly applies to other types of documents such as bank checks shipping labels health claim forms beneficiary forms invoices and other varieties of printed forms. The type of data being captured in addition to handprint could also be machine print cursive writing marks in check boxes filled in ovals MICR font characters barcodes etc. The special test materials might include printed test decks or in some cases when testing only a sub system of the overall forms processing system such as keying just the electronic snippets or images of these forms may suffice. The special test materials for which the truth is perfectly known by design may be used and or it is possible to determine the truth of a collection of real production data if that is desired.

