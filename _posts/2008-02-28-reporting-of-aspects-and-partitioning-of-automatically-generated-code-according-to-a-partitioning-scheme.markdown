---

title: Reporting of aspects and partitioning of automatically generated code according to a partitioning scheme
abstract: A method and system are described for generating a performance prediction report to assist finalizing a partitioning scheme of a block diagram model. Providing a user-defined partitioning scheme and information describing a target hardware platform used in that partitioning scheme, the present invention can generate a performance prediction report by analyzing the computational characteristics of the block diagram model.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08402411&OS=08402411&RS=08402411
owner: The MathWorks, Inc.
number: 08402411
owner_city: Natick
owner_country: US
publication_date: 20080228
---
This application claims priority to and the benefit of U.S. patent application Ser. No. 11 025 747 filed on Dec. 29 2004 and U.S. Provisional Application No. 60 611 907 filed on Sep. 20 2004 for all subject matter common to both applications. The disclosure of the above mentioned application is hereby incorporated by reference herein in their entirety.

The present invention relates to a method and system for partitioning a block diagram model and generating reports to assist in partitioning the block diagram model.

In graphical modeling environments such as a model based design tool block diagram models can be created to represent a design or algorithm of an implementation for a computational hardware. One or more block diagram models may represent a design for a target hardware platform. A target hardware platform used in this application may include a single computational hardware component or multiple computational hardware components. A target hardware platform may also have other elements such as memory interfaces or other integrated circuits ICs . A computational hardware component is used to mean any hardware component with computational capability such as a digital signal processor DSP general purpose processor GPP microcontroller application specific integrated circuit field programmable gate arrays FPGA or the like.

An automatic code generation application can automatically generate code and build programs from the block diagram model for implementation on the computational platform based on the design. In this manner the design of a block diagram model behaves as an implementation specification for automatic code generation. A graphical block diagram modeling environment can be used to produce designs and algorithms that can be implemented on computational hardware components such as DSPs GPPs microcontrollers ASICs or FPGAs.

For a model based design that represents a computational algorithm intended for implementation on a single computational component such as one processor the automatically generated code implementation of the design can be relatively straightforward. However a block diagram model may represent a design for implementation on a hardware platform that may include multiple computational hardware components for example an embedded control system with multiple different processors and FPGAs. In another example a block diagram model design which may be implemented in code to run on a single processor instead may be retargeted to run on multiple processors for a variety of design reasons such as quality cost speed etc. In these examples of multiple computational hardware components it is much more challenging and difficult to generate an implementation by automatic code generation from a model based design since the implementation is dependent on the topology of the hardware. An arbitrary partitioning of a block diagram model across the multiple computational hardware components or a partitioning scheme that does not consider attributes of one or more computational hardware components may not yield intended results and fail to meet certain design requirements.

The process of partitioning a block diagram model is not a trivial task because there are many degrees of freedom one can consider to partition a block diagram model. Requirements of a block diagram model such as input and output signal rates timing constraints computational load maximum system latency and attributes of a target hardware platform such as computational processing capacity of each computational component the speed of different interfaces the type and size of memory are variables one may consider while determining a partitioning scheme of a block diagram model. Moreover with the wide range of possible combinations of processor and integrated circuit IC components that can reside on a target hardware platform the partitioning of a model based design is further complicated as these components can be interfaced together by a wide range of different types of physical communication and control interfaces.

Furthermore computational hardware components such as GPPs microcontrollers and DSPs often include one or more types of memory with varying memory access rates. For example an embedded system may have DSPs where each processor may contain on chip random access memory RAM and or read only memory ROM . Furthermore each DSP may also access external RAM memory and the multiple DSPs may share the same external memory. On chip internal memory is typically faster but smaller in size than the external memory. How code and data are placed in memory affects the performance of a model based design implementation and the burden on the partitioning in terms of memory mappiong of the model further increases.

It is often burdensome to translate a model created in the graphical modeling application to a format suitable for execution on a target hardware platform. This burden becomes greater when the code representative of the model is partitioned across multiple computational components. This process of partitioning is often performed in manual trial and error fashion with little or no assurance of convergence to an optimal solution or a suitable solution. Given a specific model design it is often hard to discern whether a particular target hardware platform is suited for the implementation of a specific design without actually testing the implementation on the target hardware platform. It is often impractical costly and time consuming to acquire multiple hardware platforms variants to perform actual trial implementations. Additionally the hardware platform may not be available. On the other hand given a particular target hardware platform it is hard to determine whether a design will execute as intended on the particular target hardware platform without actually generating code from the model design and executing the code on the target hardware platform. Therefore there is a need for a tool that provides a quantitative assessment of any candidate partitioning scheme on the target hardware platform prior to code generation and actual execution.

The present invention provides a method and system for generating a performance prediction report of a partitioning scheme of a block diagram model before any code is generated and executed on the target hardware platform. A partitioning scheme is a representation of how the block diagram model is to be partitioned across multiple components such as computational hardware components or memory components of the target hardware platform. The data presented in the performance prediction report provides metrics that assist a user to arrive at a desirable partitioning scheme and actual partitioning for a block diagram model in a graphical modeling environment.

To practice the present invention information about the attributes of a selected target hardware platform is needed for performance analysis. Specifications of a target hardware platform may include any objective properties of each the computational hardware component of the target hardware platform such as processor clock rate internal memory size of a particular computational hardware component and the like. Additionally the present invention analyzes the computational requirements memory sizing and data routing aspects of the model and applies this information to predict the operational performance of a candidate partitioning scheme on the selected computational hardware components.

Specifically the performance prediction report can provide memory utilization interface loading and throughput estimates of a user defined partitioning scheme and a user can further specify the granularity of the data in the report to be represented. The level of granularity can be any of a block a subsystem a model or other unit element within the block diagram model. Based on the report the user can choose to modify the partitioning scheme selection of computational hardware component and or the block diagram model to achieve a user specific goal.

Given a particular partitioning scheme for a model design the present invention allows a user to estimate the performance of one or more computational hardware components knowing the specifications of these components. Thus the present invention provides a user a tool to determine what type of target hardware platform resource is required to satisfy requirements of the model design without an explicit implementation trial generating code and executing the generated code on any hardware platform. On the other hand many partitioning schemes for a given model design may be assessed for a particular hardware platform without explicit implementation trials on the particular hardware platform.

In one embodiment of the present invention a method for generating a report on a partitioning scheme for a block diagram model is provided. The method includes the step of providing information describing a target hardware platform. The method further includes the step of providing the partitioning scheme for said block diagram model. The method also includes the step of generating a performance prediction report predicting one or more operational characteristics of the target hardware platform when executing a representation of the block diagram model on the target hardware platform based on the partitioning scheme.

In one aspect of the present invention the target hardware platform includes one or more computational hardware components. In another aspect of the present invention the target hardware platform includes one or more interfaces. In one aspect of the present invention the information describing the target hardware platform is received from a storage location from a user input via an API call to the target hardware platform or any combination thereof. In another aspect of the present invention the information describing the target hardware platform includes any one of memory information interface information processing speed information of a computational hardware component or any combination thereof.

In one aspect of the present invention the memory information includes a description of a size of a memory element and or an access speed of the memory element. In another aspect of the present invention the interface information includes a description of a type of an interface element a speed of the interface element and or two hardware components that use the interface element to communicate with each other. In still another aspect of the present invention the processing speed information includes a description of a clock rate and or a processor speed of the computational hardware component.

In one aspect of the present invention the performance prediction report indicates any one of insufficient memory in a computational hardware component for a segment of code generated from the block diagram model overloading of an interface excess CPU usage of a computational hardware component failure to meet a system requirement failure to meet a user preference or a combination thereof.

In one aspect of the present invention the partitioning scheme represents a memory mapping scheme for associated code and data of the block diagram model. In another aspect of the present invention the one or more operational characteristics of the target hardware platform includes any one of memory usage data transfer times throughput of the block diagram model or a combination thereof. In still another aspect of the present invention the one or more operational characteristics of the target hardware platform are presented in the performance prediction report at a granularity level of any one of a block a partition a subsystem a model or any combination thereof.

In one aspect of the present invention the performance prediction report includes a timing diagram showing interface loading and computational hardware component usage as a function of time. In another aspect of the present invention the partitioning scheme partitions said block diagram model into two or more partitions.

In one embodiment of the present invention the method for generating a report on a partitioning scheme for a block diagram model further includes the step of modifying at least one of the block diagram model the information describing the target hardware platform and the partitioning scheme to reach a user specific goal. In another embodiment of the present invention the method includes the step of generating code from the block diagram model for execution by the target hardware platform. In still another embodiment of the present invention the method includes the step of executing the code on the target hardware platform and generating a statistical report presenting one or more real time operational characteristics of the target hardware platform. In yet another embodiment of the present invention the method includes the step of comparing at least one of the one or more predicted operational characteristics with at least one of the one or more real time operational characteristics. In still another embodiment of the present invention the method includes the step of altering generation mechanics of the performance prediction report to generate other prediction results based on the comparison of the at least one of the one or more predicted characteristics with the at least one of the one or more real time operational characteristics. In yet another embodiment of the present invention the method further includes the step of modifying at least one of the partitioning scheme the block diagram model and the information describing the target hardware platform based on the comparison of the at least one of the one or more predicted characteristics with the at least one of the one or more real time operational characteristics.

In one embodiment of the present invention a method for obtaining a report on a partitioning scheme for a block diagram model is provided. The method includes the step of providing information describing a target hardware platform. The method further includes the step of defining the partitioning scheme for said block diagram model. The method also includes the step of partitioning said block diagram model based on the partitioning scheme. The method includes the step of receiving a report predicting one or more operational characteristics of the target hardware platform when executing the block diagram model based on the partitioning scheme.

In one aspect of the present invention the partitioning scheme is a user defined partitioning scheme. In another aspect of the present invention the information describing the target hardware platform is provided from a storage location from user input via an API call to the target hardware platform or any combination thereof. In yet another aspect of the present invention the partitions are represented by a graphical representation. In one embodiment of the present invention the method for obtaining a report on a partitioning scheme for a block diagram model further includes the step of modifying at least one of the block diagram model information of the target hardware platform and the partitioning scheme based on the report.

In one embodiment of the present invention a medium in an electronic device holding computer executable instructions for a method generating a report on a partitioning scheme for a block diagram model is provided. The method includes the step of providing information describing a target hardware platform. The method further includes the step of providing the partitioning scheme for said block diagram model. The method also includes the step of generating a performance prediction report predicting one or more operational characteristics of the target hardware platform when executing a representation of the block diagram model based on the partitioning scheme.

In one embodiment of the present invention a system for generating a report on a partitioning scheme for a block diagram model is described. The system includes an application for manipulating the block diagram model a build tool for receiving the partitioning scheme and information describing a target hardware platform and a report generator for generating a performance prediction report predicting one or more operational characteristics of the target hardware platform when executing a representation of the block diagram model based on the partitioning scheme.

In one aspect of the present invention the target hardware platform includes one or more computational hardware components. In another aspect of the present invention the target hardware platform includes one of one or more interfaces. In one aspect of the present invention the information describing the target hardware platform is received from a storage location from a user input via an API call to the target hardware platform or any combination thereof. In another aspect of the present invention the information describing the target hardware platform includes any one of memory information interface information processing speed information of a computational hardware component or any combination thereof.

In one aspect of the present invention the memory information includes a description of a size of a memory element and or an access speed of the memory element. In another aspect of the present invention the interface information includes a description of a type of an interface element a speed of the interface element and or two hardware components that use the interface element to communicate with each other. In still another aspect of the present invention the processing speed information includes a description of a clock rate and or a processor speed of the computational hardware component.

In one aspect of the present invention the performance prediction report indicates any one of insufficient memory in a computational hardware component for a segment of code generated from the block diagram model overloading of an interface excess CPU usage of a computational hardware component failure to meet a system requirement failure to meet a user preference or a combination thereof.

In one aspect of the present invention the partitioning scheme represents a memory mapping scheme for associated code and data of the block diagram model. In another aspect of the present invention the one or more operational characteristics of the target hardware platform includes any one of memory usage data transfer times throughput of the block diagram model or a combination thereof. In still another aspect of the present invention the one or more operational characteristics of the target hardware platform are presented in the performance prediction report at a granularity level of any one of a block a partition a subsystem a model or a combination thereof.

In one aspect of the present invention the performance prediction report includes a timing diagram showing interface loading and computational hardware component usage as a function of time. In another aspect of the present invention the partitioning scheme partitions said block diagram model into two or more partitions.

In one aspect of the present invention the build tool generates code of the block diagram model for execution by the target hardware platform. In another aspect of the present invention the report generator generates a statistical report presenting one or more real time operational characteristics of the target hardware platform after the code is executed on the target hardware platform. In still another aspect of the present invention the report generator compares at least one of the one or more predicted operational characteristics with at least one of the one or more real time operational characteristics. In yet another aspect of the present invention the report generator alters generation mechanics of the performance prediction report to generate another prediction results upon comparing the at least one of the one or more predicted operational characteristics with the at least one of the one or more real time operational characteristics. In still another aspect of the present invention the partitioning scheme is user defined partitioning scheme.

The present invention discloses a method and system for generating a performance prediction report to assist in evaluating a partitioning scheme of a block diagram model. Given the specification description of a target hardware platform and a partitioning scheme the present invention relates the given information and data to the computational characteristics of the block diagram model and generates a performance prediction report predicting the operational characteristics of the computational hardware platform running code automatically generated from the block diagram model. The present invention applies to one or more computational hardware components with both like processors and unlike processors. The present invention also applies to other computational hardware components such as FPGAs ASICs and the like. The present invention also applies to a hardware platform comprising both processors and IC devices such as FPGAs and ASICs. The present invention enables a user to make further changes to the partitioning scheme or the block diagram model after reviewing the performance prediction report to achieve a user specific partitioning goal.

For example a user wants to achieve lowest possible system latency the delay between when an input enters the system and when its corresponding output is emitted by the system for a block diagram model but unfortunately the fastest processor available does not have the sufficient memory capacity to fit the most computational intensive subsystem. Therefore the best partitioning scheme for achieving lowest system latency with the available computational hardware components may not be easily obtained. The generation of performance prediction reports can therefore provide a user with performance estimates of a partitioning scheme. The user can use the predictive reports as a basis for iterative partitioning trials to arrive at a partitioning scheme that achieves the user specific goal of satisfying model design requirements in the implementation on the hardware platform.

Once a satisfactory partitioning scheme is reached code can be generated for execution on the target hardware platform. A statistical report presenting the real time operational characteristics of the multiple computational hardware components is generated. Optionally the user performs final tuning of the final partitioning scheme based on the statistical report from actual execution on the target hardware platform. Any discrepancies between the statistical report from actual execution and the performance prediction report is provided to the report generator as feedback so that future generated performance prediction reports provides better predictive results.

The application program will be described below for illustrative purposes as a block diagram modeling application such as Simulink from The MathWorks Inc. of Natick Mass. Nevertheless those skilled in the art will appreciate that the principles and concepts described below are equally applicable to other graphical modeling applications such as LabView System View Signal Processing Workstation HyperSignal COSSAP Angeles Ptolemy and other like graphical model applications. Furthermore those skilled in the art will also appreciate that the principles and concepts are equally applicable to non graphical modeling applications as well.

In one embodiment of the present invention the library is a repository of blocks that exist in the graphical modeling environment provided by the application program . The registry in conjunction with the user interface allow a user to register with the application program information describing the first and second computational hardware components and . Those skilled in the art will appreciate that the illustration of the user interface the registry and the library is merely illustrative and these elements can be physically or logically located in the application program . The electronic device also includes a keyboard and a pointing device such as a mouse trackball or lightpen for interfacing with the application program .

The user interface programmatically registers with the registry the various information describing the target system wherein the information is inputted by a user or retrieved from a storage location within the storage device or from the target system . Alternatively registry may obtain the information describing the target system from an application programming interface API call to another application from an integrated development environment IDE or via an interface to the target system or other system. One of ordinary skill in the art will recognize that there are many ways registry may obtain the information describing the target system including the first and second computational hardware components and and various interfaces and the examples given here are not meant to be limiting.

The registry stores the registered information and build tool receives the information of selected computational hardware component s . Those skilled in the art will recognize that the read tool build tool report generator may each be a separate program like a plug in from the application program or a tool in the application program . Additionally read tool build tool and report generator maybe be combined to become one or more programs or tools. Furthermore those skilled in the art will recognize that the user interface can take the form of a graphical user interface that provides a user of the system with textual and graphical information to allow the user to browse and select computational hardware components interfaces and their specification information modify such information or create a new hardware target system profile. Those skilled in the art will appreciate that the user interface is also implementable as a command line interface or other suitable interface that allows a user to register a computational hardware component with the application program .

In another embodiment the server coupled to the network is adaptable to include an application program and using the local registry to register the processors profiles and store the blocks used in the block diagram model in the library on the client machine as well. In this manner the application is on the server but runs on the client. In still another embodiment the server coupled to the network is adaptable to include a library and a build tool with a registry working with the application on the client. In this configuration the application program resides on a client computer but can be run on a server computer using the server s storage mediums and processing capabilities. Those skilled in the art will recognize that there are many other different configurations possible in a non distributed computing environment for practicing the present invention.

Besides the information describing the computational hardware component s build tool can also receive a user defined partitioning scheme. Additionally build tool can evaluate the individual blocks in the block diagram model to obtain block attributes for use in dividing the block diagram model into partitions.

Block attributes are characteristics of a block such as block name data type sampling time the number of computations required memory size interface type e.g. serial or parallel the number of times the memory is accessed and the like. By way of example Simulink creates and stores block diagram models into model files with a .mdl file extension. Therefore read tool can read the .mdl file storing the specifics of all the blocks in the block diagram model and determine for example the computational requirements of each block and the amount of data transfer there is between each block.

Read tool can further determine latency and real time related issues by reading associating data and account for these issues in partitioning the block diagram model. For example latency associated with memory access durations by the multiple computational hardware components and data transfers times between them given the operational characteristics of the computational hardware components. Those skilled in the art will appreciate that there are many other parameters the read tool may collect upon partitioning.

Once the read tool has the information describing the one or more computational hardware components and the partitioning scheme from build tool read tool retrieves model attributes and provides the data to report generator . Read tool may obtain the block diagram model by reading a block diagram model file or another file representing a preprocessed or intermediate representation of the block diagram model. In one embodiment the block diagram model may be obtained by making an API call to another application such as application .

In another embodiment the application may trigger the model reading process by sending the block diagram model to report generator and requesting the report generator to generate a report. There are many different ways that read tool may obtain information regarding the block diagram model. For example read tool can read from a file and obtain that for a specific block in the block diagram model that the block requires three thousand four hundred and eleven clock cycles to execute all the computation required within the block and the code size is two thousand and four hundred bytes. Alternatively a function can be used to calculate exactly or approximately how many clock cycles or how much memory is needed for each block in the block diagram model given the information from the block diagram model itself and or information describing the computational hardware component. The information can be the number of samples in an input signal the data type of the input signal and how long it takes to access these data in memory and the like. For signal information read tool may obtain the corresponding attributes by reading a file that gives the data types of each input the number of samples for each input the size of each input and other information related to a signal.

Alternatively a formula can be used to calculate the total memory access time of a signal given information such as the number of samples of input and the memory access time or number of wait states of the memory. One skilled in the art will appreciate that there is many possible ways for read tool to obtain information within a block diagram model to generate a performance prediction report. One skilled in the art will also appreciate that the functionality of read tool may be built into application build tool or report generator .

After the read tool obtains the information describing computational hardware components the partitioning scheme and the computational information of the block diagram model read tool provides the obtained data to report generator . Using the data report generator can generate a performance prediction report. The report generator may map component specific program code corresponding to a block to memory and estimate how much memory is needed for the block. The report generator may further add up all the program code by memory bank to give a total estimate of how much memory is needed in total for a subsystem a partition or the whole model. The granularity of the report may be at any level that is supported by the block diagram model. For example the report generator may generate a report at the granularity level of a function if individual function information can be retrieved from the block diagram model. The report generator may further compute predicted CPU usage of a block by using information on estimates of number of clock cycles required to execute all the computations needed within the block. Report generator may further receive system requirements or user preferences such as maximum system latency maximum memory usage maximum computational utilization maximum interface loading and the like to include in predicting the performance of the one or more computational hardware components.

If multiple computational hardware components are used in the partitioning scheme the interfaces used by the computational hardware components to communicate with each other are important to the performance of the partitioning scheme on the multiple computational hardware components. Information describing the interfaces would be registered with registry if multiple computational hardware components are used. Using the model information provided by read tool and interface information provided by registry report generator may compute predicted transfer time for each signal data from a first computational hardware component to a second computational hardware component. Report generator may also compute the time the second computational hardware component waited for receiving data from the first computational hardware component.

The present invention can also be applied to co pending U.S. application Ser. No. 11 021 760 the content of which is incorporated herein by reference. In one embodiment of the present invention component interfaces are automatically generated for a target hardware platform from a block diagram model as described in the above referenced application and report generator generates a report presenting properties of the component interfaces generated from the block diagram model.

The partitioning scheme partitions the block diagram model into partitions. In one embodiment of the present invention application may provide a grouping and selection mechanism to group one or more blocks of the block diagram model into partitions. Application may further provide a graphical representation of the partitions. For example each partition may be highlighted with a certain color shade or a box with a certain line type that visually groups the blocks that make up the partition. Alternatively read tool may provide the graphical representation of the partitions instead of application . There can be additional programs or tools that provide the partitioning mechanism and the examples listed here are not meant to limit the scope of the present invention.

The report generator may further provide a predicted timeline of CPU usage and interface loading for each partition in a timing diagram. The report generator may generate the performance prediction report displaying predicted memory and CPU usage and interface loading. A timing diagram showing the predicted timeline of CPU usage and interface loading for each partition may also be provided in the performance prediction report. One skilled in the art will appreciate that there are many aspects of the partitioning scheme the report generator can present in the performance prediction report as long as the aspects can be predicted using data obtained from the block diagram model information describing the one or more computational hardware components and the partitioning scheme and the examples given here are not to be used to limit the scope of the present invention. One skilled in the art will also appreciate that the performance prediction report may be presented at a granularity level of many kinds such as by function by block by subsystem by partition by the whole model and the examples given here are not meant to be limiting the scope of the present invention.

The present invention can also be applied to co pending U.S. application Ser. No. 11 019 044 the content of which is incorporated herein by reference. In one embodiment of the present invention report generator utilizes the memory mapping technique described in the above referenced application and generates a report presenting the memory mapping of the block diagram model to several memory with different sizes and access speeds.

In one embodiment of the present invention build tool can generate component specific executable code of the block diagram model using the partitioning scheme. In one aspect build tool divides the block diagram model into partitions and assigns each partition to a computational hardware component. During code generation each partition is translated to a component specific implementation of the assigned computational hardware component.

The computational hardware components and will be described below for illustrative purposes as microprocessors which can be one of DSP GPP specific purpose processor and the like. Other types of computational hardware components suitable for use in practicing the present invention includes microcontrollers ASIC FPGA and other computational hardware components capable of carrying out a computation. Those skilled in the art will appreciate that computational hardware component can be of the type DSP whereas computational hardware component can be of the type of FPGA and both of these computational hardware components can be used in one partitioning scheme. One skilled in the art will appreciate that the computational hardware components do not need to be the same or of the same type to practice the present invention.

During code generation in one preferred embodiment each partition is translated to a component specific implementation. In operation an automatic code generator reads the partitions of the block diagram model and associated input files to generate source code by translating the block diagram model into one or more source code files. Associated input files may include templates commands input parameters configuration data source code data and class definition or any other information that may be used by the automatic code generator to generate source code for the block diagram model. Associated input files can also include files to provide input to and configure the automatic code generator to generate source code files for a specific target hardware platform for example a DSP. By way of example the automatic code generation can be discussed in terms of generating code with Real Time Workshop from a block diagram model generated by Simulink . Real Time Workshop uses target language compiler script files with a .tic file extension as input files to the code generation process. The .tic files provide sections of programming instructions to be implemented for block references as they may be found in a block diagram model during the code generation process. The .tic files also can provide data and class definitions for data element references found in the block diagram model.

Additionally the .tic files also include compiler directives built in functions and other code generation commands to direct Real Time Workshop during the code generation process. Simulink creates and stores block diagram models into model files with a .mdl file extension. As part of the code generation process Real Time Workshop reads in a .mdl model file and analyzes the model to generate an intermediate model file with an .rtw extension. This intermediate .rtw model file includes a hierarchical structure of records describing systems and their blocks and connections analyzed from a block diagram model of the .mdl file.

A language compiler called the target language compiler of Real Time Workshop works with .tic files and .rtw files to produce code. The target language compiler interprets a program that reads the intermediate model file description of a .rtw file. As the target language compiler encounter a record in the .rtw file it uses directives in the .tic files corresponding to the record to direct the code generation process for the specific record. As such the target language compiler works much like a text processor. For example the target language compiler uses block .tic files which specify the particular code for a block to transform each block into code. When it reads a record in the .rtw file that references a block the target language compiler applies code from the corresponding block .tic file to generate code for the block in source code files. Additionally model wide .tic files are also used to provide input to the target language compiler for global customization of the code. Model wide .tic files may direct the target language compiler to generate main routines to provide entry points into the program source code header files to setup data structures and utility functions to support code for particular blocks. The block and model wide .tic files can be modified to generate customized code for blocks and to generate any desired global customizations to the code.

The source code files generated from the automatic code generator such as Real Time Workshop may include program instructions of a programming language such as C which may further be in a format and style following the ANSI ISO C standard. Additionally the source code files may be generated to include fixed point or floating point source code. The program instructions of the source code files may be generated to run on any operating system such as a real time operation system or for a specific processor.

In another embodiment instead of generating code from partitions the automatic code generator can generate code for each block in the block diagram model. The code generated and associated files for each block can include not only block attributes but also functions and operations the block performs. After each block is translated into code build tool groups the translated code into partitions according to the partitioning scheme of the block diagram model and assigns each partition to a computational hardware component.

Report generator may generate a statistical report after code is run on the one or more computational hardware components. Report generator may communicate with target system and obtain real time statistics via an interface. One example of such an interface is implemented by an API of the target system s IDE. The API is usually a programming interface accessible through C C Basic or other programming or scripting languages. For example Texas Instruments Code Composer Studio allows C and Perl programs to communicate with its IDE.

In one embodiment of the present invention code is inserted to measure processing time or data transfer time of a unit element of the block diagram model such as a block or a signal. In one aspect code is inserted during code generation. Alternatively code is inserted according to a user s preference after code generation. For example a timer is set at the start of a block s program code and the timer is stopped at the end of the code the processing time can then be calculated by finding out the time difference between the time the timer starts and the time the timer stops. The recorded times may then be stored in the target platform s memory where it can be retrieved later using a call to an API via an interface. This method can similarly be applied to measuring the transfer time of data from one component to another. This method can also be used to measure other time aspects that a user is interested in knowing. Memory usage statistics may be retrieved from the linker generated memory mapping file and the symbol table.

If multiple computational hardware components are used in the partitioning scheme the interfaces used between the computational hardware components are also important for evaluating a partitioning scheme. The type of interfaces used also effect the performance of the computational hardware components. illustrates an exemplary graphical user interface that a user can use to enter properties of an interface. In interface property window there are a drop down menu a name entry a connectivity entry a type entry and a speed entry . One skilled in the art will appreciate that these different entries need not be listed in such a way the illustrative example has shown but rather the entries could also be displayed using different tabs or other graphical representation. In the illustrated example of Interface as specified in menu is given the name p p in the name entry . The connectivity entry includes a first and second drop down menu and listing registered computational hardware components. In the given example the connectivity for Interface is chosen to have Processor and Processor in the first drop down menu and second drop down menu respectively. In the illustrative example Interface is further given 2 MB s in the speed entry and Serial DMA from the drop down menu in type entry . One skilled in the art will appreciate that the interface may be an interface between a computational hardware component and an I O device or other interfaces within the target hardware platform.

Those skilled in the art will appreciate that information about the computational hardware components interfaces etc. can be entered using a command line interface or other suitable interface and are not limited to using a graphical user interface. The graphical user interface presented here are merely for illustrative purposes and a graphical user interface that utilizes selection tools other than drop down menus or text fields is also possible. Those skilled in the art will recognize that the computational hardware component properties depicted in the memory properties depicted in and the interface properties depicted in are merely illustrative and the listing of parameters in these figures and the specific examples given here are not meant to be limiting.

After a user is presented with the performance prediction report the user can decide if any of the block diagram model the selection of computational hardware components and the partitioning scheme needs to be modified in step . If a modification is made report generator generates another performance prediction report based on the most updated block diagram model and the partitioning scheme in step . If no modification is made build tool generates code of the block diagram model in step for execution by the one or more computational hardware components. Code is generated for each computational hardware component using the platform specific implementation. Code is then executed on the one or more computational hardware components in step . Report generator then generates a statistical report in step presenting one or more operational characteristics of the one or more computational hardware components.

Data for the statical report can be retrieved for example from an integrated development environment IDE that works with the computational hardware components where the IDE has control of the execution of the computational hardware components i.e. read memory profile its execution etc . For example the user interface and the build tool can be adaptable and configurable to work with a COM compliant application that interfaces with the computational hardware components. For example for memory usage a linker outputs a file that shows memory usage as well as how code and data are mapped to physical memory. The report generator may consult these files and extract desired usage information for generating a report. For execution performance many IDE s include profiling tools to measure for example how long a series of executions take by individual processor and the report generator can get this information through an API application program interface and generate reports accordingly. Alternatively instrumentation code can be implemented in the embedded software with the build tool to measure the real time partitioning performance. One skilled in the art will recognize that there are many other ways to obtain data to generate a statistical report. Reports can be in the form of a pie chart a bar graph an html file and the like. One skilled in the art will recognize that the format of the report can be implemented in many different ways.

In step the report generator can compare the one or more operational characteristics of the one or more computational hardware components in the performance prediction report with the one or more operational characteristics of the one or more computational hardware components in the real time statistical report. In step feedback may be provided to report generator based on the comparison results of the two different reports so that report generator may adjust its prediction methodology to provide better projection of a partitioning scheme. A user may decide if further modification is needed for any of the partitioning scheme the block diagram model and the selection of computational hardware components in step . If yes modification is made and a user can decide to generate a prediction report in step or decide to generate code based on the modified partitioning scheme block diagram model and or selection of computational hardware components in step .

When two computational hardware components may not communicate directly with each other they could communicate with each other via other components such as first computational hardware component communicates with fourth computational hardware component through either second computational hardware component or third computational hardware component . Optionally all the computational hardware components may have access to a common bus not shown that provides them access to a common FIFO queue memory not shown . Additionally the computational hardware components may optionally have access to an I O device such as an analog digital converter ADC not shown . Two or more computational hardware components may optionally have access to a shared memory. One ordinarily skilled in the art will appreciate that the target system may include many different computational hardware components with different types of memories and different interfaces to communicate with each other or the different memories and the illustrative embodiment described herein is merely given as an example.

Given information describing each of the computational hardware components and interfaces the present invention can generate a performance prediction report predicting one or more operational characteristics of the computational hardware components running code automatically generated from the block diagram model. The report can take a number of format and style for example it may be a table a bar diagram a pie chart a histogram or other suitable representation. In the preferred embodiment of the present invention the report includes memory usage computational utilization interface loading and timing diagram.

In the performance prediction report generated using the partitioning scheme shown in the report includes an exemplary timing diagram as shown in . In the exemplary timing diagram the horizontal axis shows time whereas the vertical axis shows partitions and the interfaces between the partitions. The timing diagram may include other interfaces other than the interfaces between the partitions. For example an interface between an input device to a computational hardware component may be shown on the timing diagram to illustrate the amount of time needed to receive input data samples from the input device. In tdenotes the beginning of execution of the block diagram model and t denotes the time variable. Along the vertical axis labels p p p and p represents the partitions corresponding to computational hardware component computational hardware component computational hardware component and computational hardware component respectively. Additionally label p p represents the interface for transferring data from p computational hardware component to p computational hardware component . Similarly label p p means the interface for transferring data from p to p label p p means the interface for transferring data from p to p . . . label p p means the interface for transferring data from p to p. In the exemplary timing diagram B to B represents the time required for the assigned computational hardware component to perform computations defined by the block on the sample data received for the corresponding block B to B in . For example B B and B appear on the row for label p which means that B B and B are assigned to partition p which is to be processed by computational hardware component .

Of the signals S to S from the signals that interconnect blocks in different partitions computational hardware components are represented in the timing diagram of . The representation of a signal in the timing diagram indicates the transfer time required for data associated with the particular signal to transfer from one partition to another partition. For example S is shown in the row for label p p which means that S represents the time required for the signal to transfer from p computational hardware component to p computational hardware component . For signals transfer between partitions computational hardware components that have a direct communication link the signal utilizing the direct communication link for data transfer appear only once in the timing diagram and specifically only on the row of the label which represents the direct communication channel. S is one of such example signal.

However the signals not utilizing a direct communication link to transfer data will appear more than once in the timing diagram. Specifically it is shown on the corresponding row of the links which represent all the communication links that the signal utilizes to transfer data. S is such an example and it appears twice in the timing diagram. In signal S represents data transfer from partition p computational hardware component to partition p computational hardware component . However in there is no direct communication channel between computational hardware component p and computational hardware component p . Therefore for data transfer from computational hardware component to computational hardware component the transfer process has to go through either computational hardware component or computational hardware component . In the referenced example data on signal S is transferred from computational hardware component to computational hardware component via computational hardware component interfaces p p and p to p . Hence S appears twice in the timing diagram. Similar reason applies to S in this example.

Assuming the block diagram designed application samples the input signal at a rate of 10 milliseconds and the system requirements for maximum latency is 20 milliseconds the timing diagram as shown in shows that computational hardware component takes too long more than 10 milliseconds to process the assigned partition p and computational hardware component does not finish processing partition p within the maximum latency required 20 milliseconds . In other words both the real time and latency requirements are violated since the processing time for p is too long processing cannot keep up with input rate and p is unable to finish processing within the 20 milliseconds window as specified by the maximum latency requirement. For ease of understanding the timing diagram only shows the processing sequence that a specific input data follows and one of ordinary skill in the art will recognize that each process is replicated in all periodic processing intervals. For example when p is processing B p is processing B concurrently but on a different input data set. Based on the performance prediction report the partitioning scheme the block diagram model and or the selection of computational hardware components may be changed.

Given that a user can elect to change the partitioning scheme shows a new partitioning scheme for the same block diagram model as shown in . The partition p is reduced in size to process B and B only and therefore might solve the problem of p being unable to keep up with real time processing reported in . p p and p are reassigned to process different blocks to minimize transfer of data between computational hardware components without direct communication channel interface . Specifically in the partitioning scheme as shown in S and S take extra transfer time because there is no direct communication channel between p computational hardware component and p computational hardware component or between p computational hardware component and p computational hardware component . Therefore the partitioning scheme as shown in S is the only signal data that require extra transfer time due to lack of direct communication interface between p and p.

A performance prediction report based on the new partitioning scheme as shown in is generated. Different aspects of the report are illustrated in to . illustrates the new timing diagram and as illustrated the problems illustrated in have been solved using the new partitioning scheme. to illustrate an exemplary report in table format showing the memory usage computational utilization and interface loading respectively for each partition computational hardware component . A user may read the report on memory usage to ensure none of the partition requires more than 100 of memory for all the code and data within a partition. Similarly a user may read the computational utilization report to check if any partition utilizes more than 100 of the processing interval. Additionally the user may look at the computational utilization report to try to distribute computational load required for each partition.

Alternatively the performance prediction report may give indications graphically or by text that memory usage computational utilization or other aspects of the partitioning scheme is problematic instead of relying on a user to inspect data on the report. The performance prediction report may indicate insufficient memory in a computational hardware component for a segment of code generated from the block diagram model. The performance prediction report may also indicate excess CPU usage of a computational hardware component. The performance prediction report may also indicate failure to meet a system requirement. The performance prediction report may also indicate if a user preference is not met. For example if a user has a preference of memory usage of less than 70 on Partition the performance prediction report may report that there is insufficient memory in Partition to place the segment of code generated for this partition to meet the user preference of less than 70 memory usage in Partition. The performance prediction report may also indicate if there is overloading of an interface. Overloading may occur if an interface is utilized for more than 100 of the processing interval or if the interface is utilized more than required by the system requirement or a user preference.

Referring to memory usage for each partition is shown based on the partitioning scheme shown in . Methods for predicting memory usage and obtaining real time memory usage statistics have been discussed above with respect to . Within each partition program code size for each block sample data of each signal and other data that is stored in memory are reported. For example memory usage for Partition corresponding to p that is executed on computational hardware component is reported that 1237 bytes of program code of block B is stored in local memory which has the capacity of 65536 bytes in other words 1.89 of the capacity of the local memory is used to store the 1237 bytes of code of block . Additionally memory usage in bytes and percentage for block program code S signal data S signal data S signal data S signal data stack and constants are reported. Finally adding up all memory usage in partition results in total memory usage of 51971 bytes in size and a percentage of 79.30 of the total capacity of local memory for computational hardware component . For partition to partition blocks program code signal data stack and constants are reported in a similar manner with a total memory usage calculated for each partition.

Referring to computational utilization for each partition is reported. As discussed above with respect to computational utilization may be estimated by model information read by read tool and hardware information registered with registry . In other words given the number of clock cycles needed for processing a block and the clock rate of the computational hardware component processing the block the amount of time that is spend on processing the block may be calculated. The methods for estimating required clock cycles for execution has been discussed with respect to . The real time statistics of computational utilization may be retrieved as discussed with respect to as well. In this figure the percentage is calculated with respect to a processing interval however one skilled in the art will recognize that this processing interval repeats itself over and over again and therefore the percentage also represents the percentage of computational power of the corresponding computational hardware component.

Referring to interface loading for transferring each signal through an interface is reported. For example interface represents signal transfer from p to p p p and S and S are the signals that utilizes this interface. Within the processing interval of 10 milliseconds which is defined by how fast input signals are sampled by the model each signal S and S takes a transfer time of 2 milliseconds to transfer 10 000 bytes for a total of 4.0 milliseconds to transfer 20 000 bytes of S and S. The transfer time is estimated dividing the amount of data that needs to be transferred by the speed of the interface. The real time statistics for transfer time of data may be measured using methods as discussed with respect to above. The percentage usage of the interface is calculated using the amount of transfer time divided by the processing interval which is 40 in this example of two signals S and S. Similarly interface loading is estimated measured and reported for other interfaces used in the partitioning scheme shown in .

Referring to total memory usage and computational utilization are shown for each partition. Partition is shown to utilize approximately 80 of the total memory and 60 of the computational power of the computational hardware component . These numbers correspond to memory usage report and computational utilization report shown in and respectively. Assuming computational hardware components and are given identical hardware information initially for simplicity then shows that partition requires the most memory usage and computational power. Therefore a user may choose to assign partition to the fastest available computational hardware component with sufficient memory. A user may also change the hardware information related to computational hardware to match the real computational hardware component used and have a report generated by report generator based on the information of the real target system.

Referring to total percentage of interface loading for each interface is shown. Interface p p is shown to take up 40 of a processing interval. This data corresponds to what is reported in . As shown by this figure interface loading by interface p p p p and p p are 40 and others are only as high as 20 . Therefore if a user has several available interface types the user can choose to select the fastest available ones for p p p p and p p so that to shorten the total transfer time of signal data on these three interfaces and hence reduce the corresponding percentage of interface loading.

The statistical report may look exactly like the performance prediction report or it may be different in presentation or format. Therefore to may represent exemplary statistical reports. One skilled in the art will recognized that the illustrative examples are given merely as an example and are not meant to limit the form or format of either the performance prediction report or the statistical report.

Many alterations and modifications may be made by those having ordinary skill in the art without departing from the spirit and scope of the invention. Therefore it must be expressly understood that the illustrated embodiments have been shown only for the purposes of example and should not be taken as limiting the invention. Moreover it is to be understood that the features of the various embodiment described herein are not mutually exclusive and can exist in various combination and permutations even if such combination or permutation are not expressly made herein without departing from the spirit and scope of the invention which is defined by the following claims. These claims are to be read as including what they set forth literally and also those equivalent elements which are insubstantially different even though not identical in other respects to what is shown and described in the above illustrations.

