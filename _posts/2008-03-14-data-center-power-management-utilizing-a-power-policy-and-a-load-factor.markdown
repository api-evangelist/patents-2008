---

title: Data center power management utilizing a power policy and a load factor
abstract: An exemplary method for managing power consumption of a data center includes monitoring power consumption of a data center, assessing power consumption with respect to a billing equation for power, based on the assessment, deciding whether to implement a power policy where the power policy reduces instantaneous power consumption by the data center and increases a load factor wherein the load factor is an average power consumed by the data center divided by a peak power consumed by the data center over a period of time. Various other methods, devices, systems, etc., are also disclosed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08001403&OS=08001403&RS=08001403
owner: Microsoft Corporation
number: 08001403
owner_city: Redmond
owner_country: US
publication_date: 20080314
---
Running an internet scale service successfully is not an easy or cheap business to be in. Managing a set of thousands of servers running different components spreading over various data centers in order to achieve high availability and performance is a challenging task.

One aspect that is usually overlooked or prioritized lower when it comes to the manageability of a data center is the energy cost of running the data center. With new higher end hardware needing more energy to run and with energy prices going up the energy cost of running a data center is becoming a bigger part of the total cost. Indeed power costs are rising while the cost of computing is rapidly declining which means that power costs will soon dominate.

An exemplary method for managing power consumption of a data center includes monitoring power consumption of a data center assessing power consumption with respect to a billing equation for power based on the assessment deciding whether to implement a power policy where the power policy reduces instantaneous power consumption by the data center and increases a load factor wherein the load factor is an average power consumed by the data center divided by a peak power consumed by the data center over a period of time. Various other methods devices systems etc. are also disclosed.

Various exemplary methods devices and system described herein pertain to managing power in one or more data centers. An exemplary management system is configured to manage power consumption based on data center resources and workload. Such a system can also account for how power is billed to increase data center efficiency and reduce operating costs.

In general a data center is designed to provide some number of watts of critical load i.e. the electrically powered equipment can consume no more than a critical load. In practice most data centers are not fully populated because the critical load is reached at prior to using all available floor space. In the example of the server farm may have about 100 000 square feet and a critical load of 250 watts per square foot for a total critical power consumption of about 25 MW.

In a data center servers are the primary equipment responsible for directly handling workload as indicated by a pie chart . Other equipment includes routers other network devices storage and the like. With respect to data center usage a plot of data center usage versus time shows that traffic in requests per second can vary greatly for example by a factor of two or more. Other measures of workload include megabits per second. A selected measure or measures for workload for purposes of management may depend on type of service or services operating in a server farm. In general for particular services a data center will experience a somewhat regular usage cycle with a maximum usage falling within a maximum usage band see e.g. dashed lines and a minimum usage falling within a minimum usage band see e.g. dashed lines .

As described herein a power model can be specified in terms of total power what a utility or utilities provide and critical power what is supplied to servers in a server farm . In such a model cooling and other overhead power usage can be defined as the difference between current supplied power by the utility or utilities and current consumed power by the IT equipment.

As described herein an exemplary power management system can acquire information from equipment in a server farm and manage power based in part on such information. Such a system may rely on a power model to manage power consumption of a server farm. For example a system can include a processor based controller that relies on control logic where the control logic manages power consumption based in part on a power model.

A pie chart indicates how electricity usage may be divided amongst various tasks. While the chart shows servers as consuming the greatest percentage of power depending on design climate etc. cooling may consume the greatest percentage of power.

In many instances servers consume the most power assuming they are powered for use. Power consumption for a server differs depending on whether it is powered for use or in actual use where power consumed in actual use typically further depends on workload. For example servers at idle on but doing nothing consume X power and servers at a maximum load consume Y power where Y is usually more than 1.3 X and sometimes more than 1.5 X. A server operating at less than maximum workload consumes somewhere between X and Y which may vary non linearly with workload.

As discussed in more detail below various mechanisms exist for matching server processing power to workload. When such mechanisms are implemented power consumption follows workload however with some damping as indicated in a plot of power consumption versus time. This damping occurs in real time. In other words with conventional mechanisms power reduction occurs in a direct response to workload. The coupling between workload and processing power reduction may reduce peak power consumption compared to a system with no power saving mechanism however coupling alone does not address some power billing parameters e.g. load factor with respect to time which are discussed further below.

Variations can occur in power consumption for example seasonal or diurnal differences may exist as factors such as sunshine and ambient temperature can affect power consumed in cooling a data center. In other words on a hot and sunny summer day a CRAC will require more power to cool a server farm compared to a cold and overcast winter day as the temperature difference between the heated cooling fluid and the external environment is reduced. This temperature difference acts as a driving force for removing heat from the cooling fluid.

As described herein cooling equipment can include a variety of equipment. While a CRAC unit is mentioned where AC refers to air conditioning other types of cooling equipment may be used alternatively or in addition to an air system e.g. water based etc. . A conventional CRAC unit normally uses a self contained refrigeration cycle to remove heat from a space s and a heat rejection system to transfer the heat to the environment. The heat rejection system typically takes one of the following forms condensing unit fluid cooler or cooling tower to discharge heat to the environment e.g. surrounding air water of a data center .

Additional factors that can complicate data center power management include i increase in power consumption of modern server components primarily processors and ii industry direction towards creating more densely packed server form factors e.g. blades. With respect to server power consumption while computational performance has been increasing according to Moore s law energy efficiency is increasing at a lesser rate. For example between the years 2000 and 2006 computational performance increased by a factor of nearly 300 while energy efficiency increased by a factor of about 8. Consequently during this period power consumption per computational unit decreased by about 90 percent and power consumption rose by a factor of about 3.5.

Some processor vendors have focused on lowering the power footprint in an effort to decrease power levels. Some processor vendors offer multiple power options for each processor family. These options enable server vendors to offer a family of servers for each processor family where each server has a different cost point. To date most enterprise customers have chosen expensive servers for data centers where these servers offer higher levels of performance but consume more power.

With the increase of power consumption within the server design of server cooling systems has become quite complex. This is especially true in blade form factors. Server vendors typically design their enterprise servers to support an entire processor family. In order to offer the highest performing processors they have to deal with the worst case scenario for data center power and cooling.

As a moderate example consider a fully populated 42U rack that contains six blade chassis and 168 processors. In this configuration the processors alone would require a power budget of over 20 KW. Most modern blade chassis have very complex cooling systems. What makes matters worse is that the data center cooling infrastructure consumes a large amount of power. As indicated by the pie chart estimates on the overhead on power consumption for cooling of a server range from 40 to 100 of the power drawn by the server.

Accounting for all of the foregoing factors the capacity of a data center is typically limited by its power and cooling capacity. The major problem that customers are experiencing today is that data center power and cooling infrastructure cannot support the increase in server power density. The capacity of a data center is measured by its ability to cool the heat that is dissipated. Conventional data centers typically support around 100 watts per square foot of cooling when using industry standard 19 inch racks while newer data centers may support over 200 watts per square foot. In either instance a fully populated blade chassis can have a power rating higher than the power rating for an entire rack. As mentioned with respect to the power model the limiting factor for a data center is total power the data center can make available to critical load and the data center cooling capacity. The result of this problem is that customers sparsely utilize space in their data center. They limit the number of servers deployed to each rack and also leave gaps between racks to facilitate cooling.

The data center of today is a mission critical resource. Down time in the data center and the disruption this causes to the business is not acceptable. To address this customers typically deploy redundant paths for cooling and power. In a failure situation each of these redundant paths must provide enough capacity to keep the entire data center operational. This further restricts the capacity of the data center. Additionally many data centers include a power plant with enough power generating capacity to keep the data center online indefinitely if mains power is disrupted. A data center may also have one or more redundant generators.

Problems resulting from under utilization of data center space vary depending on data center construction. Legacy data centers have limited cooling capacity and many customers are running out of capacity quickly. In some cases there is an abundance of physical space to deploy more racks and servers but power or cooling capacity is exhausted. There are other scenarios with more modern data centers that have limited physical space and don t have enough cooling capacity to increase the utilization level of the racks within the physical constraints. Whatever the problem solutions can be expensive. Many customers are facing a situation where they know they will run out of data center capacity at some time in the future and the cost to build a new data center typically runs into hundreds of millions of dollars.

With respect to billing peak power demand is an often used parameter that factors into a monthly billing equation. Peak demand can be defined for example as the highest consumption of electricity over a set time window e.g. 15 or 30 minutes which is then converted to an hourly interval. The time window is often referred to as a demand window which is a period of time in which peak demand is determined. Peak demand factors into billing because capacity must exist to meet the highest instantaneous demand. In other words demand increases require utility peaking capacity new generation delivery and transmission assets. On the other hand uneven load results in idle utility resources.

Conservation alone is not always an effective means of reducing energy costs. As mentioned conventional mechanisms to reduce power consumption operate largely based on conservation principles coupled with data center workload. As described herein various exemplary techniques can improve scheduling efficiency which in turn can increase load factor and even demand i.e. reduce peaks and even load . In general load factor is defined as an average power consumed by a data center divided by a peak power consumed by the data center over a period of time. Changes in electric profile may make alternative rates more attractive noting that it is often the customer s responsibility to determine the optimum schedule.

The resources and other resources associated with individual servers in the server block typically consume power in a non linear fashion with respect to workload. A plot of power consumption versus workload indicates that a server is less efficient at lower loads e.g. in Mb s than at higher loads noting that a server consumes more power at higher loads. Data from a two year old server SKU shows idle power consumption of about 160 W and full load consumption of about 230 W. A modern server with multiple processors consumes significantly more energy. For example when fully loaded the active components in a typical high performance dual processor server consume about 380 W.

A pie chart shows an approximate breakdown of how power is consumed in a server. In general CPUs consume around 50 or more of the power to a server while other components account for around 220 W which takes the power budget to 600 W per server. In an example where 50 of the power dissipated by a server is consumed by the cooling infrastructure to cool the server power consumption for a single dual processor server can be around 900 W.

A common design point for modern data centers calls for a power usage effectiveness PUE of about 1.7 i.e. for each 1 W of critical load requires roughly 0.7 W of cooling power transmission losses and other overhead . For example if a server demands 900 W and the PUE for the datacenter is 1.7 then the power from the utility needed to deliver 900 W to the server is 1530 W. The reciprocal of PUE is data center efficiency DCE for example a DCE value of 0.59 equivalent to a PUE of 1.7 indicates that the IT equipment consumes 59 of the power to the data center. Another metric is data center performance efficiency DCPE which is defined as useful work divided by total facility power. Control logic for a data center power management system optionally includes criteria parameters etc. specified in terms of PUE DCE and or DCPE.

As described herein a management system reduces power consumption of a data center based in part on data center usage information and power consumption. Power consumption is included as it is directly related to cost of operating a data center. Such an approach includes aspects of scheduling rather than merely conserving power in direct response to current workload. For example given the data center usage information in the plot adjustments can be made to how servers handle workload in a data center to reduce power consumption. In particular adjustments can be made to reduce power consumption during server usage peaks. Reductions in peak power consumption can greatly reduce power costs. Various techniques clip peaks and fill valleys e.g. smear workload forward in time .

With respect to reduction of peak power consumption power is usually charged at the 95th percentile over the course of a month. While some negotiated power purchase rates are more complex than this percentile approach a management system can be applied to any of a variety of billing equations.

An exemplary management system optionally acquires billing information e.g. billing equations from one or more power providers e.g. utility companies and manages power consumption based in part on such information. An exemplary management system optionally acquires power demand and or generation information e.g. from one or more power providers such as a utility companies and manages power consumption based in part on such information. An exemplary management system may manage power consumption of a data center or data centers based in part on billing information and power demand and or generation information. With respect to power demand and or generation information such information may be historical information real time information and or predicted information for some future time. For example if a utility knows that a reactor will be shut down for service then an exemplary power management system may rely on such information to manage power consumption of a data center and or manage power supply to the data center e.g. using supplemental power from an on site generator an alternative utility etc. .

The server server block modules include a computational storage resources module for information germane to resources under control of an OS of an individual server e.g. chip disk etc. . Another module pertains to information for non computational storage resources such as the power supply fans temperature s etc.

Each individual server in the server block may operate using an OS that includes one or more features of a Microsoft Windows Server OS. A server may include a reliability and performance monitor e.g. as a snap in that provides for performance logs and alerts server performance system monitoring customizing data collector sets and event trace sessions.

In general terms performance is the measure of how quickly a computer completes application and system tasks. Overall system performance might be limited by the access speed of the physical hard disks the amount of memory available to all running processes the top speed of the processor s or the maximum throughput of one or more network interfaces.

The reliability of a system is the measure of how often the system operates as it is configured and expected to perform. Reliability can be reduced when applications stop responding services stop and restart drivers fail to initialize or in the worst case when operating systems fail. A reliability monitor can indicate stability of a system and track events that can help identify causes of reductions in reliability. Such a monitor can record failures including memory hard disk application and operating system failures and key events regarding the configuration of a system including installation of new applications and operating system updates . A monitor can provide a timeline of changes in both the system and reliability and can help identify how to get a system back to optimal reliability when it does not behave as expected.

The module optionally allows for a display of real time information for CPU disk network and memory usage. The module optionally allows for display of real time information related to non computational storage resources such as the power supply the fan s etc.

The data center management modules include a data center computational storage resources module and a data center non computational storage resources module . The module can aggregate information from various servers or server blocks while the module can handle information from infrastructure components . The modules may be implemented using a computing device located in a data center or using a computing device located remotely from a data center e.g. in communication with a data center via a network .

As mentioned the module can operate cooperatively with one or more other modules such as the non computational storage resource module and one or more of the data center management modules . In general the module operates at a level that allows for adjustments to resource parameters of an individual server. The module may be part of an operating system of a server.

The module can adjust one or more resource parameters that affect power consumption of an individual server. When a server is active e.g. processing a request the module can adjust a P state and or a T state of one or more cores. More specifically P states are core performance states associated with dynamic voltage and frequency scaling that allow for a reduction in power consumption by a core for every step back in core performance which results in more than linear savings in power. T states are throttle states that involve linear scaling of a core clock to reduce instantaneous power of a core. For example a 50 reduction in core clock speed will cause a work task to be processed in twice as much time. C states are often referred to as sleep states and used when no work is being performed by a core.

In conventional optimization for maximum performance at the minimum power consumption a combination of software e.g. operating system and hardware elements are used. For example Intel Duo Core processors Intel Corporation Santa Clara Calif. include power and thermal management technology PTMT . According to the Intel PTMT when a core is active it always runs at C0 but when the core is idle an OS typically tries to maintain a balance between the amount of power it can save and the overhead of entering and exiting to from that sleep state. Thus C1 represents the power state that has the least power saving but can be switched on and off almost immediately while an extended deep sleep DC4 represents a power state where the static power consumption is negligible but the time to enter into this state and respond to activity back to C0 is quite long.

According to the PTMT when an OS scheduler detects there are no more tasks to run it transitions the processor into the selected idle state by executing a BIOS defined instruction sequence. The processor will remain in that idle state until a break event occurs and then return to the C0 state. Break events would typically be interrupts and similar indicators that new tasks need to be executed.

PTMT aims to achieve a progressive increase in static power savings per state by employing more aggressive means as the C state deepens. In C1 idle state only processor centric measures are employed instruction execution is halted and the core clocks are gated. In C2 states and above platform level measures are also added to further increase the power savings. While in the C2 state the processor is obligated not to access the bus without chipset consent. Thus the front side bus can be placed in a lower power state and the chipset can also initiate power saving measures. In C3 the processor also disables its internal Phase Locked Loops. In C4 the processor also lowers its internal voltage level to the point where only content retention is possible but no operations can be done. A Deep C4 is also available in the PTMT.

The ACPI standard sets forth a mechanism to control dynamic power consumption by adjusting active power consumption to the computational needs of the software. The PTMT controls the P state of the system by a combination of hardware and software interfaces the hardware defines a set of operational points where each one defines a different frequency voltage and therefore different levels of power consumption. The OS aims to keep the system at the lowest operational point yet still meet the performance requirements. In other words if it anticipates that the software can efficiently use the maximum performance that the processor can provide it puts it in the P0 state. When an OS predicts e.g. based on history that the lower and slower operational point can still meet the performance requirements it switches to that operational point to save power.

Power consumption produces heat that needs to be removed and conventional PTMT usually chooses the maximum frequency the core can run without overheating when running in a normal usage model this is also called the Thermal Design Power TDP . Thus when an unexpected workload is used the thermal control logic aims to allow the system to provide maximum performance under the thermal constraints.

Threading can also affect power utilization. Various threading models exist such as balanced threading and imbalanced threading. According to an Intel Corporation study minimizing synchronization points between threads leads to more time spent in the parallel section which translates to better power savings. Further thread imbalance may cause performance degradation and may not provide power benefits as compared to balanced threading. Specifically applications with an imbalanced threading model may show lesser performance improvement as compared to a balanced threading model and hence consume more power than the balanced implementation.

Given the foregoing description on resource controls as described herein various exemplary techniques aim to control power consumption of an entire data center. shows an exemplary system that includes a data center or server farm along with power consumption and heat removal equations. The server farm consumes power Paccording to an overall load or demand which in this example is the sum of the power consumed by servers and server specific infrastructure Pin the server farm and power consumed by cooling equipment e.g. CRAC etc. Pin cooling the server farm . Heat transfer considerations include the temperature of the server environment the temperature and condition of cooling fluid provided by the CRAC and ambient temperature and conditions.

An exemplary method manages Pby managing P which in turn has an affect on P. For example an exemplary method for managing power consumption of a data center includes monitoring power consumption of a data center comparing power consumption to a power consumption threshold based on the comparing deciding whether to implement a power policy where the power policy reduces power consumption by the data center and increases a load factor where the load factor is an average power consumed by the data center divided by a peak power consumed by the data center over a period of time.

The modules and operate on a server or server block level and at a data center level to shape power consumption by the data center as indicated by a plot of electricity usage versus time. In particular the modules and shape power consumption by clipping peaks and filling valleys. The modules and can be instructed to account for billing equations provided by a utility or utilities. In some instances a data center may have a choice on sources of power. A data center may have one or more on site power generators and one or more off site providers. In such a scenario a data center level module may call for switching to a more optimal source or sources to avoid an increase in cost.

An exemplary method may adjust one or more server parameters based in part on cyclical information for cooling e.g. CRAC operation . For example if workload is increasing for a data center and power consumption rising responsive action may account for trends in CRAC operation such as decreased cooling requirements for nighttime operation or expected weather related changes.

According to the scenario at a time To the power consumption for a data center exceeds a power threshold P. In turn an exemplary method implements a corresponding policy Policy which adjusts one or more P states for servers in the data center. As workflow to the data center may continually increase or ambient conditions may change to increase power demands for cooling the power consumption may still rise to a threshold Pat a time T. In this instance the method implements Policy which halts non critical tasks. By itself this may not prevent the data center from consuming more power. As indicated in the scenario power consumption continues to rise. Intermediate thresholds may call for policies such as Policy C consolidate workload Policy F adjust T state s Policy K adjust C state s Policy L adjust to Deep C state s and Policy N shut down some servers . In some instances a policy to consolidate workload e.g. Policy C can be used in conjunction with one or more other policies e.g. Policy F K L N etc. .

Such policies can aim to achieve optimal handling of workload given the goal of keeping power consumption below some peak value. For example with respect to T states as mentioned a reduction in clock speed can increase the time of handling a task. Referring to the plot of data center usage such a policy may effectively handle workload without significant disruption to customers. In particular a T state policy can act to flatten peaks over an extended period of time such that power consumption for a data center does not exceed a power consumption target. In this example customers may wait a bit longer for workload to be processed however opportunities exist to pass along saving to the customers where the savings are achieved by reducing peak power consumption over a billing cycle.

After implementation of Policy X the method continues in a decision block that decides whether implementation of Policy X reduced power consumption of the data center. If implementation of Policy X reduced power consumption then the method returns to the monitoring block otherwise the method calls for more aggressive action in an implementation block that implements Policy Y which calls for shutting down some servers .

As described herein an exemplary system can defer batch e.g. batch jobs and non synchronous workload as a data center approaches a power peak threshold. Such a system can exploit the difference between idle and max load power consumption and reduce overall peaks. As the server power consumption moves away from a peak the system can optionally reschedule this non critical workload. Using such a technique the system throttles back the power consumption and knock off the peaks by filling the valleys. In other words the system can forward time flatten power peaks.

An exemplary system can take any of a variety of actions to flatten power peaks. For example a system may shut off non needed servers and use workload peak clipping and trough filling to allow the workload to be run with fewer servers turned on. Using this technique it may actually be possible to run the service with fewer servers overall.

Keeping servers off conserves power and not needing them at all would save even more. In some instances workload peaks are driven by batch jobs and SQL Server checkpoints. An exemplary method can reduce both by changes in the service and in SQL Server and as a consequence reduce peaks by spreading the workload out more evenly.

Applying such a technique to power has a huge potential upside because power provisioning and cooling dominates the cost of a data center. Filling valleys allows better data center utilization in addition to lowering power consumption charges.

In general various exemplary techniques described herein may be referred to as resource shaping techniques. Such techniques smooth power consumption spikes by knocking off peaks and filling valleys which can apply to any of a variety of data center resources.

With respect to data center design conventional design calls for buying servers to meet the highest workload requirements. To some degree knocking off peaks and filling valleys can translate to fewer servers. Such an approach also applies to internal networking. Resource shaping as a technique applies to all resources across the data center.

In a very basic configuration computing device typically includes at least one processing unit and system memory . Depending on the exact configuration and type of computing device system memory may be volatile such as RAM non volatile such as ROM flash memory etc. or some combination of the two. System memory typically includes an operating system one or more program modules and may include program data . The operating system include a component based framework that supports components including properties and events objects inheritance polymorphism reflection and provides an object oriented component based application programming interface API such as that of the .NET Framework manufactured by Microsoft Corporation Redmond Wash. The device is of a very basic configuration demarcated by a dashed line . Again a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.

Computing device may have additional features or functionality. For example computing device may also include additional data storage devices removable and or non removable such as for example magnetic disks optical disks or tape. Such additional storage is illustrated in by removable storage and non removable storage . Computer storage media may include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. System memory removable storage and non removable storage are all examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of device . Computing device may also have input device s such as keyboard mouse pen voice input device touch input device etc. Output device s such as a display speakers printer etc. may also be included. These devices are well know in the art and need not be discussed at length here.

Computing device may also contain communication connections that allow the device to communicate with other computing devices such as over a network. Communication connections are one example of communication media. Communication media may typically be embodied by computer readable instructions data structures program modules or other data forms. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. The term computer readable media as used herein includes both storage media and communication media.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

