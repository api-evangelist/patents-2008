---

title: Managing data transfer between endpoints in a distributed computing environment
abstract: A file fetcher manager provides commonly-utilized management and resource allocation for multiple file fetchers that each implement a different type of mechanism or protocol for transferring data files between peer endpoints in meshes associated with a cloud-computing service. Each file fetcher is configured with both client-side and server-side components to retrieve and serve out data files. The file fetcher manager encapsulates the file fetchers to provide an abstract interface to callers while hiding the underlying details of the file fetchers. The file fetcher manager is arranged for managing simultaneous operations of the multiple file fetchers to route requests from the callers to the appropriate file fetchers, and for scheduling work items for the file fetchers so that data transfers efficiently utilize available resources while keeping the entire transfer process coherent and organized.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08620889&OS=08620889&RS=08620889
owner: Microsoft Corporation
number: 08620889
owner_city: Redmond
owner_country: US
publication_date: 20080327
---
During approximately the last 30 years dramatic advances in technology for example the development of the minicomputer the rise of the personal computer and the emergence of the Internet have revolutionized the way information is created stored shared and used. Today as technology continues to advance and improve new breakthroughs are transforming the world once again. The foundation for the current transformation is the combination of an increasing diversity of ever more powerful devices and the expanding data storage capacity in large scale networked data centers the cloud that are accessed through the growing ubiquity of broadband networks that comprise the Internet. The capabilities of such technologies are supporting the movement of computing resources including both consumer and business oriented applications from the desktop or enterprise environment out to the Internet as hosted services.

Under such a cloud computing model locally installed software on a client platform may be replaced supplemented or blended with a service component that is delivered over a network. Such models can often give customers more choices and flexibility by delivering software solutions and user experiences that can typically be rapidly deployed and accompanied by value added services. In addition to providing application services cloud based computing can also typically provide data sharing and storage capabilities for users to access collaborate in and share rich data that leverages the global cloud computing footprint. Here files are typically transferred between endpoint devices on the cloud such as PCs personal computers and portable devices like mobile phones. Typically one single file transfer protocol is utilized where all the endpoints sharing files must communicate using that particular protocol. For example file sharing systems and services like BitTorrent and Napster might use a variety of different network structures including client server pure peer to peer hybrid peer to peer and combinations thereof but the endpoints need to utilize a globally consistent protocol to ensure proper communication.

While service platforms in the cloud are expected to provide attractive feature rich solutions to customers that are well managed robust and cost effective it is desirable to have effective and efficient ways to transfer data between client devices using the service. In particular more flexible and extensible transfer mechanisms are desirable.

This Background is provided to introduce a brief context for the Summary and Detailed Description that follow. This Background is not intended to be an aid in determining the scope of the claimed subject matter nor be viewed as limiting the claimed subject matter to implementations that solve any or all of the disadvantages or problems presented above.

A file fetcher manager provides commonly utilized management and resource allocation for multiple file fetchers that each implement a different type of mechanism or protocol for transferring data files between peer endpoints in meshes associated with a cloud computing service. Each file fetcher is configured with both client side and server side components to retrieve and serve out data files. The file fetcher manager encapsulates the file fetchers to provide an abstract interface to callers while hiding the underlying details of the file fetchers. The file fetcher manager is arranged for managing simultaneous operations of the multiple file fetchers to route requests from the callers to the appropriate file fetchers and for scheduling work items for the file fetchers so that data transfers efficiently utilize available resources such as network bandwidth and connections. Using multiple fetchers enables file transfers to be performed in a variety of ways. For example a data file can be transferred in its entirety as an HTTP Hypertext Transfer Protocol stream from a single peer or cloud resource or in chunks in fixed or variable sizes from multiple peers or resources in parallel.

The file fetcher manager provides an extensible approach to file transfer that can be tailored to meet the needs of a dynamic cloud environment where peer endpoints go on and offline bandwidth availability changes etc. A file can be transferred through the combined efforts of multiple file fetchers and work may be transferred among fetchers while the file fetcher manager keeps the entire transfer process coherent and organized.

In an illustrative example remote differential compression RDC capable file fetchers are implemented that may supplement normal chunking i.e. non RDC na ve fetchers to optimize away the need to fetch file chunks that are already locally present in an endpoint. In addition different endpoints in the meshes can run different sets of file fetchers to match the capabilities of the file fetchers to those of the endpoints.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Like reference numerals indicate like elements in the drawings. Elements are not drawn to scale unless otherwise indicated.

Cloud services may replace supplement or blend with features and capabilities provided by applications and software that run locally. Offerings may include for example one or more of identity and directory services device management and security synchronized storage and data services across multiple devices or platforms and services pertaining to activities and news. The cloud services may be provided under a variety of different business models including free advertising supported and subscription based models.

As shown in a group comprising N different endpoint devices referred to simply as endpoint s is present in the environment . In this example a user has a PC and a portable laptop computer that are arranged to access the service resources exposed by the cloud based services platform under the user s credentials or identity as indicated by reference numeral which is trusted by the cloud services . Another user maintains a trusted identity so that that user may couple a laptop computer a mobile device and a PC to the Internet to utilize the cloud services .

The endpoints shown in can typically be expected to have differing features and capabilities which may vary widely in some cases. For example the PCs and may utilize different operating systems respectively including the Microsoft Windows operating system and the Apple Mac OS operating system. In addition the portable device will typically be configured with fewer resources such as processing power memory and storage compared to the PCs and laptops and will use a different operating system.

It is emphasized that the endpoints shown in are merely illustrative and a variety of different endpoint devices may be utilized with the present file fetcher manager and file fetchers. These include for example media center PCs game consoles set top boxes ultra mobile computers handheld game devices mobile phones PDAs personal digital assistants pocket PCs personal media players such as MP3 players Moving Pictures Expert Group MPEG 1 audio layer 3 and similar devices.

As shown in the resources that are exposed by the cloud services may be logically arranged to form meshes. In this example a mesh is associated with each of the identities and and the endpoints associated therewith as respectively indicated by reference numerals and . The meshes include those resources which are utilized to implement a given service offering for the user and the endpoints which are associated with and can access the resource. In this example resources and are associated with the user having identity and the endpoints and in mesh . The user having identity receives services that are implemented with mesh which includes resources and that are accessible to the user s devices and . As shown an endpoint can traverse a mesh and move from resource to resource in order to gain access to a desired service .

Meshes can overlap as shown in . In this example resource is commonly utilized in both meshes and . Resource could be for example a folder on the cloud to which one user as an owner has given the other user permission to access as a member. It is noted that the number and configuration of resources shown here in this example are arbitrary and the particular resources used in a given mesh to implement a specific service offering for a user can be expected to vary to meet the needs of a particular scenario.

An illustrative architecture for a representative endpoint is shown in which includes several functional components including one or more applications and an instance of a mesh operating environment MOE runtime . The MOE runtime is generally configured to expose services to help the applications running on endpoints to create cached or offline based experiences to reduce round trip interactions with the cloud services or to enable the endpoints to morph data into a more consumable form.

More specifically the MOE runtime in this example includes a file fetcher manager that interfaces with a caller in the application . As shown in components in the file fetcher manager include a dispatcher a peer file fetcher list cache a request handler and a multiplicity of file fetchers . The particular file fetchers used in a given endpoint will typically take the capabilities of the endpoint into account and not all endpoints necessarily need to utilize the same number and kinds of file fetchers. For example the mobile device will not ordinarily run an RDC enabled file fetcher.

The dispatcher is utilized to queue work items associated with incoming requests to serve and retrieve files from the file fetchers . The file fetcher manager and the file fetchers use the work items to perform work on threads from a global thread pool. Accordingly throttling and prioritization are implemented at the dispatcher to ensure that a surge of requests does not impair operation of the MOE runtime . Throttling is implemented by requiring the file fetchers to queue their work items with the dispatcher in a centralized work queue. The dispatcher will queue the work item if the number of currently active requests is less than a maximum value. The maximum value may be specified in a configuration file stored on the endpoint which provides policies that are applicable to the endpoint.

The peer file fetcher list cache maintains an in memory cache of peer endpoints and a list of associated file fetchers which may identify capabilities such as whether RDC capability is supported. Entries in the peer file fetcher list cache will typically be subject to expiration i.e. time out where the time to live TTL values will be retrieved from the configuration file . The peer file fetcher list cache in this particular example is not persisted and will be rebuilt between restarts of the MOE runtime . However in other implementations the file fetcher list cache could be persisted through specification of an appropriate TTL value that dictates the length of time that such cache data is considered valid.

The peer file fetcher list cache will be used by the file fetchers in an endpoint to determine file fetcher compatibility of a peer endpoint before establishing communication with it. Upon encountering a time out or a null entry for an endpoint device the file fetcher will query that endpoint s file fetcher manager for a current list of file fetchers for the endpoint and then update the peer file fetcher list cache with the query results. Maintaining the peer file fetcher list cache at the file fetcher manager level enables all the file fetchers in the endpoint to share it.

The request handler is responsible for tracking the incoming requests from local callers to download files and the incoming requests from callers at remote peer endpoints to serve files and then routing the requests to the appropriate file fetchers . The request handler maintains a list of locally available file fetchers that are sorted in order of preference i.e. rank ordered for handling the requests. When an incoming request is queued the request handler will route the request to the most preferred file fetcher or as discussed below may route the request to enable multiple file fetchers to be utilized simultaneously. Upon successful completion of a request the request handler passes a pointer to the file to the caller . The caller is normally expected to move the file for example when the file is part of an installation.

If a failure is encountered when a file fetcher is attempting to service a file request then an error message is passed to the request handler . The request handler may choose to retry the request with a different file fetcher depending on the nature of the failure. For example if the failure occurred because none of the peer endpoints containing the requested file are compatible with the currently utilized file fetcher then the next file fetcher in the rank ordered list may be tried. If no other file fetchers remain in rank ordered list then another error message can be returned.

If the failure occurred because none of the endpoints containing the requested file are online then the request cannot be satisfied by the file fetcher manager . In this case the failure is communicated to the caller which will be responsible for retrying the request at another time.

The request handler is also responsible for serving requests from remote endpoints for information about its locally associated file fetchers and their capabilities e.g. whether or not RDC enabled .

Details of an illustrative file fetcher are shown in . The file fetcher is configured to support chunked file uploads and downloads. Chunked is a type of transfer encoding by which the message body is transmitted to an endpoint as chunks that are stamped with the size of the chunks see for example section 14.40 of RFC 2068 Requests for Comments from the Internet Engineering Task Force or IETF . In addition in this example RDC capability is built on top of the chunking layer for fetchers that operate on a Microsoft Windows platform. However it is emphasized that other fetchers with other capabilities may also be utilized using other file transfer protocols including both client server and peer to peer protocols that can utilize a single TCP transport control protocol socket or different TCP sockets. For example one file fetcher may be configured to download a file using HTTP from a storage service on the cloud while another file fetcher may be configured to download a file in chunks using a peer to peer protocol and another file fetcher is RDC compatible.

The file fetcher includes both client side and server side components so that the file fetcher may both retrieve files and serve up files. In this illustrative example an RDC component is commonly utilized across the client side component and server side component . However it is emphasized that the use of RDC is illustrative and that other types of file replication and transfer mechanisms may also be utilized as may be required to meet the needs of a particular implementation.

In the RDC component an RDC algorithm component implements a conventional RDC algorithm where the parameters for the algorithm including for example minimum and maximum block size are retrieved from the configuration file . An RDC signature store contains the logic used to maintain a store for caching the RDC signatures for a given file. The RDC signatures are stored in a separate file named after the file s URI Uniform Resource Identifier under a directory that is picked up from the configuration file . Using a naming scheme that relies upon the file URI ensures that out of date signatures cannot be erroneously utilized since subsequent versions of the same file will have different URIs. The RDC signature store maintains in memory state about the signatures but not the signatures themselves. This in memory state is rebuilt each time the endpoint and MOE runtime is started up by implementing a simple directory scan and updating the cache during execution as appropriate.

The caller can explicitly delete signatures in the RDC signature store or let cleanup logic implemented by the store flush them away at restart. The cleanup logic is run once at start up and then again after a configurable time period. The cleanup logic will check if any signatures are older than a configurable age and if so delete those signatures. Changes to files are subscribed to here as well so that out of date signatures may be aggressively cleaned up as out of date signatures have no utility .

In the client side component which handles requests to retrieve files an ELS Enclosure Locator Service proxy is provided. Typically the different file fetchers may utilize different ELS implementations. The ELS is typically implemented as a cloud service which can map files to endpoints . The file fetcher interacts with the ELS proxy to obtain a list of endpoints that contain a file notify the ELS proxy of the local availability of a file and notify the ELS proxy of the local removal of a file. The ELS proxy not only contains the asynchronous call stubs for these operations but is also the first point of entry for file requests entering the file fetcher . That is when the request first comes in it is queued for ELS lookup in the ELS proxy . The queue is arranged in this example as a priority queue with dual priority levels normal and high. The priority level for the request is typically specified by the caller .

Lookups to the queue are performed by de queuing file requests from the queue on a priority basis with ties in priority being broken randomly. If a lookup is successful the results are passed to an endpoint work list so that the file may be queued for download. If the lookup failed which would occur if no peer endpoint has the requested file then an error is generated and delivered to the caller . The ELS proxy is also configured for batching communications to the ELS to enable better scalability in the cloud where parameters for the batching are provided by the configuration file .

Every endpoint with pending work has an associated work list which is maintained by the endpoint work list . The work list supports dual level priorities normal and high where the priority for each item in the work list is specified by the caller . A best effort will normally be made to honor the specified priority.

Requests to the work list are queued after a successful ELS lookup for a file. The incoming requests contain details regarding the file to be fetched including a list of endpoints on which the file can be found. This list is inverted and the request is added to the work list that is associated with each endpoint from which the file can be retrieved. When the scheduler which implements file retrieval according to policies as described below in more detail is ready for more work it uses the endpoint work list to obtain more. The scheduler may either allow the endpoint work list to randomly pick an endpoint and choose a request on that endpoint s work list with the highest priority where ties are broken randomly or it may specify an endpoint from the which the endpoint work list will choose a request to de queue.

If the file returned is present on the work lists of other endpoints it is not removed from them. This behavior implies that the scheduler could pick up the request for the same file for the work lists of different endpoints at different times. This simply results in the file being downloaded in parallel from the multiple chosen endpoints. Chunks will not be downloaded redundantly as ensured by the file assembler as described below in more detail .

The file assembler is implemented in this example as a thin layer on top of the file system API application programming interface that assists in reconstructing a file from a stream of bytes. The file assembler stores files in a directory that is specified by the configuration file . The file assembler maintains two different kinds of files files that have been completely downloaded .final file extension and files that are still being downloaded .partial file extension . In memory state for both file types is maintained and is rebuilt each time the endpoint and MOE runtime is started up. The in memory state is updated during execution as appropriate.

When generating the state for a new file the file assembler requires the caller to specify the number of chunks and the size of each. The file assembler allocates space of the required size beforehand for the entire file to ensure less fragmentation. The file is named after the URI of the file with the .partial file extension. At the end of the file a bitmap is appended to indicate which chunks have been fetched and written to the file. This bitmap is also maintained in an in memory cache. It is noted that the file assembler does not make any assumptions regarding the sizes of the chunks. Accordingly RDC or a fixed size chunking protocol may be employed on top of the same file assembler.

Before retrieving a file chunk the scheduler uses the in memory bitmap to lock the chunk to avoid duplicate requests for the same chunk. After the last chunk is written to the file the file assembler removes the bitmap that was appended to the file and renames the file to have the final file extension. The caller can then make use of the fetched data.

The file assembler is configured with logic to garbage collect unused files that have been left in a partially assembled state or which appear to have been forgotten by the caller. The files are cleaned up after installation. In addition cleanup logic runs at startup and once every configurable period of time that will delete files that are older than a configurable age.

The bitmap and the dual extensions i.e. .final file extension and .partial file extension enable recovery to be very straightforward in cases where the MOE runtime crashes or in case of an abrupt shutdown when the file is in the process of being downloaded. The dual file extensions cleanly separate files that have been downloaded from those that are still being downloaded and which thus have a bitmap appended . The bitmap can be used to indicate how much of the file has been retrieved.

The scheduler is fundamental to the file retrieval process. It picks up work from the endpoint work list in the form of tuples where is a work item to retrieve the file specified by fileUri from endpointid. In the case of a sudden surge of requests if the scheduler starts working at all the requests concurrently very little progress will be made on each request and overall system performance will degrade. To avoid such a scenario the number of concurrent work items is restricted to a maximum specified in the configuration file .

When the number of active work items falls below the maximum more work is obtained from the endpoint work list if available. It is noted that there could be multiple work items active for a given file but with different endpoints . This implies that the file chunks will be retrieved in parallel from all of such endpoints .

When a work item is obtained the file fetcher checks to see if the remote endpoint contains the same file fetcher. This is accomplished by checking the peer file fetcher list cache in the file fetcher manager for valid entry for the given endpoint . If no valid entries are found then the remote file fetcher manager is queried for file fetcher information. This information is added to the cache and is used to determine the remote presence of the same file fetcher. The work item is removed if an incompatibility is detected. If the work item to be removed is the last work item for the file in an endpoint work list then an error is returned to the caller .

In this example the entire file transfer process irrespective of whether RDC capability is provided or not is based on chunking. A list of chunks to be fetched for each file is maintained by the file assembler . Before downloading chunks from a remote endpoint a list of available chunks on the remote endpoint is fetched. This list is used to query for new file chunks. As chunks flow in they are written to the file assembler and new file chunk requests are made. Before making a request for a chunk the chunk is locked to the file assembler to avoid duplicate requests for the chunk. One benefit of RDC in this chunking driven downloading is optimization away from the need for fetching locally present identical chunks that would normally be implemented by a na ve chunking based fetch.

The scheduler is configured to first check to see if the file assembler already contains an entry for the file. If an entry does not exist one is created and initialized. An entry could exist in the file assembler if another work item for the same file is being serviced by the scheduler or if the download was interrupted for some other reason during previous attempts. These reasons could include for example an MOE runtime crash or abrupt shutdown. If a previous in memory state exists then the only work left to be performed is to fetch the remaining chunks and complete the download irrespective of whether RDC is used or not.

If no previous in memory state for the file exists in the file assembler then the local endpoint negotiates the mode of file transfer e.g. RDC versus na ve chunking depending upon a variety of factors including the size of the file and the availability of RDC capability on both endpoints. RDC overhead which may include for example computing signatures and accessing the file system multiple times is typically not justified for small files because the bandwidth savings in such cases is minor.

When RDC is selected for the work item then the RDC signatures are requested from the remote endpoint and are written to the local RDC signature store . The signature store is checked for RDC signatures for locally present previous versions of the same file. If such RDC signatures are not present they are then generated. Using the generated signatures the locally available chunks of the target file are written to the file assembler . If RDC was not selected then a na ve chunking driven download is utilized. If multiple work items are de queued for a given target file and one of the work items is for performing the RDC steps of retrieving and comparing signatures then subsequent work items will wait before they start fetching chunks.

Once a file download is completed all the work items associated with the file both in the endpoint work list and scheduler are removed and a pointer to the file in a store of the file assembler is passed out to the caller . If an error was encountered for example no endpoint has a compatible fetcher or stores the target file an error code is returned to the caller . As noted above the ELS proxy is used to notify the ELS about the local presence of the file.

An RDC signature chunk server is the only component that is specific to the server side of the file fetcher and is responsible for handling requests from the remote endpoints to serve files that are stored on the local endpoint. Upon receipt of an incoming request for a chunk the requesting endpoint is subjected to authorization to the core object e.g. a resource in from which data is requested. If the authorization is successful a check is made to verify that the request can be satisfied by the file assembler . If so then the data is served out. Otherwise the core object manager e.g. a handler for a resource is called to obtain a reader to the item for serving out the bytes from the file. The reader is typically required to detect any changes to the item while reading it.

Upon an incoming request for RDC signatures the RDC signature chunk server checks the signature store for the RDC required signatures. If the signature store does not have them they are generated and checked into the store and served out.

Work is scheduled in the file fetcher by the scheduler using an additional set of components as shown in components not related to scheduling are not shown that are arranged with an objective of accommodating a variety of scheduling policies. In addition work for the client side component and server side component of the file fetcher is scheduled and implemented with the following objectives 

The device manager maintains a multiplicity of device contexts where a device context maintains state associated with an endpoint . State is maintained on a per device basis and includes i incoming and outgoing channels to the endpoint ii online offline status i.e. whether the endpoint is online or offline and iii request serving history. The request serving history can include for example quality of service data pertaining to the latency to serve a request success rate available bandwidth and the like. The state information will typically be used by the scheduler when deciding the endpoints from which to download a file. The device context may or may not include an active communication channel and terminating a channel will not eliminate an associated device context.

The device manager communicates with the scheduler when an endpoint comes online or goes offline when an endpoint attempts to establish a connection to make requests to a local file fetcher and when responses to local and remote requests have been made. The scheduler may potentially make endpoint selections at the times such events occur.

The pending incoming requests component maintains pending requests shown as a plurality of FileDownload tasks from remote endpoints that still need to be served. Whenever new requests have been queued the pending incoming requests component calls the scheduler .

The pending fetch requests component tracks pending fetch requests indicated by reference numerals for files located on remote endpoints . It will also call the scheduler whenever a new request for a file has been queued or when a file download is completed or cancelled.

Not all fetch requests need to be immediately serviced. The scheduler will decide which fetch requests will be actively serviced. Accordingly each fetch request will have an associated FileDownloadTask at the remote endpoint . The FileDownloadTask has the device context and logic that is necessary to drive the fetch process at the remote endpoint including performing an ELS lookup download metadata and file chunks etc. Each FileDownloadTask will typically wait at various states until the scheduler allows it to proceed.

The scheduler decides how to schedule work in accordance with client side and server side scheduling policies. Events from other scheduling components in the file fetcher will trigger the scheduler to run the policies to pick up more work and or assign resources to active work.

The ACTIVE FILES SELECTION policy indicated by reference numeral is applied to pick the set of active requests from the list of all pending requests. The policy is configured with the authority for example to convert pending requests to active requests and drop active requests back to the list of pending requests.

In an illustrative implementation the ACTIVE FILES SELECTION policy picks high priority files over low priority files and breaks ties in priority by picking files that have the least amount of work remaining. This approach may be expected to decrease mean download time while not significantly starving large requests.

The policy may implement the following method each time a file request is added completed or cancelled.

The OUTGOING CONNECTION SELECTION policy indicated by reference numeral is arranged to decide the set of endpoints to which connections are opened. The policy decides which new connections to establish and which existing connections should be terminated.

In an illustrative implementation an endpoint s state including history and current usage from the device context is utilized for decision making. The policy may implement the following method which is applied every time an endpoint comes online goes offline and periodically in the absence of these two events to re evaluate the set of active endpoints in the environment.

The OUTGOING CONNECTION USE policy indicated by reference numeral decides how requests are queued on an open connection. The policy controls how many requests may be outstanding on a connection and which files may be requested on a connection.

In an illustrative example the policy will attempt to achieve a configurable maximum bit rate. It will queue requests until the maximum bit rate is reached or the remote endpoint returns a busy error code. The ramping up to the maximum bit rate will normally need to be performed with some care to avoid flooding the connection with a burst of requests in an attempt to fill the connection pipe quickly.

The INCOMING CONNECTION SELECTION policy indicated by reference numeral selects the set of active incoming connections. The policy determines which peer endpoints can make requests to a local file fetcher .

The INCOMING CONNECTION USE policy indicated by reference numeral determines the allowable usage of an incoming connection. The policy ensures that a remote peer endpoint does not flood the local file fetcher with requests. Any request allowed by this policy is queued for servicing.

In an illustrative example the policy accepts new requests on an incoming connection as long as the number of requests already made from the connection does not exceed a configurable maximum and the bit rate on the connection has not reached the maximum configurable allowed bit rate. Once a request is accepted it is queued. If the request is not accepted a busy status message is communicated to the peer endpoint .

The REMOTE REQUEST SERVICING policy indicated by reference numeral determines how queued requests get de queued and serviced. The policy ensures local activity at the endpoint is not affected by the servicing of remote requests. In an illustrative implementation the total number of requests being actively serviced will be limited to a configurable maximum.

Several sample data flows are now presented to illustrate the principles of the present arrangement for managing date transfer.

Results of the ELS lookup are returned to the ELS proxy server . If an error is encountered during the lookup or if the file is not found on any of the endpoints then an error is returned. If no error is encountered then the lookup results are sent to the endpoint work list .

The endpoint work list inverts the list of ELS lookup results and adds it to the work list of each endpoint . The work item s for the requested file will eventually be received by the scheduler . The scheduler checks to see if the file fetcher at the remote peer endpoint is compatible with the local file fetcher . If it is not the work item is removed. If it represents the last work item for a file then an error is returned else the method goes back to step and waits for another work item for the file to be de queued.

The file assembler checks for the previous in memory state for the file . If not found a new state is created and initialized. If a previous state is found then the method goes to step . The file fetcher negotiates which type of download in this example either RDC or na ve chunking should be used for the file transfer . For RDC the RDC signatures for the file are requested from the remote peer endpoints . In addition the RDC signatures for locally present previous versions of the file are generated if they are not already part of the RDC signature store .

Based on the RDC signatures the locally available chunks are written to the file assembler for the file . Chunks are locked to avoid duplicate requests and are fetched from the remote peer endpoint . As the chunks flow in they are written to the file assembler and the method step is repeated . When a file download completes the file assembler moves the file from the partial state i.e. .partial file extension to the final state i.e. .final file extension .

If the request is successful then a pointer to the file is passed out to the caller . Otherwise if an error is encountered for example no endpoint has a compatible fetcher or stores the target file an error code is returned to the caller .

The file assembler is checked to determine if the incoming request can be satisfied from the file assembler. If so then the file chunk is served out and the method ends . If not the core object manager e.g. a handler for a resource is called to obtain a reader . An error is returned to the caller at the endpoint if the call fails . Otherwise the reader is used to serve out the appropriate bytes . The reader will normally be expected to detect any changes while reading an item.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

