---

title: System for prevention of buffer overflow intrusions
abstract: Disclosed as a system and method for providing enhanced security to processes running on a data processing system. The disclosed system and method selectively revokes execute privileges from memory pages being used for stacks of susceptible processes running on the data processing system. By selectively resetting execute privileges on a per page and per process basis, the system and method maintains system performance and provides enhanced security to processes running on the data processing system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07793349&OS=07793349&RS=07793349
owner: International Business Machines Corporation
number: 07793349
owner_city: Armonk
owner_country: US
publication_date: 20080124
---
The present application is a continuation application of pending U.S. patent application Ser. No. 09 708 397 which was filed on Nov. 8 2000 which is assigned to the assignee of the present invention. The present application claims priority benefits to U.S. patent application Ser. No. 09 708 397.

The instant invention relates in general to a system and method for preventing buffer overflow infiltration in a stack based data processing system operating environment. More specifically the instant invention is a system and method for adding a user accessible interface that will limit the execute on stack capability on a per process basis.

The increase in connectivity between computers has led to a proliferation of interface data processing and computing devices in a large network commonly referred to as the Internet. The interconnection of the large number of resources has had many benefits key among them the ability to share resources between a number of remotely located processing machines.

Key among the disadvantages of such wide area networked schema however are the security issues raised by the increased access of computing devices to the entire membership of the Internet. As the Internet grows access for the membership of the Internet becomes increasingly synonymous with access for the public at large.

In answer to this reality many popular security measures have been instituted on data processing machines throughout the Internet. These security measures range from the very simple e.g. requiring a user name and password as a precondition for access to a machine to the complex e.g. construction of sophisticated firewalls which limit access to certain machines from certain addresses and through certain ports . As the number of different security systems has increased so have the opportunities for rogue users to attempt to access remote systems improperly. These attempts commonly called hacking give rise to the need to design more sophisticated security systems to meet and defeat these attacks.

One of the most common methods for compromising the security of a data processing machine in a networked environment is called a buffer overflow attack.

Buffer overflow attacks may be perpetrated against a remote machine which accepts user commands and data remotely called the server . The server receives the commands and data from a user s machine called a client . Upon receipt the server may store the commands and or data received from the client in a temporary memory location called the buffer. In most server operating and memory management systems the buffers made available for storing commands and data received from a client are within a data structure called the stack .

The stack is a contiguous block of logical memory space which is made available to an application or process for the storage of data. The data stored by an application or process within the stack may comprise local variable definitions and values the definitions and values of parameters passed to the application or process a return address or instructions as well as temporary buffers.

Attacking users called hackers may attempt to use the fact that all these data items are stored within a contiguous memory structure to their advantage in infiltrating the security measures of an application or process. A hacker wishing to infiltrate such a system may send a block of data from a client to the server where the data is longer than the application or process is expecting. The server for the application or process stores the data within the buffer on the stack. However the server places the data in a buffer sized to receive a normal data block. The result is that the data is written past the end of the buffer. On a server machine having a stack architecture this overflow results in the portion of the stack holding the application or process other data being overwritten. Notably the return address for the application or process may be some of the very data that is overwritten. A clever hacker can design such a buffer overflow so that a processee s return address is overwritten with a reference to programming code placed in the buffer or an overwritten portion of the stack.

In such a case when the process owning the stack ceases execution and attempts to return to a calling application or process the return address it executes causes an infiltrator s code to execute instead. The infiltrating code may create new user IDs with superuser authority delete files copy files or provide other security breaches to the benefit of the hacker.

Once the system is compromised in this manner a hacker may fully compromise the system removing suspicious entries from logging files and modifying programs to introduce very sophisticated subversive access routes called back doors resulting in a fully compromised system.

What is needed is a system and method for improved security on data processing archives to prevent these types of buffer overflow attacks. Such a system and method must provide the flexibility of access to authorized users of the server machine while denying hackers the ability to place inappropriate return addresses and executable code on the process stack. What is also needed is a system and method which performs these tasks so that undue overhead and expense in processor time and memory. Several authors in the art have suggested placing a checking variable adjacent to the return address within the stack in order to thwart these attacks. The checking variable called a canary can be checked prior to any return address call in order to ensure that the stack has not been overwritten. If the stack has been overwritten the value of the canary will be changed and execution of the application may be aborted before an inappropriate return call is made.

The deficiency in the canary approach is that it adds excessive instructions to verify the canary upon each return address call. Accordingly what is needed is a mechanism to prevent buffer overflow attacks which does not cause such an increase in processing time.

The present invention addresses the foregoing needs by providing a system and method for protecting stacks against buffer overflow attacks. The invention takes advantage of Application Programming Interfaces APIs of modern applications to create a new form of adaptation to prevent the execution of rogue programming code introduced during a buffer overflow attack.

The invention comprises the addition of a user space accessible API to the memory management reuters that can change the execute on stack capability of the memory model on a per process basis. Accordingly a process that begins which is known to be subject to these kinds of buffer overflow attacks can call the API indicating to the underlying operating system and memory manager that the application is disallowing any execute on stack capability for itself. The memory manager or operating system records this fact in the process structure. The operating system and or memory manager can mark the process stack structure accordingly.

When the memory manager or the operating system receives a page fault and attempts to allocate a new page of memory it looks to the process structure for the current process to determine if execute on stack capability is TRUE or FALSE. If the process has not called the API to disable execute on stack then the execute on stack capability will be TRUE and execution and memory allocation proceeds in a normal fashion. No additional overhead is used.

If on the other hand the process has called the API causing the execute on stack flag to be set to FALSE the memory manager will execute a routine in accordance with the present invention to determine whether the page to be allocated is a stack page. If the page is to be a stack the memory manager resets the bit in the pagetable that permits execute privileges for that page. If a hacker attacks with a buffer overflow attack and the process executes its return instruction attempts to execute code from the corrupted stack area will immediately be prohibited by the hardware. The hardware will trigger an interrupt that causes the process to be killed without executing any of the hacker s code.

The foregoing outlines broadly the features and technical advantages of the present invention in order that the detailed description of the invention that follows may be better understood. Additional features advantages and a preferred embodiment of the invention will be described hereinafter which form the subject of the claims of the invention.

In the following description numerous specific details are set forth such as computer applications operating systems programming languages data structures microprocessors network systems communications systems bus systems protocols input output I O systems etc. to provide a thorough understanding of the invention. However it will be obvious to those skilled in the art that the present invention may be practiced without such specific details. In other instances well known applications operating systems network systems protocols data structures equipment and I O systems have been shown in block diagram form in order to not obscure the present invention in unnecessary detail. For the most part details concerning timing considerations specific applications used specific languages used specific operating systems used specific data formats used and the like have been omitted inasmuch as these details are not necessary to obtain a complete understanding of the present invention and are well within the skills of persons of ordinary skill in the art.

It should be understood that in the context of this disclosure a server or a server machine or the like is a reference to a data processing system which executes a server program. A server program is one which responds to requests from a client machine received from a network connection. In fact the data processing system that acts as a server may also run other programs including programs which interact with the server program within the instant invented system. Likewise a client is a data processing system which runs a client program and may run other programs including a server program. Accordingly it is possible although those skilled in the art will appreciate it may not be practical in many instances for a single data processing system to act as server and client simultaneously.

Those skilled in the art will also appreciate that throughout this specification the terms data processing system machine and computer are used interchangeably each having the meaning of a data processing system in its broadest sense.

A hardware environment for practicing the present invention and its components as depicted with reference to which illustrates a hardware configuration of a data processing system in accordance with the subject invention. The data processing system includes a central processing unit CPU such as a conventional microprocessor and a number of other units interconnected via a system bus . The data processing system includes a random access memory RAM and a read only memory ROM . Also included are an I O adapter for connecting peripheral devices such as disks units and tape drives to the bus a user interface adapter for connecting a keyboard a mouse and or other user interface devices such as a touch screen device not shown to the bus a communications adapter for connecting the data processing system to a data processing network and a display adapter for connecting the bus to a display device . CPU may include other circuitry not shown herein which will include circuitry found within a microprocessor e.g. execution unit bus interface unit arithmetic logic unit ALU etc. The CPU may also reside on a single integrated circuit IC .

Those skilled in the art will appreciate that the representative hardware environment depicted in may be suitable not only to execute the server processes and methods described in this disclosure but also to execute networked functions such as routing and communication serving and the like which may be necessary for the proper and efficient operation of the underlying network environments. It should also be noted that these routing and communication serving operations may be performed in accordance with the instant invention to provide enhanced security to the network system.

The RAM included within a data processing system is organized as an ordered set of registers as depicted in . The registers of the RAM are numbered 0 through 21 where X is the address length of the RAM addressing system. While the RAM is at least logically a contiguous string of registers it is often convenient as those skilled in the art will appreciate to view this one dimensional array of memory elements as if it were partitioned into smaller sub memories called pages . Those familiar with the art will recognize that depending upon the specific hardware and operating systems involved pages may also be called banks blocks or segments . The pages into which the physical RAM is divided are logical partitions and may or may not have a direct relationship to the physical structure of the RAM within the data processing system.

It will also be appreciated that the size of the logical partitions may vary depending upon the specific data processing system involved. However due to addressing considerations within the binary environment the partition size is generally equal to a perfect power of two number of registers .

It is common within the art to view RAM as partitioned into pages in order to allocate different pages of memory for different purposes. For example each computer program or process executing on a data processing system may be assigned one or more separate pages of RAM . Each of the pages of memory may be allocated to hold various and different data patterns. One page may be allocated to store the process code which the CPU will execute. Another register may contain an execution stack for use by a process. Other registers may be allocated to data. Other pages may be used solely as caches or buffers.

The allocation and use of the pages of memory within the RAM may be managed by a computer program. Such management functions may be incorporated into the operating system of the data processing system may be a function of an add on to an operating system or may be managed in a stand alone program such as a terminate and stay resident TSR memory management program. Such memory management program does not only control which pages of memory are addressed under various conditions. It may perform additional functions. For example the memory manager may have the ability to indicate when a non existent memory area is addressed such as in the case where a portion of the RAM is not physically populated with memory chips. Also the memory manager may have the ability to flag whether data within a certain memory page is executable by the CPU. Similarly the memory manager may track the different types of data stored within the page.

The memory manager may also provide additional critical functions such as the allocation and reclamation of memory pages. When a memory page gets full a page fault is triggered and the memory manager is called to allocate an additional page of memory. Similarly when a page of memory is no longer needed by an application or to store data that page may be returned to the memory manager for reallocation at a later time.

The use of the memory manager and memory pages as a means to allocate and deallocate memory space provides flexibility of the memory space. However the overhead created by the memory manager particularly when it performs special functions such as checking for memory page executability can cause increase load on the CPU and draining critical system resources.

A buffer overflow attack is described in detail with reference to . depicts a page of memory dedicated to an execution stack for a procedure . Each memory location within the stack has associated with it an address . Locations within the stack may be referenced by calling or referring to an offset address which is relative to the bottom of the stack . Accordingly the bottom of the stack begins at offset address 0000.

Each memory location also has associated with it a value . While descriptions are also shown on those skilled in the art will appreciate that the descriptions shown are for annotations only and do not actually appear within the memory model. The descriptions for illustration annotate the stack as it is used by a processes running on a network server system. Other stack configurations may be used all falling within the present invention. The bottom of the stack beginning with offset 0000 is used to store parameters passed to the process from other applications. As depicted on those values may be FFFF but those skilled in the art will readily appreciate that passed parameters may have meanings beyond such generic hexadecimal values.

Next on the stack at an address represented by the value R is the return address . This return address represents the address in memory where the CPU will return execution once the process controlling the stack terminates execution. For example the value of the return address may be the value 1234.

Next on the stack beginning at address R 2 are the local variables . While the local variables will have values which are used by the application locally for purposes of the illustration in those values are each set to 1111.

Also present on the stack at an address denoted by B is the buffer . The buffer shown in occupies two address locations and is filled with the values 0000 for illustration.

The buffer is used to store temporary data such as transmission data passing through a server process. Such data is written into the buffer in the direction indicated by the arrow .

An example of buffer overflow is demonstrated with reference to . demonstrates a stack having a buffer at addresses B through B 2 . However in this instance the size of the data value in has exceeded the size of the buffer such that the balance of the data has infringed on the local variables in the address space they have been overwritten with new values . For purposes of illustration the values in are CCCC. Thus a user by submitting to the process data of a size larger than that allocated to the buffer has overwritten the data values for the local variables within the stack .

An accomplished buffer overflow intrusion is demonstrated with reference to . depicts a stack having a buffer filled with data values . The data values have exceeded the limits of the buffer and have overwritten the local variable value space at . Additionally the user has introduced an additional element of data which is stored in the next memory location as indicated by the arrow . That memory address is at address R where the return address is normally stored. Accordingly rather than 1234 being stored as the value in location the value has been overwritten with the address R 2.

A stack buffer overflow intrusion is accomplished by replacing the return address with the address of a location within the local variables . If a user fills the local variable memory space with data value which represents the executable code xCCCC when execution of the application finishes the return address is referenced at address offset R . The CPU introduces the value into the CPU pointer register causing execution to begin at address R 2 . Address R 2 corresponds to the local variable portion of the stack which was overwritten with the code data xCCCC . Accordingly the code data executes.

Through this series of steps an intruder is able to insert his own code and execute the code from within the stack .

This kind of buffer overflow attack is completely dependent on the ability to execute machine instructions that reside in the memory belonging to the stack. While the memory management hardware makes it possible to disallow this execution several major operating systems and memory management softwares do not utilize this feature. For example Linux Solaris and Windows NT are three popular operating systems which incorporate memory managers which do not take advantage of this feature. One reason why these programs do not take advantage of the ability to prohibit execution of programs on the stack is that there are limited circumstances where certain programs might want to dynamically build a small block of code on the stack and execute it. Additionally there are some performance issues indicated when the memory management software or hardware must examine every page fault which triggers allocation of another memory page to determine if the memory page to be allocated is a stack page.

The instant invention adds a user space accessible API to the memory management model. The API can change the execute on stack capability on a per process basis. Accordingly when a process running on the server is of a type that is subject to these kinds of buffer overflow attacks such as the FTP daemon the process can call the API with the parameter indicating to the underlying operating system or memory manager that the process is choosing to disallow all execute on stack capability for its memory space. The operating system or memory manager marks flags in the process structure accordingly.

When the memory manager receives a page fault and attempts to allocate a new page to an application it examines the process structure for the current process. If the execute on stack capability is TRUE the default it proceeds in a normal fashion without modification. No additional overhead is used. If on the other hand the memory manager examines the process structure and determines that execute on stack capability is FALSE the memory manager performs the extra processes necessary to determine whether the page being allocated is to be allocated as a stack page. If the page is to be a stack page the memory manager removes the bit in the page table that permits execute privileges for that page.

When an infiltrator attacks with a buffer overflow and the process owning the stack executes it return instruction the fact that execute privileges for the page have been denied causes a hardware interrupt which results in the process being killed without executing any of the infiltrator s code. With many perpetually running services the init process can then rerun the application so that subsequent users are not impeded by the results of the potential attack. Those skilled in the art will recognize that the init process is unique to the Unix environment however similar processes exist or can be mimicked in other systems so that the present invention may be applied to a wide variety of operating systems.

The methodology of the instant invention is demonstrated with reference to . The method begins with the precondition of a calling application and an API in conformity with the current invention.

The method begins with a call to a subject process from a parent application. Immediately upon call a determination is made as to whether the process called is susceptible to buffer overflow attacks. If that determination is in the negative then the process allocates the stack . If however the process is found to be susceptible the process calls the API of the subject invention . The API makes reference to the process structure and sets flags within the process structure to indicate that execute on stack privileges for memory pages allocated for the stack of the instant process should be limited. Execution then continues with the process allocating a stack .

Under either event the subject process executes . The process method ends without particular post condition.

The methodology followed under the instant invention to allocate additional memory pages is depicted with reference to . In most cases the process depicted in will occur during the execution of the underlying process in . Accordingly the methodology for allocating memory begins with the precondition of a process executing which has been evaluated according to the methodology described in . The methodology waits for a page fault. Upon page fault the instant invention allocates a memory page for use by the subject process. A determination is then made as to whether the process is susceptible to buffer overflow attacks. This determination is made with reference to the process structure which is the same process structure previously written at . If the process is not susceptible execution proceeds to the end and the memory allocation process is finished.

If however the subject process is susceptible to buffer overflow attacks the memory management methodology proceeds to determine whether the memory page allocated will be used for a stack by the subject process. If the subject process is not using the allocated memory as a stack execution terminates .

If however the page to be allocated is to be used for stack space by the subject process the memory management methodology resets the execute privileges bit on the page table for the page . Execution then terminates . Upon termination a memory page has been allocated to the subject process and if the memory page is a stack of a susceptible program the execute privileges for that page have been revoked.

Those skilled in the art will appreciate that when a memory page is allocated the execute privileges bit for the page is set by default. Accordingly only those memory pages which are allocated by a susceptible process to a stack configuration are denied execute privileges.

The instant invention has many distinct advantages over the prior art memory allocation system and buffer overflow attack detection and handling systems. These advantages will be readily apparent to those skilled in the art but include the fact that the instant process does not drain system resources by making evaluations of every process running on a system. Rather only the processes which are susceptible to buffer overflow attacks are maintained under the instant process. Additionally the instant method and system does not require execute privileges to be denied for those applications that take advantage of the ability to execute small portions of code on the stack. Also the instant system and method protects against buffer overflow attacks in a more secure way as it does not depend upon a canary test value within the stack to determine if the stack has been overwritten. Rather all execution on the stack for at risk processes is revoked.

As to the manner of operation and use of the present invention the same is made apparent from the foregoing discussion. With respect to the above description it is to be realized that although embodiments of specific operating systems memory structures hardware representations databases and languages may be disclosed those enabling embodiments are merely illustrative and the optimum relationships for the parts of the invention are to include variations in composition form protocol function and manner of operation which are deemed readily apparent to one skilled in the art in view of this disclosure. All relevant relationships to those illustrated in the drawings and this specification are intended to be encompassed by the present invention.

Therefore as the foregoing is considered as illustrative of the principles of the invention numerous modifications will readily occur to those skilled in the art. It is not desired to limit the invention to the exact construction and operation shown or described. All suitable modifications and equivalents may be resorted to falling within the scope of the invention.

