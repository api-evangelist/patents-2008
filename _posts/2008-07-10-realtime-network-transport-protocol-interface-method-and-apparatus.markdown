---

title: Real-time network transport protocol interface method and apparatus
abstract: An electronic device has a multimedia framework including an integration layer and a higher-level application layer. The integration layer includes predefined media processing components having an input/output interface configurable based on the type and format of media content delivered to the device during a streaming media session and that provide predefined media processing functions to the application layer for enabling playback of the media content. A streaming media management component included in the integration layer establishes and manages the streaming media session and configures one or more content pipes used during the streaming media session to access packets received by one or more sockets. The component also maps the content pipes to the sockets and processes packets retrieved from the one or more content pipes such that the media content is provided to corresponding ones of the predefined media processing components in accordance with their configured input/output interfaces.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08095680&OS=08095680&RS=08095680
owner: Telefonaktiebolaget LM Ericsson (publ)
number: 08095680
owner_city: Stockholm
owner_country: SE
publication_date: 20080710
---
This application claims priority to U.S. Provisional Patent Application No. 61 015 283 filed on Dec. 20 2007.

The present invention generally relates to media content and more particularly relates to processing media content delivered to an electronic device in accordance with a real time network transport protocol.

The OpenMAX Integration Layer IL API application programming interface is an open standard developed by the Khronos Group for providing a low level interface for audio video imaging and timed text media components used in embedded and or mobile devices. The principal goal of the OpenMAX IL is to give media components a degree of system abstraction for the purpose of portability across an array of different hardware and software platforms. The interface abstracts the hardware and software architecture in the system. Each media component and relevant transform is encapsulated in a component interface. The OpenMAX IL API allows the user to load control connect and unload the individual components enabling easy implementation of almost any media use scenario and meshing with existing graph based media frameworks.

The OpenMAX IL API defines media components such as audio video image decoders encoders audio video image readers writers audio renderers video schedulers container demuxers muxers clocks audio video image processors and the like. The OpenMAX IL API allows a client such as an application or media framework to create a media processing chain by connecting together various components. Content data is typically fed into the chain at one end and sequentially processed by each component in the chain. The data is transported between components using ports and buffers.

The OpenMAX IL API also defines an interface for accessing data from a local file or from a remote location. This concept is referred to as a content pipe and is described in chapter 9 of the OpenMAX IL API specification. A content pipe is an abstraction for any mechanism of accessing content data i.e. pulling content data in or pushing content data out . This abstraction is not tied to any particular implementation. Instead a content pipe may be implemented for example as a local file a remote file a broadcast multicast or unicast stream memory buffers intermediate data derived from persistent data etc. Moreover a content pipe need not be limited to a single method of providing access. For example a single pipe may provide content via both local files and remote files or through multiple transport protocols. A system may include one or many content pipes.

There are various methods for operating a content pipe such as creating a content pipe based on a URI uniform resource identifier reading writing a number of bytes from to the content pipe and setting getting byte position inside the content. In addition asynchronous methods can be used for remote access such as by checking available bytes getting a large buffer from the content pipe that the content pipe user can read from and providing a large buffer to write to the content. In each case the OpenMAX IL API essentially models content pipe access like traditional file access.

One mechanism for remotely controlling the delivery of media content is the real time streaming protocol RTSP defined by the IETF Internet Engineering Task Force in RFC 2326. RTSP is a client server text based protocol that enables a client to remotely control a streaming server. The client transmits RTSP method requests and the server replies with RTSP method responses. Typical RTSP commands include DESCRIBE SETUP and PLAY. The packet switched streaming service PSS is defined by 3GPP and is based on RTSP but defines a complete service for streaming. To establish a streaming session the streaming client needs a session description. A streaming session is defined via the session description protocol SDP which may be obtained in practice from either an .sdp file downloaded from e.g. a WAP wireless access protocol page or an SDP retrieved in a response from a streaming server to the use by a client of the DESCRIBE command towards an RTSP URI e.g. rtsp server.com clip . The SDP information includes configuration parameters for the streaming session and for the corresponding media streams and decoding thereof.

A media stream e.g. audio video images and or timed text is established when a client requests the server to set up an RTP real time protocol connection with the client the media format being described in the SDP. Thus RTSP is used to establish the streaming session and to control the server while RTP is used to carry the actual media content once the streaming session is established. A typical streaming client has one TCP transmission control protocol connection for RTSP signaling. In addition for each media type that the session includes the streaming client will have two UDP user datagram protocol connections. The first UDP connection is used for reception of RTP traffic and the second UDP connection is used for exchange of RTCP real time control protocol packets both RTP and RTCP are carried over UDP . RTCP packets are sent by both the server and the client enabling both devices to give feedback about the RTP transmission progress.

RTP packets include payload data typically encoded media frame data provided in a format favorable for streaming. Typically the payload data may need some processing e.g. de packetization before the coded frame can be sent to the media decoder. De packetization involves extracting the encoded media frame data by removing the packet header information and other packet encapsulation information. RTP packets also include a time stamp which indicates when the content of the frame was sampled relative to other frames in the same stream. The timestamp information together with inter media synchronization information transmitted by the server which is received in either an RTSP message or an RTCP message can be used to establish the local time of the client at which each frame should be rendered and presented. This way the client can maintain synchronization between different media streams. The streaming client also typically deploys jitter buffers that hold some RTP data before decoding and rendering. Buffering the RTP data enables the client to account for variations in transmission delays that arise from the server to the client. Buffering is also used to reorder packets that arrive out of sequence during a streaming media session.

Real Media streaming is a type of media streaming that differs from 3GPP streaming. Real Media streaming uses only one UDP connection to carry multiple streams unlike 3GPP streaming which uses multiple UDP connections. Also media packets are distinguished by a stream identifier. Real Media streaming uses a proprietary transport format called Real Data Transport RDT . With Real Media streaming it is possible to use a proprietary mechanism for feeding back information to the streaming server but the feedback mechanism does not require a separate UDP connection. Thus Real Media streaming only requires one UDP port in total.

Windows Media streaming is yet another way of transporting streaming media content. Windows Media streaming uses RTP to transport an Advanced Systems Format ASF file to the client. The ASF file is a container format which holds frames for all media types and streams. Windows Media streaming thus also uses only one UDP connection to carry all media content. As such both Real Media and Windows Media streaming need some form of de multiplexing before the media content can be de packetized and decoded.

MBMS multimedia broadcast and multicast service is a mechanism for remotely delivering media content to a client in a cellular environment. MBMS defines a bearer service and a user service. The bearer service allows efficient use of broadcast or multicast bearers in the cellular environment. Traditionally bearers over cellular networks are bidirectional point to point bearers. MBMS allows for the setup of unidirectional downlink bearers to multiple receivers. The MBMS User Service allows streaming and downloading of multimedia content over unicast multicast or broadcast bearers. Mobile TV services can be realized over MBMS User Service using the streaming protocols defined in the 3GPP TS 26.346 specification. MBMS streaming uses RTP for transporting multimedia data and mobile TV sessions are described using SDP. MBMS protocols and codecs are aligned with PSS. However RTSP is not used when only unidirectional bearers are employed.

DVB H Digital Video Broadcasting Handheld is another way to remotely deliver media content to a client in a wireless environment. As its name indicates DVB H is the handheld version of a broadcast standard which includes the well known satellite DVB S terrestrial DVB T and cable DVB C versions. DVB H was specified by the DVB project and subsequently endorsed by regional standardization bodies e.g. ETSI EN . DVB H is an adaptation of DVB T that takes into account the specific requirement of handheld devices with respect to power consumption processing capabilities and multimedia rendering capabilities. Mobile TV services over DVB H use the DVB IPDC service layer where IPDC stands for IP datacasting. The DVB IPDC service layer describes the Electronic Service Guide ESG the content delivery protocols CDP and service purchase and protection SPP . An alternative service layer to DVB IPDC is OMA BCAST. The transport protocol for DVB IPDC is RTP and mobile TV sessions are described using SDP. RTSP is not used with DVB H because of its unidirectional nature.

Still another media content distribution technology is MTSI Multimedia Telephony Service over IMS where IMS stands for IP Multimedia Subsystem. MTSI is specified by 3GPP. MTSI is an evolution of traditional telephony and Voice over IP VoIP whereby traditional speech telephony calls are enriched with multimedia content such as video and text and during which users can share multimedia files e.g. images and video clips . MTSI protocols are based on IMS protocols for session description and control Session Initiation Protocol SIP and SDP . MTSI uses RTP to transport multimedia content between parties.

OpenMAX only defines playback of media content from a local or remote location i.e. a file but it does not address media carried over RTP. The OpenMAX IL API does not playback and record from to transport over RTP. Thus OpenMAX is generally incapable of directly supporting streaming including streaming implemented using RTSP and RTP. Further as shown below attempts to implement streaming using OpenMAX IL content pipes are unlikely to work correctly.

There are at least two general ways to support PSS with OpenMAX IL. One way is to bridge OpenMAX IL to the network stack with the IL client and or application. Another way is to use content pipes as suggested in the OpenMAX IL specification. OpenMAX IL can handle PSS using the client application as a bridge. The application has control of the server via an RTSP control interface. The client also has control of the media decoders via the IL client e.g. OpenMAX AL or any other multimedia framework and the OpenMAX IL API. RTP RTSP e.g. control RTP buffers RTCP etc. function outside the OMX IL implementation. Media decoders synchronization and rendering are performed within the OpenMAX IL implementation.

Once a PSS session is established the IL client can setup the decoder and renderer components. Data in the form of audio video stream buffers are transferred from the RTP stack to the multimedia decoders via the application or the IL client. Timestamps must be added to the audio and video flows for proper synchronization in OpenMAX IL. However functions such as seeking play and pause are not possible from within the OMX IL interface with such an approach. In addition the IL client must introduce time stamps in payloads from the RTP stack to feed into the OMX IL filter graph. Moreover RTP and RTSP are not integrated into OpenMAX even though they are a part of the player chain.

Streaming may also be handled in Open MAX IL using content pipes. The IL client can create its own custom content pipes. In this case each custom content pipe can be used to transfer audio and video streams from RTP to OMX IL decoders as described above. However it is unclear what type of component can leverage the content pipe because decoders cannot be directly connected to a content pipe according to OpenMAX. The IL client can also provide a streaming URI to a reader demuxer component. The reader demuxer would then create its own content pipe using the URI provided. The implementation of the content pipe in theory can then provide all streaming functionalities. However with only one content pipe opened for the streaming session all audio and video streams would go through that pipe. OpenMAX does not define how such multiplexed media data would be subsequently formatted and de multiplexed. Also no control is given via the content pipe to control the streaming session. Moreover synchronization information coming from RTP must be translated into timestamps but this is not currently supported in OpenMAX. OpenMAX also does not define how to retrieve the information about content format i.e. the SDP via the content pipe to setup the correct decoders. Thus there is no benefit for using a content pipe for processing PSS media data in view of the current OpenMAX standard.

OpenMAX primarily focuses on the media plane. For RTSP streaming and other datacom protocols a control plane towards the server is also needed. This not addressed by OpenMAX. Further the content pipe concept is modeled after file access i.e. read a number of bytes and all media data is indexed by bytes . A streaming server cannot be controlled according to how many bytes the client reads from the incoming RTP packets. The streaming server must be controlled via RTSP. In addition RTSP typically uses time measured in seconds normal play time NPT as the index not bytes.

Moreover it is also unclear how seeking should be performed in OpenMAX with an RTP source. If the potential demuxer is set to a new position OpenMAX does not define how the streaming server should be informed when the RTSP implementation is in a content pipe. As noted above content pipes only index bytes not NPT as in RTSP. Further synchronization in OpenMAX is not defined for RTP based protocols. With streaming the synchronization information may come from RTCP or from RTSP. This information must be at hand for the component handling the time stamping.

Yet another issue is how multiple streams as in 3GPP streaming and multiplexed streams as in Real and WMT streaming are handled by the same client. For example it is not clear how many content pipes should be used in such a scenario. If only one content pipe is used 3GPP streams would have to be multiplexed before the content pipe. If more than one content pipe is used Real and WMT streams would have to be de multiplexed before the content pipes.

Also there is no payload handling required for playback from a file as the reader or demuxer is already able to locate and extract the coded media frames in the file. The decoder could possibly be responsible for RTP payload handling in the streaming case but it would be seen as something outside the typical responsibility of a decoder and might contradict the input format for OpenMAX IL decoders . The demuxer reader could also possibly be responsible for RTP payload handling in the streaming case but it would be less flexible. It would be desired to be able to route RTP packets between OpenMAX IL components e.g. for recording.

Finally MTSI which provides speech related services requires very short delays in the buffers and must be able to handle variations of the amount of data in the jitter buffers. Again an RTP element is required to perform rate adaptation time stamping and de packetization. A jitter buffer is inserted and the time scaler unit placed after the speech decoder requires a fast two way control interface with the jitter buffer to allow for minimization of speech path delay.

According to the methods and apparatus taught herein an electronic device has a multimedia framework including an integration layer and a higher level application layer. The integration layer includes predefined media processing components having an input output interface configurable based on the type and format of media content delivered to the device during a streaming media session. The streaming media processing components provide predefined media processing functions to the application layer for enabling playback of streaming media content and non streaming media sessions such as local file playback. A streaming media management component included in the integration layer establishes and manages the streaming media session and configures one or more content pipes used during the streaming media session to access packets received by one or more sockets. The packets are encapsulated according to a real time network transport protocol. The component also maps the content pipes to the sockets and processes packets retrieved from the one or more content pipes such that the media content is provided to corresponding ones of the predefined media processing components in accordance with their configured input output interfaces.

Of course the present invention is not limited to the above features and advantages. Those skilled in the art will recognize additional features and advantages upon reading the following detailed description and upon viewing the accompanying drawings.

The media components reside within an integration layer of the electronic device . The integration layer together with a higher level application layer form a multimedia framework. The application layer provides an interface between one or more multimedia applications such as a media player and the integration layer . The integration layer provides an interface between the application layer and the multimedia components . The integration layer includes a streaming media management component for providing application layer access to the streaming media control and transport functions and feeding media data to components . In one embodiment the media components and streaming media management component comply with the OpenMAX IL standard. The application layer can be embodied as an application that is part of the higher level multimedia framework and has access to the lower level multimedia components and streaming media management components . In one embodiment the application layer is the OpenMAX AL

From time to time the application layer requests delivery of media content from the remote host via a streaming media session. The streaming media management component establishes and manages the streaming media session with the remote host . In response to the request the remote host delivers packet data transmitted in accordance with a real time network transport protocol such as RTP RDT etc. The electronic device includes one or more content pipes for accessing the media content upon receipt. Each content pipe can be a local file remote file broadcast stream one or more memory buffers intermediate data derived from persistent data or the like. In each case the media content accessed by the content pipes is formatted in accordance with a real time network transport protocol.

The streaming media management component configures one or more of the content pipes for accessing packets during the streaming media session. In one embodiment the streaming media management component configures one or more of the content pipes as a network socket. The predefined media processing components have an input output interface configurable based on the type and format of the media content delivered to the device . The multimedia components provide predefined media processing functions to the higher level application layer for enabling playback of the media content. The streaming media management component processes the packets retrieved from the one or more content pipes such that the media content is provided to corresponding ones of the predefined media processing components in accordance with their configured input output interfaces ensuring proper playback of the media content.

In one embodiment the streaming media management component converts the packet data by buffering and de interleaving the packet data based on header information extracted from the received packets. The streaming media management component extracts payload data from the de interleaved packet data and converts the extracted data to coded media frame data capable of being decoded by one or more of the media components . The streaming media management component also maintains synchronization between different ones of the media components based on timing information delivered to the electronic device from the remote host . Broadly the streaming media management component forms a common boundary between the content pipes and media components for seamlessly handling streaming media content delivered to the electronic device in a real time network transport protocol format. This way neither the content pipes nor the media components have to be capable of processing real time network transport protocol packet data.

According to one embodiment the streaming media management component comprises first and second processing components . The first processing component configures the one or more content pipes based on the number type e.g. audio video still images timed text etc. and possibly format e.g. WAV PCM MIDI MPEG WMV H264 etc. of the media content and the real time network transport and control protocols used to transmit the media content e.g. RTP RDT RTSP etc. . In an embodiment one of the content pipes is configured as a TCP socket and the first processing component initiates a streaming media session with the remote host via the content pipe configured as a TCP socket. For example the first processing component can issue RTSP commands such as DESCRIBE SETUP PLAY etc. to the remote host via the TCP configured content pipe . One or more other ones of the content pipes are configured as UDP sockets for receiving packet data during the streaming media session. The TCP configured content pipe can also be used to control the remote host during the streaming media session e.g. by issuing additional RTSP commands such as PAUSE.

One or more other ones of the content pipes can also be configured as UDP sockets for providing receiver statistics information to the remote host . In one embodiment RTCP reports are exchanged over UDP between the remote host and electronic device for indicating how many packets have been sent and received in both directions . The RTCP reports can also be used for synchronization. QoE feedback information can be sent from the electronic device to the remote host via RTSP over TCP. The QoE information relates to the media content delivered to the electronic device during the streaming media session. In an embodiment one or more content pipes can be configured as a UDP socket for each media stream being delivered e.g. audio video images and or timed text so that the remote host is provided receiver statistics information for each stream.

The first processing component also reads packet data from the content pipes and converts synchronization information delivered to the electronic device and timestamps extracted from the packet data to new timestamp information compatible with the media components . Moreover the first processing component which may include jitter buffers can use the extracted timestamp information to output the packet data in proper order with the new timestamp information. Alternatively the first processing component can use RTP sequence numbers associated with the packet data to re order the packet data. In each case out of order packets are placed in proper sequence. In one embodiment the packet data is output directly to the second processing component for additional handling. In another embodiment the first processing component outputs the packet data to a local file stored in memory at the electronic device . The second processing component subsequently retrieves the local file from memory and converts the stored packet data to coded media frame data for decoding and playback.

In either embodiment the second processing component converts the packet data output by the first processing component to coded media frame data based on the type and format of the media content and the real time network transport protocol used to transmit the media content. In one embodiment the second processing component removes packet header and other encapsulation information from the packet data. In some cases the packet data may require de multiplexing before the packet encapsulation is removed. For example both Real Media and Windows Media ASF streaming media types typically use a single UDP connection for all media streams. Accordingly the second processing component de multiplexes Real Media and Windows Media ASF streams into separate media streams such as audio and video e.g. based on stream identifiers embedded in the packet data. The packet encapsulation information is then removed from the de multiplexed packet streams and converted to separate coded media frame data.

The coded media frame data is then output in a format compatible with the one or more media components operable to decode the media content. In one embodiment each processing component of the streaming media management component has a number of input and output ports that are configurable based on the type and format of the media content and the real time network transport protocol used to transmit the media content. Each input of the first processing component is compatible with an output of each content pipe . Similarly each output port of the second processing component is compatible with an input port of the media components . In an embodiment the media content includes multiple media streams e.g. audio and video . In turn the first processing component has an input for each media stream that is configured based on the type and format of the media stream and the real time network transport protocol used to transmit the media stream. The second processing component likewise has an output port for each media stream configured to match an input port of the media component operable to decode the media stream.

The corresponding decoder one of the media components decodes the media frame data provided to it. Other ones of the media components process the decoded data for playback. For example an audio decoder one of the media components decodes the audio portion of the media content while a decoder one of the media components decodes the corresponding video portion. Other ones of the media components provide additional functions such as rendering synchronization error correction etc. Synchronization is maintained between the media components included in the electronic device based on the new timestamp information provided by the streaming media management component .

Either way the RTSP logic also sets up UDP connections via the content pipes for RTP RTCP and or RDT transmissions. The RTSP logic performs additional tasks such as managing media buffers such as audio and or video buffers . The re positioned content is used to issue PAUSE and PLAY RTSP commands to the remote server with a new range. The RTSP logic further maintains tight coupling between RTSP and RTP most notably for decoder configuration e.g. from the SDP media setup from the SDP and RTSP SETUP and synchronization either from RTSP PLAY or RTCP receiver report . Moreover the RTSP logic of the first processing component collects QoE quality of experience information generated by a QoE gathering and reporting entity of the application layer collects QoE measurements from RTP jitter buffers not shown and reads an OpenMAX clock input media time updates.

The first processing component of the streaming media management component also has additional logic for handling RTCP based transactions. In one embodiment the additional logic connects input ports of the streaming media management component to each content pipe configured as a UDP socket implements jitter buffers for each media stream e.g. for packet reordering de interleaving and elimination of network jitter effects and in one embodiment de multiplexes Windows Media ASF container streams. The additional logic also de multiplexes Real Media streams and puts OpenMAX timestamps on all packets based on synchronization information received from the remote server e.g. from an RTCP sender report and timestamps extracted from the received packet data. Moreover the additional logic enables the audio and video buffers to output synchronized RTP RDT streams. For Windows Media the output is preferably a single RTP stream not yet de multiplexed that is connected to a single RTP de packetizer which in turn is connected to an ASF container demuxer not shown . The additional logic also provides QoE measurements to RTSP and may use a local file as input rather than using UDP sockets as described above. The local file may be a recorded streaming session containing RTP packets in a file container format accessible by the streaming media management component . The first processing component of the streaming media management component outputs the local file to one or more de packetizers included in the second processing component for further processing.

The second processing component includes a de packetizer for each media stream. Two media streams audio and video are shown in however any desired number of media streams may be processed. The RTP de packetizers input packet data from the audio and video RTP buffers extract coded media frame data from the packet data and output the coded media frames to the audio and video decoders respectively. For Windows Media streaming the output is an ASF file stream which is de multiplexed by an ASF demuxer in one embodiment. The audio and video decoder components provide QoE measurements to the streaming media management component via a QoE gathering and reporting function of the application layer or a feedback channel. A clock component maintains proper synchronization between the different media streams based on the timing information provided by the streaming media management component . This way the clock component need not process RTP timestamps. Audio processing and rendering as well as video scheduling and sink components provide additional functions used by the application layer of the electronic device to play the media content.

Audio processing and rendering as well as video scheduling mixing and sink media components provide additional functions used by the application layer of the electronic device to play the media content. Moreover an additional content pipe can be allocated for accessing media containers read by a file reader component . The file reader component parses each media container such as a 3GPP file and determines the type of decoders needed to decode the media streams. The file reader component also configures its port settings once the media stream formats are determined because the encoding types of a media container can vary.

The streaming media management component also includes an RTP packetizer for inputting coded media frame data from audio video and timed text encoders . Audio signals are captured by an audio capturer and sent to an audio processor . The audio processor outputs an audio stream to the audio encoder for encoding. The video encoder similarly receives video data from a video processor that processes the output of a video camera . The timed text encoder can be provided for encoding timed text media. Each RTP packetizer encapsulates the corresponding media stream into RTP packets having correct header information including timestamps. The RTP packets are output to an RTP muxer having an RTP muxer component .

The RTP muxer component connects to the UDP configured content pipe . RTCP logic builds optional RTCP sender reports based on the packet data input to the RTP muxer . The RTP muxer then outputs the multiplexed synchronized RTP media stream to the content pipe plus any optional RTCP reports . The electronic device transmits the multiplexed MTSI media stream to the remote host for processing. Broadly the RTP muxer may have one or more outputs each coupled to a different one of the content pipes . The RTP muxer uses each UDP configured content pipe for transmitting RTP packets. Optionally one or more UDP configured content pipes can be used to send and receive RTCP packets. The RTP muxer also has a clock port for receiving Media Time updates. The RTP muxer can transmit RTP packets as they become ready. Moreover as the RTP muxer receives Media Time updates with scale change notifications the muxer can adjust transmission speed according to the clock scale.

The RTP muxer also has a different input port coupled to the different RTP packetizers . The input port receives RTP or RDT packets with RTP and OMX IL time stamps attached. The RTP muxer accepts the following OpenMAX controls SetParam for OMX IndexParamCustomContentPipe used when the content pipe is provided by the application layer and not acquired by the RTP demuxer set connection container get connection container set get execution state idle executing paused set get port state enabled disabled set get port definition set get content time position i.e. IndexConfigTimePosition get RTCP sender and receiver report information and buffer flush. The RTP muxer may also indicate when port settings have changed.

Each RTP packetizer that provides input to the RTP muxer has an input port coupled to the corresponding encoder . The input port format is the same as the corresponding encoder output port format. Each RTP packetizer also has an output port for the media type handled by that packetizer e.g. audio video images timed text etc. . The format of each output port is RTP or RDT packets with RTP and OMX IL time stamps attached. The RTP packetizers accept the following OpenMAX controls set get RTP packetization method set get execution state idle executing paused set get port state enabled disabled set get port definition and buffer flush.

The RTP de packetizers of the different embodiments disclosed herein also have different input and output ports for each media type under consideration e.g. audio video still images timed text etc. . The format of each input port is RTP RDT packets with OMX IL presentation time stamps attached. The format of each output port is the same as the corresponding decoder input port format. The RTP de packetizers accept the following OpenMAX controls get RTP packetization method set get execution state idle executing paused set get port state enabled disabled set get port definition and buffer flush. The following is a purely exemplary embodiment of how the RTP de packetizers are controlled and used. In this example two RTP media streams are being de multiplexed. The first stream is an audio stream with AMR WB format and the second stream is a video stream with H.264 format. The application layer loads and sets a de packetizer for each stream. The de packetizer associated with the audio stream is configured with an RTP packetization method e.g. RFC 4867 RTP Payload Format for AMR and AMR WB as indicated to the application layer by the RTP demuxer e.g. using SDP fields . The video de packetizer is configured based on e.g. RFC3984 RTP Payload Format for H.264 Video . The output port format is also set to match the decoder preferred input format. For example for audio the output frame format could be OMX AUDIO AMRFrameFormatRTPPayload or OMX AUDIO AMRFrameFormatlF2 . Each incoming RTP packet has a corresponding OMX IL time stamp attached to it. The RTP packets received are de interleaved by the streaming media management component according to the RTP header information. The de packetizer extracts the payload of the RTP packets converts the extracted payload to the proper output format and transmits the media data to the audio and video decoders with the corresponding time stamps.

The first processing component of the streaming media management component also has different input and output ports for each media type under consideration e.g. audio video still images timed text etc. . One or more of the content pipes coupled to the first processing component are configured as UDP sockets for receiving RTP or RDT packets. One or more additional ones of the content pipes may be optionally configured as a UDP socket for sending and receiving RTCP packets. Yet one more of the content pipes may be optionally configured as a TCP socket for enabling RTSP signaling.

The first processing component of the streaming media management component has one or more clock ports for receiving Media Time updates. Scale changes can be received on the clock ports. The scale factor can be used to indicate normal playback e.g. scale 1 or pause e.g. scale 0 . The first processing component also has a different output port for each media type under consideration e.g. audio video still images timed text etc. . The format of each output port is RTP RDT packets with OMX IL presentation time stamp attached. The output ports can be scanned by the application layer for alternative content.

The first processing component of the streaming media management component accepts the following OpenMAX controls set content URI either RTSP link or SDP file path get content URI actual session URI extracted from the SDP and after any redirection SetParam for OMX IndexParamCustomContentPipe used when the content pipe is provided by the application layer and not acquired by the RTP demuxer set connection container get connection container get session name session information and URI of description from the SDP get connection information from the SDP get bandwidth information from the SDP get availability and time zone information attribute information and encryption keys from the SDP get media description information from the SDP set get execution state idle executing paused set get port state enabled disabled set get port definition query number of alternative versions per output port i.e. get highest port streams index set alternative version on an output port i.e. set active port streams index set get content time position i.e. IndexConfigTimePosition get RTCP sender and receiver report information and buffer flush. The first processing component of the streaming media management component may send the following events port settings changed port format detected format not detected and redirected.

In one embodiment connection containers are used to pass connection information in and out of the first processing component . The connection containers include the following fields content pipe handle set by the first processing component connection ID to the data bearer set by the application layer protocol type such as UDP or TCP set by the application layer or first processing component local and remote IP addresses set by the first processing component local port number set by the first processing component available guaranteed maximum bit rate set by the application layer . The application layer may set some fields based on the connection establishment handled outside of OpenMAX. These fields are then passed to the first processing component which uses the information and fills in other information during execution. The container is passed up to the application layer again which can use the new information in order to continue the connection establishment.

The following is a purely exemplary embodiment of how container content is modified for unicast media content. In this example the application layer establishes a background radio bearer e.g. a non real time primary PDP context for RTSP signaling done outside of OpenMAX. Particularly the application layer receives a connection ID handle which is an abstract reference to the bearer. The application layer fills in the connection ID handle and the protocol type e.g. TCP for RTSP in the connection container and passes it to the first processing component of the streaming media management component . The first processing component uses the information in the connection container to create a socket content pipe for the specified protocol e.g. TCP in this case for RTSP . The connection ID handle extracted from the connection container is used as a default connection ID handle since the connection container does not yet have the content pipe handle field set i.e. it was not initially associated with a specific content pipe already created . The corresponding content pipe is associated with the connection ID handle by some means outside of the definition of OpenMAX. The outgoing traffic from this content pipe is now sent on the radio bearer that the connection ID handle represents. The connection container is updated with a content pipe handle the local and remote IP addresses and the allocated local UDP port number.

The first processing component of the streaming media management component gets the SDP either from the remote server or by opening a local file which was downloaded in advance. The content presented in the SDP is selected on output ports by the application layer . An RTSP SETUP command is then sent to retrieve UDP port numbers from remote host . Next the first processing component configures one or more of the content pipes as a UDP socket for each media stream and RTCP connections if applicable . The first processing component creates connection containers for each UDP socket content pipe created and fills in the following fields content pipe handle protocol type e.g. UDP local and remote IP addresses and allocated local UDP port number. The first processing component passes all the connection containers including the initial one for RTSP to the application layer .

The application layer now has enough information for establishing any real time radio bearers for media traffic. For instance the local and remote IP addresses as well as the local UDP port numbers can be used as input for downlink packet flow mapping in the network onto the correct bearer. The application layer can use any content bit rate information as input to what quality of experience requirements the real time bearer should have. The application layer receives new connection ID handles for the new bearers and updates the connection containers for the associated UDP socket content pipes with the connection ID handles and available guaranteed maximum bit rate information. The application layer passes the updated connection containers to the first processing component . The first processing component re associates any UDP socket content pipes with their new connection ID handles by some means outside of the definition of OpenMAX.

The following is yet another purely exemplary embodiment of how container content is modified for broadcast multicast media content. In this example the application layer passes no connection containers to the first processing component in the initial stage. The first processing component opens the SDP from a local file which was downloaded in advance. Based on the content description in the SDP the first processing component configures UDP socket content pipes for the media streams and RTCP connections if applicable . The first processing component creates connection containers for every UDP socket content pipe created and fills in the content pipe handle and protocol type e.g. UDP . The first processing component passes the connection containers up to the application layer .

The application layer establishes one or more radio bearers for reception of broadcast media traffic. The application layer receives new connection ID handles for the new bearers and updates the connection containers accordingly. The application layer passes the updated connection containers to the first processing component . The first processing component re associates any UDP socket content pipes with their new connection ID handles by some means outside of the definition of OpenMAX. The binding of UDP socket content pipes to actual IP addresses and port numbers may be done here or earlier in the process. The electronic device is then ready to receive streaming media content such as audio video still images timed text etc.

In one embodiment the streaming media management component of the electronic device inherits the same possibilities as any OpenMAX framework component. This means that a component such as the first processing component of the streaming media management component can be integrated into the application layer or implemented as separate hardware e.g. as a companion chip or a hardware accelerator . Further different components of the streaming media management component may be provided by a separate vendor. Moreover the streaming media management component enables the use of real time network transport protocols such as RTSP RTP and RDT for transmitting streaming media without requiring the application layer to handle the protocol. In one embodiment the application layer is not in the media data path meaning that it should not handle RTP . Many different server and client implementations are available and behave differently requiring a vast amount of interoperability testing in order to ensure server client compatibility. Such effort is worth much and it is not easy to successfully develop a new RTSP client from scratch. Thus the streaming media management component provides a seamless way for providing streaming media using real time network transport protocols with little or no modification to the application layer and media components of the electronic device . For example the first processing component of the streaming media management component can be delivered as a component including everything needed for streaming.

RTP recording and time shifting is possible useful for DRM protected media such as SRTP and ISMACryp but not IPSec because the output from the first processing component is RTP packet data. The second processing component of the streaming media management component can be used when streaming from the remote server or when performing local playback of earlier recorded material. The decoder media components do not need to be extended to support payload formats so their function can remain clear and well defined. Moreover RTSP and RTP functions are kept together simplifying the input and output ports of the streaming media management component . More specifically the first processing component of the streaming media management component is responsible for maintaining synchronization between the media streams of a session. Even if no streams need to be de multiplexed e.g. because the number of input streams are the same as the number of output streams of the first processing component there needs to be a function that translates inter system synchronization to intra system synchronization. The first processing component provides this function by mapping the inter media synchronization information from the remote server together with the incoming packet time stamps to the internal presentation time per media. This mapping is kept internal to the first processing component simplifying the inputs and output ports of the streaming media management component .

The first processing component of the streaming media management component also receives the SDP from the remote server and configures the media streams and handles the reception of the same. Decoder configuration is also received in the SDP and used to configure the output ports of the first processing component reducing the complexity of the streaming media management component . In addition the first processing component is responsible for providing data to the media components for decoding and other processing. The first processing component controls when data shall start flowing and at what position of the content that shall be received. Particularly the first processing component controls the transmission of the content which is done using RTSP. In other words the client side of the RTSP control plane is located in the streaming media management component . OpenMAX allows the platform vendor to add third party components such as audio and video hardware decoder companion chips in a standardized way. For example the components of the streaming media management component can themselves be supplied as third party components either in software or in hardware.

With the above range of variations and applications in mind it should be understood that the present invention is not limited by the foregoing description nor is it limited by the accompanying drawings. Instead the present invention is limited only by the following claims and their legal equivalents.

